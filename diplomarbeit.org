#+TITLE: Selective Cover Traffic
#+TODO: KEYWORDS WRITE CHECK EVA DANIEL | FINAL
#+TODO: TODO WAIT | DONE
* Configuration							    :ARCHIVE:
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure}[H] \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {5 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
* WAIT Abstract
* TODO Introduction (=basics) [0%]
** TODO Website Fingerprinting [0/24]
*** EVA Intro (remove this headline when reviewed)
    Website fingerprinting\cite{hintz02} aims to deduce which web page
    a user is visiting via an anonymizing proxy. It does so based on
    order, size, and timing of data packets.

    This chapter contains first [[#visual][a visual introduction to the subject]],
    then a short review of [[#http][what happens during website
    retrieval]]. Afterwards, [[#wf1.0][early methods of website fingerprinting]]
    will be presented. The next section explains [[Hurdles][why these early
    techniques no longer work]], especially on anonymity networks like Tor.

    Finally, [[#wf2.0][current attacks]] will be explored.
*** EVA Visual Representation of Traces
    :PROPERTIES:
    :CUSTOM_ID: visual
    :END:
    To illustrate the task of a website fingerprinter, consider these
    graphical traces[fn::see appendix [[#wf-pictures]] for the creation of
    these pictures] of two web pages, where box height signifies
    amount of data and width the duration until the next packet:

    craigslist.org

    #+CAPTION: example 1 of craigslist.org traffic
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[./pictures/craigslist_org@1445352269.png]]
    #+CAPTION: example 2 of craigslist.org traffic
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[./pictures/craigslist_org@1445585277.png]]
    #+CAPTION: example 3 of craigslist.org traffic
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[./pictures/craigslist_org@1445486337.png]]
    #+CAPTION: example 4 of craigslist.org traffic
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[./pictures/craigslist_org@1445527033.png]]

    facebook.com
    
    #+CAPTION: facebook.com example 1
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[./pictures/facebook_com@1445350531.png]]
    #+CAPTION: facebook.com example 2
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[file:pictures/facebook_com@1445422155.png]]
    #+CAPTION: facebook.com example 3
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[file:pictures/facebook_com@1445425799.png]]
    #+CAPTION: facebook.com example 4
    #+ATTR_LATEX: :float nil :width 0.24\textwidth
    [[file:pictures/facebook_com@1445429729.png]]

    According to Cai et al. \cite{a-systematic}, the tuples (delay,
    packet size) visualized above contain all relevant information of
    a trace.
**** WAIT when done, format in LaTeX: caption to group of images
*** EVA What happens during a (HTTP) website request
    :PROPERTIES:
    :CUSTOM_ID: http
    :END:
    #+CAPTION: HTTP/1.0\cite{rfc1945} example from \cite{ssl-traffic-analysis}: page with 2 images, ACKs omitted
    #+ATTR_LATEX: :float wrap :width 0.4\textwidth
    [[./pictures/cheng-http-request.pdf]]

    When a browser such as Mozilla
    Firefox[fn::\url{https://www.mozilla.org/firefox/}] retrieves a
    web page, it does many things under-the-hood.

    First, it retrieves the main object: the browser sends a
    HTTP\cite{rfc2616} request over the TCP/IP\cite{rfc793} protocol
    stack for the HTML page, which contains the main page
    information. The webserver answers with the requested HTML page,
    or redirects the browser to another address, which it continues
    until the HTML is loaded.

    Afterwards, the browser parses the HTML page and requests all
    objects embedded into the page, such as images, fonts, scripts,
    videos, stylesheets, etc. These can be identified using the
    HTML-tags =<img>=, ~<link rel="stylesheet">~, =<script>=, and
    =<video>=, the CSS-property =@font-face=, etc.
**** TODO [#C] if time: link to mozilla-docs
*** CHECK Website Fingerprinting 1.0
    :PROPERTIES:
    :CUSTOM_ID: wf1.0
    :END:
    
    To make sense of the noisy data retrieved via traffic sniffing,
    the first fingerprinters distinguished by object sizes. Each
    requested file has a specific size[fn::except for
    dynamically-generated objects] and is transferrend in one or
    several (IP) packets. In the first versions[fn::up to/including
    1.0] of HTTP\cite{rfc1945}, these sizes were clearly visible, as
    each HTTP request-response pair was transmitted over a separate
    TCP connection. They could be found by splitting traffic by the
    connection's port numbers, as of \cite{quantifying},
    \cite{hintz02}, and \cite{ssl-traffic-analysis}.

    Against SSL, which reveals the site being browsed to but hides the
    pages accessed, successful attacks to determine the web
    page[fn::cryptographers talk of attempts to circumvent a protocol
    as /attack/\cite{applied96}] were carried out by both Mistry and
    Raman \cite{quantifying} and Cheng and
    Avnur\cite{ssl-traffic-analysis} at Berkeley in 1998.

    The term /website fingerprinting/ was coined in Hintz's analysis
    of the SafeWeb anonymizing proxy\cite{hintz02}, where he
    qualitatively classified which web site was being visited.

    All of these approaches determined which page was visited via
    object sizes. Cheng and Avnur \cite{ssl-traffic-analysis}
    additionally proposed a hidden markov model for page
    /transitions/, for hard-to-identify pages.

    As the above attacks, Herrmann et al\cite{ccsw09-fingerprinting}
    used only packet sizes, but employed machine learning
    techniques. They applied a Naïve Bayes
    Classifier\cite[ch.1.3.1]{intro2ir} which gave them good results
    against one-hop anonymizing proxies, yet yielded only 3% accuracy
    against 775 distinct pages when retrieved via Tor\cite{tor-design}.
*** CHECK Hurdles to website fingerprinting
    <<Hurdles>> The refinement of web protocols and anonymization
    systems such as Tor\cite{tor-design} made website fingerprinting
    harder. 

    Building a new connection for each transferred object is
    inefficient\cite[sec.2.2.2]{DBLP:books/daglib/0001977}. Thus, some
    early HTTP/1.0 implementations used persistent
    connections\cite{rfc2068}. These were standardized in HTTP/1.1
    \cite{rfc2616}. It was no longer trivial to extract the files'
    sizes. You had to determine the start and end of each
    request. (which was still possible by seeing when the client sent
    a new request).

    [[./pictures/HTTP_persistent_connection.png]]

    In addition to persistent connections, HTTP/1.1 allowed pipelining
    several HTTP requests in a single connection without waiting for
    the files to arrive in between.

    [[./pictures/HTTP_pipelining2.png]]

    As this created problems with some servers, pipelining was
    disabled in
    Firefox[fn::\url{https://bugzilla.mozilla.org/show_bug.cgi?id=264354}}
    and Google
    Chrome[fn::\url{https://www.chromium.org/developers/design-documents/network-stack/http-pipelining}}
    and not implemented in Internet
    Explorer[fn::\url{http://wayback.archive.org/web/20101204053757/http://www.microsoft.com/windowsxp/expertzone/chats/transcripts/08_0814_ez_ie8.mspx}].

    After Panchenko et al.'s first successful attack\cite{panchenko},
    Firefox's built-in request pipelining was enabled with added
    request order randomization as an additional no-cost defense
    prototype in the Tor-Browser-Bundle \cite{experimental}. Yet, Cai
    et al.\cite{ccs2012-fingerprinting} found fingerprinting to be
    easier with this defense enabled than without.
**** TODO caption for pictures
**** EVA Tor [0/3]
     The Onion Router\cite{tor-design} (short: /Tor/) is an anonymity
     system: While encryption hides the /content of communication/, Tor
     also attempts to hide /metadata/: Who communicates with whom, for
     how long, when, how frequent, ...?
***** CHECK History of Tor
      Tor inherits its onion design from the Onion Routing Project
      \cite{anonymous-connections}. It was originally developed by the
      Naval Research Laboratory of the US Navy with the primary purpose
      of protecting government communication.\cite{who-uses-tor}

      In recent years, Tor has also provided censorship
      circumvention\cite{tor-spec-pt}.
***** EVA Who uses Tor
      As more and more (internet) users wish to increase their
      anonymity for various reasons, one of Tor's main design goals is
      usability\cite[Sec.3]{tor-design}, this increases
      anonymity\cite{usability:weis2006}. This has led to a diverse
      user base\cite{who-uses-tor}: The network consists of over six
      thousand nodes and is used by about two million people
      daily[fn:metrics:\url{metrics.torproject.org}].

      As of \cite{who-uses-tor}, the groups[fn::actual or recommended]
      who increase their anonymity via Tor are: journalists and their
      audience, military, law enforcement officers, activists &
      whistleblowers, high & low profile people, business executives,
      bloggers, IT professionals, and "normal people". Actual and
      recommended use is for diverse purposes: privacy, censorship
      avoidance, covert ops, publishing, safety, online surveillance,
      anonymous tip lines, whistleblowing, blogging private opinions,
      evaluating competition, and troubleshooting IT systems.
***** CHECK How does Tor Work
      The Tor network consists of volunteer servers, called /onion
      routers/. Each connection through the network is facilitated by
      a proxy implementing the SOCKS5\cite{rfc1928} protocol called
      /onion proxy/.

      A connection is routed through three onion routers (=OR), each of
      which can only see the previous and next. Thus, no router has
      knowledge of both origin and destination of traffic.

      The messages look different from OR to OR due to same-length
      encryption.

      Tor's data cell have a fixed size of 512 bytes to prevent cell
      identification, routing through 4 globally-distributed hops
      increases latency, and Tor multiplexes all data cells through a
      single TCP-connection.
*** (wf 2.0)
    :PROPERTIES:
    :CUSTOM_ID: wf2.0
    :END:
**** WRITE panchenko v1
     - first to work
     - limited world size
     - svm
     - hand-tuned features
       - several evaluated
**** WRITE SVM
     - how works
     - multi-class strategies
     - draw border(s) (linear/polynomial/rbf) between points
     - ovr vs ovo
**** WRITE wang
     - improved detection
     - many features
     - weighting/learning weights
     - faster
**** WRITE KNN
     - simple
     - often effective
     - how works
       - for point, determine (f.ex. k=5) closest neighbors by metric
       - majority decision (or only if all agree), put in that group
**** WRITE cumul
     - better features
     - svm
     - picture
     - understandeable
     - faster than knn
**** WRITE features
     :PROPERTIES:
     :CUSTOM_ID: features
     :END:

    - patterns to features
    - why features
    - sizes, sizes, sizes
    - total_size
    - top-five
    - CUMUL
    - dyer: features count
      #+CAPTION: CUMUL\cite{panchenko2} features example at \url{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}
      [[./pictures/cumul_aus_paper.pdf]]
***** TODO visual representation of CUMUL
     - Panchenko et al.'s recent approach allows for the visual
       comparison of website traces.
     - see images etc
     - see how it's done
     - example
       #+CAPTION: CUMUL example from {\url https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}
       #+ATTR_LATEX: :width \linewidth
       #+BEGIN_EXAMPLE
       [[./pictures/cumul_resized_aus_paper.jpg]]
       #+END_EXAMPLE
****** TODO or just link here to CUMUL
****** TODO think about order of this (at cumul, at visual, mention other...)
      then formulate correctly
***** TODO get picture to work
    - features to classes: classifiers
*** brainstorm                                                      :ARCHIVE:
    - induce pattern
      - naive
      - machine learning
      - features
    - kind of traffic analysis
      - without seeing content, deduce information
    - made harder by protocol changes and tor
      - *hope that spdy makes it harder again*
        - ref mike perry
    - no cacheing
    - current tbb (auto-update)
    - scripts etc
    - xpra for slow network connection
    - xvfb for local display
    - marionette
    - others, other tools
    - bit on tor in [[Hurdles]]
      - also that use tor to avoid wf, then tracked again (if it works)
      - maybe also on ta
    - dyer: most important are the features, similar results for naive
    - on feature importances:
      - panchenko
      - k-forest
      - wang implicit
    - not mentioned/omitted in related work?
      - schneier
      - liberatore
      - microsoft hintz-successor(?)
    - classifiers
      - features important or also classifier
      - no classifier fits all
      - maps features to classes (or probabilities)
      - classifier
**** WRITE attacks
    - hintz
    - herrmann (bayes)
    - panchenko (svm)
    - cumul + k-forest + wang-knn
**** WRITE Related Work
     - mitchell
     - sklearn
     - herrmann: breakable shown via naive bayes
     - panchenko: svm
     - dyer: most important are the features, similar results for naive
       bayes and svm
     - cheng:?
     - wang: knn
     - dts-approach (?)
     - k-forest: specific classifier based on randomized trees with
       hadamard-distance on leaves
     - panchenko1 and 2: (ovr?) svm

    Schneier's seminal books /Applied Cryptography/\cite{applied96}
    and (with Ferguson) /Practical Cryptography/\cite{practical} deal
    briefly with /Traffic analysis/, of which website fingerprinting
    is a subtask. The first mention of applying it against encrypted
    internet communication dates to Wagner and Schneier's analysis of
    the SSL 3.0 protocol\cite{SSL}, and is attributed to Bennet Yee.
*** unused
**** from [[*What%20happens%20during%20a%20website%20request][What happens during a website request]]
    Thus, the objects embedded within a page could allow a local
    passive observer to infer which web page from a set of pages the
    user requested.
**** from [[*Hurdles%20to%20website%20fingerprinting][Hurdles to website fingerprinting]]
    Originally, a browser should open at most two connections per host
    \cite{rfc2616} to retrieve the files one-by-one. An update
    \cite{rfc7230} removed this fixed limit, but encouraged clients
    "to be conservative when opening multiple connections".
** TODO Defenses [0/6]
*** WRITE How to defend against this
    Hintz\cite{hintz02} was the first to suggest defenses against
    website fingerprinting. He lists three ways to do so: adding noise
    to traffic, reducing the transferred data, and transferring
    everything in one connection.

    The first approach was followed by almost all researchers
    afterwards.

    As the sizes and interconnection of HTML and embedded content is
    what makes a webpage easily identifyable, using a text-only
    non-javascript browser such as Lynx, or disabling f.ex. images,
    might be a mitigation for those who consider this trade-off
    acceptable. Yet, this reduces usability and thus conflicts with
    one of Tor's design goals\cite{tor-design}.

    The third approach --- while valid --- would require modifying the
    server. This would conflict with Tor's design goal of
    deployability.\cite{tor-design}

    Later defenses can be divided as of \cite{wang2015walkie} into
    specific and general defenses.
*** WRITE specific features
    The first website fingerprinters used only packet lengths. This
    made it seem sensible to alter the lengths of packets by padding,
    as evaluated f.ex. by Liberatore et al\cite{Liberatore:2006}.

    As more and more features were used to classifify the traces,
    different ways of altering the data were evaluated by several
    researchers (\cite{oakland2012-peekaboo}
    \cite{ccs2012-fingerprinting}, \cite{a-systematic},
    \cite{morphing09}, \cite{httpos}.
*** CHECK general defense
    To stop the arms race between attacks and defenses - the attacks
    finding new feature combinations to use, the defenses obfuscating
    these - the idea of a /general defense/ was presented first by
    Dyer et al.\cite{oakland2012-peekaboo}. They proposed what had
    been the mainstain of traffic-flow
    security\cite[ch.10.3]{applied96}: fixed-rate transmission of
    data, modified here to be only for the estimated duration of the
    download.

    This idea was improved on by Cai et al\cite{a-systematic} while
    Wang et al\cite{effective} proposed the (offline) defense of
    morphing all traffic to supersequences of traffic patterns.
*** CHECK deterministic vs stochastic
    The latest defenses were proposed by Juarez et al\cite{wtfpad} and
    Wang et al\cite{wang2015walkie}: These both used a stochastic
    approach to generate additional traffic, with Wang et al
    additionally modifying the browser to send "half-duplex" only,
    either only sending or only receiving data at the same
    time. Juarez et al adapted the ideas from Shmatikov and
    Wang\cite{ShWa-Timing06} to distinguish active and non-active
    periods, having a certain probability of sending dummy packets in
    each, omitting the sending when the browser generated packets
    itself.
*** brainstorm                                                      :ARCHIVE:
    - obfuscate features
    - specific features
      - morphing
    - general obfuscation
      - deterministic
        - fixed data rate
        - supersequence if known
      - stochastic
    - [ ] traffic analysis assumes crypto is perfect
*** WRITE Related Work
    - hintz: 3 ways to do it
    - wright: morph
    - luo: also morph (HTTPOS)
    - panchenko decoy (add)
    - padding (sslv2 \to 3)
    - requestpolicy (hintz 2nd way)
    - text-only browsing (hintz 2nd way)
* TODO Methods (=methods) [0%]
** brainstorm                                                       :ARCHIVE:
  - make wf/ml harder, fudge features
    - problems wfpad: modify all of tor,
      - yet problem is browser traffic
        - and traffic is app-dependent
      - deployability: all/nothing
      - modify firefox codebase, when addon suffices
      - maybe also efficiency
        - histograms
          - not fitting: no need to hide *that* traffic occurs, just where to,
          - compare to real fingerprints
          - less efficient
    - problems walkie-talkie: also modify all
      - bit slower
      - not preferred method
      - TD: compare to wfpad accuracy/efficiency
    - conversely:
      - addon: easier to modify/implement/test
        - *easy to use* if not default (currently needs server, but
          others need too, does not need by default)
        - HTTP traffic properties used
        - "general defense": not trying to modify specific settings
** TODO Motivation [0/3]
   When this thesis was started, there existed mostly deterministic
   defenses, with both Walkie-Talkie\cite{wang2015walkie} and
   wtf-pad\cite{wtfpad} not having been published yet.

   The deterministic approaches had the major shortcoming of
   introducing additional delay into the traffic, which conflicted
   with Tor's design goal of usability\cite{tor-design}, increasing
   f.ex. the sometimes bothersome delay of using Tor for browsing the
   web. For the positive side higher latency, see
   \cite[sec.4.2]{challenges}.

   While also providing this functionality through an easy-to-add
   browser extension, keeping the Tor Browser code as-is, this
   thesis's approach uses properties of web traffic to determine when
   and how much traffic to send. This stands in contrast to both Wang
   et al.'s Walkie-Talkie\cite{wang2015walkie}, which offers sampling
   from both uniform and normal distributions, and Juarez et al.'s
   Wtfpad\cite{wtfpad}, which creates histogram-based traffic, but
   does not specify the size of packets to retrieve, and, critically,
   adapts a method that tries to do more (hiding from a global
   adversary), instead of hiding which site was browsed to.
** TODO Design and Implementation (=Implementation) [0/6]
   As detailed in section [[#features]], there are key features that are
   hard to cloak except by extra traffic, f.ex. total bytes up-/ and
   downstream.

   Given that, the next question is how to shape traffic in order to
   effectively cloak the fingerprint.
*** WRITE How to hide: addon
    - deployable
    - module that is separateable
      - easier to use and test
      - some things harder (f.ex. shaping)
*** WRITE Aim: selective cover traffic
    - based on target web site
    - simultaneous to real traffic
*** KEYWORDS Algorithm
*** KEYWORDS Modules
*** KEYWORDS Server
*** brainstorm                                                      :ARCHIVE:
    - aim: selective cover traffic
      - select based on web site
      - and target
      - simultaneous to real traffic
    - firefox browser extension / addon
      - addon sdk
      - maybe mention next generation
    - good code
      - tests
        - unit tests
        - by hand
      - good parts
      - js garden
      - style guide
      - version control
    - algorithm
    - implementation
      - classes
    - server
      - later: .onion (link to related work)
    - http traffic distribution
*** TODO Bloom Filters
    rewrite this as a subidea
**** WRITE General Idea
     - stochastic fixed-width data structure
     - works flawlessly if element is inside
       - might fail if not
**** WRITE Application: Bloom Sort
     - sort into bins
       - based on target distribution
       - one bloom filter per bin
     - check size: check all filters
       - if one returns: fine
       - if none returns: ok: clear that not inserted, default value
       - if two return: error, fall back to default value
**** TODO Error estimation of Bloom Sort
     - error both ways, and difference bin-size to real size
**** brainstorm                                                     :ARCHIVE:
     - stochastic fixed-width data structure
     - works flawlessly if element is inside
       - might fail if not
     - based on this: bloomsort: combine filters
       - sort into bins
         - based on target distribution
         - one bloom filter per bin
       - check size: check all filters
         - if one returns: fine
         - if none returns: ok: clear that not inserted, default value
         - if two return: error, fall back to default value
       - error estimation?
       - +: fixed size
       - -: error both ways, and difference bin-size to real size
*** KEYWORDS Related Work?
    - bloom paper
    - network applications
* TODO Results and Evaluation [0%]
** WRITE panchenko v1 vs cumul
   - both: similar results for different classifiers
   - panchenko v1:
     - takes longer
     - is less accurate
     - is more work: vector length normalisation [for cross-test]
     - first to really work
   - accuracy hit of about 20%?
   - best parameters sometimes outside of panchenko's range
   - same classifier
   - state of the art:
     - wang-knn
       - knn with parameter weighting step
       - first to 92% accuracy (current limit)
     - cumul:
       - faster
       - easier to see
       - bigger dataset
     - k-fingerprinting
       - accuracy
     - all similar accuracies (as of k-fingerprinting)
** TODO Evaluation of Defenses [0/3]
*** KEYWORDS Evaluation of Addon
    - different versions
      - 0.18 over-engineered?
      - scenarios
        - evolution:
          - 15.3 first results
          - buggy: did not match spec (only did html requests)
        - continue to 18:
          - much more traffic
        - try to fix at 19 (and backport to 15.3, codename retro)
          - all on same curve
        - keep in mind: only 10 sites: 10% accuracy is random guessing
        - 20 limits number of embedded requests
    - different factors
*** KEYWORDS sota (practical): wtfpad
*** KEYWORDS sota (theoretical): walkie-talkie
*** KEYWORDS (maybe) vs optimal attacker
** brainstorm                                                       :ARCHIVE:
   - addon
     - different versions
     - different factors
   - does it work?
   - does it work better?
   - which variant works?
* KEYWORDS Conclusion
* TODO topics [0/118]
** Teaser
   #+BEGIN_QUOTE
   "They who can give up essential liberty to obtain a little temporary
   safety, deserve neither liberty nor safety" - Benjamin
   Franklin\cite{franklin}
   #+END_QUOTE

   In Germany, the basis of all laws is the "Grundgesetz", which
   ensures free speech (Art. 5 GG) and protects private communication
   (Art. 10 GG).

   The Fourth Amendmend to the United States Constitution is
   interpreted as providing similar protections to Art. 10 GG, as of
   \cite{katz}.

   With new technologies for communication and information emerge new
   challenges to secure these rights.

   The internet has offered many new ways to communicate and,
   conversely, wiretap, of which website fingerprinting and page-marker
   detection are examples.

   Yet, there may exist user-friendly ways to hamper, or even to deter,
   this surveillance.
*** evtl rein
    Auch in der DDR gab es das Recht auf freie Meinungsäußerung, nur
    hat es niemand genutzt, da durch die Ueberwachung die Angst vor
    Repressalien zu groß war. (td: quote)
*** WAIT may to (ohne) (falls richtig)
** TODO Website Fingerprinting [0/55]
*** tools
**** TODO capture alternatives [0/1]
     Several applications can capture network traffic to files. The most
     well-known and oldest of these is tcpdump
     [fn:: \url{http://tcpdump.org}] It is a command-line utility, which is
     available on many UNIX-like systems and Windows.

     A modern contender with a GUI is wireshark. It also sports a
     command-line version, tshark. As it offers TLS packet reassembly,
     tshark was used in this thesis.

     Both programs rely on the libpcap library for access to network
     packets.
***** TODO subsect to [[*by-hand initialization to retrieve websites][by-hand initialization to retrieve websites]]
**** shell script
     Simply calling =firefox website= loads the website in Firefox. This
     is the approach Wang recommended(\cite{wang-scripting}.
***** TODO how to check that page has loaded
**** Selenium
     Selenium is the de-facto standard for testing web applications. It
     has drivers for several browsers, allowing it to control them, and
     evaluate the retrieved page. Its documentation is currently
     transferring from Version 1 to Version 2.
**** Chickenfoot
     Chickenfoot was a Firefox addon which allowed browser scripting. It
     was developed at MIT\cite{chickenfoot}. The most recent GitHub
     release[fn:: \url{https://github.com/bolinfest/chickenfoot}] is for
     Firefox 4.
**** CHECK Marionette
     <<Marionette>> Marionette is the next generation mozilla testing
     framework. It is works just like Selenium and was designed to be
     integrated into it. It was chosen for this thesis, as it made the
     Tor Browser Bundle easily accessible.

     After installation of the library (see below), controlling the browser
     takes two easy steps:

     1. start the Tor Browser Bundle with the `-marionette` switch

        #+BEGIN_SRC sh
          cd tor-browser_en-US/Browser
          ./firefox -marionette
        #+END_SRC

     2. attach to a running browser in Python

        #+BEGIN_SRC python
          from marionette import Marionette
          client = Marionette('localhost', port=2828);
          client.start_session()
          client.navigate('http://cnn.com'); # navigate loads a website
        #+END_SRC

     Marionette has the benefit that the =client.navigate()= call
     returns only after the page has loaded, (and throws an error if
     the page could not be loaded). This obsoletes f.ex. Panchenko et al.'s
     \cite{panchenko} need to test whether a page loaded completely.
**** CHECK Marionette installation
     Marionette exists as a Python Package. It is thus easily installed
     via

     pip install marionette_client

     After installation pip via =sudo apt-get install python-pip=). Using
     a virtualenv is highly recommended in the documentation. If using
     only Marionette, it proved to be unnecessary. The combined
     installation of Marionette with Mozmill broke Marionette.
***** TODO merge with above and split out pip install (also needed for wsgi)
**** criteria for tool to retrieve websites
     - script tor browser: load new page
     - easy set-up
     - should
       - register page load or error
     - might
       - set tor's paranoia slider
       - install extra addon
**** TODO who used which retrieval method
     - who did sth
       - p: 
         1. chickenfoot only
         2. Chickenfoot, iMacros, and Scriptish
       - h
       - ll
       - w
       - c
       - d
       - j
     - what did they use
       - list
       - chickenfoot
       - modified browser
       - selenium: daniel
       - plain tor bundle
*** setup
**** TODO by-hand initialization to retrieve websites
     After installation, the tor browser bundle performs some
     initialization steps. To complete these easily, start the tor
     browser bundle-firefox by hand once, set the connection type and
     have it load any website via Tor. All this also downloads Tor
     metadata, which allows to connect more quickly later on.

     Once the Browser Bundle is working, as it runs when starting
     manually, webpages can be retrieved automatically. This is done
     via the [[one-site.py][one-site.py]] script.

     The script

     1. starts the Tor Browser Bundle's =firefox= binary, enabling
        remote-control via the [[Marionette][=-marionette=]] command-line argument,
        waiting up to 60 seconds for its initialization
     2. starts the =tshark= capture
     3. loads the page (given as first parameter) via Marionette
     4. waits up to 600 seconds for the page load to finish
     5. waits 3 more seconds (for the last cover traffic to finish)
     6. ends the capture
     7. ends Firefox

     This setup (restart after each trace) avoids caching issues with
     website fingerprinting, as the Tor Browser Bundle cleans the
     cache between restarts (as mentioned f.ex. in \cite{critique}). If a
     browsing-session scenario is desired, the script could be
     modified to omit terminating the browser instance.
***** TODO this is not [only] by-hand! split or change title
**** tshark installation
     You also need to install =tshark= [fn:: via f.ex. =sudo apt-get
     install tshark= on Debian-based systems] and enable the user to
     capture packets [fn:: via (Debian-based) =sudo dpkg-reconfigure
     wireshark-common= and adding the user to the =wireshark= group
     (in =/etc/groups=)].
**** TODO how to get tor browser bundle to work
     In order to start the tor browser bundle via the =./firefox=
     command, you need libraries, which are bundled with the binary.
     They can be found inside the =/TorBrowser/Tor= directory.

     The library path environment variable can be set on the command-line via
     #+BEGIN_SRC sh
     export LD_LIBRARY_PATH=/lib:/usr/lib:/path/to/bundle/Browser/TorBrowser/Tor
     #+END_SRC
     The script [[one-site.py][one-site.py]] uses this internally.

     - install xpra
***** TODO is old, still use, or remove?
**** Avoiding safe mode on restart
     If Firefox was killed via a signal (as opposed to closing the
     window), it prompts to start in Safe Mode afterwards.

     This behavior can be avoided in three ways:

     You can set the firefox preference
     =toolkit.startup.max_resumed_crashes= to -1, you can set the
     environment variable =MOZ_DISABLE_AUTO_SAFE_MODE= (did not work
     in Tor Browser Bundle version ...), or --- as a last resort ---
     you can remove the =toolkit.startup.recent_crashes= line in the
     =prefs.js= config file which saves the number of consecutive
     kills via =sed -i '/toolkit\.startup\.recent_crashes/d'
     /path/to/prefs.js=.
***** TODO TBB current version
**** headless configuration
     If you want to capture on a headless server, you can use the
     =xvfb=-package. which is installed via =sudo apt-get install xvfb=.

     Then, you can run the X Virtual Framebuffer via

     =Xvfb :1=

     tell the browser to use it via

     =export DISPLAY:1=

     and start the retrieval as mentioned above.
**** TODO thoughts on size of data set
     - computable (n^2 for svm with good results)
     - number of instances negligible for computation
       - check this
     - stable results
     - recent papers
       - Panchenko: 775 a 20
       - Wang:
         - 100 a 90 of sensitive pages
         - 5000 a 1 of non-monitored pages
       - Cai: 400 samples of bbc.co.uk
         - 100 \to 800 once a 20 \to 40 twice
     - (currently closed world)
***** TODO more complete list?
**** TODO filtering tshark files [0/4]
     Although this requirement might later be removed (see [[*Further%20work][further
     work]]), the addon currently needs a generator of cover traffic to
     work. While it can be set in the add-on's preferences, this
     generator ran on the same host as the tor client. Thus, the
     capture files also contained traffic of the cover traffic
     server. As they do not belong to the Tor traffic, are not what
     the adversary sees, and might distort the result, they were
     filtered. (Even though the accuracy results were not greatly
     changed by this).

     Fortunately, =tshark= offers a way to filter these files as
     mentioned in \cite{splitcap}. The (read) filter commands are
     described in the manual \cite{wireshark-filter}, with the tcp
     protocol specific fields as given in \cite{tcp-filter-fields}.

     The script to solve this is in the appendix [[7777]]. As the server
     ran on port 7777, which was allowed only as an incoming port by
     the firewall, it suffices to filter by port name. (Otherwise, the
     read filter would need to be modified).
***** TODO implementation
     - summary approach: file 7777.sh takes each (pcap) file in
       current directory, filters the port 7777 out
     - apply this to each subdirectory
     - then move all files to a common directory
****** TODO include script from duckstein
***** TODO link to man tshark
*** TODO example: single files of a website
    The complete data of google.com can be retrieved via

    =mkdir site; cd site; wget -p -H google.com=

    which yields (in germany) the files (=find . -type f -ls=, formatted)

    |  size | url                                                               |
    |-------+-------------------------------------------------------------------|
    |       | <65>                                                              |
    | 18979 | google.com/index.html                                             |
    | 17284 | www.google.de/images/nav_logo229.png                              |
    |  1834 | www.google.de/images/icons/product/chrome-48.png                  |
    |  5482 | =www.google.de/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png= |
    |  5430 | =www.google.de/images/branding/product/ico/googleg_lodp.ico=      |
    |  8080 | www.google.de/robots.txt                                          |

    thus, there should be 5-6 (depending on robots.txt) requests
**** TODO tshark for normal (non-tor) retrieval
**** TODO mention redirects
*** practical wf: analyzing traces
**** TODO how to process the data
     The aim of processing is to extract features relevant for machine
     learning from the original trace files, which are in =pcap= format.

     Of the several tools available for reading =pcap=, =tshark= was
     chosen. It is the command-line version of the Wireshark protocol
     analyzer[fn:: \url{http://www.wireshark.org}].

     - tshark internally
     - python triggers
     - collects,
     - sums in the end
     - displays
     - =Counter=-class
***** TODO why filtering allowed
***** TODO see if merge/unify with [[transform to panchenko-features]]
***** TODO and if include stuff from [[*from%20%5B%5B*transform%20to%20panchenko-features%5D%5D][from {{*transform to panchenko-features}}]]
**** transform to panchenko-features
     In encoding packet sizes, this thesis follows Panchenko et al.'s
     approach, who recorded "incoming packets as positive, outgoing ones
     as negative numbers."\cite{panchenko}
**** usage of counter.py to extract features from pcap
     Once the website traces are stored in pcap-files, feature vectors
     need to be extracted. A feature vector is represented by a Python
     class `Counter`, which can be created from a pcap file, or persisted
     to a json file containing timing and packet size information (to
     save time and space).

     To create a counter, you can use `counter.Counter.from(filename1,
     filename2, ...)`. This is also called indirectly when using
     `counter.py` from the command line, as in

     python -i /path/to/counter.py

     This extracts data from all pcap files in the current directory and
     subdirectories (excluding Address Resolution Protocol messages and
     ACKs). The filename of the pcap files needs to be `domain@tstamp`,
     for example `craigslist.org@1445352269`. The part up to the
     separator `@` is treated as the URL. If JSON-files of the name
     `domain.json` (for example `craigslist.com.json`) exist, those are
     preferred instead of the pcap files.

     In the interactive shell, there is a dictionary called `COUNTERS`,
     with the domain names as keys and an array of `Counter`s as
     values. To persist these to JSON, you can use `save` in the
     python interactive shell, for example

     >>> Counter.save(COUNTERS)

     To distill the features from a single `Counter`, call its
     `panchenko()`, to inspect single features, call
     `get('feature_name')` (for example
     =COUNTERS['cnn.com'][0].get('duration')')=.

     `panchenko()` yields a feature vector with default padding of
     Panchenko's variable-length features. Since Panchenko et
     al\cite{panchenko} gave explicit size conversions, the sizes have
     not been normalized further. The default padding (300 per
     feature) might not be large enough for some traces.
***** TODO maybe rename counter.py to trace.py
**** WAIT and MAYBE how to get wang/goldberg to work
     As the =notes= file says:

     "svm-train and svm-predict come from the libSVM package."
***** maybe to unused
**** TODO libsvm (short)
     LibSVM is a library for support vector machine classification and
     regression. It is used under-the-hood for scikit-learn, yet one part
     of functionality required a specific module which was not
     integrated.

     Its input format is very simple: First a number determining the
     class of the data, then a colon, finally all the data for an
     instance, separated by whitespace.
***** TODO link to code to generate
**** TODO transform features to vector
     Once the =Counter=s data is obtained, it needs to be transformed
     to input for scikit-learn's\cite{scikit-learn} classifiers.

     The code to convert these features to classification input can be
     found in `analyse.py` (see [[analyse.py][appendix]]). This determines the maximum
     length of all variable-length features, 0-pads Panchenko's features
     with zeroes to the same length, and converts them to an array fit
     for input into scikit-learn's classifiers. When called from the
     command line, as

     python -i /path/to/analyse.py

     , it will extract the feature vectors from JSON or pcap files in
     the current directory, and run 5-fold cross-validated classifiers
     against the data.

     =Counter= input features are transformed into scikit-learn input
     in the =to_features()= function, which normalizes all vectors to
     have the same size (padding with 0s), and creates the feature
     matrix =X= with numeric class labels =y= (and class names in
     =y_domain=).

     If you wish to run LibSVM on the command-line, there is also
     =to_libsvm(X, y, fname='libsvm_in')=, which can be called with the
     output of =to_features=. It writes lines in X with labels in y to the
     file 'libsvm_in' (by default).
***** TODO ref stackoverflow why 0 padding
****** TODO or better, some statistics text
***** TODO see also =to_features_cumul=
**** WAIT effect of panchenko's weighting schema
     Currently, fixed attributes are weighted heavily in favor of total
     incoming/outgoing bytes.
***** maybe
**** TODO scikit-learn
     The python module scikit-learn\cite{scikit-learn} is described as a
     collection of "tools for data mining and data analysis".

     It combines python's ease-of-use with the efficiency of libraries
     written in C, such as LibSVM. It offers many different classifiers
     and regressors, such as K-NN, SVM, decision trees, linear
     approximation, random trees, etc.
***** TODO regressor? wording
*** TODO History of Website Fingerprinting
    The idea of using traffic analysis to gather information about
    encrypted traffic was mentioned in \cite[10.3]{applied96} and
    applied in the analysis of SSL 3.0 by Wagner and
    Schneier\cite{SSL}

    - quantifying etc

    The term /website fingerprinting/ was coined by Hintz in 2002. A
    successful attack against single-hop proxies was carried out by
    Herrmann et al. in 2009.

    The website fingerprinting attack scenario is already described in
    the original Tor design paper\cite{tor-design}. Previous to
    Panchenko et al.\cite{panchenko}, it was considered "less
    effective"\cite{tor-design} against Tor, due to stream/circuit
    multiplexing and fixed cell sizes.
**** index? traffic analysis
*** TODO defenses
    - walkie-talkie
    - wtfpad
    - supersequence
    - tamaraw
    - buflo

    There are other methods of defense, which might help mitigate
    website fingerprinting. A certain browser extension and text-only
    browsing might reduce the fingerprint.
**** CHECK Additional Plugin: requestpolicy
     In addition to the security-centric addons deployed with the
     Tor-Browser-Bundle, there is an additional addon with orthogonal
     protection:
     RequestPolicy[fn::\url{https://requestpolicycontinued.github.io/}]
     controls which third-party content to load on a given page. Every
     query to the original domain is allowed, while requests to other
     domains must be temporarily or permanently approved. It comes
     with a restrictive set of pre-defined rules (for example google
     pages are allowed to access gstatic). Both a blacklist and a
     whitelist mode exist.

     This could easily (and individually) alter the request/response
     characteristic of a website. More study might shed some light.

     RequestPolicy hindered early versions of the Addon, as it blocked
     [[page-worker]]s. If both are deployed alongside, it should be
     carefully checked.
***** TODO move below tbb
***** MAYBE also cite requestpolicy (orthogonal)
**** CHECK write new plugins
     Instead of inserting dummy traffic into the connection, one could
     throttle the "data rate" of request and responses (or only
     requests or the ratio) --- optionally padding with dummies up to
     the maximum rate.

     This approach has been used by f.ex. \cite{effective}, and has
     been proven to work, albeit requiring higher latency, it has not
     been explored further, as
     - it might be hard to implement in a plug-in, and
     - randomized defenses seem offer adequate defense at reduced
       latency and bandwith
***** TODO move to description of other defenses
**** CHECK tor browser bundle defense
     After the attack by Panchenko et al. \cite{panchenko}, the Tor
     Project deployed an experimental defense \cite{experimental} in
     the Tor Browser Bundle.

     This defense enables HTTP pipelining and randomizes both the
     number of concurrent requests and their order.  It was shown to
     be ineffective by \cite{ccs2012-fingerprinting}, and confirmed by
     \cite{wpes13-fingerprinting} and \cite{effective}.
***** TODO HTTP pipelining refer to/elaborate, make own show subsubsection
**** TODO running an OR
     - hinted by ...
     - extra traffic
     - depends on data rate: if all is easily decorrelatable, maybe no
       extra protection
**** CHECK text-only
***** TODO lynx link
*** distribution of (main) features
    These distribution histograms show how Panchenko's main features
    are distributed. They are stacked histograms with classes
    separated by colors. They are compared (visually) to the HTTP
    Traffic Model\cite{newtrafficmodel}.

    [[file:pictures/all_count_in.png]]
    shows the number of downstream/incoming packets.

    The general form of a gamma distribution may be
    fitting. Conceptually, this should be approximately

    num_embedded (gamma) * size_embedded (lognormal) / packet_size

    [[file:pictures/all_count_out.png]]
    shows the number of upstream/outgoing packets.

    Conceptually, the

    [[file:pictures/all_length_0.png]]
    the length of the Size Marker feature vector.

    [[file:pictures/all_num_sizes_in.png]]
    number of different packet sizes downstream/incoming.

    [[file:pictures/all_num_sizes_out.png]]
    number of different packet sizes upstream/outgoing.

    [[file:pictures/all_percentage_in.png]]
    percentage of incoming bytes (of total).

    [[file:pictures/all_total_in.png]]
    total bytes downstream/incoming.

    [[file:pictures/all_total_out.png]]
    total bytes upstream/outgoing.
**** TODO compare to HTTP model
*** Who could attack via WF
    As website fingerprinting requires very litte resources, a specific
    attacker could be a WLAN sniffer, an ISP, up to maybe even a nation
    state.
*** CHECK Panchenko et al.'s Attack via Website Fingerprinting
    The first website fingerprinting
    attack\cite{ccsw09-fingerprinting} to also target Tor had yielded
    little accuracy. This was due\cite{experimental} to Tor's
    multiplexing and fixed cell-size.

    Panchenko et al\cite{panchenko} were the first to publish a
    successful website fingerprinting attack on Tor. They extracted
    HTTP-specific features from the packet trace and used those in a
    hand-tuned support vector machine with a radial basis function
    kernel.
**** practical wf: Capturing traces
**** TODO move to subsection related work
*** CHECK which features work well
    As stressed by Perry in \cite{critique}, analysis of which feature
    contribute the most towards classification is important. Panchenko
    et al. \cite{panchenko} provided a qualitative analysis. Hayes and
    Danezis \cite{kfingerprint} used forests of randomized trees, which
    provide feature importance estimation.[fn:: f.ex. in scikit-learn
    \cite{scikit-learn} via the =_feature_importances= attribute]

    As Dyer et al \cite{oakland2012-peekaboo} noted and experiments with
    Panchenko et al's \cite{panchenko} estimator support (see
    [[different-classifiers]]), you can get good accuracy with several
    classifiers, given the right features.

    As determined by Hayes and Danezis \cite{kfingerprint}, the top-five
    features are the number (both absolute and percentage of total) of
    both incoming and outgoing packets. The standard deviation of the
    packet ordering list [fn:: Panchenko et al \cite{panchenko} call
    these features /Number Markers/] completes the top five. Each added
    feature increases accuracy, yet with nearly the same accuracy for 30 as
    for the total of 150 features.
** TODO Addon Design and Implementation [0/58]
*** [[*description of add-on][description of add-on]]
*** Defenses
*** CHECK Variations of Cover Traffic
    There are two variations how to generate Cover Traffic.

    1. Does knowing web page characteristics, such as [[#find sizes of HTML-documents][the size of the
       HTML-webpage]] and [[number_embedded][number of embedded objects]] help in generating
       cover traffic? While it seems so at first, it can be evaluated if
       [[HTML traffic
        model][educated guessing]] might work better.

       In a closed world, it is possible to always know these sizes
       beforehand. If unknown, the random variates from the [[HTML traffic model][HTML traffic
       model]] are used. [fn:: The size of each embedded element is always
       drawn from the HTML traffic model.].

    2. Given a webpage and its size, how much traffic should be generated?

       While just adding random traffic to each page might enhance
       anonymity, always adding from the same distribution would
       probably be wasteful, as site-specific values might prove to
       obfuscate better.  A target size and number of objects has to be
       determined. How to choose this is the second parameter.

       One approach is to group the webpages by their size into bins and
       to set the bin border as the target size, as all webpages in the
       bin must have a size less than or equal the border. This approach
       mimics that taken by Wang et al. in \cite{effective} with the
       bins being equivalent to the anonymity sets / partitions. For the
       biggest bin, its median size is currently chosen.[fn:: The optimal
       size for the biggest bin is a parameter that should be evaluated
       as well.]

       The other approach is to have a single target distribution from
       which values are sampled each time, once again from the [[HTML traffic model][HTML
       traffic model]].
**** TODO move this f.ex. to coverTraffic, maybe move this section below
    From the target values, the webpage's html-size and number of
    elements (variation A/B) is subtracted.

    At the same time of the HTML-query, another query for the remaining
    HTML-size (or a token amount if too small) is sent. Concerning the
    embedded elements, the ratio of (target-site)/site is computed. For
    each element, this ratio determines the number of requests for
    embedded elements (these are always of random sizes, once again from
    the [[HTML traffic model][HTML traffic model]]. See [[*cover%20traffic%20distribution%20generation][cover traffic distribution generation]]
    for the algorithms.
**** TODO end move section
    This leads to the following variations:

    1) bloom binning (I) with known sizes (A)
    2) bloom binning (I) with random sizes (B)
    3) one target distribution (II) with original size from bloom (A),
    4) one target distribution (II) with random sizes (B)

    | SIZES \ TARGETS | I: bloom binning | II: one distribution |
    |-----------------+------------------+----------------------|
    | A: known sizes  |                  |                      |
    | B: random sizes |                  |                      |
*** TODO Mozilla Add-On Sdk [0/12]
**** CHECK Introduction to the Mozilla Add-On Sdk
     #INDEX: XUL
     #INDEX: XML User-interface Langage
     The Add-on SDK by Mozilla facilitates the development of
     Firefox-Addons.

     It allows users to create addons using HTML and Javascript only, as
     opposed to the previous use of
     XUL[fn:: \url{https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL}],
     the XML User-interface Language.

     The addon execution entry point (like =main= in C and Java) can be
     configured via the =preferences.json= file. By default, the main
     addon-script is called =index.js=.

     The SDK contains many tools to interact with the browser. URLs can
     be loaded in the background via the =page-worker= module; the
     =page-mod= module injects JavaScript code into the page the user is
     browsing to. User-created code can be tested via unit tests.

     If none of the easily accessible high-, or low-level modules
     suffice, much of the browser's functionality is accessible via
     the Components
     object[fn:: \url{https://developer.mozilla.org/en/Components_object}],
     which can be accessed as =require("chrome")=.
**** TODO Debugger
**** TODO Available Data
     Firefox offers several ways for an add-on to listen for web activity.

     - contents of main page
       \to links to each domain
     - page-mod
       - problems: only when page is loaded, problem for cover traffic
       - but +: ends of all the loading (and processing)
     [[file:docs/lit.org::*%5B%5B./Intercepting%20Page%20Loads%20-%20Mozilla%20|%20MDN.html%5D%5BIntercepting%20Page%20Loads%5D%5D][Intercepting Page Loads*]] lists several
     - load events
     - http observer
     - webprogersslistener
     - xpcom
       - policymanager
       - documentloader
***** each load of page
***** end of page load
***** TODO as references or as footnote?
      ref, as completely read?
**** separation of scripts
     As a security measure, there is a separation between

     1) /add-on scripts/, which are run in the browser context, but
	cannot access the web page, and
     2) /content scripts/, which are run in the page context. They can
	access the DOM, but not add-on scripts. nor
     3) /page scripts/, which are those included in the website via
	f.ex. =<script>= tags

     Bridging this separation, f.ex. accessing page scripts (and vice
     versa) is possible, but needs some extra work.
***** WAIT index: page scripts, content scripts, add-on scripts
**** CHECK message-passing
     There is a mechanism to pass content from the add-on to the
     content scripts, as shown in the example.

     A single string can be passed. As this string can be any serialized
     JSON\cite{rfc7159} object, this is not much of a limitation. (It
     effectively disallows the passing of functions and circular
     objects).

     In a content-script, a message can be sent via
     =self.port.emit('message_type', param)= and received via
     =self.port.on('message_type', function(param))=.

     In the Addon-Context, a =worker= object is used and the
     content-script's =self= is replaced by a =worker=. The worker is
     initialized via the =onAttach= parameter of f.ex. the page-mod.
**** TODO collect/list all addon sections
**** CHECK page-worker
     <<page-worker>>
     A =page-worker= creates "a permanent, invisible page and access[es]
     its
     DOM."[fn:: \url{developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-worker.html}]

     New pages can be loaded in the background, which would allow for the
     retrieval of camouflage traffic, as described by \cite{panchenko}.

     A minimal new page-worker is created via

     #+BEGIN_SRC js
       var pageWorker = require("sdk/page-worker").Page({});
     #+END_SRC

     The page-worker's page can be set dynamically via

     pageWorker.contentURL = "http://en.wikipedia.org/wiki/Cheese"

     This fetches only the file pointed to. The retrieval of included
     images, stylesheets, etc, is not automatic.

     A page-worker was used in the initial prototype. The RequestPolicy
     addon blocked this method of retrieval.

**** TODO page-mod
     <<page-mod>>
     The
     page-mod[fn:: \url{http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html}]
     module injects "scripts in the context of web pages whose URL
     matches a given pattern."

     The pattern can be given as ="*"= or =/.*/= to run on every
     user-visited page.

     It thus offers the possibility to check for the end of a web page
     load by the user.

     A page-mod example is

     #+BEGIN_SRC js
       const pageMod = require("sdk/page-mod");
       pageMod.PageMod({
           include: /.*/,
           contentScriptFile: "./getLinks.js",
           onAttach: function(worker) {
               worker.port.on("links", function(JSONlinks) {
                   addToCandidates(JSON.parse(JSONlinks));
               });
           }
       });
     #+END_SRC

     , which is run on every page, applies the =getLinks.js= script and
     listens for its feedback, which is then used via
     =addToCandidates()=.

     The page-mod has a =contentScriptWhen= parameter, which specifies
     when to attach the script to the page. Valid values are =start=,
     =ready=, and =end=, the last of which triggers at the
     =window.onload= event, when the complete page, including
     JavaScript, CSS, and images has loaded.

     A page-mod offers many other options such as f.ex. stylesheets,
     script parameters, etc.
***** link page-mod
      http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html
**** CHECK Installation and Use of Jpm (the build tool)
     (SDK-)addons can be built via the =jpm=-tool. It is available as a
     NodeJS-Module via the built-in NodeJS Package Manager =npm=.

     Installing =jpm= is a two-step process. Firstly, install NodeJS
     either via built-in tools[fn:: for example =apt-get install
     nodejs-legacy= in Debian and Ubuntu] or via
     download[fn:: \url{https://nodejs.org}] then, do a

     npm install jpm

     to install jpm[fn:: for the current user, global installation is done
     via =npm install -g jpm=].

     Once =jpm= is installed, new addons can be created via =jpm init=,
     unit-tested via =jpm test=, live-tested via =jpm run=, the addon
     package built via =jpm xpi=.

     Another command that may be of use is =jpm sign=: as of Firefox
     version 47, Mozilla enforces that all addons be
     signed\cite{addon-signing}. If they are distributed via Mozilla's
     Addon Marketplace[fn:: \url{https://addons.mozilla.org}], they are
     checked and signed automatically. Otherwise, you can request an
     API key for signing and sign via the command
     [fn:: \url{https://developer.mozilla.org/en-US/Add-ons/SDK/Tools/jpm\#jpm_sign}]
     =jpm sign --api-key $SIGNING_KEY --api-secret $SIGNING_SECRET=.
**** TODO interacting with page-scripts
     By default, content-scripts are isolated from the modifications
     done by page-scripts.[[Interacting with page scripts]]

     To access object inside the page-scripts context, you can use
     =unsafeWindow=.

     The reverse is only true for primitive values. If page-scripts
     need to see altered behavior, it is possible to override
     functionality of the page by using =exportFunction=, as in

     exportFunction(open,
		    unsafeWindow.XMLHttpRequest.prototype,
		    {defineAs: "open"});

     This exports the (previously-defined) function =open()= to the
     XMLHttpRequest.prototype, where it replaces the built-in
     functionality.
***** Interacting with page scripts
=developer.mozilla.org/en-US/Add-ons/SDK/Guides/Content_Scripts/Interacting_with_page_scripts.html=
**** TODO [#C] <<<DOM>>>
     domain-object-model
**** TODO unit tests jpm
     JPM also offers the ability to write unit-tests.
*** WAIT [#B] Design
    #+BEGIN_LATEX
    \begin{adjustbox}{max width=\textwidth}
    \input{pictures/model.tex}
    \end{adjustbox}
    #+END_LATEX
**** needs to do
     - make wf harder such that it is impossible
**** by
     - generating cover traffic
**** procedural
***** check which urls user loads
****** aggregate by domain
***** for each loaded url, maybe load something else
      - this generates the cover traffic over the loading of the website
      - yet only augments bursts, does not equalize them
**** TODO modules [0/14]
***** TODO replace with pic [[shell:dia pictures/model.dia &]]
***** TODO how to show (singleton)-module in jUML
***** TODO Watcher
      - notifies when user loads sth, and when finished
      - implements nsIObserverService
****** TODO link nsIObserverService
****** methods
       - loads
       - endsLoad
***** CHECK CoverTraffic(Loader=default)
      <<CoverTraffic>> The =CoverTraffic= module provides requests for a
      single host contacted. This is the only module used with a
      constructor, as it requires several instances, one for each host.

      The cover traffic provided tries to mimic the [[HTML traffic
      model]]s parameters.

      There are two strategies implemented which have to be set by
      modifying the source code.

      One strategy deals with size estimation: for each page, the size
      of its HTML request and the number of embedded elements are
      stored in a statistic data structure depending on bloom filters,
      called [[SizeCache]]. If the size is known, it can be used or
      randomly guessed from the HTML traffic model.

      Another strategy deals with target sizes. The size-cache stores
      approximate sizes, due to binning of values. One strategy is to
      pad both the HTML request size and the number of embedded
      elements up to the bin border. The other strategy determines a
      target distribution for each parameter, multiplies by the
      overhead parameter, and tries to attain that.

      Thus, on creation the site's and a target HTML size and number
      of embedded elements are determined. As creation is synchronous
      with the first HTML request, another request to pad up to the
      target HTML size is sent. As the target number of requests for
      embedded elements is a certain multiple of the actual number of
      requests, on each such request[fn:: signaled by the =loadNext=
      call], a probability is sampled, potentially resulting in a
      request for a cover embedded element. The cover element sizes
      are once again drawn from the HTML traffic model.

      After the page has finished loading, the =CoverTraffic='s
      =finish()= method is called. If the number of embedded elements
      requests has been to low, the remainder are then dispatched.
****** WAIT check if still two strategies
****** TODO link to number of embedded elements and HTML request
****** TODO link to sizecache
***** TODO Loader(Source=default2)
      loads new cover page (mockable)
***** TODO Stats - Static functions
      statistical distributions (html, embedded, etc)
***** TODO CoverUrl
      source for cover traffic
      fixed domain, size as parameter
***** TODO BloomSort
      <<bloomsort>>
      sorts elements by size using Bloom Filters
      +add(id, value)
      +query(id): value
      +save
      +restore
***** TODO Random
      provides randomization methods
      +string(length:number) pseudo-random string
      +uniform01() secure random float in the range [0,1)
***** CHECK SizeCache
      <<SizeCache>> The SizeCache element stores approximations for
      both the HTMLsizes (=htmlSize()=) and number of embedded objects
      (=numberEmbeddedObject()=) per URL, using a [[bloomsort][BloomSort]] data
      structure for each.

      Exceptions from the BloomSort are passed on. This module is a
      facade \cite{gof} that initializes the bloom filters and
      simplifies access.
***** CHECK User
      The [[user.js][User]] module handles user action. It is the main controller.

      On each loading of a object via HTTP(S), it receives a message
      from the =Watcher= module via =loads()=, with the loaded URL as
      parameter.

      If it is a new request to the host, loading of an HTML page is
      assumed and a new =CoverTraffic=-Object is generated.

      If the host is known (as defined below), an embedded page is
      assumed and the (existant) related =CoverTraffic=-Object is told
      that an embedded element was loaded.

      After the first request, the host is known. At completion of the
      page load, indicated either by a [[page-mod]]'s integration into the
      page (at =end=), or the end of a timeout of =User.TIMEOUT=
      seconds, the CoverTraffic-object is notified of the ending and
      removed from the internal host-to-CoverTraffic mapping.
**** TODO cover traffic distribution generation
     - each retrieval maybe triggers additional retrieval(s)
       - based on statistical model
     - for each page being retrieved
       - either size can be estimated or it must be guessed
       - either number of embedded elements can be estimated or must
         be guessed
       - determine target size and number of embedded elements
       - fill up HTML traffic with another request with content size
         page.size - target.size
       - for each embedded element
         - generate request for additional embedded element(s) with
           probability (target.number_embedded - page.number_embedded)
           /page.number_embedded
           - if probability > 1, generate those certainly and iterate
             with probability -1, until probability < 0
**** TODO HTML traffic model
     <<HTML traffic model>>
     - intel
       - html object lognormal with params \mu = 7.90272, \sigma = 1.7643
       - embedded objects
     - test
       - download html top 10000
       - analysis
     - link to
**** TODO browser caching
     - browsers cache
     - only helps in cover traffic, (unless warm/cold site model is used)
***** WAIT where to put this?
**** TODO Parameter: Sizes of HTML-Documents
     :PROPERTIES:
     :CUSTOM_ID: find sizes of HTML-documents
     :END:
     The statistical size generation works with application-level
     sizes on the network, as the authors of the HTML traffic
     model\cite{newtrafficmodel} analysed logfiles of the Squid
     proxy[fn:: \url{http://www.squid-cache.org}].

     The HTML-sizes could not be trivially obtained from the
     =Content-Length= in the browser, as there are additional headers
     and size-reduction via compression. The sizes were determined by
     retrieving the files with =wget= via squid. This is implemented
     via the [[./bin/html_top_100.sh]] script (see appendix).

     It empties the =access.log= file and the squid cache by
     restarting. Afterwards, the top-100 files are retrieved with
     =wget= via squid.

     From the log file =access.log=, the sizes are extracted via the
     command sequence

     #+BEGIN_SRC sh
       sudo cat /var/log/squid3/access.log | tr -s ' ' | cut -d ' ' -f 5,7 > /mnt/data/HTML-sizes
     #+END_SRC

     These sizes are then converted to a JSON-array via the
     [[./htmlSizeToJSON.py]]-file. It also does a check for duplicate
     values, choosing the lower one. This increases traffic, but the
     opposite might be too little traffic, thus easier website
     fingerprinting, which should be avoided.
**** TODO Estimate Parameter: Number of Embedded Objects
     <<number_embedded>>
     The second parameter for generating cover traffic is the number
     of embedded objects per HTML-page.

     These are extracted via the python script [[htmlToNumEmbedded.py][htmlToNumEmbedded.py]]
     which is called for each of the top-100's main web pages by
     [[retrieve-100-embedded.sh][retrieve-100-embedded.sh]].

     To extract, python's lxml module to parse the HTML's
     DOM extracts the URLs of embedded files from the attributes of
     several tags, f.ex. the =src= element of =img= tags.

     This implementation currently omits some possibly embedded
     elements, f.ex. those embedded in css files and =style= tags via
     the =@url= css-directive. It seems better for cover traffic to
     slightly underestimate the number of embedded elements. This
     might generate more traffic than strictly necessary, but here,
     safe seems better than sorry. Extracting just the right URLs is a
     matter of [[*Further%20work][further research]].
***** TODO read dom reference
***** TODO link to lxml website
**** TODO bloom-sort usage
     It is impractical to store the sizes of all URLs. Another
     possibility is to use Bloom Filters to aggregate groups of URLs
     with similar values, as described in [[*bloom-sort][bloom-sort]].

     Each groups gets borders (/splits/) and a size which represents each
     contained element.

     Determining the optimal number of groups, splits and sizes is a
     topic of [[*Further%20work][Further work]]. Here, initially the quantiles of the
     HTML-model (see [[*HTML%20traffic%20model][HTML traffic model]]) were used. When the data were
     to be inserted, it turned out that especially the numbers of
     embedded elements did not match the theoretically proposed groups:

     For three groups, the splits would be given by the 33 1/3 and 66
     2/3 quantiles, as 0.0107 and 1.481. As the number of embedded
     elements is a whole number, two thirds of the information would
     be if an element is 0, the next group would contain all other
     elements: The (representative) sizes of the groups were given as
     7.915E-05, 0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

     The data to be inserted (see previous section) had the splits
     (quantiles) at 10 2/3 and 36 2/3 and the sizes at 6, 20, and 59
     2/3.

     In addition to using the observed sizes for the bloom filter, the
     number of groups was increased to 5.
***** TODO error rate computing
      - sources of error
        - filter tells that is has element when it has not
      - how does error appear
        - collision: one of several, the other might be true
        - replacement: simulates being another url
      - rates of error
        - "add" the error rates of the filters? (times population density?)
***** maybe graphics?
***** WAIT check "see previous section"
**** KEYWORDS negative values in distribution
     - occur with real size > target size
     - solution
       - if small /negative request value:
         - save value (min size is 160, thus =160 - requested_size=)
       - else:
         - get value at random up to min(request size, saved values)
         - subtract from request size, and from saved value
*** TODO Implementation [0/5]
**** TODO js coding best practices
     JavaScript\cite{ecma} is arguably a language with some great
     parts, but also several bad ones\cite{javascript}. Approaches to
     mitigate these include

     - "use strict";
     - unit tests
     - mention "good parts"?
       - for what exactly?
       - and javascript garden
     - jshint
***** mention bad parts?
**** Unit Testing
**** WAIT Cover add-on
     Defends against website fingerprinting by injecting artificial
     cover traffic into the communication.
***** when stable
      also cover against website fingerprinting by injecting really
      artificial cover traffic

      for every request, do one as well,
***** why as an add-on
      This is one of the few low-latency communication methods, Instead
      of burdening all of Tor with extra bells and whistles, this solves
      this deanonymization problem at the application layer, where its
      origins are. (Separation of Concerns)
**** TODO http server for testing
**** TODO description of add-on
     The add-on tries to defend against website fingerprinting by
     adding HTTP-distributed extra traffic.

     To do so, it detects the start of each web request. If it is a
     request for a HTML page, an additional HTML page is requested.

     If the request is determined to be for an embedded
     object[fn:: currently, the first page from a domain is the HTML,
     all others within a certain time window are considered embedded],
     an additional embedded object-size page is requested with a
     certain probability.

     Both the sizes of the extra HTML and the extra number of embedded
     objects are determined based on the [[HTML traffic model][HTML traffic model]].

     - detect start of transmission
       - request extra HTML doc to obfuscate that
       - maybe do something to IPP-model (trigger off/on-state on some)
     - always send dummy traffic
       - on each request
       - better: leave some out
     - better: delay some requests (f.ex. images)
     - detect end of page load
       - maybe do something to IPP-model (trigger on/off-state on some)
     - request size uniform [0, 300)
       - except if request.len > 300
     - source: cover traffic server
     - size distributions
       - html
       - numembedded
     - td describe best algo only?
**** TODO Apache mod_wsgi
     =mod_wsgi= is a module for the Apache web
     server[fn:: \url{https://httpd.apache.org/}]. It executes python
     scripts which implement the WSGI standard\cite{pep3333}. An
     apache httpd serving only WSGI is easily set up via the
     =mod_wsgi-express= command, which is included in the =mod_wsgi=
     python package[fn:: \url{https://pypi.python.org/pypi/mod_wsgi}].

     Installation (Ubuntu Server Edition and Linux Mint 17.1 Rebecca)

     - apt-get install apache2-bin apache2-dev python-dev
     - pip install mod_wsgi

     start via

     - ~/.local/bin/mod_wsgi-express start-server wsgi.py

     (here, also --port 7777), as for the script wsgi.py see appendix [[wsgi.py][appendix]].
**** CHECK python web server nichol.as
     The naïve implementation based on Python's BaseHTTPServer did not
     perform flawlessly (see [[*Non-parallelized-based web server for cover traffic][Non-parallelized-based web server for
     cover traffic {0/1}]]), even for the queries of a single
     addon. This prompted the search for a python-based,
     adequately-performing technology stack.

     Luckily, an evaluation of Python web server performance had been
     performed by Nicholas Piël \cite{nicholas}. It shows the apache
     server with the mod_wsgi module as well-performing. As it was noted
     to be very easy to set up, it was chosen for this evaluation.
*** Evaluation
**** add-on
***** TODO differences to adaptive padding/wtfpad
- delay of some possible (f.ex. images)
- knowledge of packets
- end of transmission detectable
- different target distributions
- multiple distributions
- optionally no cooperator necessary
    dummy packets chosen as response to real request (as in web traffic)
- add evaluation values
- similarities: no delay
  - also has app_hint
- currently uses exit nodes
- this has no gap traffic, aims less at global adversary, more at ISP
****** TODO understand adaptive padding histogram
***** TODO differences to walkie-talkie
***** TODO differences to panchenkos
      - feature extraction via python class directly from pcap
        - packet data saveable to JSON
***** TODO why several covers
      - competition
      - when this started, walkie-talkie and juarez had not yet published
      - harder to break
        - more effort: one classifier for each cover scheme
** TODO Bloom Filters
*** TODO what is a bloom filter
    A Bloom Filter is a data structure to test membership in a set. It
    has a fixed size and a certain one-way error rate. If an item is in
    the set, the Bloom Filter is guaranteed to report this. If an item
    is not in the set, there is a certain probability, the /error rate/,
    of reporting that it belongs.

    This error rate is dependent on the size of the bloom filter and the
    number of inserted elements.
*** TODO bloom usage and implementation
    - bloom sort
      - error rate computation
    - size taken from example...
      - maybe change when altered
*** CHECK bloom-sort
    By ordering data into bins, it becomes possible to use bloom filters
    for the estimation of sizes, using one bloom filter for each bin.

    To achieve this, sensible separation criteria (called /splits/) for
    the bins need to be found. Afterwards, each bin needs to be assigned
    a value (called /size/) for all contained elements. See section
    [[*bloom-sort%20usage][bloom-sort usage]] on determining the sizes and splits.

    This data-structure, called /bloom-sort/ is initialized with an
    array of splits, and an array of sizes. The sizes-array needs to
    have one more element than the splits-array, as the bins are bounded
    on the left by 0, and on the right by infinity.

    #+BEGIN_SRC js
      /**
       ,* @param {sizes Array} array of values for each bin, must be sorted
       ,* @param {splits Array} array of bin borders, must be sorted
      ,*/
      function BloomSort(sizes, splits) {
          this.sizes = sizes;
          this.splits = splits;
          this.filters = [];
          for ( let i = 0; i < sizes.length; i++ ) {
              this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
          }
      }
    #+END_SRC

    Thus, you get

    -\infty \le size0 \le split0 \le size1 \le split1 \le ... \le split(n-1) \le sizen < \infty

    Given the splits, it becomes possible to add the elements to their
    bins:

    #+BEGIN_SRC js
      BloomSort.prototype.add = function(id, size) {
          this.filters[_.sortedIndex(this.splits, size)].add(id);
      };
    #+END_SRC

    where =_.sortedIndex()= gives the index at which =size= would be
    inserted into the sorted =this.splits= array.

    The retrieval of element sizes looks into each bloom filter,
    checking whether it might contain the element =id=. If one bloom
    filter reports containment, its corresponding element- =size= is
    returned. If several or no bloom filters report containment, an
    exception is thrown. The exception is used to allow all possible
    return values, not blocking one of them, say =-1=, for the error
    condition.
    #+BEGIN_SRC js
      /** determines size of element, raises exception if unclear */
      BloomSort.prototype.query = function(id) {
          let pos = -1;
          for ( let i = 0; i < this.filters.length; i++ ) {
              if ( this.filters[i].test(id) ) {
                  if ( pos === -1 ) {
                      pos = i;
                  } else {
                      throw {
                          name: 'BloomError',
                          message: 'Contains multiple entries'
                      };
                  }
              }
          }
          if ( pos === -1 ) {
              throw {
                  name: 'BloomError',
                  message: 'Contains no entries'
              };
          }
          return this.sizes[pos];
      };
    #+END_SRC

    It can be used by initializing with
    #+BEGIN_SRC js
    let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
    #+END_SRC

    then adding elements via =htmlSizes.add("http://google.com/", 613)=
    and querying via =htmlSizes.query("http://google.com/")=, which
    would yield =400=. (see usage in [[file:cover/js/size-cache.js::let%20numEmbeddeds%20%3D%20new%20BloomSort.BloomSort(NUM_EMBEDDED_SIZES,][size-cache]])
* MAYBE-then-WAIT torben
  Torben is a deanonymization attack based on injected website content
  in combination with Pattern Recognition. The authors show that when
  the user's browser sends requests of certain sizes for responses of
  certain sizes, this can be recognized in the encrypted TLS-Traffic
  from the Guard Node to the Onion Proxy.

  Each request/response pair corresponds to a certain amount of
  information (the authors show their approach with four request and
  response sizes, yielding a four-bit side-channel per request). This
  channel is used to encode a hash of the currently visited page.

  The requests are performed via XMLHttpRequest, but they authors also
  mention using HTTP redirects for the same effect.



  inject additional traffic into communication via JS XMLHttpRequest
  fixed request/response sizes of 2k, 4k, 6k, 8k bytes
  \to quad bits, concatenate, data transfer rate rate
  after 30 or 120 ms (tor latency bigger)
  detect via svm (how)
  setzt auf tcp an statt auf ip, (weil tor ja tcp ! yeah!)
** WAIT talk to daniel whether mention or not
* MAYBE why privacy
  - fundamental human need
  - concentration camp:
    "solitude in a Camp is more precious and rare than bread." -- primo levi
* TODO extract dom tags python
  - diveintopython
  - see code
* CHECK modified top-100
  The files for retrieval were from the alexa-top-1m[fn:: Available
  at \url{ http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}],
  from September 30, 2015. Similar to \cite{wpes13-fingerprinting},
  similar sites were removed. Also removed were those sites which
  failed to respond to python's =urllib=. The list of sites with their
  Alexa index can be found in appendix [[top-100]].
** TODO complete list in appendix
* CHECK wsgi.py cover traffic server and generator
  With the technology stack to implement the cover traffic generator
  being settled, implementation becomes a single-page file, see
  [[wsgi.py]].

  One detail is that the length of the content gets inflated by the
  content-headers. To decrease this again, the length (which in turn
  depends on the required length) needs to be calculated and
  subtracted from the body-length. Some uncertainty arises because the
  =Proxy-Connection: keep-alive= header is headed in some
  circumstances. The implementation errs on the side of returning too
  much data.

  Once the size is computed, a pseudo-random choice from the list of
  all printable characters is returned to the HTML query.

  To test this algorithm, the first 1000 sizes are retrieved via
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          curl -D /tmp/curlheaders/$i.head 127.0.0.1:8000/?size=$i > /tmp/curlheaders/$i.body; 
      done
    #+END_SRC
  which outputs the header and body of each query to the files,
  f.ex. =134.head= and =134.body=.

  This data is then evaluated by hand to check the sizes:
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          echo "$i: $(cat ${i}.* | wc -c)";
      done
    #+END_SRC
* TODO differences theoretical HTML-num embedded and observed
  - redirects
    - html had 176 elements, embedded only 100
    - the others were redirects (f.ex. from google.com to
      www.google.de)
    - these could be counted as having 0 embedded elements,
      - yet still a difference remains
  - it fits better if you enlarge the sizes by 0 for each redirected
    element (there are 176 elements in the html filter, including
    redirects, and only 99 in the embedded filter, if you pad the
    embedded filter by 0 for each of those, it is not a perfect fit,
    but better)
  - growth of websites
  - [[*Further%20work][Further work]]
* TODO panchenko CUMUL
* TODO truncated distributions
  - html: truncated lognormal instead of lognormal
    truncated at 0.999918739 quantile
  - embeddedSize: ebd
  - numEmbeddedObjects: truncated gamma instead of gamma
* TODO does this hide bursts?
  - meta-bursts as described in walkie-talkie
  - are those hidden, too, or can the number of bursts be found out
  - easy to implement, maybe do this
** maybe see cumul-graphics
* TODO mention tor browser bundle version etc
* TODO why defense better
* TODO which sites well-protected, which less
* TODO bursts on addon site load finish
  One characteristic which identified sites well as per Dyer et
  al.\cite{oakland2012-peekaboo} and Wang and Goldberg
  \cite{wang2015walkie} is the number of bursts.

  As the addon would conceptually only increase burst sizes, and not
  alter their number, this should be covered as well. To address this,
  the per-site traffic module [[CoverTraffic]] remembers the number of
  unsent requests for embedded elements. When the page loading is
  finished, this number (which should be 0 or less in more than half
  the cases) of embedded objects is requested. As the cover traffic
  currently comes from a single server, the multiple connection limit
  (compare [[Hurdles]]) should automatically lead to multiple bursts if
  the number of embedded objects is high enough.

  This should emulate normal browser traffic better than the proposed
  probabilistic schemes by Wang and Goldberg (normal and uniform
  distribution). It might be that Wang and Goldberg's deterministic
  padding to common values performs better, but that seems require a
  priori knowledge of website burst sizes.
** TODO maybe move to [[CoverTraffic]]
* TODO machine learning
** TODO knn
** TODO svm
** TODO features
** TODO extremely randomized trees
*** brainstorm
    - decision trees
    - ensemble methods
* TODO addon weaknesses/uncertainties
  - all HTTP gets treated the same
    - redirects
    - iframes
    - normal pages
  - request sizes not altered
    - can clearly see each cover request (as each should have size < 500)
  - sizes have grown since 2007
* TODO strong assumptions on feasibility
  - as critiqued in \cite{critique}
  - if protects against this, should also protect against worse
  - additional (?defense?) as proposed in critique
** TODO follow critique at all?
* TODO npm short installation/description
* TODO panchenko v1 different classifiers
  <<different-classifiers>>
  - experiment
  - different classifiers, different results
  - much easier to just use knn
  - more work for svm parameter estimation
* TODO outlier removal
  As described by \cite{panchenko2}, the CUMUL approach is greatly
  enhanced by outlier removal. In his software, he uses both a
  median-based as well as a 25%/75%-quantile-based approach.

  - implemented in [[file:bin/extract_attribute.py::def%20remove_quantiles_panchenko_2(counter_list):][ex-att]]
  - quantiles:
    - numpy instead of his original code for code clarity
    - just take quantiles, use his limits
** TODO run test, include results
** TODO link to panchenko's software
* TODO how to set up wfpad
  - tor server: listen on ORPort X
  - wfpad server script: send to X, listen on Y
  - wfpad client script: send to Y, listen on Z
  - in tbb/on 2nd tor (a.k.a. client): send traffic to bridge
    =Bridge 127.0.0.1 Z=
  - modify capture
    - localhost (=-i lo=)
    - =port Y=
  - start capture
  - bug on multiple uses:
    #+BEGIN_EXAMPLE
    exceptions.IOError: [Errno 24] Too many open files: '/proc/23634/stat'
    #+END_EXAMPLE
    - try temporary fix: increase number of file descriptors, set
      #+BEGIN_SRC sh
        username        hard    nofile  10000
      #+END_SRC
      in =/etc/security/limits.conf=
    - bug report in appendix, needs some code to mitigate
** TODO scripts
* misc: tex bibliography
\bibliography{docs/master}
\bibliographystyle{plain}
* WAIT Discussion
  intel model: interdependences (html bigger \to more embedded) not mentioned
* WAIT Acknowledgements
  - Dr. med. Dr. phil. Eva
  - Daniel Arp
  - Prof. Dr. Konrad Rieck
  - Tao Wang
  - ...
  - Elena
* TODO Further work
  - bigger world sizes
  - open world
  - source cover traffic: user gives domain as starting point
  - how to generate
    - how often, which parameters
    - just triggered by start and until end, or for each load
  - background if non-active (IPP self-similar)
    - 802.16 model
  - does a new connection to another site create a measurable tor-response
    (with variable-length packets)?
  - provable protection
  - size of bloom filter
  - number of bloom filters,
  - which and how many items to prepopulate
    - country-specific f.ex. google.com
    - leave out redirect from prepopulation
  - automatic update of bloom-filter
    - with currently visited sites
  - loading further items
  - The choice of cover traffic domains was explicitly taken out of
    the research focus. Currently, all cover traffic is dynamically
    generated by a web server written in Python.

    There exists basic code to use a list of webpages, given their
    sizes. It could be augmented by following links.
    - update from visited URLs
  - no morphing (delay, segmentation)
    - justify why good idea
  - bloomsort save/restore
  - number of embedded elements lacks <style> tags and some in <link>
    - does not honor reloads/cacheing
      - or does it? (maybe only called on cacheing)
    - but better than too many?
      - some approaches yes, binning no
  - elaborate on [[number_embedded]]
  - how to set splits and sizes
  - [[differences theoretical HTML-num embedded and observed]]
  - improve code to include css, (iframes?), js in number of embedded elements
  - web pages got bigger. See if \cite{newtrafficmodel}'s values are
    still accurate.
    - or only rely on quantiles of observed data
      - but these are hard to gather
	- use networkmanager code to do that

    - cite web-doom
  - more elaborate tests with different world sizes / open world / etc
  - User class: should aggregate smarter, not by-host, but by-page
    with every page-embedded element as just that.
    - indexed by host as workaround, can do better later
      - hard to find out which is HTML, which is non-HTML-traffic
      - so all is lumped together per domain
	- first request seen as HTML
	- other requests as non-HTML
    - == determine if HTML page by suffix (not clear as of ... and
      ... (link to SO))
  - bursts maybe less hidden (number of)
  - time not hidden (no delays of single files)
  - firefox e10n multiprocess
** TODO also helps against global observer if .onion generator is used
   - murdoch/danezis: correlation
   - this creates additional traffic which might hinder correlation attacks
   - further work
   - if cover traffic server is used by enough clients at once
   - or is unobservable (hidden service)
   - information-theoretical / stochastical analysis
   - quote perry critique
*** TODO first read murdoch/danezis paper
** onion host for cover traffic
   As indicated f.ex. by Wang and Goldberg,
   \cite{wpes13-fingerprinting}, network load already is a bottleneck
   on Tor, with the key bottleneck being exit nodes\cite{wtfpad}. The
   exit nodes might be spared the extra traffic by using =.onion=
   traffic generators (or, alternatively, hosts). A traffic generator
   could be further optimized by using tor proposals ... (see todo) to
   reduce latency, if this does not reduce privacy.
*** TODO tor proposals as of tor.sx
*** TODO read/skim and cite "on performance..."
** more thorough evaluation
   - only two panchenko approaches
   - assumption: can split traces
** TODO always also link in text
*** TODO check with darp
** TODO links to original, back to further work
** Exactly distinguishing HTML and embedded requests
   The current version of the [[user.js][User module]] separates
   CoverTraffic by DNS-domainname. As it often happens that one HTML
   page has embedded elements from different domains, this does not
   perfectly represent reality. It would be more exact to analyse the
   HTML page and at least return the domains of all embedded elements.
* TODO appendices [0/2]
\appendix
** Script =one-site.py=: capture pcap traces
   <<one-site.py>>
   #+INCLUDE: "./bin/one_site.py" src python
** Script =analyse.py=: classify the data
   <<analyse.py>>
   #+INCLUDE: "./bin/analyse.py" src python
** Script =counter.py=: parse pcap files
   #+INCLUDE: "./bin/counter.py" src python
** Cover Traffic Server: =wsgi.py=
   <<wsgi.py>>
   #+INCLUDE: "./bin/wsgi.py" src python
** Script =htmlToNumEmbedded.py=: extract embedded objects
   <<htmlToNumEmbedded.py>>
   #+INCLUDE: "./bin/htmlToNumEmbedded.py" src python
** Script =html-top-100.sh= to retrieve html pages via squid
   #+INCLUDE: "./bin/html_top_100.sh" src sh
** Script retrieve-100-embedded.sh run htmlToNumEmbedded
   <<retrieve-100-embedded.sh>>
   #+INCLUDE: "./bin/retrieve_100_embedded.sh" src python
** modified top-100
   <<top-100>>
   #+INCLUDE: "./data/top-100-modified.csv" example
** TODO Remove same-host cover traffic server from traces: =7777.sh=
   <<7777>>
   #+INCLUDE: "./bin/7777.sh" src sh
** Addon
*** Control module User
    <<user.js>>
    #+INCLUDE: "./cover/js/user.js" src js
** WF-Trace Pictures
   :PROPERTIES:
   :CUSTOM_ID: wf-pictures
   :END:
   The pictures were created by the commands

   #+BEGIN_EXAMPLE
    for fb in $(ls | grep facebook); do
      python ~/da/bin/counter.py ./$fb  | tail -1 | sed 's/),/\n/g' | \
          tr -d "'()][" > /tmp/times;
      gnuplot -e "set terminal png size 1024,680; \
              set output \"/tmp/${fb}.png\"; plot '/tmp/times' with boxes;";
    done
   #+END_EXAMPLE

   and respectively for craigslist, in the directories containing the
   pcap files.

   These commands first extract the timing attributes (at git commit
   791af76 the last line of the output of counter.py), format it for
   gnuplot (inserting appropriate newlines via =sed= and removing
   extra characters via =tr=), and =gnuplot= s it to a png file with
   the name of the trace file as prefix.
** KEYWORDS Tor Browser despite --sync problems
   - broke with error (quote)
   - solution1: --sync
     - but verrry slow
   - solution 2: xpra
     - install via instructions at ...
       - use their repo
       - quote fp
   - use as
     #+BEGIN_SRC sh
       xpra start ssh:user@host:37 --start=path/to/tor-browser_en-US/Browser/start-tor-browser     
     #+END_SRC
* unused
** from [[*transform to panchenko-features]]
  The code to examine a single trace file is in =analyze_file()=
  It
  - opens the filename in tshark
  - splits the output by tokens
  - gives the relevant values (source IP, size, timestamp) (with the
    timestamp not used by Panchenko) to a =Counts=-object, which
    aggregates it

  [...]
  For a single line, a =Counter=-object aggregates bytes (incoming,
  outgoing), packets (incoming/outgoing), distills into a size/packets
  array and (size+timestamp)/packets array.
  [...]
  This is used in =postprocess()= to determine
  - size markers, (via the =_sum_stream()=-function),
  - the html marker as the first of those
  - the total transmitted bytes incoming and outgoing
  - number marker (via the =_sum_numbers()=-function)
    - slightly extended, as the number 16 was occuring
      everything above 14 was mapped to the same as 14
    - a bit unclear, currently, 3-5 \to 3, 6-8 \to 4, 9-13 \to 5, 14-\infty \to 6
  - occurring packet sizes incoming and outgoing (binned in steps of 2)
  - percentage of outgoing packets
  - number of packets incoming and outgoing.
** start browser with -marionette parameter
   Each modern Firefox, and thus also the tor-browser-bundle, has
   marionette-support built-in. It needs to be enabled on the
   command-line via the =-marionette= switch, for example


   This starts the Tor browser with marionette enabled.
*** marionette support page link
** Sally installation
   Sally is a tool to transfer text into points in a vector space.

   It is installed on Ubuntu Vivid Vervet by following the official
   instructions, then changing =vivid= in the file
   =/etc/apt/sources.list.d/mlsec-ubuntu-sally-vivid.list= to
   =devel=.
** from getting tbb to work
  One external repository is required, which can be installed via

  =add-apt-repository ppa:ubuntu-toolchain-r/test=
  =apt-get update=
  =apt-get dist-upgrade=

  Furthermore, the binary needs some firefox libraries, which can be
  retrieved most easily via =apt-get install firefox=.

  Afterwards, the binary can be started by typing =./firefox=.
** throttling
   As especially outgoing web requests are often quite small, and this
   paper has at the moment a 1:1 rate of outgoing vs incoming for the
   requests, throttling the amount of data leaving the end user might
   well suffice for reducing the bandwidth of the side-channel enough
   to make it insignificant.
** in-browser vs tcp-level ( ???) (generation?)
** how sally works
   - configuration file
     - input
     - features
     - output

** problematic websites
   The above setup worked on most websites.
   The websites sina.com.cn and xinhuanet.com both did not terminate loading.
   This might need further looking into.

   - do they load completely when not Tor, repeat necessary
   - is this by design?
*** exclude
    "scheint sonst zu klappen"
** Plugins: noscript and requestpolicy
   There exist two plugins, which should both allow mitigation of this
   attack. Used in parallel, they may hinder normal browsing somewhat
   (which is why they are not enabled/installed by default in the Tor
   Browser Bundle).

   The first is NoScript, which selects which Javascript sources to
   run and which to block. This is installed by default in the Tor
   Browser Bundle for the additional security benefits it brings (XSS
   defense etc), but not fully enabled. It is recommended by Edward
   Snowden and many others\cite{noscript}.

** what sets Tor apart / other anonymity networks
   There are other anonymity networks, such as JonDonym, I2P, MixNet
   and freedom.

   Tor is an anonymity service.
   - decentralized
   - biggest
   - high throughput
   - rather low latency, usable for web browsing
   - also hidden services

   Using a client called /Onion Proxy/ on the local computer, almost all
*** TODO ref onion routing
*** TODO onion routing
** TODO Non-parallelized-based web server for cover traffic [0/1]
   This approach did not scale to several parallel connections, so it
   was not used. It is included as a reference of what seems to work,
   but did not.

   The python module =TrafficHTTPServer= can be started on the
   command-line via

   python TrafficHTTPServer.py portname

   with portname set to 8000 by default. It generates cover traffic of the
   size given by the =size= parameter, for example the command

   wget 'http://localhost:8000/?size=10'

   retrieves a document with 10 bytes content from a TrafficHTTPServer
   running at localhost port 8000.
*** Problems
    - did not scale: did not respond immediately for parallel
      connections
      - obviously delays problematic as they in effect create less
        cover traffic
    - maybe further work: test that really works worse
*** Script
    #+INCLUDE: "./bin/unused/TrafficHTTPServer.py" src python
** CHECK Why could website fingerprinting be a problem
   As a typical scenario, consider the government of some state. A
   whistleblower posts something very critical of the regime on a
   well-known critical website. The whistleblower uses Tor or some
   other anonymity service to protect his identity. The government
   monitors and records all Tor connections. Even though Tor
   obfuscates the user's traffic, the specific data-pattern of the
   website allows the government to limit its search to, say three,
   subjects. This gives the whistleblower away.[fn:: Such has not been
   observed.]

