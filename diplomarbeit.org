#+TITLE: Selective Cover Traffic
#+TODO: KEYWORDS WRITE CHECK | EVA DANIEL FINAL
#+TODO: TODO WAIT | DONE
#+TODO: | APPENDIX_DONE
\listoffigures
\listoftables
* Configuration							    :ARCHIVE:
#+LATEX_CLASS: scrreprt
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \usepackage{numprint}
#+LATEX_HEADER: \npdecimalsign{.}
#+LATEX_HEADER: \nprounddigits{4}
#+LATEX_HEADER: \npthousandthpartsep{}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure} \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {3.1 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte % mkreik <2016-07-11 Mo>: war {5 cm}
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
#+OPTIONS: H:6
* WAIT Acknowledgements
  - parents H.H. K.K.
  - Elena
  - Daniel Arp
    - so much, surely missed some
    - binning approach
  - Prof. Dr. Konrad Rieck
  - Tao Wang
  - Dr. Dr. Eva
* WAIT Summary
* WRITE Basics (=basics) [40%]
** CHECK Introduction
   To protect online communication, journalists, whistleblowers,
   "normal people", and among others even military have
   resorted\cite{who-uses-tor} to the Tor\cite{tor-design} anonymity
   network.

   Unfortunately, Tor's anonymity protection has been compromised
   under laboratory conditions. \cite{panchenko} were the first to
   show that just looking at the encrypted and location-anonymized
   packets of Tor, the target web page can be determined with high
   accuracy. The [[#wf][next section]] explains how this works: how data
   packets are captured, how relevant identifying data is extracted,
   etc.

   Fortunately, as soon as these weaknesses became known, defenses
   were proposed. [[#Defenses][The subsequent section]] shows different classes of
   defenses and examines them in detail.
** WRITE Website Fingerprinting [15/33]
   :PROPERTIES:
   :CUSTOM_ID: wf
   :END:
*** DANIEL Introduction
    #+INDEX: attack
    Website fingerprinting\cite{hintz02} aims to deduce which web page
    a user is visiting via an anonymizing proxy. It does so based on
    order, size, and timing of data packets.

    This chapter contains first [[#visual][a visual introduction to the subject]],
    then a short review of [[#http][what happens during website
    retrieval]]. Afterwards, [[#wf1.0][early methods of website fingerprinting]]
    will be presented. The next section explains [[#Hurdles][why these
    early techniques no longer work]] especially on anonymity networks
    like Tor.

    Finally, [[#wf2.0][current website fingerprinting attacks]][fn::the attempt to
    break a cryptologically secured system is called an /attack/]will
    be explored.
*** DANIEL Visual Representation of Traces
    :PROPERTIES:
    :CUSTOM_ID: visual
    :END:
    #+INDEX: trace
    To do website fingerprinting, you collect the sizes, order, and
    timing of IP packets, called /traces/. To illustrate the task of a
    website fingerprinter, consider this visualization[fn::see
    appendix [[#wf-pictures]] for the creation of these pictures] of two
    web pages in Figure \ref{fig:traces}.

#+BEGIN_LaTeX
\begin{figure}[htb]
Craigslist.org\\
\includegraphics[width=0.22\textwidth]{./pictures/craigslist_org@1445352269.png}
\includegraphics[width=0.22\textwidth]{./pictures/craigslist_org@1445585277.png}
\includegraphics[width=0.22\textwidth]{./pictures/craigslist_org@1445486337.png}\includegraphics[width=0.22\textwidth]{./pictures/craigslist_org@1445527033.png}\\
Facebook.com\\
\includegraphics[width=0.22 \textwidth]{./pictures/facebook_com@1445350531.png}
\includegraphics[width=0.22 \textwidth]{./pictures/facebook_com@1445422155.png}
\includegraphics[width=0.22 \textwidth]{./pictures/facebook_com@1445425799.png}
\includegraphics[width=0.22 \textwidth]{./pictures/facebook_com@1445429729.png}
\caption{Web trace data visualized. Box height signifies
    amount of data, width the duration until the next packet.}
\label{fig:traces}
\end{figure}
#+END_LaTeX

    According to Cai et al. \cite{a-systematic}, the tuples (delay,
    packet size) visualized in Figure \ref{traces} contain all
    relevant information of a trace.
*** DANIEL What Happens During a (HTTP) Website Request
    :PROPERTIES:
    :CUSTOM_ID: http
    :END:
    #+CAPTION: HTTP/1.0\cite{rfc1945} example from \cite{ssl-traffic-analysis}: page with 2 images, ACKs omitted
    #+ATTR_LATEX: :float wrap :width 0.4\textwidth
    [[./pictures/cheng-http-request.pdf]]

    When a browser such as Mozilla
    Firefox[fn::\url{https://www.mozilla.org/firefox/}] retrieves a
    web page, it does many things under-the-hood.

    First, it retrieves the main object: the browser sends a
    HTTP\cite{rfc2616} request over the TCP/IP\cite{rfc793} protocol
    stack for the HTML\cite{html5} page, which contains the main page
    information. The webserver answers with the requested HTML page,
    or redirects the browser to another address, which it continues
    until the HTML is loaded.

    Afterwards, the browser parses the HTML page and requests all
    objects embedded into the page, such as images, fonts, scripts,
    videos, stylesheets, etc. These can be identified using the
    HTML-tags =<img>=, ~<link rel="stylesheet">~, =<script>=, and
    =<video>=, the CSS-property =@font-face=, etc.
**** TODO test link to firefox (and all links)
**** TODO [#C] format this in latex: left pic, right text, float, ...
**** TODO [#C] if time: link to mozilla-docs
*** DANIEL Website Fingerprinting 1.0
    :PROPERTIES:
    :CUSTOM_ID: wf1.0
    :END:
    To make sense of the noisy data retrieved via traffic sniffing,
    the first fingerprinters distinguished by object sizes: Each
    requested file has a specific size[fn::except for
    dynamically-generated objects] and is transferrend in one or
    several (IP) packets. In the first versions[fn::up to/including
    1.0] of HTTP\cite{rfc1945}, these sizes were clearly visible, as
    each HTTP request-response pair was transmitted over a separate
    TCP connection. They could be found by splitting traffic by the
    connection's port numbers, as done by Mistry and Rahman
    \cite{quantifying}, Cheng and Avnur \cite{ssl-traffic-analysis},
    and Hintz \cite{hintz02}.

    Against HTTPS[fn::HTTP over SSL\cite{sslv3}, the attacks were
    carried out against SSL 3.0], which reveals the domain being
    browsed to but hides the URLs accessed[fn::a web site/domain is
    f.ex.\space{}\url{wikipedia.org}, while an example page/URL is
    https://en.wikipedia.org/w/index.php?title=Quantile_function],
    successful attacks to determine the web page[fn::cryptographers
    talk of attempts to circumvent a protocol as
    /attack/\cite{applied96}] were carried out by the formers
    (\cite{quantifying}, \cite{ssl-traffic-analysis}) at Berkeley
    in 1998.

    The term /website fingerprinting/ was coined in the latter's
    analysis of the SafeWeb anonymizing proxy\cite{hintz02}, where he
    qualitatively classified which web site was being visited.

    All of these approaches determined which page was visited via
    object sizes. Cheng and Avnur \cite{ssl-traffic-analysis}
    additionally proposed a hidden markov model for page
    /transitions/, for hard-to-identify pages.
*** EVA Hurdles to Website Fingerprinting: HTTP Protocol Evolution
    :PROPERTIES:
    :CUSTOM_ID: Hurdles
    :END:
    As a side-effect of web protocol evolution, website fingerprinting
    became harder:

    Building a new HTTP connection for each transferred object turned
    out to be
    inefficient\cite[sec.2.2.2]{DBLP:books/daglib/0001977}. To improve
    on this, some early HTTP/1.0 implementations used persistent
    connections\cite{rfc2068}, see Figure \ref{http_persistent}. These
    were standardized in HTTP/1.1 \cite{rfc2616}.

#+BEGIN_LaTeX
\begin{figure}[H]
\centering
\includegraphics[width=.9\linewidth]{./pictures/HTTP_persistent_connection.png}
\caption{HTTP persistent connections, source \url{https://en.wikipedia.org/wiki/File:HTTP_persistent_connection.svg}. The left sequence includes a TCP handshake for each object, while the right keeps the TCP connection alive until the whole document can be displayed.}
\label{http_persistent}
\end{figure}
#+END_LaTeX

    Before, it was trivial to find out the number of bytes of each
    file's connection. Persistent connections made it harder to
    extract the files' sizes. You had to determine the start and end
    of each request.[fn::f.ex.\space{}by checking gaps in the transmission;
    the more difficult problem of estimating web page retrieval splits
    seems possible over Tor, see f.ex.\space{}\cite{realistic} and
    \cite{DBLP:journals/tifs/FeghhiL16}].


    In addition to this, HTTP/1.1 allowed pipelining several HTTP
    requests in a single connection without waiting for the files to
    arrive in between, see Figure \ref{http_pipelining}.

#+BEGIN_LaTeX
\begin{figure}[htb]
\centering
\includegraphics[width=.9\linewidth]{./pictures/HTTP_pipelining2.png}
\caption{HTTP pipelining. The left sequence waits for each file to be received before the next request is sent. In the right one, all requests are sent at once. source: \url{https://commons.wikimedia.org/wiki/File:HTTP_pipelining2.svg}}
\label{http_pipelining}
\end{figure}
#+END_LaTeX

    Pipelining was disabled[fn::Firefox:
    \url{https://bugzilla.mozilla.org/show_bug.cgi?id=264354}, Chrome:
    \url{https://www.chromium.org/developers/design-documents/network-stack/http-pipelining}]
    or not implemented [fn::in Internet Explorer
    \url{http://wayback.archive.org/web/20101204053757/http://www.microsoft.com/windowsxp/expertzone/chats/transcripts/08_0814_ez_ie8.mspx}]
    in all major browsers. After \cite{panchenko}, Firefox's built-in
    request pipelining was enabled[fn::with added request order
    randomization] as part of a no-cost defense prototype in the
    Tor-Browser-Bundle
    \cite{experimental}. \cite{ccs2012-fingerprinting} found
    fingerprinting to be slightly easier with this defense enabled than
    without.
*** DANIEL Challenges to Website Fingerprinting: Tor [3/5]
    #+INDEX: Tor
    /The Onion Router/\cite{tor-design} (Tor) is an anonymity system:
    While encryption only hides the /content of communication/, Tor
    also attempts to hide some /metadata/: Who communicates with whom,
    for how long, when, how frequent, ...?

    This metadata is important, as it can reveal "[a] lot of good
    information"\cite{applied96}.
**** DANIEL History of Tor
     Tor inherits its onion design from the Onion Routing Project
     \cite{anonymous-connections}. It was originally developed by the
     Naval Research Laboratory of the US Navy with the primary purpose
     of protecting government communication.\cite{who-uses-tor}

     In recent years, Tor has also provided censorship
     circumvention\cite{tor-spec-pt}.
**** DANIEL Who uses Tor
     As more and more (internet) users wish to increase their
     anonymity for various reasons, one of Tor's main design goals is
     usability\cite[Sec.3]{tor-design}, which increases
     anonymity\cite{usability:weis2006}. This has led to a diverse
     user base\cite{who-uses-tor}: The network consists of over six
     thousand nodes and is used by about two million people
     daily[fn:metrics:\url{metrics.torproject.org}].

     As of \cite{who-uses-tor}, the groups[fn::actual or recommended]
     who increase their anonymity via Tor are: journalists and their
     audience, military, law enforcement officers, activists &
     whistleblowers, high & low profile people, business executives,
     bloggers, IT professionals, and "normal people". Actual and
     recommended use is for diverse purposes: privacy, censorship
     avoidance, covert ops, publishing, safety, online surveillance,
     anonymous tip lines, whistleblowing, blogging private opinions,
     evaluating competition, and troubleshooting IT systems.
**** DANIEL How does Tor Work
     :PROPERTIES:
     :CUSTOM_ID: tor-how-work
     :END:
     The Tor anonymity network consists of volunteer servers, called
     /onion routers/. A connection through the network is
     created by a SOCKS5\cite{rfc1928} proxy, the /onion proxy/.

     As illustrated in Figure [[tor-network]], a connection is routed
     through three onion routers, each of which can only see the
     previous and next hop, due to encryption[fn::the messages look
     different each hop, but have the same length]. Thus, no router
     has knowledge of both origin and destination of traffic.

     #+CAPTION: Connection through the Tor network. Alice's computer's onion proxy connects to Bob via three onion routers. Source \url{https://www.torproject.org/about/overview.html.en}
     #+NAME: tor-network
     [[./pictures/htw2.png]]

     Tor's side-effects which hinder website fingerprinting are:
     - Tor's data cells have a *fixed size of 512 bytes* to prevent cell
       identification, obfuscating packet sizes used by [[#wf1.0][all early methods]],
     - Routing through several globally-distributed hops *increases
       latency*, and
     - Tor *multiplexes all data* cells through a single
       TCP-connection, yielding an effect similar to [[#Hurdles][HTTP
       pipelining]][fn::but arguably enhanced].

     Tor's cryptography will be omitted, because website
     fingerprinting, as all traffic analysis, assumes that a
     cryptography is unbreakable, relying only on traffic
     characteristics.[fn::for a general introduction to cryptography,
     consider Schneier's book /Applied Cryptography/\cite{applied96},
     for a high-level introduction to traffic analysis, see
     \cite{introta}].
***** WAIT high-resolution picture
***** WAIT maybe remove Schneier...phy/
      wait for [[file:~/Desktop/main.org::*entfernen%20der%20autorennamen%20(und%20titel)%20aus%20text,%20nur%20bib-referenz?][entfernen der autorennamen (und titel) aus text, nur bib-referenz?]]
*** DANIEL Website Fingerprinting 2.0
    :PROPERTIES:
    :CUSTOM_ID: wf2.0
    :END:
    Similarly to the above website fingerprinting attacks,
    \cite{Liberatore:2006} and\cite{ccsw09-fingerprinting} use only
    packet sizes, but employ machine learning
    techniques. \cite{Liberatore:2006} use both a Naïve Bayes
    Classifier[fn::described f.ex.\space{}in \cite[ch.1.3.1]{intro2ir}] and
    Jaccard's classifier $s_{AB} = \frac{|A \cap B|}{|A \cup B|}$[fn::described
    f.ex.\space{}in \cite{ccsw09-fingerprinting}], while
    \cite{ccsw09-fingerprinting} use a Naïve Bayes Classifier with
    various metrics and preprocessing. This yields high accuracy
    against one-hop anonymizing proxies, but only 3% accuracy against
    775 distinct pages retrieved via Tor\cite{tor-design}. As this is
    still significantly better than random guessing, they
    conjecture[fn:: \cite{Liberatore:2006} mention that they also did
    some experiments which lead to this conjecture] that this result
    could be improved with attacks specifically tailored to Tor.
*** DANIEL Features
    :PROPERTIES:
    :CUSTOM_ID: features
    :END:
    As stressed by \cite{critique}, analysis of which features
    contribute the most towards classification will help to defend
    against website fingerprinting. \cite{panchenko} provides a
    qualitative analysis. \cite{kfingerprint} classify using random
    forests[fn::forests of randomized trees, see
    \cite{DBLP:journals/ml/Breiman01}], which provide quantitative
    feature importance feedback.[fn:: f.ex.\space{}in scikit-learn
    \cite{scikit-learn} via the =_feature_importances= property]

    As \cite{oakland2012-peekaboo} noted and experiments with
    \cite{panchenko}'s estimator support[fn::see
    [[#different-classifiers]]], you can get good accuracy with several
    classifiers, if proper features are available.

    According to \cite{kfingerprint}, the top-five features are the
    number (both absolute and percentage of total) of both incoming
    and outgoing packets. The standard deviation of the packet
    ordering list[fn:: Panchenko et al \cite{panchenko} call these
    features /Number Markers/] completes the top five. Each added
    feature increases accuracy, yet with nearly the same accuracy
    for the best 30 as for the total of 150 features.

    Both \cite{effective} and \cite{panchenko} also consider the first
    packets of a transmission/the HTML page as effective features.

    \cite{panchenko2} allows for the visual comparison of website
    traces. See for example Figure [[CUMUL_traces]].
    #+CAPTION[CUMUL features example]: CUMUL\cite{panchenko2} features example at \url{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}.
    #+NAME: CUMUL_traces
    [[./pictures/cumul_aus_paper.pdf]]
*** WAIT Panchenko Version 1
    #+INDEX: Panchenko~1
    #+INDEX: classifier!Panchenko~1
    #+INDEX: closed-world
    #+INDEX: open-world
    #+INDEX: world!closed
    #+INDEX: world!open
    While \cite{ccsw09-fingerprinting} classified one-hop proxies
    well, they had achieved low accuracy against Tor due to Tor's
    multiplexing and fixed
    cell-size\cite{experimental}. \cite{panchenko} were the
    first\cite{critique} to publish a successful website
    fingerprinting attack on Tor: They extracted HTTP-specific
    features from the packet trace, adjusted their weights, and
    selected only those found to have the greatest impact. The
    features are weighted heavily in favor of total
    incoming/outgoing bytes. Support Vector Machines then classified
    these features.

    In addition to \cite{ccsw09-fingerprinting}'s data set, a new
    open-world[fn::The /closed-world/ assumption means that the web
    browser only visits /k/ web pages, while in the /open-world/
    scenario, /k/ web pages out of an arbitrarily large number are to
    be detected. Thus, the closed-world scenario is a
    simplification.\cite{panchenko2}] dataset was used. The size of
    the open-world data set was five foreground and 5000 background
    sites.

    \cite{panchenko} achieved closed-world recognition rates of 54.61%
    and open-world true-positive rate of up to 73% for Tor. While the
    Tor Project considers these and better rates both insufficient and
    inapplicable\cite{critique} to pose a real threat, they still
    consider this paper "the first work to successfully apply website
    traffic fingerprinting to Tor"\cite{critique}.
    
    - true-positive rate etc footnote (with \cite[sec.5.1.3]{rieckdiss})
**** WAIT for [[file:~/da/da.org::*daniel%20panchenko%201][daniel panchenko 1]]
*** DANIEL CUMUL
    #+INDEX: CUMUL
    #+INDEX: attack!CUMUL
    CUMUL\cite{panchenko2} is a state-of-the-art\cite{kfingerprint}
    website fingerprinting attack. It sums the number of incoming and
    outgoing bytes, which results in an easily-understandable
    cumulative-size feature. As a reminder, see Figure [[CUMUL_traces]].

    CUMUL keeps a cumulative list of sizes: For each new packet in a
    trace, its byte size is added[fn::\cite{panchenko2} treat incoming
    byte sizes as positive and outgoing as negative values] to the
    last cumulative size, and appended to the list.

    To obtain a fixed size feature vector, values are interpolated
    from the list values to $n$ equidistant points. As of
    \cite{panchenko2}, "$n=100$ yields the best trade-off between
    classification accuracy and computational efficiency."

    This vector is then normalized to $[-1, 1]$. As additional
    features, the total number of both bytes and packets, both
    incoming and outgoing are appended to this list.

    Support vector machines classify these.
*** WRITE Support Vector Machines
**** EVA Introduction
     #+INDEX: Classifier!Support Vector Machine
     #+INDEX: Classifier!SVM
     #+INDEX: Support Vector Machine
     #+INDEX: SVM
     #+INDEX: linear classifier
     #+INDEX: binary classification
     #+INDEX: classification!binary
     Support Vector Machines (short: SVM) are a /linear classifier/:
     they find a linear boundary between points, see Figure
     [[fig:linear_boundary]] for a simple example.

     #+CAPTION[Example binary linear classification]: Example binary linear classification from \cite[Figure 1.5]{iml}.
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:linear_boundary
     [[./pictures/iml_fig1.5.png]]

     (This and the following parts are mostly based on
     \cite[ch.6f]{iml}). Given a set $X = \{x_1, ..., x_n\}$ with a
     dot product $\langle\cdot, \cdot\rangle: X \times X \to \mathbb R$ and tuples $(x_1, y_1),
     ..., (x_m, y_m)$, with $x_i \in X, y_i \in \{-1, 1\}$ as a /binary
     classification/ task.

     The SVM's job is to find a hyperplane[fn::as \cite[ch.4.1]{esl}
     mention, this is actually an affine set, as it need not pass
     through the origin. Keeping with tradition, it will be called
     hyperplane in this thesis (as long as those things formed by
     quarks are still called atoms \ldots).]
     #+BEGIN_LaTeX
       \[\{x \in X \mid \langle w, x \rangle +b = 0\}\]
     #+END_LaTeX
     such that $\langle w, x_i \rangle +b \ge 0$ whenever $y_i = 1$, and $\langle w, x_i \rangle
     +b < 0$ whenever $y_i = -1$. With added normalization, this can
     be compressed to the form \[y_i \cdot (\langle w, xi \rangle +b) \ge 1.\]
**** EVA Soft Margin Classifiers
     #+INDEX: margin
     A support vector machine tries to find a hyperplane between two
     groups of points and maximize its distance to the closest points,
     called /margin/. What happens if the points lie such that a line
     cannot be found, as f.ex. in Figure [[fig:non-linear-data]]?

     #+CAPTION[Example simple non-linearly separable data]: Non-linearly separable data; source: \url{https://en.wikipedia.org/wiki/File:Separability_NO.svg}
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:non-linear-data
     [[./pictures/Separability_NO.eps]]

     To solve this, a /soft-margin classifier/ introduces slack
     variables $\xi \ge 0$, which it tries to reduce while maximizing the
     margin.

     This alters the equations to $y_i( \langle w, xi \rangle +b) \ge 1 - \xi_i$ for the
     optimization problem

     \[\min_{w, b, \xi} 1/2 ||w||^2 + C/m \sum_{i=1..m} \xi_i.\]
**** EVA Multi-Class Strategies
     #+INDEX: binary classification
     #+INDEX: multi-class classification
     #+INDEX: classification!binary
     #+INDEX: classification!multi-class
     The SVMs as described above solve the binary classification
     problem \cite[sec.1.1.3]{iml}: they propose a boundary between
     two classes of objects.

     In website fingerprinting[fn::as in f.ex.\space{}handwriting
     recognition], there are most often more classes than two.

     Several strategies exist to distinguish more than two
     classes. The main are to train one classifier for each class ---
     called /One-Vs-Rest/ (OVR) --- and one for each class-class
     combination --- called /One-Vs-One/ (OVO). One-Vs-Rest trains
     fewer classifiers, while One-Vs-One trains more, but evaluates
     fewer samples per fitting.\cite[sec.4.12.3]{scikit-learn}.
**** WRITE Kernel Trick
     Linear boundaries seem overly limiting. Yet, SVMs can compute the
     boundary not only on the original data, but also on a projected
     space. This allows for complex decision boundaries.

     The above all find a linear boundary between classes. What if
     this does not model reality, as f.ex. example in Figure
     [[hastie_kerneltrick]]? The solution to this is that a SVM can not
     only use the dot product, but any kernel function.

       - instead of dot product $\langle.,.\rangle$ use =kernel= $k(., .)$
       - same effect as mapping each point in set to dot product
         space, and applying $\langle.,.\rangle$ there, $k(x, x') = \langle\Phi(x), \Phi(x')\rangle$
         - but need not compute complete mapping
       #+CAPTION: Kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left side shows linear boundaries on $X$ and $Y$ --- the right side linear boundaries computed with added input data $X^2$, $Y^2$ and $XY$
       #+NAME: hastie_kerneltrick
       [[./pictures/hastie.png]]
     - rbf kernel
     - effect: no longer needs to find a straight line (yes, but in
       transfomed space, so could be others, too)
**** WRITE parameter estimation
    #+CAPTION[Example svm-rbf classification with different parameters for C and \gamma]: Example svm-rbf classification with different parameters for C and \gamma. Source \cite[Figure 42.328]{scikit-learn}
    #+ATTR_LATEX: :float wrap :width 0.4\textwidth :placement {l}{0.42\textwidth}
    [[./pictures/skl-fig-42.328.png]]
     - each soft margin SVM has an error term C which states how
       much to penalize outliers
       - see [[*Soft Margin Classifiers][Soft Margin Classifiers]]
     - rbf kernel also has a gamma term which gives the width of the
       gaussian rbf
       - picture
     - formula rbf kernel
     - pic from scikit-learn Figure 42.328: RBF SVM parameters
***** TODO formula rbf kernel
***** TODO picture first from sklearn, better from rieck
**** TODO link to basic book (bronstein) for vector space etc
**** need more preparation than other classifiers
     - outlier removal
     - scaling
     - parameter estimation
*** WRITE Wang's KNN
    - improved detection
    - many features
    - weighting/learning weights
    - faster
*** WRITE KNN
    - simple
    - often effective
    - how works
      - for point, determine (f.ex.\space{}k=5) closest neighbors by metric
      - majority decision (or only if all agree), put in that group
    - as seen in table in appendix: similar results to extratrees,
      randomforest, decisiontrees
      - one or the other slightly better
*** WRITE Extremely Randomized Trees
    #+INDEX: Classifier!Extra-Trees
    #+INDEX: Classifier!Extremely Randomized Trees
    #+INDEX: Extra-Trees
    #+INDEX: Extremely Randomized Trees
    - more random: (here only classifiction)
      - M trees, independent
      - split training set S into K subsets
        - split by single non-constant, randomly-selected feature
        - return best split
    - reduce variance by randomness
    - reduce bias by several instances M
    - efficiency by basing on decision trees
**** brainstorm                                                     :ARCHIVE:
     - decision trees
       - read up
     - ensemble methods
       - read up?
     - more random: (here only classifiction)
*** brainstorm                                                      :ARCHIVE:
    - induce pattern
      - naive
      - machine learning
      - features
    - kind of traffic analysis
      - without seeing content, deduce information
    - made harder by protocol changes and tor
      - *hope that spdy makes it harder again*
        - ref mike perry
    - no cacheing
    - current tbb (auto-update)
    - scripts etc
    - xpra for slow network connection
    - xvfb for local display
    - marionette
    - others, other tools
    - bit on tor in [[Hurdles]]
      - also that use tor to avoid wf, then tracked again (if it works)
      - maybe also on ta
    - dyer: most important are the features, similar results for naive
    - on feature importances:
      - panchenko
      - k-forest
      - wang implicit
    - not mentioned/omitted in related work?
      - schneier
      - liberatore
      - microsoft hintz-successor(?)
      - bissias
      - wright
    - classifiers
      - features important or also classifier
      - no classifier fits all
      - maps features to classes (or probabilities)
      - classifier
*** WRITE k-fingerprinting
    - similar accuracy to wang-knn and CUMUL
    - about 150 different features evaluated
    - classifier is random forest
    - different multi-class metric leaf label, knn with hamming
      distance
      - leaf labels, see scikit-learn sec 42.9.9.apply
    - feature importances
    - \cite{kfingerprint}
*** WRITE Related Work
    :PROPERTIES:
    :CUSTOM_ID: wfRelated
    :END:
    - mitchell
    - sklearn
    - herrmann: breakable shown via naive bayes
    - panchenko: svm
    - dyer: most important are the features, similar results for naive
      bayes and svm
    - cheng:?
    - wang: knn
    - dts-approach (?)
    - k-forest: specific classifier based on randomized trees with
      hadamard-distance on leaves
    - panchenko1 and 2: (ovr?) svm

    - schneier
    - liberatore
    - microsoft hintz-successor(?)
    - bissias
    - wright
    - feghhi\cite{DBLP:journals/tifs/FeghhiL16}

   Schneier's books /Applied Cryptography/\cite{applied96}
   and (with Ferguson) /Practical Cryptography/\cite{practical} deal
   briefly with /Traffic analysis/, of which website fingerprinting
   is a subtask. The first mention of applying it against encrypted
   internet communication dates to Wagner and Schneier's analysis of
   the SSL 3.0 protocol\cite{SSL}, and is attributed to Bennet Yee.

   The website fingerprinting attack scenario is already described in
   the original Tor design paper\cite{tor-design}. Previous to
   Panchenko et al.\cite{panchenko}, it was considered "less
   effective"\cite{tor-design} against Tor, due to stream/circuit
   multiplexing and fixed cell sizes.
*** Who Could Attack via WF
    As website fingerprinting requires very litte resources, a specific
    attacker could be a WLAN sniffer, an ISP, up to maybe even a nation
    state.
** WRITE Defenses [3/8]
   :PROPERTIES:
   :CUSTOM_ID: defenses
   :END:
*** WRITE Introduction
    As soon as it became clear that anonymized (web) traffic could be
    analysed to guess the destination, defenses were proposed.

    The general idea is to obfuscate characteristics of the
    data. Defenses progressed from obfuscating specific features
    that were used for fingerprinting, to trying to obfuscate the
    whole of traffic, including features not previously thought of.


    The next sections explode early defense proposals, then address
    defenses tailored to specific attacks, and /general defenses/ that
    hope to defend against all possible attacks. Last are general
    stochastic defenses that add randomness to achieve meaningful
    reduction in accuracy with less bandwith and time overhead than
    deterministic ones.

      - deterministic
        - fixed data rate
        - supersequence if known
    - [ ] traffic analysis assumes crypto is perfect
*** EVA Early Defenses
    As defense against fingerprinting webpages when retrieved via SSL,
    \cite{ssl-traffic-analysis} mainly proposed using
    proxies[fn::as well as HTML- and protocol modification]. Addressing
    weaknesses when using proxies, \cite{hintz02} proposed
    three defenses: (1.) adding noise to traffic, (2.) reducing
    transferred data, and (3.) transferring everything in one connection.

    As for the (2.) approach of reducing transferred data: As the
    sizes and interconnection of HTML and embedded content is what
    makes a webpage easily identifyable, either using a text-only
    non-javascript browser such as Lynx[fn::available at
    \url{http://lynx.invisible-island.net/}], disabling f.ex.\space{}images,
    or reducing cross-site requests via f.ex.\space{}the
    RequestPolicy[fn::RequestPolicy
    \url{https://requestpolicycontinued.github.io/} uses domain-based
    filters, which is considered undesirable in the Tor Browser
    Bundle\cite[sec.2.3]{tor-browser-design-impl}] extension all might
    mitigate the threat of website fingerprinting for those who
    consider this trade-off acceptable. Yet, this reduces usability
    and thus conflicts with one of Tor's design
    goals\cite{tor-design}. It is also mostly deterministic, so that
    an adversary could simply train on modified
    data.

    The (3.) approach of transferring everything in one connection
    --- while valid --- would require modifying the HTTP
    protocol. This would conflict with Tor's design goal of
    deployability\cite{tor-design}.

    Several researchers used the (1.) approach of /adding noise to
    traffic/. Others --- additionally or exclusively --- /alter traffic/
    by reducing the data rate, splitting and merging packets, etc.
*** DANIEL Specific vs General Defenses
    As of \cite{wang2015walkie}, defenses designed against website
    fingerprinting can be divided into specific and general defenses.
    Both types can either add noise, modify existing traffic, or
    combine both.

    The first website fingerprinters considered only packet
    lengths. This made it seem sensible to alter the lengths of
    packets by padding, as evaluated f.ex.\space{}by
    \cite{ssl-traffic-analysis}.

    As more and more features were used to classify the traces,
    different ways of altering the data were evaluated by several
    researchers: several ways of padding (\cite{Liberatore:2006},
    \cite{oakland2012-peekaboo}, \cite{a-systematic},
    \cite{ccs2012-fingerprinting}, \cite{wang2015walkie}), or altering
    traffic sizes to fit another web page's (\cite{morphing09},
    \cite{httpos}).

    Specific defenses alter specific features, mostly single packet
    size.

    To stop the arms race between attacks and defenses - the attacks
    finding new feature combinations to use, the defenses
    obfuscating these - the idea of a /general defense/ was presented
    first by \cite{oakland2012-peekaboo} in the context of website
    fingerprinting. They proposed a traffic-flow
    security\cite[ch.10.3]{applied96} solution: fixed-rate
    transmission of data, modified here to be only for the estimated
    duration of web site retrieval.

    This idea was improved on by \cite{a-systematic} while
    \cite{effective} proposed the (offline) defense of morphing all
    traffic to supersequences of traffic patterns.
*** WRITE Tamaraw
    - fixed-rate sending, other fixed-rate receiving
    - different rate up-/downstream
      aka throttle if more data, send dummies if less
    - evolution of BuFLO\cite{oakland2012-peekaboo}
    - longer sending cloaks end of transmission
    - tunable overhead
*** DANIEL Stochastic Defenses
    Previous general defenses were mostly deterministic. The latest defenses by \cite{wtfpad}
    and \cite{wang2015walkie} both use a stochastic approach to
    generate additional traffic, with the latter\cite{wang2015walkie}
    additionally modifying the browser to send "half-duplex", either
    exclusively sending or exclusively receiving data at the same
    time. The former \cite{wtfpad} adapted the ideas from
    \cite{ShWa-Timing06} to distinguish active and non-active periods,
    with a certain probability of sending dummy packets in each,
    omitting the sending when the browser generated packets itself.
*** WRITE WTF-PAD
    - Juarez\cite{wtfpad}
    - Website Traffic Fingerprinting Protection with Adaptive Defense (WTF-PAD)
    - adapts adaptive padding\cite{ShWa-Timing06}
      - hide from global adversary's correlation attack
    - defense + crawler and modifier
    - packet histogram-based
      - when packet is sent, timer from (one of two) histogram is started
      - if timer is finished without another packet, send dummy request
      - else (if another packet): restart timer with new values from histogram
    - built using Tor's pluggable transport\cite{tor-spec-pt}
      censorship avoidance layer
*** WRITE Walkie-Talkie
    - Wang and Goldberg\cite{wang2015walkie}
    - half-duplex
      - send XOR receive
      - kurose 3.4.1: unidirectional
    - with additional traffic
    - browser modification
    - only detectable metric: number of bursts
      - (and also total data, which is a powerful metric)
*** WRITE Related Work
    - hintz: 3 ways to do it
    - wright: morph
    - luo: also morph (HTTPOS)
    - panchenko decoy (add)
    - padding (sslv2 \to 3)
    - requestpolicy (hintz 2nd way)
    - text-only browsing (hintz 2nd way)
* WRITE Methods (=methods) [54%]
** WRITE Introduction
   - short what could be done better
   - longer: approach / variants
** EVA Motivation [2/2]
*** brainstorm                                                      :ARCHIVE:
   - make wf/ml harder, fudge features
     - problems WTF-PAD: modify all of tor,
       - yet problem is browser traffic
         - and traffic is app-dependent
       - deployability: all/nothing
       - modify firefox codebase, when add-on suffices
       - maybe also efficiency
         - histograms
           - not fitting: no need to hide *that* traffic occurs, just where to,
           - compare to real fingerprints
           - less efficient
       - not tunable, bridge-dependent
     - problems walkie-talkie: also modify all
       - bit slower
       - not preferred method
       - TD: compare to WTF-PAD accuracy/efficiency
     - conversely:
       - add-on: easier to modify/implement/test
         - *easy to use* if not default (currently needs server, but
           others need too, does not need by default)
         - HTTP traffic properties used
         - "general defense": not trying to modify specific settings
   - design
     - different versions
     - different factors
   - aim: selective cover traffic
     - select based on web site
     - and target
     - simultaneous to real traffic
*** DANIEL Introduction
    As emphasized both by \cite{wang2015walkie} and \cite{wtfpad},
    deterministic approaches have the major shortcoming of introducing
    delay into the traffic, which conflicts with Tor's design goal of
    usability\cite{tor-design}, increasing the delay[fn::which already
    hinders adoption\cite{pets2011-defenestrator}] of using Tor for
    browsing the web.[fn::As for the positive side of higher latency,
    see \cite[sec.4.2]{challenges}.]

    This thesis's approach uses properties of web traffic to determine
    when and how much traffic to send, providing this functionality
    through an easy-to-add browser extension, thus keeping the Tor
    Browser code as-is. This approach stands in contrast to both
    \cite{wang2015walkie}, which offers sampling from both uniform and
    normal distributions, and \cite{wtfpad}, which creates
    histogram-based traffic, but works at Tor's cell level, adds
    overhead to non-web-based traffic, and adapts a method that tries
    to do more (hiding from a global adversary), instead of hiding
    which site was browsed to from a local passive observer.[fn::only
    the second of these is included in Tor's design goals, see
    \cite[sec.3.1]{tor-design}]

    Additionally, when this thesis was started, both \cite{wang2015walkie} and
    \cite{wtfpad} had not been published yet.
*** DANIEL Aim: Selective Cover Traffic
    As detailed in section [[#features]], there are key features that
    are hard to cloak except by extra traffic, especially the total
    number of bytes up-/ and downstream.[fn::For Tor, the number of
    bytes is an almost exact multiple of the total number of packets,
    due to fixed data cell sizes].

    If some extra traffic needs to be created, the question is how to
    shape this traffic in order to effectively cloak the fingerprint.

    Additionally, it might be advantageous to be able to adjust the
    data overhead of cover traffic to some user-settable privacy
    level, as a whistleblower might need more anonymity protection
    than Jane Doe who just reads the latest news, possibly censored in
    her country.
** WRITE Design and Implementation (=Implementation) [10/19]
*** DANIEL Introduction
    This thesis' aim is to create cover traffic to hinder website
    fingerprinting[fn::and, coincidentally, correlation attacks]. The
    question is how, and how much traffic to create.

    All of website fingerprinting is an application-layer
    problem[fn::for an introduction to protocol layering etc, see
    f.ex.\space{}\cite[ch.1.7]{DBLP:books/daglib/0001977}]. It exploits HTTP
    characteristics, so a same-level application-layer solution would
    be to generate additional HTTP-shaped traffic to make the
    classifier misclassify.

    When a new webpage is opened by the user, the browser creates a
    sequence of HTTP requests as detailed above (see [[#http]]). As of the
    [[#HTTP traffic model][HTTP traffic model]], embedded elements have a different size
    distribution than the HTML document, but both come from
    (heavy-tailed) lognormal distributions.

    The next sections describe in detail how the add-on distinguishes
    initial traffic from embedded objects, how to create traffic and
    describe different versions of the add-on.
*** DANIEL HTTP traffic model
    :PROPERTIES:
    :CUSTOM_ID: HTTP traffic model
    :END:
    \cite{newtrafficmodel} models web traffic via statistical
    distributions.

    The size of both HTML documents and embedded objects is each
    modeled by truncated lognormal distributions. The number of
    embedded objects is modeled by a truncated gamma function. See
    illustration in Figure \ref{distribution}. They offer further
    parameters to fully model web browsing.

    #+BEGIN_LaTeX
    \begin{figure}[htb]
      \begin{adjustbox}{max width=\textwidth}
        \input{pictures/fig_html_embedded.pgf}
      \end{adjustbox}
      \caption{Distribution of sizes for the HTTP traffic model}
      \label{distribution}
    \end{figure}
    #+END_LaTeX

    These distributions have two drawbacks. Firstly, web traffic has
    evolved since 2007, when the paper was written, as documented for
    total web page size in \cite{web-is-doom}. Secondly, as mentioned
    in \cite{newtrafficmodel}, the number of embedded objects are
    computed per each HTML page, including frames, and possibly
    including redirects. How the number of embedded elements is used
    in this thesis differs from how it should be used, see next
    section.
**** TODO footnote for other web traffic work
***** wang paper (see [[file:~/da/da.org::*web%20studies%20in%20%5B%5Bfile:~/da/git/docs/lit.org::*%255B%255B./cacr2015-09.pdf%255D%255BWang%2520-%2520On%2520Realistically%2520Attacking%2520Tor%2520with%2520Website%2520Fingerprinting%255D%255D%5D%5BWang%20-%20On%20Realistically%20Attacking%20Tor%20with%20Website%20Fingerprinting*%5D%5D%206.2][web studies in Wang - On Realistically Attacking Tor with Website Fingerprinting* 6.2]])
**** TODO redo graph: side-by-side, remove "probablity density of"
     - (Byte) to [Byte]
*** DANIEL How to Distinguish HTML and Embedded Objects
    :PROPERTIES:
    :CUSTOM_ID: distinguish_HTML_embedded
    :END:
    To tune traffic generation, it makes sense to distinguish between
    HTML and other requests. This is usually done via HTTP's
    =content-type= header\cite[sec.14.18]{rfc2068}, yet that is only
    accessible when the content has been received, whereas this
    defense needs to distinguish at the time it is requested.

    World Wide Web URLs increasingly move away from including a file
    type suffix\cite{cooluri}, so that distinguishing HTTP elements at
    request time by just looking at the URL is not recommended[fn::See
    \url{http://stackoverflow.com/questions/34656221/}]. The solution
    in this add-on is to consider the first request to a host as the
    HTML page, while subsequent requests while the page is being
    loaded[fn::until the body's =load=
    event\cite[ch.1.6.5]{dom2-events}] are considered requests for
    embedded objects.

    This accurately distinguishes between start of a page load and the
    loading of its embedded objects, but a drawback is that is does
    not recongnize embedded iframes etc. as HTML.

    Providing an accurate estimate of embedded objects /per web page/
    would be [[#future-work][future work]].
*** DANIEL Why HTTP-shaped Cover Traffic
    HTTP-shaped cover traffic might prove more effective, as this
    would make it harder to separate cover and real traffic. In
    addition, it works at the layer where the problem
    originates[fn::For a treatment of /separation of concerns/,
    \cite[ch.1.7]{DBLP:books/daglib/0001977} is recommended], as it
    mimics the HTTP-specific request-response interaction.


    There are several approaches on how to generate HTTP-shaped
    traffic. The naïve way, using HTTP dummy
    traffic[fn::a.k.a. loading another page in the background
    a.k.a. decoy pages a.k.a. multi-tab browsing], has been evaluated
    several times (\cite{ccs2012-fingerprinting}, \cite{a-systematic},
    \cite{kfingerprint}, \cite{effective}, \cite{panchenko},
    \cite{wtfpad}) and is surprisingly effective for all its
    simplicity, albeit at a high overhead.
*** DANIEL How to Generate Cover Traffic
    The cover add-on defends against website fingerprinting by
    injecting artificial cover traffic into the communication.

    When the user requests a page, be it by clicking on a hyperlink, a
    bookmark, or entering an address via the location bar, the add-on
    always creates additional traffic simultaneous to the first HTTP
    request.

    Several versions, and flavors, were evaluated. The next sections
    describe the main add-on branch with its configuration
    possibilities, the simple version, and an older version with very
    low, but non-adjustable overhead. Version evolution is described
    in appendix [[#versions]].
**** WRITE Add-on Version: Main
***** EVA Introduction
     :PROPERTIES:
     :CUSTOM_ID: addon_main
     :END:
     The main version works with the retrieved page's[fn:kg:known or
     guessed, see [[#cache_size][description in the next subsection]]] HTML size, and
     number of embedded objects, adding to these to reach target
     values.

     At the web page's first HTML request, a /target HTML size/ is
     determined, the HTML size of the current request is subtracted
     from the target size and a request for the remaining bytes is
     sent[fn::or a token amount if too small].

     At the same time, a /target number of embedded requests/ is
     determined. From this and the page's number of embedded
     items[fn:kg], the probability of embedded requests is
     computed. For each request to embedded objects, this is the
     probability that embedded-object-sized dummy data is requested.
***** EVA Choice: Cache (approximate) sizes using Bloom Filters
      :PROPERTIES:
      :CUSTOM_ID: cache_size
      :END:
      #+INDEX: a
      #+INDEX: flavor!a
      #+INDEX: b
      #+INDEX: flavor!b
      A webpage is modeled by its HTML size and its number of embedded
      objects. In a closed world[fn::if the client can only browse to
      a limited number of URLs], it is possible to cache all
      sizes[fn::See appendices [[#find sizes of HTML-documents]] and
      [[#number_embedded]] for how the sizes were determined]
      beforehand. If a size is unknown, random variates from the [[#HTTP traffic model][HTTP
      traffic model]] are used.[fn:: The size of each embedded dummy
      object is always drawn from the HTTP traffic model.]

      Using known sizes is called the /cache/ flavor and denoted by
      =a= in tables etc. In the /nocache/ flavor, denoted by =b=,
      sizes are always guessed.

      To cache sizes, an approximate-size data structure based on
      Bloom filters is used.
****** EVA Bloom Filter
       Bloom filters\cite{Bloom70space/timetrade-offs}[fn::
       \cite{Broder02networkapplications} contains a Bloom filter
       overview with applications] are a stochastic fixed-width data
       structure to test membership in a set. They work similarly to
       hash-coding[fn::see \cite{Bloom70space/timetrade-offs} for
       hash-coding. Shortly: elements are hashed and certain places
       is checked for whether it contains this hash.], yet require
       significantly less space, which they trade for a small /error
       rate/[fn::This error rate is dependent on the size of the Bloom
       filter and the number of inserted elements.]: they work
       flawlessly if an element is inside the set, yet might wrongly
       state that an element is inside, when it is not.
****** EVA Application: Bloom Sort
       Sizes can be saved approximately based on Bloom filters: they
       are sorted into bins based on the target distribution. For each
       bin, a Bloom filter is created. An element is added to this
       filter if its size is inside the bin.

       To approximate the size of an element, all filters are
       checked. If one filter reports containment, its size is
       chosen. If zero report containment, the size is not known; if
       two or more report containment, it is saved wrongly. In the
       latter cases, the default distribution is used.[fn::See [[#bloom-sort]]
       for implementation details.]

       This data structure has the advantage that, even if visited
       page sizes were saved, an adversary could not safely detemine
       that pages were visited due to the Bloom Filter's false
       positive errors.
****** MAYBE Error estimation of Bloom Sort
       - error both ways, and difference bin-size to real size
      - sources of error
        - filter tells that is has element when it has not
      - how does error appear
        - collision: one of several, the other might be true
        - replacement: simulates being another url
      - rates of error
        - "add" the error rates of the filters? (times population density?)
      - error estimation?
        - +: fixed size
        - -: error both ways, and difference bin-size to real size
***** WRITE Choice: how to set cover traffic target values
      #+INDEX: I
      #+INDEX: flavor!I
      #+INDEX: II
      #+INDEX: flavor!II
      Once the /page's values/ are thus determined[fn:kg], there are
      two tactics on how to set /target/ values.

      One tactic (=I=) is to group the webpages by their values into bins and
      to set the bin border as the target value, as all webpages in the
      bin must have a size less than or equal the border. This approach
      approximates that taken by Wang et al. in \cite{effective} with
      the bins being equivalent to the anonymity sets/partitions. As
      the biggest bin does not have a maximum size, its median value is
      chosen.[fn:: The optimal size for the biggest bin is a parameter
      that should be evaluated as well, see [[#future-work][Future Work]].]

      The other tactic (=II=) is to have a single target distribution
      from which values are sampled each time, the [[#HTTP traffic model][HTTP traffic
      model]]. This is also the fallback approach if the web page's
      values are not known.

      - picture of binning approach
    This leads to the following variations:

    1) Bloom binning (I) with known sizes (A)
    2) Bloom binning (I) with random sizes (B)
    3) one target distribution (II) with original size from Bloom (A),
    4) one target distribution (II) with random sizes (B)
**** EVA Add-on Version: Simple
     The main version had become quite complex for a Firefox extension
     with more than 500 lines of code. A simplified\cite[ch.7.2]{xp}
     algorithm triggers a HTML-sized[fn::times overhead] request at
     the beginning, and an embedded-sized request with
     overhead-related probability for each embedded object.

     This halved the number of lines of code[fn::+simplified
     debugging]. The version is called /simple/.
**** WRITE Add-on Version: Retro
     All other add-on versions reached overheads as low as 20%, but not
     lower.

     - very low overhead at one test: negative
     - previous version 0.15.3, added change 0.20 to 0.21: remembers
       negative request sizes, and randomly subtracts them for further
       requests
     - de facto: HTML-sized request for each request (target-size),
       (remembering negative sizes, subtracting them)
     - drawback: not very configurable
**** WRITE Why as an add-on
     Web browsing is the (main) target of website
     fingerprinting. Instead of burdening all of Tor with extra bells
     and whistles, this solves this deanonymization problem at the
     application layer, where its origins are. (Separation of
     Concerns)
*** WRITE Server
    - where there are several possibilities how to generate cover
      traffic,
    - here: simplest: server, GET-query with size=bytes parameter
      returns this many bytes random data
*** brainstorm                                                      :ARCHIVE:
    - firefox browser extension / add-on
      - add-on sdk
      - maybe mention next generation
    - good code
      - tests
        - unit tests
        - by hand
      - good parts
      - js garden
      - style guide
      - version control
    - algorithm
    - implementation
      - classes
    - server
      - later: .onion (link to related work)
    - http traffic distribution
*** WRITE Related Work?
    - Bloom paper
    - network applications
*** WRITE integrate into above
    This implementation currently omits some possibly embedded
    elements, f.ex.\space{}those embedded in css files and =style= tags via
    the =@url= css-directive. It seems better for cover traffic to
    slightly underestimate the number of embedded elements. This
    might generate more traffic than strictly necessary, but here,
    safe seems better than sorry.
* KEYWORDS Results and Evaluation [21%]
** EVA Introduction
   The following sections describe [[#setup][how data was collected and
   analysed]], [[#eval-wf][evaluate some website fingerprinting approaches]], and
   [[#eval-defense][various defenses]].
** WRITE Setup
   :PROPERTIES:
   :CUSTOM_ID: setup
   :END:
*** EVA Introduction
    The following sections describe [[#sites][which]] and [[#capture][how]] data was captured,
    [[#feature-extraction][how classification input was extracted from the raw data]], and
    [[*Classification][how this input was assigned to
    classes/websites]].
*** WRITE Sites
    :PROPERTIES:
    :CUSTOM_ID: sites
    :END:
**** EVA Introduction
     The following subsections describe [[#top100][the 100 sites selected for
     testing]], how a [[#30sites][subset of these was chosen]], and the
     [[#sites-bg][background-data]] sites.
**** WRITE Modified Top-100
     :PROPERTIES:
     :CUSTOM_ID: top100
     :END:
     Up to \cite{panchenko2} and \cite{kfingerprint}[fn::which were
     both published before this thesis was started], most studies used
     100 different web pages to test website fingerprinting
     classifiers.

     Here, the URLs for retrieval come from the Alexa top million
     sites list[fn:: Current version available at
     \url{http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}],
     retrieved on September 30th, 2015. As in
     \cite{wpes13-fingerprinting}, similar sites were removed. Also
     removed were those sites which failed to respond to python's
     =urllib=[fn::\cite[sec.20.5]{python-lib-ref}].

     The Marionette framework (see Appendix [[#Marionette]]) detects
     retrieval errors. Evaluation was run with erroneous captures both
     included and excluded. Sites with more than 25% erroneous
     captures are removed from the list, too.[fn::These include mostly Chinese
     sites: \url{cntv.cn}, \url{tmall.com}, \url{baidu.com},
     \url{china.com}, \url{tianya.cn}, \url{alipay.com},
     \url{163.com}, \url{taobao.com}, \url{xinhuanet.com},
     \url{soso.com}, \url{sina.com.cn}, \url{qq.com}, \url{sohu.com},
     \url{weibo.com}, \url{jd.com}, and \url{sogou.com}. The site
     \url{hao123.com} seemed to yield some errors, but less than the
     ratio, so it was retained.]

     As users would not retrieve sites over Tor when retrieval
     resulted in mostly errors, striking these sites seemed
     reasonable.

     Both \cite{ccs2012-fingerprinting} and
     \cite{wpes13-fingerprinting} show that accuracy on a top-100 list
     mirrors accuracy on 100 randomly-chosen sites.

     The list of sites with their Alexa index can be found in appendix
     [[#top-100]]. Due to the these multiple drawbacks, the 100-site list
     used by \cite{effective} was also used as an
     alternative.[fn::available at
     \url{https://crysp.uwaterloo.ca/software/webfingerprint/knnsitelist.txt}]
**** DANIEL 30 sites
     :PROPERTIES:
     :CUSTOM_ID: 30sites
     :END:
     Retrieval of the whole 100 sites to test add-on performance proved
     lengthy, a smaller set of sites was needed. The first attempts
     used the top-10 of these sites, but those differed from all sites
     distributions, f.ex.\space{}in size, as mentioned in \cite{web-is-doom}.

     To solve this, the top 100 sites's traces[fn::both for no defense
     and with add-on version 18.2 (see [[*0.18: configurability][0.18: configurability]])] were
     ordered by incoming byte size, and percentiles were
     chosen.[fn::the method to do this is =top30()= in [[#analyse][analyse.py]], the
     traces were from add-on version 0.18.2]. Due to them being well
     apart, these sites should be easy to classify, and thus harder to
     keep apart via defenses.
**** DANIEL open-world
     :PROPERTIES:
     :CUSTOM_ID: sites-bg
     :END:
     For open-world evaluation, sites from the same top-1m data were
     used, starting after the last of the modified top-100 pages. The
     next 4100 page's traces were collected, one trace per page.

     \cite{a-systematic} offers a model to compute open-world results
     from closed-world. It seemed more direct to capture background
     traces.
*** DANIEL Capture
    :PROPERTIES:
    :CUSTOM_ID: capture
    :END:
    #+INDEX: Bridge
    #+INDEX: Tor!Bridge
    #+CAPTION: Setup to capture web page traffic: Tor Browser on /Client/ machine, connects to Tor server on /Bridge/ machine, connects to Tor network, connects to web servers
    #+ATTR_LATEX: :float nil :width 0.5\textwidth
    [[./pictures/Setup.eps]]

    For the first captures, a single virtual machine was
    available. This had the main drawback that WTF-PAD used very
    little padding.[fn::see [[#eval-wtf-pad][sota (practical): WTF-PAD]] for details]

    Thus, a Tor bridge was introduced into the traffic
    flow[fn::Bridges relay Tor traffic. They act as a gateway into the
    network. Their main use is censorship avoidance\cite{tor2014}]. It
    is required for WTF-PAD anyways, but other Tor traffic also uses
    the bridge instance via Tor's =Bridge=
    directive\cite[sec.client~options]{tor-manual}, to ease
    comparability.

    One host runs the Tor Browser Bundle and the cover traffic server
    (if needed), the other runs a tor server instance in bridge
    mode. For WTF-PAD, an additional server transport program is run
    at the bridge, and a client transport at the client[fn::WTF-PAD
    did not run via the built-in =ServerTransportPlugin= and
    =ClientTransportPlugin= directives].

    This setup utilises the same bridge for WTF-PAD and the browser
    extension.

    Single traces are captured via the Python script ~one_site.py~, see
    appendix [[#1site_desc]].
**** WAIT github config files upload + mention
*** WRITE Feature Extraction
**** DANIEL Introduction
     :PROPERTIES:
     :CUSTOM_ID: feature-extraction
     :END:
    The aim of preprocessing is to extract features relevant for
    machine learning from the original trace files, which are in
    =pcap= format. From these, a vector of characteristics, called
    /features/ [fn::They are called /features/ and /attributes/ in
    machine learning; statistics calls them /independent variables/ or
    /predictors/, see\cite[sec.2.1]{esl}] is created.

    Of the several tools available for reading =pcap=-trace files,
    =tshark= was used in this thesis, as it offers TLS packet
    reassembly. It is the command-line version of the Wireshark
    protocol analyzer[fn:: \url{http://www.wireshark.org}].

    The program created to distill a feature vector from a trace is
    called [[#counter][=counter.py=]]. It creates both CUMUL\cite{panchenko2} and
    Version\nbsp1\cite{panchenko} feature vectors.
**** DANIEL split
     :PROPERTIES:
     :CUSTOM_ID: split
     :END:
     As mentioned by \cite{arp-personal}, outliers can be removed
     based on statistical class features[fn::\cite{panchenko2} used
     incoming-byte-count median and quantiles] on the training data
     only. On test data, outliers should be removed only based on
     absolute size, as the adversary does not know the trace's class
     beforehand. Splitting off the validation set thus happens before
     outlier removal.

     The data set is split twice. Once for later validation of
     results[fn::code at =tts()= in analyse.py], once implicitly
     during SVM parameter estimation, which is done via five-fold
     cross-validation.[fn::code at =_my_grid()= in analyse.py]
**** CHECK outlier removal
     :PROPERTIES:
     :CUSTOM_ID: outlier_removal
     :END:
     As of \cite{panchenko2}, CUMUL's accuracy is increased
     by outlier removal. Their
     software[fn::\url{http://lorre.uni.lu/~andriy/zwiebelfreunde/},
     see =features/feature-scripts/outlier-removal.py=] has 3--4
     different settings: a minimal --- which just removes very small
     traces ---, a median-based[fn::in addition to the median-based
     original by Wang and Goldberg\cite{wpes13-fingerprinting}], and a
     25%/75%-quantile-based approach, each adding a step to the
     previous. \cite{panchenko2} only mentions the quantile-based
     filtering, so that was mainly also used in this thesis.

     An alternative saves the highest and lowest values of previous
     outlier removal on training data and applies it to the test
     data. It did not improve on the minimal setting, see
     [[#wf-outlier-removal]].
**** WRITE scaling
     - recommended by \cite{sarle2} and \cite{Hsu10apractical}
     - same scale factor for test and train set
     - uses \cite[User Guide, sec. 42.30.7]{scikit-learn}'s =MinMaxScaler=
**** WAIT for [[file:~/Desktop/main.org::*daniel:%20merkmalsraum%20-%20musterraum%20etc%20in%20englisch?][daniel: merkmalsraum - musterraum etc in englisch?]]
*** WRITE Classification
    :PROPERTIES:
    :CUSTOM_ID: classification
    :END:
**** WRITE Introduction
     - feature vectors extracted
     - outlier removal
     - classification
**** WRITE Original Classification Methods: Panchenko and Wang's
     As the main purpose of this thesis is defending against website
     fingerprinting, it helps to measure the success against
     state-of-the-art website fingerprinting methods. At first, a
     re-implementation of \cite{panchenko2} was used for
     classification. As
     #+BEGIN_QUOTE
 Optimism is an occupational hazard of programming. Feedback is the
 treatment.\cite{xp}
     #+END_QUOTE
     this method is validated with the original implementation used by
     both \cite{panchenko2} and \cite{effective}.

     Additionally, the reimplementation has some aspects lacking in
     \cite{panchenko2}, namely those mentioned in [[#split][split]] and the
     prototype in [[#outlier_removal][outlier removal]].
 
         - and has some optimizations
           - several classifiers: extra-trees
     - wang: data sets until study (2014)
       - web site at \url{https://cs.uwaterloo.ca/~t55wang/wf.html}
         - attack code at \url{https://cs.uwaterloo.ca/~t55wang/knn.zip}
         - attack data at \url{https://cs.uwaterloo.ca/~t55wang/knndata.zip}
       - data format: one file per trace, named =class_id-trace_num=
         - lines are time(float) \t packet length/cell
         - [ *positive out, neg in* ]
       - feature extraction in =fextractor.py=
       - seems to compare different URLs whether they match: data
         format contains different classes
     - panchenko: 2016 data set plus code
       - data format: one file per class, named by url
         - *positive in, neg out*
         - line is trace, =class start_time packet_time:packet_size ...=
           - times are milliseconds since epoch
       - seems to compare whether sites are in the matched set or
         outside: data contains only two classes
**** DANIEL CUMUL Re-Implementation
    After outlier removal, splits and possibly scaling, classification
    takes as input feature vectors and tries to output the class that
    this input belongs to. This is triggered by [[#analyse][=analyse.py=]].

    Support Vector Machines need additional meta-parameters as
    input. These are determined in an additional parameter estimation
    step[fn::while it would be possible to also determine these
    meta-parameters together with the support vectors, it would pose a
    compationally hard problem, as mentioned for a similar case by
    \cite[sec.2.8.3]{esl}]. \cite{Hsu10apractical} recommend
    grid-search, which is used by \cite{panchenko2} and
    \cite{panchenko}. This happens on the training set.

    After SVM-parameters have been determined, several classifiers are
    trained on the training data and evaluated on the test data.
** WRITE Website Fingerprinting [0/13]
*** WRITE Introduction
    :PROPERTIES:
    :CUSTOM_ID: eval-wf
    :END:
    - various approaches
      - also seemingly different ow-accuracy
    - aim: validate code
    - and show lower bound for perfect attacker
    - panchenko 1
      - diff classf
    - cumul
      - my and panchenko's impl
    - wang
      - his impl, slightly modified for different number of classes/instances
        - =flearn_params.sh=
    - drawbacks
      - size changes
*** WAIT Panchenko 1
    - original wf attack
    - HTTP/HTML-specific features
    - mostly fixed-size, with Number and Size Markers variable-sized
      - lead to longer feature vectors than CUMUL'default
      - longer time
**** WAIT for [[file:~/da/da.org::*klassifikation][klassifikation {0/10}]] panchenko 1 without scaling on best
*** WAIT Panchenko 1 with Different Classifiers
    :PROPERTIES:
    :CUSTOM_ID: different-classifiers
    :END:
    Panchenko et al's first attack's\cite{panchenko} length of
    features grow with the size of the packet trace, as opposed to
    CUMUL's\cite{panchenko2}. As a result, it often has more features
    than CUMUL's default. As the SVM classifier expects each feature
    vector to have the same length, all vectors need to be padded to
    the length of the longest vector. As evaluation time scales with
    feature length, this increases classification time. In our
    experiments, SVM evaluation takes longer.

    Other classifiers evaluated included the K-Nearest-Neighbors
    classifier. These provided quicker results, which often exceeded
    the SVM's accuracy in our tests.

    In addition, CUMUL improved the classification
    result[fn::accuracy and FPR/TPR respectively].

    In our various tests with the first attack many classifiers,
    notably k-nearest-neighbors, perform better than the SVM
    classifier on the same set of features. Additionally, these are
    easier to apply, as they do not need to scale data.

 #+CAPTION: Timing comparison for various classifiers
 #+NAME: tab:timing
 #+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|
 #+INCLUDE: "data/results/alternatives.org::#timing" :only-contents t


    - [ ] [[file:~/da/da.org::*klassifikation][klassifikation {0/12}]]
      - how much better alt(knn) than svc
    - much easier to just use knn, ....
      - and slightly better results
      - knn is best, outperforms svc on our tests
    - more work for svm parameter estimation
    - pad variable-length features to maximum length of all with 0s
**** WAIT retest p1 without scale, see "panchenko 1 without scaling on best" in [[file:~/da/da.org::*klassifikation][klassifikation {0/10}]]
*** WRITE [#A] Panchenko 1 vs Cumul
    Panchenko et al proposed two \cite{panchenko} methods
    \cite{panchenko2} for analysing traces, one of which is
    considered\cite{kfingerprint} state-of-the art.

    They both use support vector machines as classifier, but differ in
    the features they select.

    Since CUMUL\cite{panchenko2} is Panchenko et al.'s newer approach
    after their first classifier\cite{panchenko} (called /Panchenko 1/
    from now on), better accuracy of the former is to expected. As
    seen in Table [[1vsCUMUL]], there is both a decrease in running time
    and an increase in accuracy for CUMUL compared to
    Panchenko 1. Still, panchenko 1 was the first method to sucessfully
    classify traces recorded via Tor.

    #+CAPTION: CUMUL and panchenko 1 accuracy and timing per classifier. Data is 30 sites, as described in section [[#30sites]].
    #+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}||n{1}{4}|n{1}{4}||n{1}{4}|n{1}{4}|n{3}{4}|
    #+NAME: 1vsCUMUL
|------+---------+---------+---------+---------+---------+---------+---------|
|      | \open ET \close  |         | \open KNN\close  |         | \open SVM\close  |         |         |
|      | \open accuracy \close | \open time\close | \open accuracy\close | \open time\close | \open accuracy\close | \open param search \close | \open time\close |
| <4>  | <7>     |     <7> | <7>     |     <7> | <7>     |     <7> |     <7> |
|------+---------+---------+---------+---------+---------+---------+---------|
| Panchenko 1 | 0.60121239709 | 4.73188591003 | 0.749585484366 | 0.933593034744 | 0.087291644353 | 1.85943317413 | 117.314565897 |
| CUMUL | 0.765525547631 | 1.89891982079 | 0.651070670389 | 0.411082983017 | 0.866125778825 | 1.08589506149 | 0.822636127472 |
|------+---------+---------+---------+---------+---------+---------+---------|

    - best parameters sometimes outside of panchenko's range
    - state of the art:
      - wang-knn
        - knn with parameter weighting step
        - first to 92% accuracy (current limit)
      - cumul:
        - faster
        - easier to see
        - bigger dataset
      - k-fingerprinting
        - accuracy
      - all similar accuracies (as of k-fingerprinting)
    - [ ] experiment
      - how much better CUMUL than v1
        - on disabled
        - accuracy, overhead, time
    - most important feature 3.3%, see [[file:data/results/alternatives.org::*feature%20importances%20et][feature importances et]]
      - outgoing packet count + sum bytes each 3%, together 6.5%
*** WRITE Trace Growth, Time Difference, Effect
    :PROPERTIES:
    :CUSTOM_ID: time
    :END:
    The trace sizes changed significantly over time, see Table
    [[tab:time]]. What at first seemed like a size increase was reduced by
    the last measurement to mere fluctuations of up to 25%. As seen in Table
    [[tab:site_sizes_relative]], this fluctuation is highly dependent on
    the url. \url{google.com}, f.ex.\space{}shows two distinct sizes, which
    could be related to whether doodles were active at these times.

#+CAPTION[Overhead and accuracy for training 07-06, testing on various times]: Overhead and accuracy at different times, training 07-06, testing on listed times, sorted by date.
#+NAME: tab:time
#+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{3}{2}|
#+INCLUDE: "data/results/alternatives.org::#tab:time" :only-contents t
    - accuracy decrease: much for svc, little for et
    - noticeable trace growth, see Table [[tab:sizes]]
#+CAPTION: Overall growth of trace sizes [byte] by date.
#+NAME: tab:sizes
#+ATTR_LATEX: :align |r||n{7}{4}|n{1}{4}|
#+INCLUDE: "data/results/alternatives.org::#size_all" :only-contents t
    - some grew a lot, some not at all, see Table [[tab:site_sizes_relative]]
#+CAPTION[Relative growth of trace sizes by date by domain]: Relative growth of trace (byte) sizes by date by domain.
#+NAME: tab:site_sizes_relative
#+ATTR_LATEX: :align |r||n{3}{3}|n{3}{3}|n{3}{3}|n{3}{3}|n{3}{3}|
#+INCLUDE: "data/results/alternatives.org::#size_rel" :only-contents t
       - google,
       - cloudflare captcha test

       - summary: compare to normal in time
         - mostly for more exact overhead
         - also for more exact accuracy
         - due to time difference accuracy lag (and some time to
           capture), all defenses compared to closest-in-time
           no-defense retrieval
       - makes it hard to evaluate overhead
**** TODO [#C] google doodle
**** TODO after removal of erroneous captures
     - due to retrieval errors?
*** WAIT CUMUL: SVM vs Extra-Trees
    :PROPERTIES:
    :CUSTOM_ID: svm_vs_et
    :END:
    wait for [[file:~/da/da.org::*panchenko's%20approach%20(10fold)%20for%20all%20defenses][panchenko's approach (10fold) for all defenses]]
    - svc very good on unadd-oned data, (if trained and or on all)
      - extratrees with 5--10% lower accuracy, but less difference
        against defenses (?)
        - similar results, but mostly bit less good: knn,
          randomforest, even decisiontrees
      - others not tested
      - exception 22.0/5aII: better for svc
        - continue here: inspect which pages, etc, show traces, table
    - different classifiers: best are svc for both, knn for 1, et for 2
      - re defenses: svc tends to overfit, yielding better results on
        regular traces, but decaying fast on data collected
        - later
        - with add-on
*** WRITE Which Features Help Most in Classification
    #+CAPTION: Importance of CUMUL features
    #+ATTR_LATEX: :width 0.4\textwidth
    [[./pictures/feature_importances.eps]]


    - include somehow [[file:data/results/alternatives.org::*feature%20importances%20ET][feature importances ET]]
    - mention that ET classifies differently from SVM
      - but easily accessible
      - compare
        - general: [[#svm_vs_et]]
        - specific: [[#websites][Websites]]
    - all 100 CUMUL-values in the 1% \pm 0.5%
      - increasing with the first CUMUL-element at 0.5%, the last at 1.5%
    - both outgoing byte and outgoing packet count at 3%
    - graph
      - data from feature_importances
      - xlabel: cumul feature number
      - ylabel: feature contribution [in %]
      - line graph
      - caption: first four are
        - inCount
        - outCount
        - outSize
        - inSize
*** WRITE outlier removal
    :PROPERTIES:
    :CUSTOM_ID: wf-outlier-removal
    :END:
     As seen in Table [[tab:outlier_removal]], a higher level of outlier
     removal on test data increases accuracy[fn:: The results are
     similar for the Extra-Trees classifier, see appendix [[*Outlier Removal Results with Extra-Trees Classifier][Outlier
     Removal Results with Extra-Trees Classifier]].], but as of section
     [[#split]] this is not applicable in a real-world scenario.

#+CAPTION: Accuracy with SVM classifier after outlier removal. The ~both~ column shows classification accuracy if the outliers are removed from both test and training set at the same time.
#+NAME: tab:outlier_removal
#+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|
#+INCLUDE: "data/results/alternatives.org::#outlier_removal" :only-contents t

     This outlier removal removed on average
       - how many removed for levels
         - both panchenko's own and mine
       - evaluation of outlier removal steps
       - results
         - rw
           - this might be why results are worse than f.ex.\space{}panchenko
           - includes variant labeled -1: remember highest and lowest
             quantile values from previous train-or, removes based on
             these
       - link to file
     problem with previous code: did not label download problems
     - some sites had mostly problems (cat.kr f.ex.)
     - outlier removal only last resort, better to remove when
       retrieval turned out faulty
     - link to [[file:data/results/alternatives.org::*sizes%20TOP-0721][sizes TOP-0721]]

     denoted as =-1=. As seen in the table, it does not show any
     accuracy gain.
**** on average/per class numbers how many removed
**** WRITE disadvantage of few instances with much outlier removal
    - problem for level 3 (only): number of splits requires more data
      than just 40 instances
      - (train-test)/validation set\cite[sec.2.2.3]{iml} 2/3 to 1/3,
        - pathological example: microsoftonline.com
          - train-test split yielded 20-20
          - removed 17 from 20 in level-3 split
          - level 2 removed 0 from 20
          - pics: all, train==lvl1==lvl2, lvl3
        - happened only in level-3-or which was not used (and gave
          little advantage, see OR), so little need to recapture
          traces
#+BEGIN_latex
  \begin{figure}[htp]
  \label{or-levels}
\includegraphics[width=0.32\textwidth]{./pictures/or/all.eps}
\includegraphics[width=0.32\textwidth]{./pictures/or/train.eps}
\includegraphics[width=0.32\textwidth]{./pictures/or/level3.eps}
  \end{figure}
#+END_latex
        - traces available in =data/path= directory
        - possible solution: validation set bigger
          - \cite{kfingerprint} captured 70 instances each
          - \cite{effective} used 90: 60 for train, 30 for test
        - this was before labelling erroneous traces
** KEYWORDS Evaluation of Defenses [1/12]
*** KEYWORDS Introduction
    :PROPERTIES:
    :CUSTOM_ID: eval-defense
    :END:
    - eval defenses
    - addon
      - main
        - flavors
      - simple
      - retro
    - wtf-pad
    - factor to overhead evaluation
    - overhead
    - look at cumul-traces (?include this?)
    - summary
*** KEYWORDS [#A] Add-on
    #+CAPTION: Different defense versions with CUMUL, SVM classifier
    #+ATTR_LATEX: :float nil :width \textwidth
    [[./pictures/svc_oh_vs_acc.eps]]

    #+CAPTION: Different defense versions with CUMUL, Extra-Trees classifier
    #+ATTR_LATEX: :float nil :width \textwidth
    [[./pictures/extratrees_oh_vs_acc.eps]]
    - problem: tunable: factor correlation to overhead
      - not given for 0.15, retro, 0.18-0.21
      - old graph: cluster for retro, etc
      - given in 0.22? (td: tests)
      - graph: accuracy vs overhead
        - mention (somewhere else that tamaraw fell through)
    - problem: bursts
      - at simple
      - at normal
    - factor at simple
      - 0.18 over-engineered?
    - optimal defenses: 22 and simple2@10
      - td: battle those
      - td: pix vs disabled
    - different factors
    - all around same curve
    - which classifier classifies which page well?
    - 5aII higher accuracy at svc than at et
      - mention still, still accurate, or obsolete, because svm increased
*** DANIEL Add-on Flavors
    As described in [[#addon_main][Add-on Version: Main]], there are different flavors
    of the main version. These were evaluated with overhead factors of
    5% and 20%. See Tables [[tab:flavors0829]] and [[tab:flavors0909]]
    respectively.

#+CAPTION[Classification accuracy and overhead for 5%-flavor of the main addon]: Classification accuracy and overhead for flavors of the main addon, at 5% overhead factor.
#+NAME: tab:flavors0829
#+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{3}{1}|
#+INCLUDE: "data/results/alternatives.org::#flavors-08-29" :only-contents t


#+CAPTION[Classification accuracy and overhead for 20%-flavor of the main addon]: Classification accuracy and overhead for flavors of the main addon, at 20% overhead factor.
#+NAME: tab:flavors0909
#+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{1}{3}|n{3}{1}|
#+INCLUDE: "data/results/alternatives.org::#flavors-09-09" :only-contents t

    The results point to major accuracy reduction for all variants,
    with almost equal accuracy for all variants with SVM-classifier,
    but slightly overhead-related accuracy for the Extra-Trees
    classifier. In both experiments, the =aI= flavor performed at the
    top or almost, with the =aII= flavor producing slightly lower
    overhead for comparable accuracy in the low-overhead setting.

    The size-guessing =b= flavors produce significantly
    higher overhead, but yield a further small reduction of classifier
    accuracy.
*** KEYWORDS Add-on overhead
*** KEYWORDS [#A] sota (practical): WTF-PAD
    :PROPERTIES:
    :CUSTOM_ID: eval-wtf-pad
    :END:
    - overhead of WTF-PAD depends on network connection from client to bridge
      - 0--1% if on localhost
      - ~12.9% for virtual machine
      - reported by Juarez et al \cite{wtfpad}, confirmed by Danezis
        \cite{kfingerprint}: 54%
    - all of mine so far add additional data /for each request/, WTF-PAD
      adds additional data /over time/, less with more requests, more
       with less
    - graph
      - disabled vs WTF-PAD
        - on each page: how much correctly classified?
        - google.com
    - reduction
      - et: 79% to 75% accuracy
      - svc: 87% to 41% accuracy

- delay of some possible (f.ex.\space{}images)
  - also pre-cache some as cover traffic
- different target distributions
- multiple distributions
- optionally no cooperator necessary
    dummy packets chosen as response to real request (as in web traffic)
- add evaluation values
- end of page load easily detectable
  - wfpad can see end of each transmission
- similarities:
  - also has app_hint
  - no delay
- currently uses exit nodes
- this has no gap traffic, aims less at global adversary, more at ISP
*** KEYWORDS Does This Hide Bursts?
    - meta-bursts as described in walkie-talkie
    - are those hidden, too, or can the number of bursts be found out
    - easy to implement, maybe do this
    - see that higher overhead per protection
    - increased overhead while only slightly increasing protection
      - show in table, where?
**** maybe see cumul-graphics
*** KEYWORDS Trace Comparison Three Main Defenses
#+BEGIN_LaTeX
\begin{table}[H]
\begin{longtable}{c c c}
   Page: google.com & Page: tumblr.com & Page: netflix.com \\
\endfirsthead
   Page: google.com & Page: tumblr.com & Page: netflix.com \\
\endhead
   \hline
   \multicolumn{3}{c}{WTF-PAD} \\
  \includegraphics[width=0.3 \textwidth]{./pictures/google.com__wfpad.eps}
  & \includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__wfpad.eps}
  & \includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__wfpad.eps}
  \\
   \multicolumn{3}{c}{Add-on Version Simple.1, Factor 10\%} \\
\includegraphics[width=0.3 \textwidth]{./pictures/google.com__simple1@10.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__simple1@10.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__simple1@10.eps}
\\
   \multicolumn{3}{c}{Add-on Version 0.22, Factor 10\%} \\
\includegraphics[width=0.3 \textwidth]{./pictures/google.com__22.0@10aI.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__22.0@10aI.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__22.0@10aI.eps}
\end{longtable}
\caption{CUMUL traces of different defenses}
\end{table}
#+END_LaTeX
*** KEYWORDS sota (theoretical): Walkie-Talkie
    - as of paper: 32%bw \to 5% fpr and 55% bw \to 10% fpr
    - how to translate to closed-world?
    - wait for [[file:~/da/da.org::*open-world?%20(vs%20erst%20mal%20fertig?)][open-world? (vs erst mal fertig?)]]
*** KEYWORDS (maybe) vs Optimal Attacker
    - show just traces of single html retrieval:
      - small page, small page with add-on, bigger page
      - does with add-on look like bigger page?
    - wait for [[file:~/da/da.org::*experimente][experimente]] plan 3
*** WRITE Websites
    :PROPERTIES:
    :CUSTOM_ID: websites
    :END:
    Some websites classified better by the Extra-Trees classifier,
    some with Support Vector Machines.

    - which websites classify well with which classifier, which badly
    - results in Tables [[tab:class_accuracy-07-06]],
      [[tab:class_accuracy-simple]], [[tab:class_accuracy-main]], and
      [[tab:class_accuracy-09-18]].
 #+CAPTION: Classification accuracy of wfpad on 30 classes, 2 classifiers.
 #+NAME: tab:class_accuracy-07-06
 #+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}|
 #+INCLUDE: "data/results/alternatives.org::#class-accuracy-07-06" :only-contents t

 #+CAPTION: Classification accuracy of "simple" addon on 30 classes, 2 classifiers.
 #+NAME: tab:class_accuracy-simple
 #+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}|
 #+INCLUDE: "data/results/alternatives.org::#class-accuracy-simple" :only-contents t

 #+CAPTION: Classification accuracy of the main addon version 0.22 on 30 classes, 2 classifiers.
 #+NAME: tab:class_accuracy-main
 #+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}|
 #+INCLUDE: "data/results/alternatives.org::#class-accuracy-main" :only-contents t

 #+CAPTION: Classification accuracy of "retrofixed" addon on 30 classes, 2 classifiers.
 #+NAME: tab:class_accuracy-09-18
 #+ATTR_LATEX: :align |r||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}||n{1}{3}|n{1}{3}|
 #+INCLUDE: "data/results/alternatives.org::#class-accuracy-09-18" :only-contents t

 # maybe later replace top tex-line in above table with =defense & \multicolumn{2}{l}{disabled} & \multicolumn{2}{l}{wfpad} &  \multicolumn{2}{l}{simple2/5} & \multicolumn{2}{l}{0.22/5aI} \\=

 - CUMUL-traces for buzzfeed.com (svc fails) and weibo.com (svc wins)

 #+BEGIN_LaTeX
 \begin{table}[H]
 \begin{longtable}{c c c}
    WTF-PAD & Simple Addon & Addon 0.22 \\
 \endfirsthead
    WTF-PAD & Simple Addon & Addon 0.22 \\
 \endhead
    \hline
    \multicolumn{3}{c}{buzzfeed.com} \\
 \includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__wfpad.eps}
 & \includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__simple2@5.eps}
 & \includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__0.22@5aI.eps}
 \\
    \multicolumn{3}{c}{weibo.com} \\
 \includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__wfpad.eps}
 & \includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__simple2@5.eps}
 & \includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__0.22@5aI.eps}
 \end{longtable}
 \caption{CUMUL traces of defenses with different classifier results}
 \end{table}
 #+END_LaTeX

   - similar, little to see
   - no-addon classifies equally well for both classifiers
   - all: \ge 0.15 accuracy difference
     - next: 0.25 diff
   - wfpad
     - apple 0.925 with et, 0.675 with svm
     - go.com: et 0.875, svm 0.725
     - yahoo.com: et 0.75, svm 0.575
     - youtube: et 0.575, svm 0.425
     - 
     - ettoday: 0.825 et, 1.00 svm
     - microsoftonline.com: et 0.6, svm 0.775
     - pornhub: et 0.6, svc 0.975
     - xvideos classifies 0.7 with et, 0.975 with svm
   - simple
     - sina.com.cn 0.95 et, 0.55 svm
     - 
     - apple et 0.5250, svm 0.9250
     - buzzfeed et 0.475, svm 0.8
     - ettoday.com et 0.2821, svm 0.5385
     - facebook.com et 0.65, svm 0.9
     - msn.com 0.35 et, 0.75 svm
     - xnxx.com et 0.5750, svm 0.8250
   - 5aI
     - sina.com.cn et: 0.953846153846, svm: 0.276923076923
     - 
     - buzzfeed.com et: 0.230769230769, svm: 0.507692307692
     - youtube.com et: 0.234375, svm: 0.6878
     - xvideos.com et: 0.0923076923077, svm: 0.430769230769
     - blogspot.com  et: 0.0153846153846 , svm: 0.215384615385
     - apple.com     et:  0.123076923077 , svm: 0.430769230769
     - xnxx.com      et: 0.0769230769231 , svm: 0.369230769231
     - yahoo.com     et:  0.538461538462 , svm: 0.753846153846
     - wordpress.com et:          0.0625 , svm:       0.296875
     - soso.com      et:         0.15625 , svm:       0.390625

   - also depending on defense: apple.com with wfpad better detected by
     et, with simple (and main) better detected by svm
**** WAIT which retro better, use just that, then addto list
**** google.com
      - check that not a robot (not captcha page with either addon or without)
      - td: estimate probability if matches traces
      - maybe see what it gets classified as
**** aliexpress.com
      - https of akamai
      - td: check with recapture both
*** KEYWORDS relation FACTOR to overhead
** brainstorm                                                       :ARCHIVE:
   - describe setup
     - which sites, why
       - some with great variance
       - top-10 did not work
     - how to capture
       - tools + scripts
       - bridge
     - how to analyse
       - json
       - script: reimplement
         - Panchenko 1: problems to achieve panchenko's accuracy
           - classifiers
         - cumul: problems to achieve panchenko's accuracy
           - outlier removal
   - addon
   - does it work?
   - does it work better?
   - which variant works?
   - difference svm others
     - other grouped
     - svm alone
       - but better for fitting original data
       - "overfitting"
     - review trace pictures
   - panchenko worse?
     - do pictures/comparisons
     - timing comparison on disabled
   - plots
     - accuracy vs overhead
       - all methods at 30
       - and vs Panchenko 1 for comparison
         - which parts?
           - unaddoned
     - cumul
       - disabled vs WTF-PAD, tamaraw, simple10, simple30, 22@best
   - compare bursts to nobursts
   - WTF-PAD pads small sites much, larger sites little
     - addon-simple does the opposite
   - have a look at [[*practical wf: analyzing traces][practical wf: analyzing traces]]
* WAIT Conclusion
** KEYWORDS Introduction
** WAIT Discussion
   intel model: interdependences (html bigger \to more embedded) not mentioned
   - wf
    - very dependent on retrieved trace time difference, see [[#time]]
    - svc slightly better on original data, fails against defense
      - still panchenko's accuracy only with combined test-train-OR
** TODO Future Work
   :PROPERTIES:
   :CUSTOM_ID: future-work
   :END:
*** KEYWORDS Introduction
*** KEYWORDS addon
   - all HTTP gets treated the same
     a.k.a. how to distinguish HTML/embedded
     a.k.a. redirects + iframes included in model's number of embedded objects
     - redirects
     - iframes
     - normal pages
   - request sizes not altered
     - can clearly distinguish each cover request (as each should have
       size < 500)
       - countable, better: randomized request size
   - how to generate
     - how often, which parameters
     - just triggered by start and until end, or for each load
   - background if non-active (IPP self-similar)
     - 802.16 model
   - does a new connection to another site create a measurable
     tor-response (with variable-length packets)?
   - provable protection
   - Bloom
     - size of Bloom filter
     - number of Bloom filters,
     - bloomsort save/restore
   - which and how many items to prepopulate
     - country-specific f.ex.\space{}google.com
     - leave out redirect from prepopulation
   - automatic update of Bloom-filter
     - with currently visited sites
   - loading further items
   - The choice of cover traffic domains was explicitly taken out of
     the research focus. Currently, all cover traffic is dynamically
     generated by a web server written in Python.

     There exists basic code to use a list of webpages, given their
     sizes. It could be augmented by following links.
     a.k.a. source cover traffic: user gives domain as starting point
     - update from visited URLs
   - no morphing (delay, segmentation)
     - justify why good idea
   - number of embedded elements lacks <style> tags and some in <link>
     a.k.a. improve code to include css, (iframes?), js in number of
     embedded elements
     - does not honor reloads/cacheing
       - or does it? (maybe only called on cacheing)
     - but better than too many?
       - some approaches yes, binning no
   - elaborate on [[#number_embedded]]
   - how to set splits and sizes
   - [[#theory-practice-numEmbedded][differences theoretical HTML-num embedded and observed]]
   - web pages got bigger. See if \cite{newtrafficmodel}'s values are
     still accurate.

     a.k.a. sizes have grown since 2007
     - or only rely on quantiles of observed data
       - but these are hard to gather
	 - use networkmanager code to do that
     - cite web-doom
   - User class: should aggregate smarter, not by-host, but by-page
     with every page-embedded element as just that.
     - indexed by host as workaround, can do better later
       - hard to find out which is HTML, which is non-HTML-traffic
       - so all is lumped together per domain
	 - first request seen as HTML
	 - other requests as non-HTML
     a.k.a. determine if HTML page by suffix (not clear as of ... and
       ... (link to SO))
   - bursts maybe less hidden (number of)
   - time not hidden (no delays of single files)
   - firefox e10n multiprocess
   - delay some requests (f.ex.\space{}images)
*** TODO also helps against global observer if .onion generator is used
    - murdoch/danezis: correlation
    - this creates additional traffic which might hinder correlation attacks
    - further work
    - if cover traffic server is used by enough clients at once
    - or is unobservable (hidden service)
    - information-theoretical / stochastical analysis
    - quote perry critique
**** TODO first read murdoch/danezis paper
**** a.k.a onion host for cover traffic
    As indicated f.ex.\space{}by Wang and Goldberg,
    \cite{wpes13-fingerprinting}, network load already is a bottleneck
    on Tor, with the key bottleneck being exit nodes\cite{wtfpad}. The
    exit nodes might be spared the extra traffic by using =.onion=
    traffic generators (or, alternatively, hosts). A traffic generator
    could be further optimized by using tor proposals ... (see todo) to
    reduce latency, if this does not reduce privacy.
***** TODO tor proposals as of tor.sx
***** TODO read/skim and cite "on performance..."
*** more thorough evaluation
    - only two panchenko approaches
    - assumption: can split traces
    As critiqued in \cite{critique}, the evaluation in this thesis
    makes copies the approach of
    - wg
    - panchenko
    - ...
    - and makes assumptions on feasibility
      - small data set
      - split sites etc
        - but feghhi and wang
    - if protects against this, should also protect against worse
    - additional (?defense?) as proposed in critique
    - bigger world sizes
    - more elaborate tests with different world sizes / open world / etc
**** TODO follow critique at all?
     - yes, shows Tor project's official position
*** Exactly distinguishing HTML and embedded requests
    The current version of the [[user.js][User module]] separates
    CoverTraffic by DNS-domainname. As it often happens that one HTML
    page has embedded elements from different domains, this does not
    perfectly represent reality. It would be more exact to analyse the
    HTML page and at least return the domains of all embedded elements.
\appendix
\part{Appendix}
* appendices (begin above this headline; this is for searching)     :ARCHIVE:
  above, as in this section cuts it out (due to ARCHIVE tag)
* Testing Results different defenses
** Accuracy on 30 Sites, Training against 2016-07-06
#+CAPTION: Testing results of various defenses and other dates when trained on capture data from 2016-07-06. The columns headers show either the classifier, or overhead.
#+NAME: some_name_test
#+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{3}{4}|
#+INCLUDE: "data/results/alternatives.org::#vs-07-06" :only-contents t
** Accuracy on 30 Sites, Training against 2016-07-21
   #+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|
   #+INCLUDE: "data/results/alternatives.org::#vs-07-21" :only-contents t
* Outlier Removal Results with Extra-Trees Classifier
  #+ATTR_LATEX: :align |r||n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{1}{4}|n{3}{4}|
  #+CAPTION: Effect of outlier removal on Extra-Trees classifier accuracy
  #+INCLUDE: "data/results/alternatives.org::#outlier_removal_et" :only-contents t
* WRITE Versions
  :PROPERTIES:
  :CUSTOM_ID: versions
  :END:
** WRITE 0.17 bursts at end - bursts on addon site load finish
   - wang: burst distinguishing feature left with
     w/t\cite{wang2015walkie}
   - solution: count how many embedded, add those as bursts at the
     end

   One characteristic which identified sites well as per Dyer et
   al.\cite{oakland2012-peekaboo} and Wang and Goldberg
   \cite{wang2015walkie} is the number of bursts.

   As the addon would conceptually only increase burst sizes, and not
   alter their number, this should be covered as well. To address this,
   the per-site traffic module [[CoverTraffic]] remembers the number of
   unsent requests for embedded elements. When the page loading is
   finished, this number (which should be 0 or less in more than half
   the cases) of embedded objects is requested. As the cover traffic
   currently comes from a single server, the multiple connection limit
   (compare [[#Hurdles]]) should automatically lead to multiple bursts if
   the number of embedded objects is high enough.
** WRITE 0.18: configurability
   - options choosing which tactic:
     - known/guess sizes
     - bins/target
     - bursts

   - much more traffic
     - try to fix at 19 (and backport to 15.3, codename retro)
** WRITE 0.19: negative values in distribution
   :PROPERTIES:
   :CUSTOM_ID: addon0.19
   :END:
   - negative values for requests are saved and randomly subtracted
   - occur with real size > target size
   - solution
     - if small /negative request value:
       - save value (min size is 160, thus =160 - requested_size=)
     - else:
       - get value at random up to min(request size, saved values)
       - subtract from request size, and from saved value

   The improvements described in this section were backported to
   version 0.15.3, with version name 0.15.3-retrofixed. This
   greatly reduced the amount of overhead, but had the same
   problem: the factor was not correlated to the overhead: it
   lacked control on how much traffic to generate.
** WRITE 0.20: bounds for probability
   - buggy html model: counts many more URLs as HTML than expected
     - fix would be: use only absolute numbers, not probabilities,
       detect HTML (by suffix as approximation, and by content-type
       when found), increase counter when found
     - workaround: bound probability
     - 20 limits number of embedded requests
** WRITE 0.21 bounds absolute number of retrieved objects
    - better workaround
      - stricter bounds on retrieval of embedded objects
      - and stop when limit reached

** WRITE 0.22: should be 0.21.1: reliability fixes
   - retrieved one-too-many on each embedded request
   - fix to /simple/: =endsLoad= was seemingly not triggered
* WRITE capture alternatives [0/1]
  Several applications can capture network traffic to files. The most
  well-known and oldest of these is tcpdump
  [fn:: \url{http://tcpdump.org}] It is a command-line utility, which is
  available on many UNIX-like systems and Windows.

  A modern contender with a GUI is wireshark. It also sports a
  command-line version, tshark.

  Both programs rely on the libpcap library for access to network
  packets.
** TODO better title
* tools
** criteria for tool to retrieve websites
   - script tor browser: load new page
   - easy set-up
   - should
     - register page load or error
   - might
     - set tor's paranoia slider
     - install extra addon
** shell script
   Simply calling =firefox website= loads the website in
   Firefox. This is the approach Wang
   recommended(\cite{wang-personal})[fn:: it seems as though they
   also switched to using the Marionette
   framework.\cite{wang-personal}}. It has the drawback that
   unsuccessful page loads are treated just like successful ones.
** Selenium
   Selenium is the de-facto standard for testing web applications. It
   has drivers for several browsers, allowing it to control them, and
   evaluate the retrieved page. Its documentation is currently
   transferring from Version 1 to Version 2.
** Chickenfoot
   Chickenfoot was a Firefox addon which allowed browser scripting. It
   was developed at MIT\cite{chickenfoot}. The most recent GitHub
   release[fn:: \url{https://github.com/bolinfest/chickenfoot}] is for
   Firefox 4.
** APPENDIX_DONE Marionette
   :PROPERTIES:
   :CUSTOM_ID: Marionette
   :END:
   Marionette is the next generation mozilla testing
   framework. It is works just like Selenium and was designed to be
   integrated into it. It was chosen for this thesis, as it made the
   Tor Browser Bundle easily accessible.

   After installation of the library (see below), controlling the browser
   takes two easy steps:

   1. start the Tor Browser Bundle with the `-marionette` switch

      #+BEGIN_SRC sh
        cd tor-browser_en-US/Browser
        ./firefox -marionette
      #+END_SRC

   2. attach to a running browser in Python

      #+BEGIN_SRC python
        from marionette import Marionette
        client = Marionette('localhost', port=2828);
        client.start_session()
        client.navigate('http://cnn.com'); # navigate loads a website
      #+END_SRC

   Marionette has the benefit that the =client.navigate()= call
   returns only after the page has loaded, (and throws an error if
   the page could not be loaded). This obsoletes f.ex.\space{}Panchenko et al.'s
   \cite{panchenko} need to test whether a page loaded completely.
** TODO who used which retrieval method
   - who did sth
     - p: 
       1. chickenfoot only
       2. Chickenfoot, iMacros, and Scriptish
     - h
     - ll
     - w
     - c
     - d
     - j
   - what did they use
     - list
     - chickenfoot
     - modified browser
     - selenium: daniel
     - plain tor bundle
* setup
** TODO by-hand initialization to retrieve websites
   After installation, the tor browser bundle performs some
   initialization steps. To complete these easily, start the tor
   browser bundle-firefox by hand once, set the connection type and
   have it load any website via Tor. All this also downloads Tor
   metadata, which allows to connect more quickly later on.
** tshark installation
   You also need to install =tshark= [fn::via f.ex.\space{}=sudo apt-get
   install tshark= on Debian-based systems] and enable the user to
   capture packets [fn::via (Debian-based) =sudo dpkg-reconfigure
   wireshark-common= and adding the user to the =wireshark= group
   (in =/etc/groups=)].
** TODO how to get tor browser bundle to work
   In order to start the tor browser bundle via the =./firefox=
   command, you need libraries, which are bundled with the binary.
   They can be found inside the =/TorBrowser/Tor= directory.

   The library path environment variable can be set on the command-line via
   #+BEGIN_SRC sh
   export LD_LIBRARY_PATH=/lib:/usr/lib:/path/to/bundle/Browser/TorBrowser/Tor
   #+END_SRC
   The script [[#1site_desc][one-site.py] uses this internally.

   - install xpra
*** TODO is old, still use, or remove?
** Avoiding safe mode on restart
   If Firefox was killed via a signal (as opposed to closing the
   window), it prompts to start in Safe Mode afterwards.

   This behavior can be avoided in three ways:

   You can set the firefox preference
   =toolkit.startup.max_resumed_crashes= to -1, you can set the
   environment variable =MOZ_DISABLE_AUTO_SAFE_MODE= (did not work
   in Tor Browser Bundle version ...), or --- as a last resort ---
   you can remove the =toolkit.startup.recent_crashes= line in the
   =prefs.js= config file which saves the number of consecutive
   kills via =sed -i '/toolkit\.startup\.recent_crashes/d'
   /path/to/prefs.js=.
*** TODO TBB current version
** headless configuration
   If you want to capture on a headless server, you can use the
   =xvfb=-package. which is installed via =sudo apt-get install xvfb=.

   Then, you can run the X Virtual Framebuffer via

   =Xvfb :1=

   tell the browser to use it via

   =export DISPLAY:1=

   and start the retrieval as mentioned above.
** TODO thoughts on size of data set
   - computable (n^2 for svm with good results)
   - number of instances negligible for computation
     - check this
   - stable results
   - recent papers
     - Panchenko: 775 a 20
     - Wang:
       - 100 a 90 of sensitive pages
       - 5000 a 1 of non-monitored pages
     - Cai: 400 samples of bbc.co.uk
       - 100 \to 800 once a 20 \to 40 twice
   - (currently closed world)
*** TODO more complete list?
** WRITE filtering pcap files [0/4]
   Although this requirement could later be removed (see [[#future-work][section
   future work]]), the addon currently needs a generator of cover
   traffic to work. While it can be set in the add-on's preferences,
   this generator ran on the same host as the tor client. Thus, the
   capture files also contained traffic of the cover traffic
   server. As they do not belong to the Tor traffic, are not what the
   adversary sees, and might distort the result, they were
   filtered. (Even though the accuracy results were not greatly
   changed by this).

   Fortunately, =tshark= offers a way to filter these files as
   mentioned in \cite{splitcap}. The (read) filter commands are
   described in the manual \cite{wireshark-filter}, with the tcp
   protocol specific fields as given in \cite{tcp-filter-fields}.

   The script to solve this is in the appendix [[7777]]. As the server
   ran on port 7777, which was allowed only as an incoming port by
   the firewall, it suffices to filter by port name. (Otherwise, the
   read filter would need to be modified).
*** TODO implementation
   - summary approach: file 7777.sh takes each (pcap) file in
     current directory, filters the port 7777 out
   - apply this to each subdirectory
   - then move all files to a common directory
**** TODO include script from duckstein
*** TODO link to man tshark
** overview
   - for the sake of comparability, also bridge for addon tests
     - and easier to filter
** Marionette installation
   Marionette exists as a Python Package. It is thus easily installed
   via[fn::After installing pip (f.ex.\space{}Debian: =sudo apt-get install python-pip=)]
   #+BEGIN_SRC sh
     pip install marionette_client
   #+END_SRC

   Using a virtualenv is highly recommended in the documentation. If
   using only Marionette, it proved to be unnecessary. The combined
   installation of Marionette and Mozmill without virtualenv broke
   Marionette.
*** TODO merge with above and split out pip install (also needed for wsgi)

** WAIT Tor Browser and Firefox despite --sync problems
   In case someone experiences graphics errors when using Tor via
   X-forwarding, there are several solutions. The first thing to check
   is that the corresponding Firefox version on the same system yields
   the same error.

   If so, the first workaround would be to use the =--sync=
   command-line option to both firefox and the TBB. As this introduces
   huge lag, it is less than optimal. A second solution is to install
   XPRA[fn::\url{http://xpra.org/}]. When it is installed[fn:: best
   install via the developer's repository] on both client and server,
   you can start it via
   #+BEGIN_SRC sh
     xpra start ssh:user@host:DISPLAY_ID --start=/path/to/start-tor-browser
   #+END_SRC
   This issue was seemingly fixed in Firefox 50, as of the bug report.
*** WAIT [#C] bug report: quote other's error messages, link to bug report
* lists of sites
** KEYWORDS modified top-100
   :PROPERTIES:
   :CUSTOM_ID: top-100
   :END:
   #+INCLUDE: "./sw/top/top-100-modified.csv" example
** KEYWORDS 30 sites
   #+INCLUDE: "./sw/top/top-30.csv" example
* APPENDIX_DONE Mozilla Add-On Sdk
  #+INDEX: XUL
  #+INDEX: XML User-interface Langage
  The Add-on SDK by Mozilla facilitates the development of
  Firefox-Addons.

  It allows users to create addons using HTML and Javascript only, as
  opposed to the previous use of
  XUL[fn:: \url{https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL}],
  the XML User-interface Language.

  The addon execution entry point (like =main= in C and Java) can be
  configured via the =preferences.json= file. By default, the main
  addon-script is called =index.js=.

  The SDK contains many tools to interact with the browser. URLs can
  be loaded in the background via several methods; the
  =page-mod= module injects JavaScript code into the page the user is
  browsing to. User-created code can be tested via unit tests.

  If none of the easily accessible high-, or low-level modules
  suffice, much of the browser's functionality is accessible via
  the Components
  object[fn:: \url{https://developer.mozilla.org/en/Components_object}],
  which can be accessed via =require("chrome")=.
** TODO page-mod
   <<page-mod>>
   The
   page-mod[fn:: \url{http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html}]
   module injects "scripts in the context of web pages whose URL
   matches a given pattern."

   The pattern can be given as \verb|"*"| or =/.*/= to run on every
   user-visited page.

   It thus offers the possibility to check for the end of a web page
   load by the user.

   A page-mod example is

   #+BEGIN_SRC js
     const pageMod = require("sdk/page-mod");
     pageMod.PageMod({
         include: /.*/,
         contentScriptFile: "./getLinks.js",
         onAttach: function(worker) {
             worker.port.on("links", function(JSONlinks) {
                 addToCandidates(JSON.parse(JSONlinks));
             });
         }
     });
   #+END_SRC

   , which is run on every page, applies the =getLinks.js= script and
   listens for its feedback, which is then used via
   =addToCandidates()=.

   The page-mod has a =contentScriptWhen= parameter, which specifies
   when to attach the script to the page. Valid values are =start=,
   =ready=, and =end=, the last of which triggers at the
   =window.onload= event, when the complete page, including
   JavaScript, CSS, and images has loaded.

   A page-mod offers many other options such as f.ex.\space{}stylesheets,
   script parameters, etc.
*** link page-mod
    http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html
*** use real script (via import?)
** APPENDIX_DONE Installation and Use of Jpm (the build tool)
   The =jpm=-tool builds (SDK-based) Firefox browser extensions. It is available as a
   NodeJS-Module via the built-in NodeJS Package Manager =npm=.

   Installing =jpm= is a two-step process. Firstly, install NodeJS
   either via built-in tools[fn:: for example =apt-get install
   nodejs-legacy= in Debian and Ubuntu] or via
   download[fn:: \url{https://nodejs.org}]. Secondly,
   #+BEGIN_SRC sh
     npm install jpm
   #+END_SRC

   installs[fn::this command installs the software for the current
   user, global installation is done via =npm install -g jpm=] jpm.

   Once =jpm= is installed, new addons can be created via =jpm init=,
   unit-tested via =jpm test=, live-tested via =jpm run=, the addon
   package built via =jpm xpi=.

   Another command that may be of use is =jpm sign=: as of Firefox
   version 47, Mozilla enforces that all addons be
   signed\cite{addon-signing}. If they are distributed via Mozilla's
   Addon Marketplace[fn:: \url{https://addons.mozilla.org}], they are
   checked and signed automatically. Otherwise, you can request an API
   key for signing and sign via the command\cite[sec.2.7]{moz-sdk-jpm}
   #+BEGIN_SRC sh
     jpm sign --api-key $SIGNING_KEY --api-secret $SIGNING_SECRET
   #+END_SRC
** WRITE message-passing
   There is a mechanism to pass content from the add-on to the
   content scripts, as shown in the example.

   A single string can be passed. As this string can be any serialized
   JSON\cite{rfc7159} object, this effectively only disallows the
   passing of functions and circular objects .

   In a content-script, a message can be sent via
   =self.port.emit('message_type', param)= and received via
   =self.port.on('message_type', function(param))=.

   In the Addon-Context, a =worker= object is used and the
   content-script's =self= is replaced by a =worker=. The worker is
   initialized via the =onAttach= parameter of f.ex.\space{}the page-mod.
*** TODO find example and move somewhere here
** TODO Available Data
   Firefox offers several ways for an add-on to listen for web activity.

   - contents of main page
     \to links to each domain
   - page-mod
     - problems: only when page is loaded, problem for cover traffic
     - but +: ends of all the loading (and processing)
   [[file:docs/lit.org::*%5B%5B./Intercepting%20Page%20Loads%20-%20Mozilla%20|%20MDN.html%5D%5BIntercepting%20Page%20Loads%5D%5D][Intercepting Page Loads*]] lists several
   - load events
   - http observer
   - webprogresslistener
   - xpcom
     - policymanager
     - documentloader
*** each load of page
*** end of page load
*** TODO as references or as footnote?
    ref, as completely read?


** separation of scripts
   As a security measure, there is a separation between

   1) /add-on scripts/, which are run in the browser context, but
      cannot access the web page, and
   2) /content scripts/, which are run in the page context. They can
      access the DOM, but not add-on scripts. nor
   3) /page scripts/, which are those included in the website via
      f.ex.\space{}=<script>= tags

   Bridging this separation, f.ex.\space{}accessing page scripts (and vice
   versa) is possible, but needs some extra work.
*** WAIT index: page scripts, content scripts, add-on scripts

** interacting with page-scripts
   By default, content-scripts are isolated from the modifications
   done by page-scripts.[fn::see
   \url{developer.mozilla.org/en-US/Add-ons/SDK/Guides/Content_Scripts/Interacting_with_page_scripts.html}]

   To access objects dynamically created the inside the page-scripts
   context, you can use =unsafeWindow= instead of =window=.

   The reverse is only true for primitive values. If page-scripts
   need to see altered behavior, it is possible to override
   functionality of the page by using =exportFunction=, as in
   #+BEGIN_SRC js
     exportFunction(open,
                    unsafeWindow.XMLHttpRequest.prototype,
                    {defineAs: "open"});
   #+END_SRC

   This exports the (previously-defined) function =open()= to the
   XMLHttpRequest.prototype, where it replaces the built-in
   functionality.

** TODO unit tests jpm
   JPM also offers the ability to write unit-tests. See
   \url{developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/Low-Level_APIs/test_assert.html}.
*** TODO test link

* Evaluation Scripts
** WRITE retrieve trace: ~one_site.py~
   :PROPERTIES:
   :CUSTOM_ID: 1site_desc
   :END:
   If the Browser Bundle runs when started manually, webpages can be
   retrieved automatically. This is done via the ~one_site.py~ script.

   The script

   1. starts the Tor Browser Bundle's =firefox= binary, enabling
      remote-control via the [[Marionette][=-marionette=]] command-line argument,
      waiting up to 60 seconds for its initialization
   2. starts the =tshark= capture
   3. loads the page (given as first parameter) via Marionette
   4. waits up to 600 seconds for the page load to finish
   5. waits 3 more seconds (for the last cover traffic to finish)
   6. ends the capture
   7. ends Firefox

   This setup (restart after each trace) avoids caching issues with
   website fingerprinting, as the Tor Browser Bundle cleans the
   cache between restarts (as mentioned f.ex.\space{}in \cite{critique}). If a
   browsing-session scenario is desired, the script could be
   modified to omit terminating the browser instance.

   This retrieval is triggered from several scripts that set up the
   environment[fn::mostly they ensure that the frame buffer is running for
   the capture].

   The script's main version captures all traffic, which needs to be
   filtered later. The script =d2g_one_site.py= contains the bridge's IP
   and captures only traffic to and from the bridge.

   The script stops the browser after 9--10 minutes of retrieving one
   page. A drawback is that this has not happened often enough that the
   code for this stopping has not been thoroughly tested.

   - pro: split not necessary

   - browsers cache
     - only helps in cover traffic, (unless warm/cold site model is used)
     - thus by disabling, cover traffic has a harder job at concealing
*** WAIT github link
** WRITE counter.py: represent trace files
   :PROPERTIES:
   :CUSTOM_ID: counter
   :END:
   Once the website traces are stored in pcap-files, feature vectors
   need to be extracted. A feature vector is represented by a Python
   class =Counter=, which can be created from a pcap file, or
   persisted to a JSON\cite{rfc7159} file containing timing and
   packet size information (to save time and space).

   To create a counter, you can use =counter.Counter.from(filename1,
   filename2, ...)=. This is also called indirectly when using
   =counter.py= from the command line, as in

   python /path/to/counter.py

   This extracts data from all pcap files in the current directory and
   subdirectories (excluding Address Resolution Protocol messages and
   ACKs). The filename of the pcap files needs to be =url@timestamp=,
   for example =craigslist.org@1445352269=. The part up to the
   separator =@= is treated as the URL. If JSON-files of the name
   =url.json= (for example =craigslist.com.json=) exist, those are
   preferred instead of the pcap files.

   In the interactive shell, there is a dictionary called
   =COUNTERS=, with the domain names as keys and an array with
   =Counter=s as values. If there were no JSON files in the
   directory, these are created automatically via the =save()=-method.

   To extract the features from a single =Counter=, the
   =panchenko()= and =cumul()=-methods can be used; to inspect
   single features of Panchenko et al.'s first
   attack\cite{panchenko} you can call
   =get('feature_name')=[fn::for example
   =COUNTERS['cnn.com'][0].get('duration')')=].

   =panchenko()= yields a feature vector with default padding of
   Panchenko's variable-length features. Since Panchenko et
   al\cite{panchenko} gave explicit size conversions, the sizes have
   not been normalized further. The default padding is computed to
   be large enough for all traces.

   =cumul()= yields a CUMUL\cite{panchenko2} vector of length
   104.[fn::as reported by Panchenko et al to be optimal}

   outlier removal
   - effected by =outlier_removal= in the [[#counter][counter.py]] module
     - numpy instead of his original code for code clarity
     - otherwise almost verbatim copy
   - quantiles:
     - just take quantiles, use his limits

   In encoding packet sizes, this thesis follows \cite{panchenko}'s
   approach, who recorded "incoming packets as positive, outgoing
   ones as negative numbers."

   - differences to panchenkos
     - feature extraction via python class directly from pcap
       - packet data saveable to JSON
*** TODO cumul
** WRITE analyse.py: code to classify etc
   :PROPERTIES:
   :CUSTOM_ID: analyse
   :END:
   Once the =Counter=s data is obtained, it needs to be transformed
   to input for scikit-learn's\cite{scikit-learn} classifiers.

   The code to classify input can be found in =analyse.py= (see
   appendix [[#analyse]]). For Panchenko 1, this determines the maximum
   length of all variable-length features, 0-pads variable-length
   features with zeroes to the same length, and converts them to an
   array fit for input into scikit-learn's classifiers. When called
   from the command line, as

   python -i /path/to/analyse.py

   , it will extract the feature vectors from JSON or pcap files in
   the current directory, and run 5-fold cross-validated classifiers
   against the data.

   =Counter= input features are transformed into scikit-learn input
   in the =to_features()= function, which normalizes all vectors to
   have the same size (padding with 0s), and creates the feature
   matrix =X= with numeric class labels =y= (and class names in
   =y_domain=).

   Scikit-learn internally uses LibSVM\cite{libsvm}, a library for
   support vector machine classification and regression. To run
   LibSVM\cite{libsvm} on the command-line, there is ~to_libsvm(X, y,
   fname='libsvm_in')~, which can be called with the output of
   =to_features=. It writes lines in X with labels in y to the file
   'libsvm_in' (by default).

    - tries to follow \cite{panchenko} and \cite{panchenko2} code
*** TODO ref stackoverflow why 0 padding
**** TODO or better, some statistics text
*** TODO see also =to_features_cumul=
*** TODO scikit-learn
    The python module scikit-learn\cite{scikit-learn} is described as a
    collection of "tools for data mining and data analysis".

    It combines python's ease-of-use with the efficiency of libraries
    written in C, such as LibSVM. It offers many different classifiers
    and regressors, such as K-NN, SVM, decision trees, linear
    approximation, random trees, etc.
**** TODO regressor? wording
** see also [[file:geloescht.org::*from%20%5B%5B*transform%20to%20panchenko-attributes%5D%5D][from *transform to panchenko-attributes]]                 :ARCHIVE:
* TODO Remove same-host cover traffic server from traces: =7777.sh=
  <<7777>>
  #+INCLUDE: "./bin/7777.sh" src sh
* Addon
** Control module User
   <<user.js>>
   #+INCLUDE: "./cover/js/user.js" src js
* WRITE WF-Trace Pictures
  :PROPERTIES:
  :CUSTOM_ID: wf-pictures
  :END:
  The pictures were created by the commands

  #+BEGIN_EXAMPLE
   for fb in $(ls | grep facebook); do
     python ~/da/bin/counter.py ./$fb  | tail -1 | sed 's/),/\n/g' | \
         tr -d "'()][" > /tmp/times;
     gnuplot -e "set terminal png size 1024,680; \
             set output \"/tmp/${fb}.png\"; plot '/tmp/times' with boxes;";
   done
  #+END_EXAMPLE

  and respectively for craigslist, in the directories containing the
  pcap files.

  These commands first extract each =Counter='s =timing= feature
  (the last line of the output of counter.py at git commit 791af76),
  format it for gnuplot (inserting appropriate newlines via =sed= and
  removing extra characters via =tr=), and =gnuplot= s it to a png
  file with the name of the trace file as prefix.
* modules [1/15]
    #+CAPTION: Modules/Classes of the Addon
    #+ATTR_LATEX: :float nil :width 0.5\textwidth
    [[./pictures/model.eps]]
** TODO update picture to reflect on current addon
** TODO how to show (singleton)-module in UML
** TODO Watcher
   - notifies when user loads sth, and when finished
   - implements nsIObserverService
*** TODO link nsIObserverService
*** methods
    - loads
    - endsLoad
** WRITE CoverTraffic(Loader=default)
   <<CoverTraffic>> The =CoverTraffic= module provides requests for a
   single host contacted. This module uses a
   constructor, as it requires several instances, one for each host.

   The cover traffic provided tries to mimic the [[#HTTP traffic model][HTTP traffic
   models]] parameters.

   There are two strategies implemented which can be adjusted in the
   add-on's preference pane[fn::In the browser, go to settings,
   Add-ons, select the addon's ~Preferences~ button].

   One strategy deals with size estimation: for each page, the size
   of its HTML request and the number of embedded elements are
   stored in a statistic data structure depending on Bloom filters,
   called [[SizeCache]]. If the size is known, it can be used or
   randomly guessed from the HTTP traffic model.

   Another strategy deals with target sizes. The size-cache stores
   approximate sizes, due to binning of values. One strategy is to
   pad both the HTML request size and the number of embedded
   elements up to the bin border. The other strategy determines a
   target distribution for each parameter, multiplies by the
   overhead parameter, and tries to attain that.

   Thus, on creation the site's and a target HTML size and number
   of embedded elements are determined. As creation is synchronous
   with the first HTML request, another request to pad up to the
   target HTML size is sent. As the target number of requests for
   embedded elements is a certain multiple of the actual number of
   requests, on each such request[fn:: signaled by the =loadNext=
   call], a probability is sampled, potentially resulting in a
   request for a cover embedded element. The cover element sizes
   are once again drawn from the HTTP traffic model.

   After the page has finished loading, the =CoverTraffic='s
   =finish()= method is called. If the number of embedded elements
   requests has been to low, the remainder are then dispatched.
*** WAIT check if still two strategies
*** TODO link to number of embedded elements and HTML request
*** TODO link to sizecache
*** WRITE [#A] see if description better than design, then integrate
** TODO Loader(Source=default2)
   loads new cover page (mockable)
** TODO Stats - Static functions
   statistical distributions (html, embedded, etc)
** TODO CoverUrl
   source for cover traffic
   fixed domain, size as parameter
** TODO BloomSort
   <<bloomsort>>
   sorts elements by size using Bloom Filters
   +add(id, value)
   +query(id): value
   +save
   +restore
** TODO Random
   provides randomization methods
   +string(length:number) pseudo-random string
   +uniform01() secure random float in the range [0,1)
** SizeCache
   <<SizeCache>> The SizeCache element stores approximations for
   both the HTML sizes (=htmlSize()=) and number of embedded objects
   (=numberEmbeddedObject()=) per URL, using a [[bloomsort][BloomSort]] data
   structure for each.

   Exceptions from the BloomSort are passed on. This module is a
   facade \cite[sec.4.10]{gof} that initializes the Bloom filters and
   simplifies access.
** APPENDIX_DONE User
   The [[user.js][User]] module handles user action. It is the main controller.

   On each loading of a object via HTTP(S), it receives a message
   from the =Watcher= module via =loads()=, with the loaded URL as
   parameter.

   If it is a new request to the host, loading of an HTML page is
   assumed and a new =CoverTraffic=-Object is generated.

   If the host is known (as defined in [[#distinguish_HTML_embedded][How to distinguish HTML and
   embedded objects]]), an embedded page is assumed and the (existant)
   related =CoverTraffic=-Object is told that an embedded element was
   loaded.
* WAIT Cached: Size of HTML-Documents
  :PROPERTIES:
  :CUSTOM_ID: find sizes of HTML-documents
  :END:
  The HTTP traffic model\cite{newtrafficmodel}'s statistical size
  generation works with application-level sizes on the network, as the
  authors of analysed logfiles of the Squid proxy[fn::
  \url{http://www.squid-cache.org}].

  The HTML-sizes could not be trivially obtained from the
  =Content-Length= in the browser, as there are additional headers
  and size-reduction via compression. The sizes were determined by
  retrieving the files with =wget= via squid. This is implemented
  via the =html_top_100.sh= script.

  It empties squid's =access.log= file and cache by restarting
  it. Afterwards, the top-100 files are retrieved with =wget= via
  squid.

  From the log file =access.log=, the sizes are extracted via the
  command

  #+BEGIN_SRC sh
    sudo cat /var/log/squid3/access.log | tr -s ' ' | cut -d ' ' -f 5,7 > /mnt/data/HTML-sizes
  #+END_SRC

  These sizes are then converted to a JSON-array via the
  [[./htmlSizeToJSON.py]]-file. It also does a check for duplicate
  values, choosing the lower one. This increases traffic, but the
  opposite might be too little traffic, thus easier website
  fingerprinting, which should be avoided.
** WAIT github link for =html_top_100= script
* WRITE Cached: Number of Embedded Objects
  :PROPERTIES:
  :CUSTOM_ID: number_embedded
  :END:
  The second parameter for generating cover traffic is the number
  of embedded objects per HTML-page.

  These are extracted via the python script ~htmlToNumEmbedded.py~
  which is called for each of the top-100's main web pages by
  ~retrieve-100-embedded.sh~.

  To extract, python's lxml module to parse the HTML's
  DOM extracts the URLs of embedded files from the features of
  several tags, f.ex.\space{}the =src= element of =img= tags.
** TODO link to lxml website
** WAIT github links
* TODO single files of a website
  The complete data of google.com can be retrieved via

  =mkdir site; cd site; wget -p -H google.com=

  which yields (in germany) the files (=find . -type f -ls=, formatted)

  |  size | url                                                               |
  |-------+-------------------------------------------------------------------|
  |       | <65>                                                              |
  | 18979 | google.com/index.html                                             |
  | 17284 | www.google.de/images/nav_logo229.png                              |
  |  1834 | www.google.de/images/icons/product/chrome-48.png                  |
  |  5482 | =www.google.de/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png= |
  |  5430 | =www.google.de/images/branding/product/ico/googleg_lodp.ico=      |
  |  8080 | www.google.de/robots.txt                                          |

  which are five to six[fn::depending on ~robots.txt~] requests.
** TODO tshark for normal (non-tor) retrieval
** TODO mention redirects
* TODO host install list
  This is a list of all that need to be done on a fresh Ubuntu server
  installation to get all parts to work.

  1. apt update
  2. apt upgrade
  3. apt install emacs tmux unison
  4. download tbb
     1. apt install firefox
     2. apt install xpra
        - installs needed x-libraries
        - and is faster
        - test via (local)
        #+BEGIN_SRC sh
          xpra start ssh:mkreik@duckstein:37 --start=firefox
        #+END_SRC
        and kill by hand (on duckstein) via
        #+BEGIN_SRC sh
          xpra stop
        #+END_SRC
     3. download tbb, gpg check, cp to hosts, test
  5. apt install python-pip; pip install --upgrade pip
  6. pip install marionette_client
  7. apt install xvfb
  8. apt install tshark
     - add user to wireshark group in /etc/group
     - log out, log back in
  9. mkdir mnt/data, chown to current user/group
  10. mod_wsgi: see appendix [[#mod_wsgi][Apache =mod_wsgi=]]
  11. apt install tor
  12. apt install python-pyptlib python-crypto python-yaml
      python-psutil
** TODO mention unison (in bib)
** todo format as source block
* WRITE how to set up WTF-PAD
  - helpful notes at scramblesuit
  - modify for WTF-PAD
  - failed to work if called from Tor
  - thus 3/4 separate parts
    - tor server: listen on ORPort X
    - WTF-PAD server script: send to X, listen on Y
    - WTF-PAD client script: send to Y, listen on Z
    - in tbb/on 2nd tor (a.k.a. client): send traffic to bridge
      =Bridge 127.0.0.1 Z=
      - here: =Bridge 134.169.109.51:40300=
      - or in tbb without the Bridge: =134.169.109.51:40300=
  - modify capture: localhost
    - =-i lo=
    - =port Y=
  - alternative: client/server on separate hosts
    - because localhost did not yield much
      - because histogram based on arrival times, which are small on
        the same host (loopback interface)
    - ohne =-i=
    - =host server_host=
  - start capture
  - bug on multiple uses:
    #+BEGIN_EXAMPLE
    exceptions.IOError: [Errno 24] Too many open files: '/proc/23634/stat'
    #+END_EXAMPLE
    - try temporary fix: increase number of file descriptors, set
      #+BEGIN_SRC sh
        username        -    nofile  10000
      #+END_SRC
      in =/etc/security/limits.conf=. (Alter 10000 to something else
      if you retrieve more than 4000 at once. It seems to be about
      1 per domain-request).
    - bug report in appendix, needs some code to mitigate
  - without WTF-PAD
    - at bridge
      tor -f torrc.no_wfpad.server (just orport)
    - at client
      configure tbb with
      #+BEGIN_EXAMPLE
        Bridge bridge.ip:40200
      #+END_EXAMPLE
** TODO scripts
* distribution of Panchenko 1's (main) features
  These distribution histograms show how Panchenko's main features
  are distributed. They are stacked histograms with classes
  separated by colors. They are compared (visually) to the HTTP
  Traffic Model\cite{newtrafficmodel}.

  [[file:pictures/all_count_in.png]]
  shows the number of downstream/incoming packets.

  The general form of a gamma distribution may be
  fitting. Conceptually, this should be approximately

  num_embedded (gamma) * size_embedded (lognormal) / packet_size

  [[file:pictures/all_count_out.png]]
  shows the number of upstream/outgoing packets.

  Conceptually, the

  [[file:pictures/all_length_0.png]]
  the length of the Size Marker feature vector.

  [[file:pictures/all_num_sizes_in.png]]
  number of different packet sizes downstream/incoming.

  [[file:pictures/all_num_sizes_out.png]]
  number of different packet sizes upstream/outgoing.

  [[file:pictures/all_percentage_in.png]]
  percentage of incoming bytes (of total).

  [[file:pictures/all_total_in.png]]
  total bytes downstream/incoming.

  [[file:pictures/all_total_out.png]]
  total bytes upstream/outgoing.
** TODO compare to HTTP model
* Cover Traffic Server
** APPENDIX_DONE Which Python Web Server to Choose
   The naïve implementation based on Python's
   =BaseHTTPServer=[fn::\cite[20.18]{python-lib-ref}] had drawbacks (see
   [[#server-non-para]]), even for the queries of a single addon. This
   prompted the search for a python-based, adequately-performing
   technology stack.

   \cite{nicholas} evaluates Python web server performance. He shows
   the Apache web server[fn:: \url{https://httpd.apache.org/}] with
   the ~mod_wsgi~ module as well-performing. As it was noted to be
   very easy to set up, it was chosen for this evaluation.
** APPENDIX_DONE Apache =mod_wsgi=
   :PROPERTIES:
   :CUSTOM_ID: mod_wsgi
   :END:
   =mod_wsgi= is a module for the Apache web server. It executes
   python scripts which implement the WSGI standard\cite{pep3333}. An
   apache httpd serving only WSGI is easily set up via the
   =mod_wsgi-express= command, which is included in the =mod_wsgi=
   python package[fn:: \url{https://pypi.python.org/pypi/mod_wsgi}].
   To install on Debian-based systems[fn::tested on Ubuntu Server
   Edition and Linux Mint 17.1 Rebecca], do
   #+BEGIN_SRC sh
     apt-get install apache2-bin apache2-dev python-dev
     pip install mod_wsgi
   #+END_SRC

   and start a server via =~/.local/bin/mod_wsgi-express start-server
   wsgi.py=[fn::to listen on a non-standard port, the =--port=
   parameter is used, here: =--port 7777=].
*** TODO github link wsgi.py
** APPENDIX_DONE wsgi.py cover traffic server and generator
   :PROPERTIES:
   :CUSTOM_ID: wsgi
   :END:
   With the technology stack to implement the cover traffic generator
   being settled, implementation becomes a single-page file, see
   ~bin/wsgi.py~.

   One implementation detail is that the length of the content gets
   inflated by the content-headers. To decrease this again, the length
   (which in turn depends on the required length) needs to be
   calculated and subtracted from the body-length. Some uncertainty
   arises because the =Proxy-Connection: keep-alive= header is added
   in some circumstances. The implementation errs on the side of
   returning too much data.

   Once the size is computed, a pseudo-random choice from the list of
   all printable characters is returned to the HTML query.

   To test this algorithm, the first 1000 sizes are retrieved via
     #+BEGIN_SRC sh
       for i in $(seq 1000); do
           curl -D /tmp/curlheaders/$i.head 127.0.0.1:8000/?size=$i > \
                /tmp/curlheaders/$i.body;
       done
     #+END_SRC
   which outputs the header and body of each query to the files,
   f.ex.\space{}=134.head= and =134.body=.

   This data is then evaluated by hand to check the sizes:
     #+BEGIN_SRC sh
       for i in $(seq 1000); do
           echo "$i: $(cat ${i}.* | wc -c)";
       done
     #+END_SRC
*** WAIT link to github wsgi.py
** APPENDIX_DONE Non-parallelized web server for cover traffic [0/2]
   :PROPERTIES:
   :CUSTOM_ID: server-non-para
   :END:
   The following approach did not scale to several parallel connections,
   causing delays of several seconds under medium load, so it was not
   used, as /unplanned/ delays are problematic because cover traffic does
   not appear when it should. It is included as a reference of what
   seems to work, but did not.

   The python module =TrafficHTTPServer= can be started on the
   command-line via
   #+BEGIN_SRC sh
     python TrafficHTTPServer.py [port]
   #+END_SRC

   with =port= set to 8000 by default. It generates cover traffic of
   the size given by the =size= parameter. For example the URL
   =http://localhost:8000/?size=1000= provides 1000 bytes of
   pseudo-random[fn::minus the static HTTP header] content from a
   =TrafficHTTPServer= running at localhost port 8000.
*** WAIT link to github script
* WRITE differences theoretical HTML-num embedded and observed
  :PROPERTIES:
  :CUSTOM_ID: theory-practice-numEmbedded
  :END:
  - redirects
    - html had 176 elements, embedded only 100
    - the others were redirects (f.ex.\space{}from google.com to
      www.google.de)
    - these could be counted as having 0 embedded elements,
      - yet still a difference remains
  - it fits better if you enlarge the sizes by 0 for each redirected
    element (there are 176 elements in the html filter, including
    redirects, and only 99 in the embedded filter, if you pad the
    embedded filter by 0 for each of those, it is not a perfect fit,
    but better)
  - growth of websites
  - [[#future-work][Further work]]
** link in [[#HTTP traffic model][HTTP traffic model]] to here when moved
* TODO Bloom Filters
** TODO Bloom usage and implementation
   - Bloom sort
     - error rate computation
   - size taken from example...
     - maybe change when altered
** WRITE Bloom-sort
   :PROPERTIES:
   :CUSTOM_ID: bloom-sort
   :END:
    By ordering data into bins, it becomes possible to use Bloom filters
    for the estimation of sizes, using one Bloom filter for each bin.

    To achieve this, sensible separation criteria (called /splits/) for
    the bins need to be found. Afterwards, each bin needs to be assigned
    a value (called /size/) for all contained elements. See appendix
    [[#bloom-params]] on determining the sizes and splits.

    This data-structure, called /Bloom-sort/ is initialized with an
    array of splits, and an array of sizes. The sizes-array needs to
    have one more element than the splits-array, as the bins are bounded
    on the left by 0, and on the right by infinity.

    #+BEGIN_SRC js
      /**
       ,* @param {sizes Array} array of values for each bin, must be sorted
       ,* @param {splits Array} array of bin borders, must be sorted
      ,*/
      function BloomSort(sizes, splits) {
          this.sizes = sizes;
          this.splits = splits;
          this.filters = [];
          for ( let i = 0; i < sizes.length; i++ ) {
              this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
          }
      }
    #+END_SRC

    Thus, you get
    \[-\infty \le \text{size}_0 \le \text{split}_0 \le \text{size}_1 \le \text{split}_1 \le ... \le \text{split}_{n-1} \le \text{size}_n < \infty\]
    Given the splits, it becomes possible to add the elements to their
    bins:

    #+BEGIN_SRC js
      BloomSort.prototype.add = function(id, size) {
          this.filters[_.sortedIndex(this.splits, size)].add(id);
      };
    #+END_SRC

    where =_.sortedIndex()= gives the index at which =size= would be
    inserted into the sorted =this.splits= array.

    The retrieval of element sizes looks into each Bloom filter,
    checking whether it might contain the element =id=. If one Bloom
    filter reports containment, its corresponding element- =size= is
    returned. If several or no Bloom filters report containment, an
    exception is thrown. The exception is used to allow all possible
    return values, not blocking one of them, say =-1=, for the error
    condition.
    #+BEGIN_SRC js
      /** determines size of element, raises exception if unclear */
      BloomSort.prototype.query = function(id) {
          let pos = -1;
          for ( let i = 0; i < this.filters.length; i++ ) {
              if ( this.filters[i].test(id) ) {
                  if ( pos === -1 ) {
                      pos = i;
                  } else {
                      throw {
                          name: 'BloomError',
                          message: 'Contains multiple entries'
                      };
                  }
              }
          }
          if ( pos === -1 ) {
              throw {
                  name: 'BloomError',
                  message: 'Contains no entries'
              };
          }
          return this.sizes[pos];
      };
    #+END_SRC

    It is initialized with
    #+BEGIN_SRC js
    let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
    #+END_SRC

    then adding elements via =htmlSizes.add("http://google.com/", 613)=
    and querying via =htmlSizes.query("http://google.com/")=, which
    would yield =400=. See usage in =size-cache=.

    - link to main bloom-sort, create custom-id there
*** WAIT github link
** TODO determining Bloom-sort parameters
   :PROPERTIES:
   :CUSTOM_ID: bloom-params
   :END:
   It is impractical to store the sizes of all URLs. Another
   possibility is to use Bloom Filters to aggregate groups of URLs
   with similar values, as described in [[#bloom-sort][bloom-sort]].

   Each groups gets borders (/splits/) and a size which represents each
   contained element.

   Determining the optimal number of groups, splits and sizes is a
   topic of [[#future-work][Further work]]. Here, initially the quantiles of the
   HTTP-model (see [[#HTTP traffic model][HTTP traffic model]]) were used. When the data were
   to be inserted, it turned out that especially the numbers of
   embedded elements did not match the theoretically proposed groups:

   For three groups, the splits would be given by the 33 1/3 and 66
   2/3 quantiles, as 0.0107 and 1.481. As the number of embedded
   elements is a whole number, two thirds of the information would
   be if an element is 0, the next group would contain all other
   elements: The (representative) sizes of the groups were given as
   7.915E-05, 0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

   The data to be inserted (see [[#number_embedded][Cached: Number of Embedded Objects]])
   had the splits (quantiles) at 10 2/3 and 36 2/3 and the sizes at
   6, 20, and 59 2/3.

   In addition to using the observed sizes for the Bloom filter, the
   number of groups was increased to 5.
*** maybe graphics?
* KEYWORDS Data Set Sizes
The following Table [[tab:data-set-size]] lists the size of the data sets
per website fingerprinting study.

#+CAPTION[Sizes of data sets]: Sizes of data sets per study.
#+ATTR_LATEX: :align |r||l|l|
#+NAME: tab:data-set-size
|-----------------------+--------------------------+---------------------------|
| dataset               | foreground               | background                |
| <21>                  |                          |                           |
|-----------------------+--------------------------+---------------------------|
| \cite{effective}      | 100@(60train+30test)     | 9000@1 (from alexa 10000) |
| \cite{kfingerprint}   | 55@100                   | 100000 alexa (-55 fg)     |
| \cite{wtfpad}         | 100@20-40                | ?                         |
| \cite{Liberatore:2006} | 109,479/(2000mostcommon) |                           |
| \cite{wpes13-fingerprinting} | 100@40                   |                           |
| \cite{panchenko2} RND-WWW | 1125@40                  | 118,884@1                 |
| \cite{panchenko2} TOR-EXIT |                          | 211,148 (total urls)      |
| \cite{ccsw09-fingerprinting} | 775@20                   | --- (none)                |
| \cite{panchenko}      | 5@(35train+25test)       | (4000train+1000test)@1    |
| \cite{wpes14-csbuflo} | 200@20                   |                           |
|-----------------------+--------------------------+---------------------------|

- seldom more than 100 fg sites
- as mentioned by f.ex.\space{}\cite{panchenko} (where exactly?), small cw-set
  for testing defenses
- caption, name, ...

* WRITE How to get Wang-kNN to work
  in directory with traces (json/pcap/...)

  0. [@0] convert traces to \cite{effective}'s =batch=-directory,
     using =counter.py#dir_to_wang= (for convenience, use
     =wang_batch.py=)
  1. python /path/to/fextract.py
  2. /path/to/flearner 0 1 $(flearn_params.sh)
** TODO github flearn_params.sh, link
* WRITE How to get Panchenko-SVM to work
** algo
   1. outlier removal
      - creates new input dir
        #+BEGIN_SRC sh
          python ./feature-scripts/outlier-removal.py -in ./foreground-data/ -out ./output/out-removal/ -setting CW -randomInstances NO -instances 40 -referenceFormat tcp -outlierRemoval Simple -ignoreOutlier YES
        #+END_SRC
        for foreground-data f.ex.
   2. generate libsvm input
      - open world
        - foreground
        #+BEGIN_SRC sh
          python ./feature-scripts/generate-feature.py -in ./foreground-data/ -out ./output/ -setting OW_FG -dataSet FG -classifier CUMULATIVE -force YES -features 100 -randomInstances NO -instances 1
        #+END_SRC
        - background
      - closed world
        #+BEGIN_SRC sh
          python ./feature-scripts/generate-feature.py -in ./foreground-data/ -out ./output/ -setting CW -classifier CUMULATIVE -force YES -features 100 -randomInstances NO -dataSet CW -instances 40
        #+END_SRC
   3. train svm
      #+BEGIN_SRC sh
        svm-train ./output/CW_TCP ./output/CW_TCP.model
      #+END_SRC
      f.ex. for closed world-data
   4. use svm
      #+BEGIN_SRC sh
        ../libsvm-3.20-src/svm-predict CW_TCP  CW_TCP.model CW_TCP.output
      #+END_SRC
** OW-data just has *two* classes: fg (=1) and bg (=0)
\bibliography{docs/master}
\bibliographystyle{plain}
\input{diplomarbeit.ind}
* END: above index, bibliography, etc                               :ARCHIVE:
