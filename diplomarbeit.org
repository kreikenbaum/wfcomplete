#+TITLE: Selective Cover Traffic
#+TODO: KEYWORDS WRITE CHECK | EVA DANIEL FINAL
#+TODO: RECHECK | EVA-AGAIN DANIEL FINAL
#+TODO: | NEEDS_SUBPARTS
#+TODO: WAIT | APPENDIX_DONE WAIT_FINISH
\pagenumbering{roman}
\listoffigures
\listoftables
* Configuration							    :ARCHIVE:
#+LATEX_CLASS: scrreprt
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{numprint}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \npdecimalsign{.}
#+LATEX_HEADER: \nprounddigits{2}
#+LATEX_HEADER: \npthousandthpartsep{}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure} \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.1 cm} % WAR: \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Defending against Tor Website Fingerprinting with Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.1cm} % WAR: \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {1.9 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte % mkreik <2016-07-11 Mo>: war {5 cm}
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
#+OPTIONS: H:6
* EVA Introduction
#+BEGIN_LaTeX
  \pagenumbering{arabic}
#+END_LaTeX
  #+INDEX: Tor
  #+INDEX: trace
  #+INDEX: website fingerprinting
  In the wake of both the Snowden revelations in the western world,
  and increased internet censorship in countries such as Iran,
  Saudi-Arabia, and China\cite{china}, more and more
  Internet users search for ways to keep online communication and web
  browsing both private and free of censorship.

  The /Tor/ project\cite{tor-design} provides this. It protects
  whistleblowers, journalists, the people in oppressive
  regimes\cite{jardine2016tor}, even the military, and regular
  internet users, against e.g.\space{}nation-states or businesses which want
  to follow user's online steps. It routes encrypted data traffic via
  intermediaries, obscuring who connects to whom.

  # NO reflow here!!!
  While basic Tor acts as a proxy for any kind of TCP traffic, web
  browsing with Tor is user-friendly\cite{usability:weis2006}: you
  just need to
  \href{https://www.torproject.org/download/download-easy.html.en}{download the Tor Browser Bundle}. It connects automatically to the Tor
  network to cloak the client's web traffic.  \\
  # NO reflow here!!!


  As has often been the case with anonymity and privacy, several
  parties try to attack Tor's protection. One mass-surveillance type
  of attack that can be carried out passively with low-level
  technology is /website fingerprinting/: The time and size of users'
  data packets (called /traces/) are recorded. See Figure
  \ref{fig:traces}. This recording is compared to previously recorded
  traces of known sites. While older attacks
  (\cite{ccsw09-fingerprinting}, \cite{Liberatore:2006}) just compared
  single packet sizes, possibly by hand, newer attacks
  (\cite{panchenko}, \cite{panchenko2}, \cite{realistic}) use machine
  learning to classify traces, achieving high accuracy rates under
  laboratory conditions.
#+BEGIN_LaTeX
\begin{figure}[htbp]
\includegraphics[width=0.12\textwidth]{./pictures/craigslist_org@1445352269.png}
\includegraphics[width=0.12\textwidth]{./pictures/craigslist_org@1445585277.png}
\includegraphics[width=0.12\textwidth]{./pictures/craigslist_org@1445486337.png}\includegraphics[width=0.12\textwidth]{./pictures/craigslist_org@1445527033.png}\includegraphics[width=0.12 \textwidth]{./pictures/facebook_com@1445350531.png}
\includegraphics[width=0.12 \textwidth]{./pictures/facebook_com@1445422155.png}
\includegraphics[width=0.12 \textwidth]{./pictures/facebook_com@1445425799.png}
\includegraphics[width=0.12 \textwidth]{./pictures/facebook_com@1445429729.png}
\caption[Web trace data visualized]{Web trace data visualized. Box height signifies amount of data, width the duration until the next packet. The left 4 are for \url{http://craigslist.org}, the right for \url{http://facebook.com}.
While some similarity can be seen for each group, the "within-group" differences are quite big between each group's traces as well.}
\label{fig:traces}
\end{figure}
#+END_LaTeX

  Web fingerprinting effectively deanonymizes the traffic that users
  thought secure, exposing for example a dissident to his nation
  state, nullifying this part of Tor's protection. To protect against
  this, most early attacks (\cite{Wagner96analysisof}, \cite{hintz02},
  \cite{ssl-traffic-analysis}) also proposed defenses. These evolved
  from /specific defenses/, f.ex. packet-size-altering methods
  (\cite{httpos}, \cite{morphing09}) to /general defenses/, which
  transform groups of web retrievals so that all members look the
  same.

  At the start of the thesis, there existed mostly deterministic
  defenses with high overhead (f.ex. over 220% bandwidth, and 300%
  time\cite{a-systematic}). During the course of this thesis, the
  stochastic defenses of \cite{wang2015walkie} and \cite{wtfpad},
  with much lower overhead, were developed. This validates a
  stochastic approach, yet improvements seem possible in two areas:
  (1) ease of installation, and configurability, and (2) more-closely
  fitting cover traffic generation.

  Ad 1: \cite{wang2015walkie} alters the Firefox binary, while the
  current version of \cite{wtfpad} needed much manual adjustment in
  our attempts.

  Ad 2: \cite{wang2015walkie} uses either a normal, or lognormal
  distribution, not adjusting to HTTP-specifics, while \cite{wtfpad}
  samples packets at the network layer. It aims at hiding that traffic
  occurs, as it derives its basic mechanism from \cite{ShWa-Timing06}.
** Thesis Contribution
   This thesis presents and tests a new defense against website
   fingerprinting. This new defense mimics HTTP\cite{rfc7230}-shaped
   cover traffic: Each web page retrieval is augmented by
   stochastically-drawn dummy HTTP traffic\cite{newtrafficmodel}. This
   could optimize the protection offered for given bandwidth
   overhead. It is implemented in a browser extension, which makes the
   defense easier to install, configure, and maintain.
** Thesis Structure
   The following chapters try to solve the question whether the new
   defense works more effectively than existing ones.

   Chapter [[#ch2-background]] provides basic background for the IT-savvy
   who have not yet encountered Tor, machine learning, or website
   fingerprinting. For the Tor network, we treat its basic structure
   and why website fingerprinting might be a credible threat. Machine
   learning basic steps and algorithms are briefly skimmed. Finally,
   website fingerprinting on Tor is presented. These parts can safely
   be skipped given previous knowledge.

   The defense's why and how (motivation and design) is described in
   chapter [[#ch3-newdefense]]. This also describes the bloom sort data
   structure for stochastically saving object sizes.

   Chapter [[#ch4-evaluation]] evaluates the defense. It first describes
   the data-gathering process. Next, the website fingerprinting
   attacks of \cite{panchenko2}, and \cite{ccsw09-fingerprinting} are
   validated on defenseless data. This is followed by the evaluation
   on data with cloaking.

   These results are discussed and compared to literature results of
   \cite{wang2015walkie} and \cite{wtfpad} in chapter [[#ch5-discussion]].

   Chapter [[#ch6-conclusion]] summarizes the results, shows a path to
   implementation, with both included and additional further work.
* NEEDS_SUBPARTS Background [0/6]
  :PROPERTIES:
  :CUSTOM_ID: ch2-background
  :END:
  Knowing [[#sub2-tor][the Tor network]], [[#sub2-ml][machine-learning basics]] and [[#sub2-wf][previous
  attacks and defenses]] helps to understand and then counter website
  fingerprinting.
** CHECK The Tor Network
   :PROPERTIES:
   :CUSTOM_ID: sub2-tor
   :END:
   #+INDEX: onion router
   #+INDEX: onion proxy
   #+INDEX: Tor!onion router
   #+INDEX: Tor!onion proxy
   #+INDEX: Tor!the onion router
   #+CAPTION: \href{https://www.torproject.org/about/overview.html.en}{Connection through the Tor network}.
   #+NAME: fig:tor-network
   #+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {r}{0.4\textwidth}
   [[./pictures/htw2.png]]

   To protect online anonymity, Tor\cite{tor-design} uses a
   semi-distributed infrastructure: volunteer-run /onion routers/ pass
   the messages, while their structure is provided by fixed /directory
   servers/. A path through the onion routers is negotiated by the
   /onion proxy/, a SOCKS\cite{rfc1928}-TCP proxy which most often
   runs on the client computer.

   The onion proxy selects three globally-distributed hops to
   initialize a connection. It makes a connection to the first,
   establishes encryption, asks the first hop to make a connection to
   the second, sets up encryption to this, and from there to the
   third. The third hop establishes a connection to its destination.

   Each message is encrypted three times using same-length encryption
   and sent along this path. The first router decrypts the first
   layer, and so on, like layers of an onion. This explains Tor's name
   /the onion router/.

   As a result of this setup, each hop can only see its direct
   neighbors along the path. Even if one hop of a three-hop setup is
   compromised, directly linking source and destination becomes pretty
   hard.
** CHECK Tor Website Fingerprinting
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf
   :END:
   #+INDEX: traffic analysis
   #+INDEX: website fingerprinting
   #+INDEX: WF
   What does an adversary do if he cannot decrypt and the message
   traffic of a cryptographic system he is interested in? One
   alternative is to inspect the traffic itself for patterns. This
   process is called /traffic analysis/\cite{introta} and yields much
   useful information\cite{applied96}.

   /Website fingerprinting/ (short: WF) needs only message meta-data:
   who sends how much data when. It assumes that the system itself is
   computationally secure\cite{applied96}: there are not enough
   resources, time, or data to break it. Analysing traffic patterns
   can circumvent the system. Anyone who can see the data stream can
   carry out this attack, without anyone being the wiser. They simply
   need to capture the data stream using f.ex. the
   =tcpdump=\cite{tcpdump8-manual} tool.\\

   From inception\cite{tor-design}, Tor provided defenses against
   traffic analysis. For one, all /data/ cells have the same size,
   which protects against identifying them by size only. Tor also
   multiplexes all its data traffic into a single stream, making it
   hard to distinguish the multiple streams that most websites
   require, let alone parallel retrieval. Tor also
   unavoidably\cite{rfc1925} increases traffic latency, so that
   attacks have a harder time relying on interpacket
   timing\cite{challenges}.\\


   This made WF harder, to the point that it was was mentioned, but
   not hindered, in \cite{tor-design}. It took five years for
   \cite{ccsw09-fingerprinting} to show better than random
   classification results against Tor traffic. This evolved to
   state-of-the-art methods like \cite{panchenko2}.

   What all methods have in common is that they extract numerical
   /features/ from the raw data, which is then classified using
   machine learning.

   \cite{ccsw09-fingerprinting} follows \cite{hintz02}, and uses
   packet sizes for features. They use a jaccard metric as classifier,
   but as seen in chapter [[#ch4-evaluation]], nothing but sizes can yield
   surprisingly good results in combination with support vector
   machines.\\

   #+CAPTION[CUMUL features example]: CUMUL\cite{panchenko2} \href{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}{features example}
   #+ATTR_LATEX: :float wrap :width 0.5\textwidth :placement {r}{0.55\textwidth}
   #+NAME: CUMUL_traces
   [[./pictures/cumul_aus_paper.pdf]] 

   \cite{panchenko2} uses a cumulative size metric, CUMUL. As seen in
   Figure [[CUMUL_traces]], these provide a graphical representation of
   traces, while still allowing for computer-based comparison after
   normalization. This is also evaluated with support vector machines.\\

   There are numerous WF attacks. F.ex. \cite{effective} achieved
   accuracy improvements using over 3000 features and a
   KNN-classification with weighted metrics while
   \cite{kfingerprint}'s approach uses approximately 150 features with
   Random Forest Classification.\\

   Extracted /features/ are assigned to web sites using machine
   learning techniques reviewed in section [[#sub2-ml]].
** CHECK Machine Learning
   :PROPERTIES:
   :CUSTOM_ID: sub2-ml
   :END:
   #+INDEX: machine learning
   #+INDEX: ML
   Let us review basics of /Machine Learning/ (ML): a
   computer\cite{turing1936a} algorithm extracts and generalizes
   patterns from learning data.\cite{rieckdiss} This is then used to
   classify further patterns (f.ex. for handwriting recognition),
   or to act on the generalizations (f.ex. for self-driving cars).

   The machine learning process consists of at least two separate
   steps: domain-specific [[#ml-features][feature extraction]] transforms the raw input
   data --- in our case, website traces --- into characteristics ---
   in our case, numbers for f.ex. the number of outgoing
   packets. [[#ml-class][Classification]] then generalizes and assigns these
   characteristics into categories.

   A last section studies [[#ml-measure][measures to evaluate machine learning
   performance]].
*** KEYWORDS Feature Extraction
    :PROPERTIES:
    :CUSTOM_ID: ml-features
    :END:
    #+INDEX: feature extraction
    #+INDEX: machine learning!feature extraction
    Feature extraction\cite[sec.1.3.1]{duda} transforms (preprocessed)
    input data into features/characteristics suitable for
    classification.


- discussion of related approaches used to evaluate your approach (e.g. Panchenko,
Hermann)

    While the boundary of feature extraction to classification is
    "somewhat arbitrary"\cite[sec.1.3.1]{duda}, feature extraction
    deals with the, well, extraction of characteristics from the
    underlying data.
*** Classification
    :PROPERTIES:
    :CUSTOM_ID: ml-class
    :END:
    #+INDEX: classification
    #+INDEX: machine learning!classification
    Once the features' values are extracted, instances can be
    assigned[fn::Here, we deal only with classification. Another
    application is regression, where a real-valued function is
    estimated.] to a class[fn::some classifiers also allow the
    computation of class probabilities, etc].

    In website fingerprinting, each instance is a numerical input
    vector, which can be directly used by many classifiers. From
    these, most classifiers, such as [[*Support Vector Machines][support vector machines]], form a
    model from which further input data can be classified. Others,
    such as [[*K-Nearest-Neighbor-Classifier][k-Nearest-Neighbors]] classify directly without an
    intermediary model.
*** Measuring Performance
    :PROPERTIES:
    :CUSTOM_ID: ml-measure
    :END:
    #+INDEX: Accuracy (acc)
    #+INDEX: Area Under Curve
    #+INDEX: AUC
    #+INDEX: AUC$_{0.01}$
    #+INDEX: AUC!bounded
    #+INDEX: confusion matrix
    #+INDEX: False Positive Rate
    #+INDEX: fpr
    #+INDEX: Receiver Operating Characteristic curve
    #+INDEX: ROC curve
    #+INDEX: True Positive Rate
    #+INDEX: tpr
    To evaluate website fingerprinting attacks and defenses, their
    success needs to be measured.[fn::this section's definitions
    follow \cite{powers}] Starting with binary classification, a
    /confusion matrix/ helps to illustrate the different cases that
    can occur. Each instance is categorized by its real and
    predicted class --- in website fingerprinting, whether it /is/ a
    sensitive website, and whether it is /classified/ as such. See
    Table [[tab:confusion_matrix]].

    #+CAPTION: Confusion matrix. Correctly classified cases are in bold.
    #+NAME: tab:confusion_matrix
    #+ATTR_LATEX: :align r|l l
    |             | real 1                | real 2                |
    |-------------+-----------------------+-----------------------|
    | predicted 1 | *True Positives (TP)* | False Positives (FP)  |
    | predicted 2 | False Negatives (FN)  | *True Negatives (TN)* |

    From these absolute values, various metrics can be derived. The
    main metrics used in website fingerprinting literature are
    /Accuracy/ (short: acc)[fn::accuracy is mostly used in the
    closed-world case], and /True/ and /False Positive Rate/ (short:
    tpr and fpr)[fn::tpr and fpr are mostly used in the open-world
    case]. These are defined as

    #+ATTR_LATEX: :align r c l
    | True Positive Rate  | := | $TP / (TP + FN)$                  |
    | False Positive Rate | := | $FP / (FP + TN)$                  |
    | Accuracy            | := | $(TP + TN) / (TP + FP + FN + TN)$ |

    To show the classifier strictness tradeoff, a /Receiver
    Operating Characteristic Curve/ (short: ROC-Curve) can be used.
    This diagram contrasts classifier tpr vs fpr, see Figure
    [[fig:roc-example]]. The /area under/ the /curve/ (short: AUC) can
    be measured. The closer this value is to 1, the better. If one
    is mainly interested in low fpr, the leftmost section of the
    ROC-curve is of particular interest. The area under the curve
    bounded up to a fpr value of 0.01 is denoted AUC_{0.01}.

    #+CAPTION[ROC curve example]: Example Receiver Operating Characteristic (ROC) curve. Source: \cite[sec.11.18.8]{scikit-user-guide}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:roc-example
    [[./pictures/plot_roc.png]]
** KEYWORDS Machine Learning Algorithms
     - algorithms in classification (?)
       - todo: ask kloft via mitarbeiter
*** Support Vector Machines
    #+INDEX: classifier!Support Vector Machine
    #+INDEX: classifier!SVM
    #+INDEX: Support Vector Machine
    #+INDEX: SVM
    #+INDEX: linear classifier
    #+INDEX: binary classification
    #+INDEX: classification!binary
    Support Vector Machines (short: SVM) are a /linear classifier/:
    they find a linear boundary between points, see Figure
    [[fig:linear_boundary]] for a simple example.

    #+CAPTION[Example binary linear classification]: Example binary linear classification from \cite[Figure 1.5]{iml}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:linear_boundary
    [[./pictures/iml_fig1.5.png]]

    [fn::This and the following parts are mostly based on
    \cite[ch.6f]{iml}] Given a set $X = \{x_1, ..., x_n\}$ with a dot
    product $\langle\cdot, \cdot\rangle: X \times X \to \mathbb R$ and tuples $(x_1, y_1), ...,
    (x_m, y_m)$, with $x_i \in X, y_i \in \{-1, 1\}$ as a /binary
    classification/ task.

    The SVM's job is to find a hyperplane[fn::as \cite[ch.4.1]{esl}
    mention, this is actually an affine set, as it need not pass
    through the origin. Keeping with tradition, it will be called
    hyperplane in this thesis (as long as those things formed by
    quarks are still called atoms \ldots).]
    #+BEGIN_LaTeX
      \[\{x \in X \mid \langle w, x \rangle +b = 0\}\]
    #+END_LaTeX
    such that $\langle w, x_i \rangle +b \ge 0$ whenever $y_i = 1$, and $\langle w, x_i \rangle
    +b < 0$ whenever $y_i = -1$. With added normalization, this can
    be compressed to the form \[y_i \cdot (\langle w, xi \rangle +b) \ge 1.\]
**** Soft Margin Classifiers
     :PROPERTIES:
     :CUSTOM_ID: soft-margin-svm
     :END:
     #+INDEX: margin
     #+INDEX: SVM!margin
     #+INDEX: soft-margin
     #+INDEX: SVM!soft-margin
     #+INDEX: classifier!soft-margin
     #+INDEX: C
     #+INDEX: SVM!C
     A support vector machine tries to find a hyperplane between two
     groups of points and maximize its distance to the closest points,
     called /margin/. What happens if the points lie such that a line
     cannot be found, as f.ex. in Figure [[fig:non-linear-data]]?

     #+CAPTION[Example simple non-linearly separable data]: Non-linearly separable data; source: \url{https://en.wikipedia.org/wiki/File:Separability_NO.svg}
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:non-linear-data
     [[./pictures/Separability_NO.eps]]

     To solve this, a /soft-margin classifier/ introduces slack
     variables $\xi \ge 0$, which it tries to reduce while maximizing the
     margin.

     This alters the equations to $y_i( \langle w, xi \rangle +b) \ge 1 - \xi_i$ for the
     optimization problem

     \[\min_{w, b, \xi} \frac{1}{2} ||w||^2 + \frac{C}{m} \sum_{i=1..m} \xi_i\]

     The /error term/ $C$ weighs minimizing training errors against
     maximizing the margin\cite[sec.7.2.1]{iml}.
**** Multi-Class Strategies
     :PROPERTIES:
     :CUSTOM_ID: multi-class
     :END:
     #+INDEX: binary classification
     #+INDEX: multi-class classification
     #+INDEX: classification!binary
     #+INDEX: classification!multi-class
     The SVMs as described above solve the binary classification
     problem \cite[sec.1.1.3]{iml}: they propose a boundary between
     two classes of objects.

     In website fingerprinting[fn::as in f.ex.\space{}handwriting
     recognition], there are most often more classes than two.

     Several strategies exist to distinguish more than two
     classes. The main are to train one classifier for each class ---
     called /One-Vs-Rest/ (OVR) --- and one for each class-class
     combination --- called /One-Vs-One/ (OVO). One-Vs-Rest trains
     fewer classifiers, while One-Vs-One trains more, but evaluates
     fewer samples per fitting.\cite[sec.4.12.3]{scikit-user-guide}.
**** Kernel Trick
     #+INDEX: kernel
     #+INDEX: kernel!radial basis function
     #+INDEX: kernel!RBF
     Straight lines do not always distinguish classes correctly, as
     f.ex. example in Figure [[hastie_kerneltrick]]. This would seem a
     drawback to using Support Vector Machines, yet they can compute
     these not only on the original data, but also on a projected
     space. This allows for complex decision boundaries. By using the
     kernel trick\cite[sec.2.2.2]{kernels}[fn::A kernel is a function
     with specific properties. The dot product is such a kernel. The
     kernel trick enables a algorithm with a kernel to use any other
     kernel], a SVM can not only use the dot product $\langle.,.\rangle$, but
     another kernel $k(., .)$ instead.

     #+CAPTION: Kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left side shows linear boundaries on $X$ and $Y$ --- the right side linear boundaries computed with added $X^2$, $Y^2$ and $XY$.
     #+NAME: hastie_kerneltrick
     [[./pictures/hastie.png]]

     The kernel used by default by \cite{Hsu10apractical} for SVMs is
     the (gaussian) /radial basis function/ (short: RBF)
     kernel\cite[sec.2.3]{kernels} \[k(x, y) = \exp \left ( - { \|x -
     y\|^2 \over 2 \gamma^2 } \right )\] This is also used by
     \cite{panchenko2}. While the algorithms still finds a straight
     line in a projected space, the resulting decision boundaries in
     the original feature space are more varied.
**** Parameter Estimation
     #+INDEX: cross-validation
     #+INDEX: grid search
     #+INDEX: $\gamma$ (gamma)
     #+INDEX: gamma
     Each [[#soft-margin-svm][soft margin classifier has an error term $C$]] which states
     how much to penalize outliers. The gaussian radial basis
     function kernel used by \cite{panchenko2} also has a $\gamma$ (gamma)
     term which varies the width of the area, see Figure
     [[fig:C-gamma-effect]].

     #+CAPTION[Example svm-rbf classification with different parameters for $C$ and \gamma]: Example svm-rbf classification with different parameters for $C$ and \gamma. Source \cite[Figure 42.328]{scikit-user-guide}, recreated for higher resolution.
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:C-gamma-effect
     [[./pictures/skl-fig-42.328.png]]
#     [[./pictures/skl-fig-42.328.eps]]

     These parameters have to be provided externally for the Support
     Vector Machine to achieve high
     accuracy. \cite[sec.3.2]{Hsu10apractical} recommend grid-search
     with cross-validation to find optimal parameters.

     In /grid-search/, several parameters of $C$ and $\gamma$ are
     evaluated, and the best one, depending on the metric, is chosen.

     In /cross-validation/, the data set is split into $k$ disjoint
     subsets, called /folds/, of equal size. Of those, $k-1$ are used
     combinedly for training the classifier, while the last is used
     for prediction evaluation. This is done $k$ times, and averaged
     for the result.

     It might be possible to evaluate these meta-parameters together
     with the main classification problem \cite[secs.2.8.3, 6.7]{esl},
     but analogously to \cite[sec.2.8.3]{esl}, this would probably
     become "combinatorially hard".

*** K-Nearest-Neighbor-Classifier
    #+INDEX: classifier!kNN
    #+INDEX: classifier!k-nearest-neighbors
    #+INDEX: kNN
    #+INDEX: k-nearest-neighbors
    The /k-nearest-neighbors/ (short: kNN) classifier
    (\cite[sec.1.3.2]{iml} \cite[sec.13.3]{esl}
    \cite[sec.8.2]{mitchell}) classifies data points based on the
    known class[es] of their neighbors: for each item to be
    classified, determine the (f.ex.\space{}k=5) closest neighbors by a
    given metric. If all neighbors' classes agree, or based on a
    majority decision, the item's class is set to theirs. See Figure
    [[fig:knn-example]].

    It is successful "in a large number of classification and
    regression problems"\cite[sec.4.6]{scikit-user-guide}, despite its
    simplicity.

    This classifier works best if all classes have the same number of
    (training) instances. Otherwise, it is of course probable that the
    classes with the higher number of instances will be chosen as
    targets of classification more often.

     #+CAPTION[k-nearest-neighbors illustrated]: The left picture shows the five closest neighbors around the test instance $x_q$, which is then classified as =-=. The right shows the k==1-decision boundary around several training instances (the area where a test instance would be classified as the point). Source \cite[Figure 8.1]{mitchell}
     #+NAME: fig:knn-example
     #+ATTR_LATEX: :width 0.7\textwidth
     [[./pictures/mitchell-fig8.1.png]]
** KEYWORDS Tor Website Fingerprinting Defenses
- discussion of state-of-the art defenses and drawbacks of these techniques
* gtd-Novel Defense                                                 :ARCHIVE:
** purpose: appetize and explain, finish thesis acceptably, defend Tor's users from this attack, get it into mainline Tor as an option/make it well-enough known, get an interesting job
** limits: see [[file:~/da/da.org::#limits][limits]]
** vision: reads, understands concept, is convinced
** brainstorm
   - remember drawbacks
     - hard to install/opt-in/configure
       - needs to opt-in
     - high bandwidth and/or latency
       - better results than Tamaraw, maybe even than others
   - new defense
     - based on Addon-SDK
       - possible to use WebExtension, but Tor's base is ESR version,
         which takes some time to get them
       - really easy to install/configure
     - based on stats model
       - creates HTML-like traffic
     - [possible to cache sizes using (new) stochastic data structure]
       - but currently needs to be tuned by hand, see future work
** organize
   1. j
* Novel Defense
  :PROPERTIES:
  :CUSTOM_ID: ch3-newdefense
  :END:
* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: ch4-evaluation
  :END:

* Discussion
  :PROPERTIES:
  :CUSTOM_ID: ch5-discussion
  :END:

* Conclusion
  :PROPERTIES:
  :CUSTOM_ID: ch6-conclusion
  :END:
\appendix
\part{Appendix}
* appendices (begin above this headline; this is for searching)     :ARCHIVE:
  above, as in this section cuts it out (due to ARCHIVE tag)
* Appendices
\bibliography{docs/master}
\bibliographystyle{plain}
\input{diplomarbeit.ind}
* END: /above/ this headline are INDEX, and BIBLIOGRAPHY, etc       :ARCHIVE:
