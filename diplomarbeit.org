#+TITLE: Selective Cover Traffic
#+TODO: KEYWORDS WRITE CHECK | EVA DANIEL FINAL
#+TODO: RECHECK | EVA-AGAIN DANIEL FINAL
#+TODO: | EVA_BUT_NEEDS_SUBPARTS
#+TODO: | DANIEL_BUT_NEEDS_SUBPARTS
#+TODO: TODO | PENDING
#+TODO: WAIT | APPENDIX_DONE WAIT_FINISH
\pagenumbering{roman}
\listoffigures
\listoftables
* Configuration							    :ARCHIVE:
#+LATEX_CLASS: scrreprt
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{numprint}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \npdecimalsign{.}
#+LATEX_HEADER: \nprounddigits{2}
#+LATEX_HEADER: \npthousandthpartsep{}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure} \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.1 cm} % WAR: \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Defending against Tor Website Fingerprinting with Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.1cm} % WAR: \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {1.9 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte % mkreik <2016-07-11 Mo>: war {5 cm}
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \def\BState{\State\hskip-\ALG@thistlm}
#+LATEX_HEADER: \makeatother
#+OPTIONS: H:6
* Introduction
#+BEGIN_LaTeX
  \pagenumbering{arabic}
#+END_LaTeX
  #+INDEX: Tor
  #+INDEX: trace
  #+INDEX: website fingerprinting
  In the wake of both the Snowden revelations in the western world,
  and increased internet censorship in countries such as Iran,
  Saudi-Arabia, and China\cite{china}, more and more
  Internet users search for ways to keep online communication and web
  browsing both private and free of censorship.

  The /Tor/ project\cite{tor-design} provides this. It protects
  whistleblowers, journalists, the people in oppressive
  regimes\cite{jardine2016tor}, even the military, and regular
  internet users, against e.g.\space{}nation-states or businesses which want
  to follow user's online steps. It routes encrypted data traffic via
  intermediaries, obscuring who connects to whom.

  # NO re-flow here!!! broke href-link
  While basic Tor can be configured as a proxy for any kind of
  TCP\cite{rfc793} traffic, web browsing with Tor is
  user-friendly\cite{usability:weis2006} and needs very little
  configuration: you just need to
  \href{https://www.torproject.org/download/download-easy.html.en}{download the Tor Browser Bundle}. It connects automatically to the Tor
  network to cloak the client's web traffic.  \\
  # NO re-flow here!!!


  As has often been the case with anonymity and privacy, several
  parties try to attack Tor's protection. One mass-surveillance type
  of attack that can be carried out passively with low-level
  technology is /website fingerprinting/: The time and size of users'
  data packets (called /traces/) are recorded. See Figure
  \ref{fig:traces}. This recording is compared to previously recorded
  traces of known sites. While older attacks
  (\cite{ccsw09-fingerprinting}, \cite{Liberatore:2006}) just compared
  single packet sizes, possibly by hand, newer attacks
  (\cite{panchenko}, \cite{panchenko2}, \cite{realistic}) use machine
  learning to classify traces, achieving high accuracy rates under
  laboratory conditions.
#+BEGIN_LaTeX
\begin{figure}[htbp]
\includegraphics[width=0.24\textwidth]{./pictures/craigslist_org@1445352269.png}
\includegraphics[width=0.24\textwidth]{./pictures/craigslist_org@1445585277.png}
\includegraphics[width=0.24\textwidth]{./pictures/craigslist_org@1445486337.png}
\includegraphics[width=0.24\textwidth]{./pictures/craigslist_org@1445527033.png}
\\
\includegraphics[width=0.24\textwidth]{./pictures/facebook_com@1445350531.png}
\includegraphics[width=0.24\textwidth]{./pictures/facebook_com@1445422155.png}
\includegraphics[width=0.24\textwidth]{./pictures/facebook_com@1445425799.png}
\includegraphics[width=0.24\textwidth]{./pictures/facebook_com@1445429729.png}
\caption[Web trace data visualized]{Web trace data visualized. Box height signifies amount of data (positive incoming data, negative outgoing), width the duration until the next packet. The top shows \url{http://craigslist.org}, the bottom \url{http://facebook.com}.
While some similarity can be seen for each group, the "within-group" differences are quite big between each group's traces as well.}
\label{fig:traces}
\end{figure}
#+END_LaTeX

  Web fingerprinting effectively deanonymizes the traffic that users
  thought secure, exposing for example a dissident to his nation
  state, nullifying this part of Tor's protection. To protect against
  this, most early attacks (\cite{Wagner96analysisof}, \cite{hintz02},
  \cite{ssl-traffic-analysis}) also proposed defenses. These evolved
  from /specific defenses/, f.ex. packet-size-altering methods
  (\cite{httpos}, \cite{morphing09}) to /general defenses/, which
  transform groups of web retrievals so that all members look the
  same.

  At the start of the thesis, there existed mostly deterministic
  general defenses with high overhead (f.ex. over 220% bandwidth, and
  300% time\cite{a-systematic}). During the course of this thesis, the
  stochastic defenses of \cite{wang2015walkie} and \cite{wtfpad}, with
  much lower overhead, were developed. This validates a stochastic
  approach, yet improvements seem possible in two areas: (1) ease of
  installation, and configurability, and (2) more-closely fitting
  cover traffic generation.

  Ad 1: \cite{wang2015walkie} alters the Firefox binary, while the
  current version of \cite{wtfpad} needed much manual adjustment in
  our attempts.

  Ad 2: \cite{wang2015walkie} uses either a normal, or lognormal
  distribution, not adjusting to HTTP-specifics, while \cite{wtfpad}
  samples packets at the network layer. It aims at generally hiding
  /that traffic occurs/, not just which website is visited, as it
  derives its basic mechanism from \cite{ShWa-Timing06}.
** Thesis Contribution
   This thesis presents and tests a new defense against website
   fingerprinting. This new defense mimics HTTP\cite{rfc7230}-shaped
   cover traffic: Each web page retrieval is augmented by
   stochastically-drawn dummy HTTP traffic\cite{newtrafficmodel}. This
   could optimize the protection offered for given bandwidth
   overhead. It is implemented in a browser extension, which makes the
   defense easier to install, configure, and maintain.
** Thesis Structure
   The following chapters try to solve the question whether the new
   defense works more effectively than existing ones.

   Chapter [[#ch2-background]] provides basic background for the IT-savvy
   who have not yet encountered Tor, machine learning, or website
   fingerprinting. For the Tor network, we treat its basic structure
   and why website fingerprinting might be a credible threat. Machine
   learning basic steps and algorithms are briefly skimmed. Finally,
   website fingerprinting on Tor is presented. These parts can safely
   be skipped given previous knowledge.

   The defense's why and how (motivation and design) is described in
   chapter [[#ch3-newdefense]]. This also describes the bloom sort data
   structure for stochastically saving object sizes.

   Chapter [[#ch4-evaluation]] evaluates the defense. It first describes
   the data-gathering process. Next, the website fingerprinting
   attacks of \cite{panchenko2}, and \cite{ccsw09-fingerprinting} are
   validated on defenseless data. This is followed by the evaluation
   on data with cloaking.

   These results are discussed and compared to literature results of
   \cite{wang2015walkie} and \cite{wtfpad} in chapter [[#ch5-discussion]].

   Chapter [[#ch6-conclusion]] summarizes the results, shows a path to
   implementation, with both included and additional further work.
* DANIEL_BUT_NEEDS_SUBPARTS Background [9/9]                     :nextaction:
  :PROPERTIES:
  :CUSTOM_ID: ch2-background
  :END:
  Knowing [[#sub2-tor][the Tor network]], [[#sub2-ml][machine-learning basics]] and [[#sub2-wf][previous
  attacks and defenses]] helps to understand and then counter website
  fingerprinting.
** DANIEL The Tor Network
   :PROPERTIES:
   :CUSTOM_ID: sub2-tor
   :END:
   #+INDEX: onion router
   #+INDEX: onion proxy
   #+INDEX: Tor!onion router
   #+INDEX: Tor!onion proxy
   #+INDEX: Tor!the onion router
   #+CAPTION: \href{https://www.torproject.org/about/overview.html.en}{Connection through the Tor network}.
   #+NAME: fig:tor-network
   #+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {r}{0.4\textwidth}
   [[./pictures/htw2.png]]

   To protect online anonymity, Tor\cite{tor-design} uses a
   semi-distributed infrastructure: volunteer-run /onion routers/ pass
   the messages, while their structure is provided by fixed /directory
   servers/. A path through the onion routers is negotiated by the
   /onion proxy/, a SOCKS\cite{rfc1928}-TCP proxy which most often
   runs on the client computer.

   The onion proxy selects three globally-distributed hops to
   initialize a connection. It makes a connection to the first,
   establishes encryption, asks the first hop to make a connection to
   the second, sets up encryption to this, and from there to the
   third. The third hop establishes a connection to its destination.

   Each message is encrypted three times using same-length encryption
   and sent along this path. The first router decrypts the first
   layer, and so on, like layers of an onion. This explains Tor's name
   /the onion router/.

   As a result of this setup, each hop can only see its direct
   neighbors along the path. Even if one hop of a three-hop setup is
   compromised, directly linking source and destination becomes pretty
   hard.
** DANIEL Tor Website Fingerprinting
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf
   :END:
   #+INDEX: traffic analysis
   #+INDEX: website fingerprinting
   #+INDEX: WF
   What does an adversary do if he cannot decrypt and the message
   traffic of a cryptographic system he is interested in? One
   alternative is to inspect the traffic itself for patterns. This
   process is called /traffic analysis/\cite{introta} and yields much
   useful information\cite{applied96}.

   /Website fingerprinting/ (WF) needs only message meta-data:
   who sends how much data when. It assumes that the system itself is
   computationally secure\cite{applied96}: there are not enough
   resources, time, or data to break it. Analysing traffic patterns
   can circumvent the system. Anyone who can see the data stream can
   carry out this attack, without anyone else learning about
   this. They simply need to capture the data stream using f.ex. the
   =tcpdump=\cite{tcpdump8-manual} tool.\\

   From inception\cite{tor-design}, Tor provided defenses against
   traffic analysis. For one, all /data/ cells have the same size,
   which protects against identifying them by size only. Tor also
   multiplexes all its data traffic into a single stream, making it
   hard to distinguish the multiple streams that most websites
   require, let alone parallel retrieval. Tor also
   unavoidably\cite{rfc1925} increases traffic latency, so that
   attacks have a harder time relying on interpacket
   timing\cite{challenges}.\\


   This made WF harder, to the point that it was was mentioned, but
   not hindered, in \cite{tor-design}. It took five years for
   \cite{ccsw09-fingerprinting} to show better than random
   classification results against Tor traffic. This evolved to
   state-of-the-art methods like \cite{panchenko2}.

   What all methods have in common is that they extract numerical
   /features/ from the raw data, which is then classified using
   machine learning.
** DANIEL_BUT_NEEDS_SUBPARTS Machine Learning
   :PROPERTIES:
   :CUSTOM_ID: sub2-ml
   :END:
   #+INDEX: machine learning
   #+INDEX: ML
   Let us review basics of /Machine Learning/ (ML): a
   computer\cite{turing1936a} algorithm extracts and generalizes
   patterns from learning data.\cite{rieckdiss} This is then used to
   classify further patterns (f.ex. for handwriting recognition),
   or to act on the generalizations (f.ex. for self-driving cars).

   The machine learning process consists of at least two separate
   steps: feature extraction and classification. Domain-specific
   [[#ml-features][feature extraction]] transforms the raw input data --- in our case,
   website traces --- into characteristics --- in our case, numbers,
   f.ex. the number of outgoing packets. [[#ml-class][Classification]] then
   generalizes and assigns these characteristics into categories.

   A last section presents [[#ml-measure][measures to evaluate machine learning
   performance]].
*** DANIEL Feature Extraction
    :PROPERTIES:
    :CUSTOM_ID: ml-features
    :END:
    #+INDEX: feature extraction
    #+INDEX: machine learning!feature extraction
    WF tries to analyse web traces. To be able to defend against WF,
    it should first be understood. To be understood, it should be
    reproduced.

    WF input data needs to be wrangled for the classification to work:
    extra information that might change from request to request ---
    such as the hosts IP address, or the absolute time of the
    retrieval --- needs to be removed or unified to a common
    format. The trick is as always: keeping the signals and discarding
    the noise, (also called "reducing intra-class variability while
    increasing inter-class variability").

    The source data in website fingerprinting are traces (f.ex. in the
    =pcap=\cite{pcap-manual} format). From this, only the size,
    direction and timing of each data item is extracted. The size of
    files is hidden by the traffic's encryption; the closest
    approximation is the size of each TLS record.

    Feature extraction\cite[sec.1.3.1]{duda} transforms (preprocessed)
    input data into features/characteristics suitable for
    classification.


    \cite{ccsw09-fingerprinting} follows \cite{hintz02}, and uses
    packet sizes for features. They use a jaccard metric as
    classifier, but as seen in chapter [[#ch4-evaluation]], nothing but
    sizes can yield surprisingly good results in combination with
    support vector machines, despite Tor's fixed data cell size.\\

    #+CAPTION[CUMUL features example]: CUMUL\cite{panchenko2} \href{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}{features example}
    #+ATTR_LATEX: :float wrap :width 0.5\textwidth :placement {r}{0.55\textwidth}
    #+NAME: CUMUL_traces
    [[./pictures/cumul_aus_paper.pdf]]

    \cite{panchenko2} uses a cumulative size metric, CUMUL. As seen in
    Figure [[CUMUL_traces]], these provide a graphical representation of
    traces, while still allowing for computer-based comparison after
    normalization. This is also evaluated with support vector
    machines.\\

    There are numerous WF attacks. F.ex. \cite{effective} achieved
    accuracy improvements using over 3000 features and a
    KNN-classification with weighted metrics while
    \cite{kfingerprint}'s approach uses approximately 150 features
    with Random Forest\cite{DBLP:journals/ml/Breiman01} classification.\\

    While the boundary of feature extraction to classification is
    "somewhat arbitrary"\cite[sec.1.3.1]{duda}, feature extraction
    deals with the, well, extraction of characteristics from the
    underlying data.
*** EVA Classification
    :PROPERTIES:
    :CUSTOM_ID: ml-class
    :END:
    #+INDEX: classification
    #+INDEX: training
    #+INDEX: classification!training
    #+INDEX: classification!test
    #+INDEX: machine learning!classification
    The feature extraction step converted raw input data to a feature
    vector. In classification, some of these are for training the
    classifier. Others for testing.

    In classifier /training/, the classifier gets as input several
    feature vectors and their respective classes and tries to
    generalize a relationship.

    In actual /classification/, the classifier only receives an input
    feature vector, and needs to predict the class label: the web
    page.\\


    Most classifiers, such as [[*Support Vector Machines][support vector machines]], form an
    internal model from which further input data is
    classified. Others, notably [[*K-Nearest-Neighbor-Classifier][k-Nearest-Neighbors]], classify directly
    without an intermediary model.
*** DANIEL Measuring Performance                                 :nextaction:
    :PROPERTIES:
    :CUSTOM_ID: ml-measure
    :END:
    #+INDEX: Accuracy (acc)
    #+INDEX: Area Under Curve
    #+INDEX: AUC
    #+INDEX: AUC$_{0.01}$
    #+INDEX: AUC!bounded
    #+INDEX: confusion matrix
    #+INDEX: False Positive Rate
    #+INDEX: fpr
    #+INDEX: Receiver Operating Characteristic curve
    #+INDEX: ROC curve
    #+INDEX: True Positive Rate
    #+INDEX: tpr
    To find out if WF attacks work, and if defenses prevent this,
    their success needs to be measured.

    A /confusion matrix/\cite{powers} helps to illustrate the
    different cases that can occur in WF. Each trace is categorized by
    whether it /is/ a sensitive website, and whether it is
    /classified/ as such. See Table [[tab:confusion_matrix]].

    #+CAPTION: Confusion matrix. Correctly classified traces are in bold.
    #+NAME: tab:confusion_matrix
    #+ATTR_LATEX: :align |l||l | l|
    |----------------------+-----------------------+-----------------------|
    | <20>                 |                       |                       |
    |                      | real wikileaks.org    | real facebook.com     |
    |----------------------+-----------------------+-----------------------|
    | predicted as wikileaks.org | *True Positives (TP)* | False Positives (FP)  |
    | predicted as facebook.com | False Negatives (FN)  | *True Negatives (TN)* |
    |----------------------+-----------------------+-----------------------|

    From these counts, other metrics can be derived. The main metrics
    used in WF literature are /Accuracy/ (acc), and /True-/ and
    /False-Positive-Rate/ (tpr and fpr). These are defined as

    #+ATTR_LATEX: :align r c l
    | True Positive Rate  | := | $TP / (TP + FN)$                  |
    | False Positive Rate | := | $FP / (FP + TN)$                  |
    | Accuracy            | := | $(TP + TN) / (TP + FP + FN + TN)$ |

    To show the classifier strictness tradeoff, a /Receiver
    Operating Characteristic Curve/ (ROC-Curve) can be used.
    This diagram contrasts classifier tpr vs fpr, see Figure
    [[fig:roc-example]]. The /area under/ the /curve/ (AUC) can
    be measured. The closer this value is to 1, the better. If one
    is mainly interested in low fpr, the leftmost section of the
    ROC-curve is of particular interest. The area under the curve
    bounded up to a fpr value of 0.01 is denoted AUC_{0.01}.

    #+CAPTION[ROC curve example]: Example Receiver Operating Characteristic (ROC) curve \cite[sec.11.18.8]{scikit-user-guide}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:roc-example
    [[./pictures/plot_roc.png]]
** PENDING Machine Learning Algorithms
     - algorithms in classification (?)
       - todo: ask kloft via mitarbeiter
*** Support Vector Machines
    #+INDEX: classifier!Support Vector Machine
    #+INDEX: classifier!SVM
    #+INDEX: Support Vector Machine
    #+INDEX: SVM
    #+INDEX: linear classifier
    #+INDEX: binary classification
    #+INDEX: classification!binary
    /Support Vector Machines/ (SVMs) are a linear classifier:
    they find a linear boundary between points, see Figure
    [[fig:linear_boundary]] for a simple example.

    #+CAPTION[Example binary linear classification]: Example binary linear classification from \cite[Figure 1.5]{iml}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:linear_boundary
    [[./pictures/iml_fig1.5.png]]

    [fn::This and the following parts are mostly based on
    \cite[ch.6f]{iml}] Given a set $X = \{x_1, ..., x_n\}$ with a dot
    product $\langle\cdot, \cdot\rangle: X \times X \to \mathbb R$ and tuples $(x_1, y_1), ...,
    (x_m, y_m)$, with $x_i \in X, y_i \in \{-1, 1\}$ as a /binary
    classification/ task.

    The SVM's job is to find a hyperplane[fn::as \cite[ch.4.1]{esl}
    mention, this is actually an affine set, as it need not pass
    through the origin. Keeping with tradition, it will be called
    hyperplane in this thesis (as long as those things formed by
    quarks are still called atoms \ldots).]
    #+BEGIN_LaTeX
      \[\{x \in X \mid \langle w, x \rangle +b = 0\}\]
    #+END_LaTeX
    such that $\langle w, x_i \rangle +b \ge 0$ whenever $y_i = 1$, and $\langle w, x_i \rangle
    +b < 0$ whenever $y_i = -1$. With added normalization, this can
    be compressed to the form \[y_i \cdot (\langle w, xi \rangle +b) \ge 1.\]
**** Soft Margin Classifiers
     :PROPERTIES:
     :CUSTOM_ID: soft-margin-svm
     :END:
     #+INDEX: margin
     #+INDEX: SVM!margin
     #+INDEX: soft-margin
     #+INDEX: SVM!soft-margin
     #+INDEX: classifier!soft-margin
     #+INDEX: C
     #+INDEX: SVM!C
     A support vector machine tries to find a hyperplane between two
     groups of points and maximize its distance to the closest points,
     called /margin/. What happens if the points lie such that a line
     cannot be found, as f.ex. in Figure [[fig:non-linear-data]]?

     #+CAPTION[Example simple non-linearly separable data]: Non-linearly separable data; source: \url{https://en.wikipedia.org/wiki/File:Separability_NO.svg}
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:non-linear-data
     [[./pictures/Separability_NO.eps]]

     To solve this, a /soft-margin classifier/ introduces slack
     variables $\xi \ge 0$, which it tries to reduce while maximizing the
     margin.

     This alters the equations to $y_i( \langle w, xi \rangle +b) \ge 1 - \xi_i$ for the
     optimization problem

     \[\min_{w, b, \xi} \frac{1}{2} ||w||^2 + \frac{C}{m} \sum_{i=1..m} \xi_i\]

     The /error term/ $C$ weighs minimizing training errors against
     maximizing the margin\cite[sec.7.2.1]{iml}.
**** Multi-Class Strategies
     :PROPERTIES:
     :CUSTOM_ID: multi-class
     :END:
     #+INDEX: binary classification
     #+INDEX: multi-class classification
     #+INDEX: classification!binary
     #+INDEX: classification!multi-class
     The SVMs as described above solve the binary classification
     problem \cite[sec.1.1.3]{iml}: they propose a boundary between
     two classes of objects.

     In website fingerprinting[fn::as in f.ex.\space{}handwriting
     recognition], there are most often more classes than two.

     Several strategies exist to distinguish more than two
     classes. The main are to train one classifier for each class ---
     called /One-Vs-Rest/ (OVR) --- and one for each class-class
     combination --- called /One-Vs-One/ (OVO). One-Vs-Rest trains
     fewer classifiers, while One-Vs-One trains more, but evaluates
     fewer samples per fitting.\cite[sec.4.12.3]{scikit-user-guide}.
**** Kernel Trick
     #+INDEX: kernel
     #+INDEX: kernel!radial basis function
     #+INDEX: kernel!RBF
     #+INDEX: radial basis function kernel
     #+INDEX: RBF kernel
     Straight lines do not always distinguish classes correctly, as
     f.ex. example in Figure [[hastie_kerneltrick]]. This would seem a
     drawback to using Support Vector Machines, yet they can compute
     these not only on the original data, but also on a projected
     space. This allows for complex decision boundaries. By using the
     kernel trick\cite[sec.2.2.2]{kernels}[fn::A kernel is a function
     with specific properties. The dot product is such a kernel. The
     kernel trick enables a algorithm with a kernel to use any other
     kernel], a SVM can not only use the dot product $\langle.,.\rangle$, but
     another kernel $k(., .)$ instead.

     #+CAPTION: Kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left side shows linear boundaries on $X$ and $Y$ --- the right side linear boundaries computed with added $X^2$, $Y^2$ and $XY$.
     #+NAME: hastie_kerneltrick
     [[./pictures/hastie.png]]

     The kernel used by default by \cite{Hsu10apractical} for SVMs is
     the (gaussian) /radial basis function/ (RBF)
     kernel\cite[sec.2.3]{kernels} \[k(x, y) = \exp \left ( - { \|x -
     y\|^2 \over 2 \gamma^2 } \right )\] This is also used by
     \cite{panchenko2}. While the algorithms still finds a straight
     line in a projected space, the resulting decision boundaries in
     the original feature space are more varied.
**** Parameter Estimation
     #+INDEX: cross-validation
     #+INDEX: grid search
     #+INDEX: $\gamma$ (gamma)
     #+INDEX: gamma
     Each [[#soft-margin-svm][soft margin classifier has an error term $C$]] which states
     how much to penalize outliers. The gaussian radial basis
     function kernel used by \cite{panchenko2} also has a $\gamma$ (gamma)
     term which varies the width of the area, see Figure
     [[fig:C-gamma-effect]].

     #+CAPTION[Example svm-rbf classification with different parameters for $C$ and \gamma]: Example svm-rbf classification with different parameters for $C$ and \gamma. Source \cite[Figure 42.328]{scikit-user-guide}, recreated for higher resolution.
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:C-gamma-effect
     [[./pictures/skl-fig-42.328.png]]
#     [[./pictures/skl-fig-42.328.eps]]

     These parameters have to be provided externally for the Support
     Vector Machine to achieve high
     accuracy. \cite[sec.3.2]{Hsu10apractical} recommend grid-search
     with cross-validation to find optimal parameters.

     In /grid-search/, several parameters of $C$ and $\gamma$ are
     evaluated, and the best one, depending on the metric, is chosen.

     In /cross-validation/, the data set is split into $k$ disjoint
     subsets, called /folds/, of equal size. Of those, $k-1$ are used
     combinedly for training the classifier, while the last is used
     for prediction evaluation. This is done $k$ times, and averaged
     for the result.

     It might be possible to evaluate these meta-parameters together
     with the main classification problem \cite[secs.2.8.3, 6.7]{esl},
     but analogously to \cite[sec.2.8.3]{esl}, this would probably
     become "combinatorially hard".

*** K-Nearest-Neighbor-Classifier
    #+INDEX: classifier!kNN
    #+INDEX: classifier!k-nearest-neighbors
    #+INDEX: kNN
    #+INDEX: k-nearest-neighbors
    The /k-nearest-neighbors/ (kNN) classifier
    (\cite[sec.1.3.2]{iml} \cite[sec.13.3]{esl}
    \cite[sec.8.2]{mitchell}) classifies data points based on the
    known class[es] of their neighbors: for each item to be
    classified, determine the (f.ex.\space{}k=5) closest neighbors by a
    given metric. If all neighbors' classes agree, or based on a
    majority decision, the item's class is set to theirs. See Figure
    [[fig:knn-example]].

    It is successful "in a large number of classification and
    regression problems"\cite[sec.4.6]{scikit-user-guide}, despite its
    simplicity.

    This classifier works best if all classes have the same number of
    (training) instances. Otherwise, it is of course probable that the
    classes with the higher number of instances will be chosen as
    targets of classification more often.

     #+CAPTION[k-nearest-neighbors illustrated]: The left picture shows the five closest neighbors around the test instance $x_q$, which is then classified as =-=. The right shows the k==1-decision boundary around several training instances (the area where a test instance would be classified as the point). Source \cite[Figure 8.1]{mitchell}
     #+NAME: fig:knn-example
     #+ATTR_LATEX: :width 0.7\textwidth
     [[./pictures/mitchell-fig8.1.png]]
** DANIEL Tor Website Fingerprinting Defenses
   This section describes defenses against WF as described
   [[#sub2-wf][previously]]. As most Machine Learning, WF uses statistical
   properties of the underlying data. It could possibly be defeated by
   shuffling these properties. The total number of incoming packets
   f.ex. is a feature used by almost all modern attacks:

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION: distribution of number of total incoming packets
   #+NAME: fig:total_packets_in
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-06_tamaraw_wtf-pad___bridge--2016-07-05__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   As seen in Figure [[fig:total_packets_in]], \cite{wtfpad}'s publicly
   available implementation of his own WTF-PAD and
   Tamaraw\cite{a-systematic} both create additional packets, but
   preserve site separation and ordering. Contrast this with this
   thesis' defense in Figure [[fig:total_packets_in_thesis]]

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION[distribution of number of total incoming packets, thesis defense]: Distribution of number of total incoming packets, thesis defense. The 5aI setting has small overhead, 30aI has average overhead.
   #+NAME: fig:total_packets_in_thesis
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-21_0.22___5aI--2016-07-19_0.22___30aI--2016-07-25__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   Each site's variances have been increased with the result of them
   overlapping. This figure hints that this thesis' defense more
   effectively munges websites fingerprinting traces, and is also
   tunable.

   While it was possible to get \cite{wtfpad} to run given [[https://bitbucket.org/mjuarezm/obfsproxy_wfpadtools][the
   provided software]], \cite{wang2015walkie}'s state-of-the-art defense
   was compared using his reported values. As \cite{wang2015walkie}
   uses simple statistical distributions in addition to a novel
   single-duplex method, it could arguably be augmented by
   HTTP-specific cover traffic distributions.

   Another point is ease-of-installation: Firefox, on which the Tor
   Browser Bundle is based, enables extensions. These already provide
   some of the Tor Browser Bundle's
   functionality\cite{tor-browser-design-impl} and were used in this
   thesis' to ease installation as compared to \cite{wtfpad} and
   arguably also to \cite{wang2015walkie}.
** DANIEL Summary
   WF can deanonymize anonymous traffic. This can pose a huge problem
   f.ex. for whistleblowers. The previous sections gave a short
   introduction to the basics of Tor, WF attacks, its basis in machine
   learning, and finally defenses against it. It also gave a first
   glimpse at this thesis' new defense.

   The next section presents the novel defense in depth.
* WRITE Novel Defense
  :PROPERTIES:
  :CUSTOM_ID: ch3-newdefense
  :END:
  This thesis' defense aims to improve installation, HTTP-specific
  cover traffic and configurability.

  Ease-of-installation and configuring the amount of cover traffic is
  provided by using Firefox's Addon-SDK. Add-ons can be downloaded via
  [[https://addons.mozilla.org][Mozilla's Addon Page]] with automatic updates.

  The created traffic is based on the HTTP model by
  \cite{newtrafficmodel}: it sets a target size of the main HTML
  document and number of embedded objects. From this, the requests's
  actual HTML size and number of embedded objects are subtracted to
  set the amount of cover traffic for this web page. The first HTML
  request is covered by an additional HTML cover-request, and each
  additional request is covered by embedded cover-requests with a
  given probability.

#+BEGIN_LaTeX
\begin{algorithm}
\caption{Generate Cover Traffic}\label{covertraffic}
\begin{algorithmic}[1]
\Procedure{OnRequest}{$\textit{url}$}
\If {$! \textit{isRegistered}(\textit{hostnameOf}(\textit{url}))$}
  \State $\textit{targetHttpSize} \leftarrow \textit{randomHttpSize}()$
  \State $\textit{urlHttpSize} \leftarrow \textit{lookupOrGuessHttpSize}(\textit{url})$
  \State $\textit{coverHttpSize} \leftarrow \textit{targetHttpSize} - \textit{urlHttpSize}$
  \State $\textit{targetNumEmbedded} \leftarrow \textit{randomNumEmbedded}()$
  \State $\textit{urlNumEmbedded} \leftarrow \textit{lookupOrGuessNumEmbedded}(\textit{url})$
  \State $\textit{coverNumEmbedded} \leftarrow \textit{targetNumEmbedded} - \textit{urlNumEmbedded}$
  \State $\textit{requestCoverSized}(\textit{coverHttpSize})$
  \State $\textit{registerHost}(\textit{hostnameOf}(\textit{url}), \textit{coverNumEmbedded}, \textit{urlNumEmbedded})$
\Else
  \State $\textit{requestProbability} \leftarrow \textit{computeProbability}(\textit{hostnameOf}(\textit{url}))$
  \While {$\textit{requestProbability} > 1$}
    \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
    \State $\textit{updateHosts}(\textit{url})$
    \State $\textit{requestProbability} \leftarrow \textit{requestProbability} -1$
  \EndWhile
  \If {$\textit{withProbability}(\textit{requestProbability})$}
    \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
    \State $\textit{updateHosts}(\textit{url})$
  \EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
#+END_LaTeX

The algorithm randomly draws from appropriate HTTP-related
distributions.\cite{newtrafficmodel}

/registerHost/, /isRegistered/, /computeProbability/ and /updateHosts/
access a data structure that saves the recently active hosts with
their number of embedded objects and the cover requests that still
need to be sent.

The /lookupOrGuess.../-functions need data structures to map urls to
both HTTP sizes and number of embedded objects. These change over
time, just compare the topmost plots of Figures [[fig:total_packets_in]]
and [[fig:total_packets_in_thesis]]. As can be seen in the next chapter,
the amazingly good defense that was achieved with up-to-date data (see
next chapter) decreased over time to that of guessing. These data
structures use Bloom-filter\cite{Bloom70space/timetrade-offs} based
binning to save values related to urls in a fixed space, while not
allowing an adversary to exactly determine which urls are saved.
** purpose
appetize and explain, finish thesis acceptably, defend Tor's users from this attack, get it into mainline Tor as an option/make it well-enough known, get an interesting job
** limits: see [[file:~/da/da.org::#limits][limits]]
** vision: reads, understands concept, is engaged emotionally and convinced intellectually
** brainstorm
   - make clearer,
   - maybe separate section for bloom-stuff
     - copy from old thesis, at least look at
       - [[file:~/da/old.org::#cache_size][Choice: Cache (approximate) sizes using Bloom Filters]]
** next action:
* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: ch4-evaluation
  :END:
** purpose
   - main gtd
** limits: [[file:~/da/da.org::#limits][limits]]
** vision
   - after reading this chapter, users have a clear picture when the
     novel defense performs better and when worse than juarez' etc's
     implementation
** brainstorm
   - check that very-low-accuracy scenario (good) are dated right after
     implementing all (0.18/0.15)
   - herrmann: good accuracy: should be random guessing (Tor should
     hide sizes)
     - seems like connection establishment leaks data
** organize
* Discussion
  :PROPERTIES:
  :CUSTOM_ID: ch5-discussion
  :END:

* Conclusions
  :PROPERTIES:
  :CUSTOM_ID: ch6-conclusion
  :END:
** Future Work
*** purpose
*** limits
*** vision
*** brainstorm
    - addon-sdk replace by webextension
      - not that much to do
      - when/if necessary for Tor's ESR-version-based browser
      - advantage: also Google Chrome
    - seems like connection establishment leaks data, as of ch4





\appendix
\part{Appendix}
* appendices (begin above this headline; this is for searching)     :ARCHIVE:
  above, as in this section cuts it out (due to ARCHIVE tag)
* KEYWORDS [#C] The Base Rate Fallacy
  :PROPERTIES:
  :CUSTOM_ID: base-rate
  :END:
  - two stats-related: psych and IT/IDS
    - psych: kahneman+..., bar-hillel
    - IT/IDS: axelsson
  - bayes
    - hard for them, easy for us (?) bayes rate fallacy
      - axelsson
      - just need a few %
      - but: theoretical concept, better be a bit sceptical
        - \cite{koehler1996base} in general (original authors sceptical, too)
        - rieck\cite{rieckdiss} had success in IDS

   This knowledge helps in understanding and creating defenses. As of
   \cite{a-systematic}, \cite{ccs2014-critical} and \cite{panchenko2},
   [[#base-rate][the Base Rate Fallacy]] creates problems for /some/
   WF-adversaries. This means that finding people who might have
   accessed a certain site is easier than making sure that they really
   visited the site.
* After Appendices, Bibliography and Index
\bibliography{docs/master}
\bibliographystyle{plain}
\input{diplomarbeit.ind}
* END: /above/ this headline are INDEX, and BIBLIOGRAPHY, etc       :ARCHIVE:
* gtd                                                 :ARCHIVE:
