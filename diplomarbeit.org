#+TITLE: Selective Cover Traffic
#+PRIORITIES: A D B
#+TODO: KEYWORDS WRITE CHECK | DANIEL FINAL
#+TODO: RECHECK | DANIEL FINAL
#+TODO: | DANIEL_BUT_NEEDS_SUBPARTS
#+TODO: TODO | PENDING
\listoffigures
\listoftables
\printnoidxglossaries
* Configuration							    :ARCHIVE:
#+BIBLIOGRAPHY: master plain option:-d
#+LATEX_CLASS: scrreprt
#+LATEX_CLASS_OPTIONS: [a4paper,12pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{booktabs} % for \toprule
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage[tracking=true]{microtype}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{numprint}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
# glossaries after hyperref
#+LATEX_HEADER: \usepackage{glossaries}
#+LATEX_HEADER: \makenoidxglossaries
#+LATEX_HEADER: \setacronymstyle{long-short}
#+LATEX_HEADER: \newacronym{ml}{ML}{machine learning}
#+LATEX_HEADER: \newacronym{wf}{WF}{website fingerprinting}
#+LATEX_HEADER: \newacronym{tn}{TN}{true negatives}
#+LATEX_HEADER: \newacronym{fn}{FN}{false negatives}
#+LATEX_HEADER: \newacronym{fp}{FP}{false positives}
#+LATEX_HEADER: \newacronym{tp}{TP}{true positives}
# end glossaries
#+LATEX_HEADER: \pagenumbering{roman}
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \npdecimalsign{.}
#+LATEX_HEADER: \nprounddigits{2}
#+LATEX_HEADER: \npthousandthpartsep{}
#+LATEX_HEADER: \makeindex
# begin HU preset
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \newlength{\parindentbak} \setlength{\parindentbak}{\parindent}
#+LATEX_HEADER: \newlength{\parskipbak} \setlength{\parskipbak}{\parskip}
#+LATEX_HEADER: \setlength{\parindent}{0pt}
#+LATEX_HEADER: \setlength{\parskip}{\baselineskip}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{minipage}[c][3cm][c]{12cm}
#+LATEX_HEADER: \textsc{%
#+LATEX_HEADER: % optischer Randausgleich per Hand:
#+LATEX_HEADER: \hspace{-0.4mm}\textls*[68]{\Large Humboldt-Universität zu Berlin}\\
#+LATEX_HEADER: \normalsize \textls*[45]{
#+LATEX_HEADER: Mathematisch-Naturwissenschaftliche Fakultät\\
#+LATEX_HEADER: Institut für Informatik
#+LATEX_HEADER: }
#+LATEX_HEADER: }
#+LATEX_HEADER: \end{minipage}
#+LATEX_HEADER: \hfill
#+LATEX_HEADER:
#+LATEX_HEADER: \sffamily
#+LATEX_HEADER:
#+LATEX_HEADER: \vfill
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER: \begin{doublespace}
#+LATEX_HEADER: \vspace{\baselineskip}
#+LATEX_HEADER: {\LARGE \textbf{Defending against Tor Website Fingerprinting with Selective Cover Traffic}}\\
#+LATEX_HEADER: %\vspace{1\baselineskip}
#+LATEX_HEADER: {\Large
#+LATEX_HEADER: Diplomarbeit\\
#+LATEX_HEADER: zur Erlangung des akademischen Grades\\
#+LATEX_HEADER: Diplominformatiker
#+LATEX_HEADER: \vspace{\baselineskip}
#+LATEX_HEADER: }
#+LATEX_HEADER: \end{doublespace}
#+LATEX_HEADER: \end{center}

#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ %
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft (Universität  Kaiserslautern)&& \\%
#+LATEX_HEADER: 				 %
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \def\BState{\State\hskip-\ALG@thistlm}
#+LATEX_HEADER: \makeatother
#+OPTIONS: H:6
* Introduction
#+BEGIN_EXPORT latex
\pagenumbering{arabic}
#+END_EXPORT
  #+INDEX: Tor
  #+INDEX: website fingerprinting
  #+INDEX: fingerprint
  Imagine you are talking in private to a good friend. You feel that
  you can talk about things you would avoid otherwise, as you are in a
  private space. How would you feel if the topics of conversation
  became known to others? Maybe it was due to your friend, maybe
  someone eavesdropped. Maybe you would stop trusting, either only the
  feeling of privacy, or also your good friend.


  Many parties like to observe users' online browsing: shops and
  advertisers want to tailor advertisement to focus groups; people who
  share your WLAN might just be curious; governments could aim for
  dragnet crime prevention, steering public discourse, and sometimes
  censorship; criminals want to create better scams. Against all
  these, Tor \citep{tor-design} helps individuals protect their online
  privacy. They can browse censored information \citep{jardine2016tor},
  and publish e.g. leaks without fear of retaliation. Several
  constitutions implicitly \citep{katz} or explicitly
  \citep[Art.10]{grundgesetz} acknowledge that communication privacy
  is fundamental to an open society \citep[ch.10]{popper2012open}. Tor
  protects many and diverse parts of society, such as journalists,
  businessmen, and military units deployed abroad.


  Tor \citep{tor-design} works by creating an encrypted path through
  its network of servers. These servers are run by volunteers located
  all over the world. A local software on a client's computer
  negotiates this path step-by-step: It uses encryption to ensure that
  none of the forwarding servers can know the full path of the
  message. A response from, say, a web server is sent back to the
  local software along the same path. This is how Tor makes it
  possible for two parties to communicate anonymously over the
  internet.


  People need to trust that Tor truly protects their online
  privacy \citep{challenges}. The more they doubt this, the less they
  would use Tor, or the more they would self-censor their online
  activities. One as-of-yet-unfixed vulnerability in Tor is
  \gls{wf}. In \gls{wf}, the sizes and timing of each data packet for
  a web page retrieval are collected to form a /fingerprint/. This is
  matched to other  observed retrievals to predict which page was
  visited. Originally, \Gls{wf} was briefly mentioned by
  \citet{SSL}. \citet{panchenko} were the first to successfully attack
  Tor by \gls{wf} under laboratory conditions. The Tor project
  encourages research into its \gls{wf} attacks and
  defenses \citep{experimental}.


  Various defenses have been proposed so far (\citet{hintz02},
  \citet{morphing09}, \citet{httpos2011},
  \citet{oakland2012-peekaboo}, \citet{effective2014},
  \citet{a-systematic2014}, \citet{wtfpad2015} \citet{wang2015walkie},
  \citet{DBLP:journals/popets/CherubinHJ17}). None have been deployed
  with the Tor Browser. The reasons are manifold: Defenses delayed
  \citep{brutlag2009speed} web browsing, required tuning by hand or
  changes to the Tor code base, complicated setup, were only presented
  conceptually, etc. Still, \gls{wf} attacks have continued to improve
  (\citet{quantifying}, \citet{ssl-traffic-analysis}, \citet{hintz02},
  \citet{ccsw09-fingerprinting}, \citet{panchenko},
  \citet{ccs2012-fingerprinting}, \citet{effective2014},
  \citet{panchenko2}, \citet{197185}).
** Contributions: a New Website Fingerprinting Defense and Tools
   The thesis tries to find if a HTTP-specific defense increases
   protection for the same overhead.


   A new defense should be easy to use, easy to deploy, and hinder
   users as little as possible. It should use resources responsibly,
   that is: it should try to get the best possible defense from a
   given resource level. Such a defense is attempted in this
   thesis. Ideally, it should also be easy to configure.


   The new defense tightly integrates into the Tor Browser; it tries
   to make each fetch of a webpage look like another random webpage;
   it also uses the Tor Browser's extension system for easy
   configurability, and deployability. The new defense sets sensible
   defaults, allowing users to adjust its resource use to account for
   a their personal security needs.


   The new defense needs to be compared to existing defenses: As a
   secondary benefit, this thesis reimplements and validates a
   state-of-the art attack by \citet{panchenko2}; it also provides a
   way to record web page browsing via Tor. The assorted data in trace
   format includes more than 150 captures. A capture took on average
   more than a day. More than 50 captures are without defense. This
   data takes more than 10GB of disk space, in a format similar to
   those of \citet{effective2014} and \citet{panchenko2}. The included
   code can convert from and to their formats.
** Structure: Foundations, Design, Evaluation
The organization of this thesis follows thesis best practices: [[#ch2-background][a
foundational chapter]] is followed by [[#ch3-newdefense][the problem statement and design]],
which is [[#ch4-evaluation][evaluated in the following chapter]]. The final chapter
contains [[#ch6-conclusion][a conclusion with future work]].


Chapter [[#ch2-background]] introduces \gls{wf}, Tor, and defenses against
\gls{wf} on a need-to-know basis. Section [[#sub2-tor]] gives a gentle
introduction to Tor, with an example how Tor achieves privacy. Section
[[#sub2-wf]] explains how \gls{wf} works in general and against Tor in
particular. A big part of \gls{wf} is \gls{ml}, which is
explained in section [[#sub2-ml]]. How to defend against \gls{wf} is
presented in section [[#sub2-wf-def]].


This thesis' new \gls{wf} defense is introduced in chapter
[[#sub3-defense]]. It also presents the aspects the defense is based
upon: section [[#sub3-http]] justifies the chosen HTTP traffic model;
whereas the stochastic size cache based on Bloom-filters
\citep{Bloom70space/timetrade-offs} is described in section
[[#sub3-bloom]]. Finally, section [[#sub3-defense]] explains the defense's
itself.


Chapter [[#ch4-evaluation]] puts the defense to the test, analysing how it
compares to the most recent \gls{wf} defense by
\citet{DBLP:journals/popets/CherubinHJ17}. The analysis follows
\gls{wf} literature best practices: it first evaluates how a subset of
sites are distinguished one from the other in section [[#sub4-closed]]; it
then tries to distinguish these sites from a bigger set of background
sites in section [[#sub4-open]]. All results are summarized in section
[[#sub4-summary]].


Chapter [[#ch6-conclusion]] summarizes the findings and presents courses
of future work in section [[#future-work]].
** old exposé                                                       :ARCHIVE:
**** Thesis Contribution
     This thesis presents and tests a new defense against \gls{wf}. This
     new defense mimics HTTP\cite{rfc7230}-shaped cover traffic: Each
     web page retrieval is augmented by stochastically-drawn dummy HTTP
     traffic\cite{newtrafficmodel}. This could optimize the protection
     offered for given bandwidth overhead. It is implemented in a
     browser extension, which makes the defense easier to install,
     configure, and maintain.
**** Thesis Structure
     The following chapters try to solve the question whether the new
     defense works more effectively than existing ones.

     Chapter [[#ch2-background]] provides basic background for the IT-savvy
     who have not yet encountered Tor, machine learning, or website
     fingerprinting. For the Tor network, we treat its basic structure
     and why \gls{wf} might be a credible threat. \Gls{ml} basic steps
     and algorithms are briefly skimmed. Finally, \gls{wf} on Tor is
     presented. These parts can safely be skipped given previous
     knowledge.

     The defense's why and how (motivation and design) is described in
     chapter [[#sub3-defense]]. This also describes the bloom sort data
     structure for stochastically saving object sizes.

     Chapter [[#ch4-evaluation]] evaluates the defense. It first describes
     the data-gathering process. Next, the website fingerprinting
     attacks of \cite{panchenko2}, and \cite{ccsw09-fingerprinting} are
     validated on defenseless data. This is followed by the evaluation
     on data with cloaking.

     Chapter [[#ch6-conclusion]] summarizes the results, shows a path to
     implementation, with both included and additional further work.
* Tor Basics, Attacking and Defending with Website Fingerprinting
  :PROPERTIES:
  :CUSTOM_ID: ch2-background
  :END:
** How Tor Works
   :PROPERTIES:
   :CUSTOM_ID: sub2-tor
   :END:
  In the wake of both the Snowden revelations in the western world,
  and increased internet censorship in countries such as Iran,
  Saudi-Arabia, and China\citep{china}, more and more Internet users
  search for ways to keep online communication and web browsing both
  private and free of censorship. The /Tor/ project \citep{tor-design}
  provides this. It protects whistleblowers, journalists, the people
  in oppressive regimes \citep{jardine2016tor}, even the military, and
  regular internet users, against e.g.\space{}nation-states or businesses
  which want to follow user's online steps. It routes encrypted data
  traffic via intermediaries, obscuring who connects to whom.



  Let us conceive of the internet as a series of tubes. Each internet
  message sent from Alice \citep{rivest1978method} to Bob passes many
  of their joinings. At each joint, there are many paths in and
  out. The message needs to find a way to Bob, so it contains Bob's
  address on the envelope. In case Bob wants to answer, the envelope
  also contains Alice's address.

  #+CAPTION[Tor Network]: Tor Network. Tor-protected links are dashed and Tor green \citep{tor-style}. Onion icon marks Tor servers, \copyright Tor project. Inspired by \href{https://upload.wikimedia.org/wikipedia/commons/a/a1/How_Tor_Works_3.svg}{Wikipedia}.
  #+NAME: fig:tor-network
  #+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {l}{0.4\textwidth}
  [[./pictures/tor_network_no_eve.pdf]]
  # todo: convert svg to pdf

  This is why the internet is not anonymous by design. To provide
  partial anonymity, a group of tube intersections can join, wrap each
  data packet in layers (of encryption), and bounce it along the group
  randomly, unwrapping a layer at each bounce. After several bounces,
  say to Carol, Dave, and Frank, the data packet is completely
  unwrapped again. Its destination is Bob, but Alice's name is blotted
  out. Frank sends the packet to Bob. To answer the packet, Bob sends
  the packet back to Frank, who sends it via Dave and Carol back to
  Alice. Because encryption, Alice knows the full path, but Carol only
  knows Alice and Dave, Dave only knows Carol and Frank, and Frank
  only knows Dave and Bob.\\


  This closely models the Internet: Each Internet Protocol
  \citep{rfc791} packet lists the sender and destination. This makes
  it easy to identify communication partners. To achieve anonymity,
  the Tor software forms a path to the destination along multiple
  hops, establishing separate encryption with each hop. The hops are
  globally-distributed volunteer servers. Each intermediary hop only
  knows its predecessor and successor. Only Alice knows the full path.


  The local Tor software selects three globally-distributed hops to
  initialize a connection. It makes a connection to the first,
  establishes encryption, asks the first hop to make a connection to
  the second, sets up encryption to this, and from there to the
  third. The third hop establishes a connection to its destination.

  Each message is encrypted three times using same-length encryption
  and sent along this path. The first router decrypts the first layer,
  and so on, like layers of an onion. As a result of this setup, each
  hop can only see its direct neighbors along the path. Even if one
  hop of a three-hop setup is compromised, directly linking source and
  destination becomes pretty hard.
** Website Fingerprinting Attack
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf
   :END:
   #+INDEX: trace
   Some groups dislike other people's privacy. It's too
   resource-intensive to protect against all of them. Tor, as any
   privacy system, has elected to protect against certain threats
   \citep{tor-design}. E.g., Tor does not protect against an adversary
   that can see all Tor network traffic; this level of observation
   would make correlation attacks (\citet{flow-correlation04},
   \citet{ccs2013-usersrouted}) easy \citep[sec.9]{tor2014}. Tor is
   designed to defend e.g., against a local passive adversary: someone
   who can see the traffic from a client to the network.

  #+CAPTION[Tor Network with Website Fingerprinter]: Tor Network as in [[fig:tor-network]]. Alice's mother Eve can see all of Alice's WLAN traffic, and tries to perform website fingerprinting.
  #+NAME: fig:tor-network-eve
  #+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {l}{0.4\textwidth}
  [[./pictures/tor_network.pdf]]


   # no re-flow in this paragraph (breaks link)
   Let us assume that Eve is a concerned mother who wants to find out
   whether Alice visits Bob's website, see Figure
   [[fig:tor-network-eve]]. If Alice is just using a vanilla web browser,
   Eve can see where Alice connects. Alice needs to use a
   middle-man. She [[https://www.torproject.org/download/download-easy.html.en][downloads the Tor Browser Bundle]]. This program
   routes all traffic via three Tor servers, say Carol, Dave, and
   Frank. These three are regularly replaced with other random
   nodes. Eve is frustrated: she can not see whom Alice connects
   to. Also, all traffic is encrypted, so she can only see that
   packets are sent, not their content.


   Eve is not to be thwarted: \Gls{wf} to the rescue! She uses Tor on
   her own computer to both connect to Bob's website, recording the
   network traffic of Bob's site, as well as other random web browsing
   traffic. (She could of course just block Tor, but then Alice might
   sneak off in the middle of the night to see Bob.) This traffic
   trains a \gls{ml} classifier. When Alice uses Tor, her traffic is
   input to this classifier, which can decide in almost real-time
   whether the site Alice visits is Bob's. If so, Eve can devise
   targeted ways to keep her daughter occupied with other things.


# ## code, last trace does not fit well
# wiki = scenario.list_all("17-12-26")[0].get_traces()["wikipedia.org"]
# mplot.traces([wiki[x-1] for x in [5, 26, 45, 35, 32, 24, 44, 1]]) #
    #+CAPTION[Trace Visualization Example]: Example of traces of wikipedia.org. Box width is the time to the next packet, box height the size of the packet (positive incoming, negative outgoing). The top traces seem similar to the naked eye (modulo time dilation). The bottom trace does not fit this pattern. The whole of Wikipedia's traces were recognized with 100% accuracy in a set of 30 sites. (overall accuracy 98.57%)
    #+NAME: fig:traces
    [[./pictures/example_traces_wiki.pdf]]

   In \gls{wf}, the time and size of users' data packets (called
   /traces/) are recorded.  Figure [[fig:traces]] shows a visualization of
   these traces: The similarities can be expected, as every retrieval
   retrieves similar content. \Gls{wf} distinguishes between 30 and
   many more sites with high accuracy.(\citet{panchenko2},
   \citet{effective2014} \citet{197185})
*** TODO write more about wf: how it works                          :ARCHIVE:
** Machine Learning
   :PROPERTIES:
   :CUSTOM_ID: sub2-ml
   :END:
   #+INDEX: machine learning
   In \glsdesc{ml}, a computer \citep{turing1936a} algorithm extracts
   and generalizes patterns from learning
   data.\citep[ch.1.2]{rieckdiss} This abstraction is used to classify
   further patterns (e.g. for handwriting recognition
   \citep[sec.11.7]{esl}), or to act on the generalizations (say, for
   self-driving cars \citep{montemerlo2008junior}). For the purpose of
   this thesis, \gls{ml} is more of a black box: it transforms traces
   into a website prediction.


   Few real world examples are just given as a vector of
   numbers.\citep[sec.1.3.1]{duda} Thus, machine learning needs some
   prior work: [[#sub2-ml-features][/Feature extraction/]] (sec. [[#sub2-ml-features]]), uses
   domain-specific knowledge to extract meaningful features. It also
   has to transform raw input data --- in our case, website traces ---
   into /features/ --- in our case, numbers, e.g. the number of
   outgoing packets. All these features are combined into classifier
   input. [[#sub2-ml-class][Classification]] (sec. [[#sub2-ml-class]]) takes feature extraction's
   output as input. Its task is two-fold: it first generalizes from
   training data. Trained, it assigns input data into categories. In
   order to evaluate how well this works, section [[#sub2-ml-measure]] presents
   [[#sub2-ml-measure][measures to evaluate \gls{ml} performance]].
*** Feature Extraction
    :PROPERTIES:
    :CUSTOM_ID: sub2-ml-features
    :END:
    #+INDEX: feature extraction
    #+INDEX: machine learning!feature extraction
    The natural world has an abundance of
    information. \citet[ch.24]{russell1995modern} mention that even a
    1 megapixel camera, sampled at 60Hz, produces more than 10 GB of
    data per minute. The more features are used in \gls{ml}, the
    higher the amount of data required to correctly train a classifier
    \citep[sec.1.2.3]{mitchell}. Knowing the subject domain can help
    in condensing information: the aim is to keep class-specific
    information, and to discard individual attributes and noise
    \citep[sec.1.3.1]{duda}. Some classifiers expect their input as
    elements of f.ex. a Hilbert space \citep[sec.1.3.3]{iml}. In
    this case, email texts e.g., /must/ be transferred to some other
    representation.


    Feature extraction is highly domain-dependent
    \citep[sec.1.3.1]{duda}. Let us examine \gls{wf} features en
    detail: \Gls{wf} input data needs to be wrangled
    \citep{kandel2011research} for the classification to work: extra
    information that might change from request to request --- such as
    IP addresses \citep{rfc791}, or the absolute time of the
    retrieval --- needs to be removed or unified to a common
    format. The trick is as always: keeping the signals and discarding
    the noise; in other words: finding those features with the biggest
    difference of the class means relative to class standard
    deviations.\citep[sec.10.14.2]{duda}


    The source data in \gls{wf} are traces, say in =pcap=
    \citep{pcap-manual} format. From these, only the size, and timing
    of each packet is extracted. The packet direction is encoded in
    the size (\citet{panchenko}, \citet{a-systematic2014}), either
    positive or negative. These uniquely describe the web retrieval
    for \gls{wf} \citep[Fact 1]{a-systematic2014}. The size of files
    is hidden by the traffic's encryption; the closest approximation
    \citep{wpes13-fingerprinting} is the size of each TLS
    \citep{rfc5246} record. Alternatives to this are sizes of TCP
    \citep{rfc793}, Tor\cite{tor-design} cell, or IP \citep{rfc791}
    packets. As of \citet{wpes13-fingerprinting}, these work similarly
    well.


    The earliest \gls{wf} attacks (\citet{quantifying},
    \citet{ssl-traffic-analysis}, \cite{hintz02}) only used packet
    sizes as features to attack SSL
    \citep{sslv3}. \citep[sec.7]{tor-design} conjectured that \Gls{wf}
    against Tor would be hampered by Tor's fixed (data) cell size
    \citep[sec.0.2,3]{tor-spec}. Five years later,
    \cite{ccsw09-fingerprinting} confirmed this resilence in
    comparison with other privacy-enhancing technologies, but still
    showed better-than-random classification.


    #+CAPTION[CUMUL features example]: CUMUL features example
    #+NAME: fig:CUMUL_traces
    [[./pictures/CUMUL_2017-12-31.pdf]]
    # code: see [[file:bin/mplot.py::traces_cumul%20usage]]

    \citet{panchenko} increased closed-world \gls{wf} accuracy on
    \citet{ccsw09-fingerprinting}'s dataset from 2.96% to 54.61%
    (section [[#sub2-ml-measure]] defines accuracy etc.). They added
    \gls{wf}-specific features, such as the percentage of incoming
    packets, the estimated size of the HTML \citep{html5} page,
    etc. \citet{effective2014} used close to 4000 features. On a
    smaller dataset, they achieved a true-positive rate of 84%, with a
    false-positive rate of 0.6%. \citet{panchenko2} use a cumulative
    size metric, CUMUL. As seen in Figure [[fig:CUMUL_traces]] these provide
    a graphical representation of traces, while still allowing for
    computer-based comparison after normalization. To create this,
    they sum the incoming and outgoing bytes. To extract the same
    number of features from each trace, they interpolate 100 data
    points from these. \citet{kfingerprint} provide a state-of-the-art
    \gls{wf} attack. They also measure how much each feature adds to
    classification. Their approach uses approximately 150 features
    with a unique Random Forest \citep{DBLP:journals/ml/Breiman01}
    fingerprint classification that appears to use error-correcting
    output codes.(\cite[sec.3.1]{197185},
    \cite{DBLP:journals/jair/DietterichB95})
*** Classification
    :PROPERTIES:
    :CUSTOM_ID: sub2-ml-class
    :END:
    #+INDEX: classification
    #+INDEX: test data
    #+INDEX: training data
    #+INDEX: classification!training data
    #+INDEX: classification!test data
    #+INDEX: machine learning!classification
    \Gls{wf} /classification/ tries to assign a website to a trace's
    features extracted in the previous step.  The previous step
    [[#sub2-ml-features][feature extraction]] transforms traces (raw input data) to
    features. In /classification/, these features are used for two
    purposes: some traces' features are used for training the
    classifier, others for testing.


    For \citet[sec.1.1]{mitchell}, in \gls{ml}, a computer program
    learns how to do a task /T/ from experience /E/ according to
    performance measure /P/, if it improves at /T/ as measured by /P/,
    given /E/. \cite{914517} provide a formal definition: The task of
    (binary) classification is to approximate a function $f:
    \mathbb{R}^n \to \{-1, 1\}$. The given /training data/ points
    $(\mathbf{x}, y) \in X \times Y = \mathbb{R}^n \times \{-1, 1\}$ are drawn
    from an independent and identically distributed (i.i.d.)
    distribution. Using these, a hypothesis $h$
    \citep[sec.18.2]{russell1995modern} can be estimated. The aim is
    to minimize the /hypothesis's error/
    #+BEGIN_EXPORT latex
    \begin{equation}
    R[h] = \int l(h(\mathbf{x}), y) \mathrm{d}P(\mathbf{x}, y)
    \end{equation}
    #+END_EXPORT
    with an adequately defined /loss function/ $l$, say $l(x, y) = 0$
    if $x == y$ else $1$. As the probability distribution $P$ is not
    known, it is often estimated as the /empirical risk/
    #+BEGIN_EXPORT latex
    \begin{equation}
     R_{emp}[h] = \frac{1}{n} \sum_{i=1}^{n} l(h(\mathbf{x_{i}}), y_{i})
     \end{equation}
    #+END_EXPORT
    over all points of training data. As of \citet[sec.1.1]{iml}, this
    can be extended to multiclass-classification, where $Y$ contains
    more than two labels. More and other classes of input and output
    are possible. \citet[sec.18.1]{russell1995modern} makes the
    distinction that class labels are not always provided. Providing
    the labels is called /supervised learning/. Finding patterns in
    unlabelled data is called /unsupervised learning/. In between
    these, there is also /semi-supervised learning/, where some data
    is labelled, and/or these labels are not necessarily accurate
    (say, user-reported ages). /Reinforcement learning/ provides
    guidance only after the fact. This guidance is only in the form of
    "yes, you did well", or "no, do better next time".



    While classification input can have many types, in \gls{wf} it is
    always a vector, most often of numbers. In classifier /training/
    \cite[sec.1.3.1]{iml}, a classifier gets as input several feature
    vectors $\{x_1, \ldots, x_n\} \subset X$ and their respective classes
    $\{y_1, \ldots, y_n\} \subset Y$ as pairs $(x_i, y_i)$ and tries to
    generalize a relationship. This combination of feature vectors and
    their classes is called /training data/ \citep[sec.2.2]{esl}. In
    actual /classification/, the classifier only receives input
    feature vectors, and needs to predict the class label. In
    \gls{wf}, this is: the web page. This data is called /test data/
    and is used to test classifier performance.


    What happens in classification steps depends on the
    classifier. Most classifiers, such as support vector machines form
    an internal model from which further input data is
    classified. Others, notably k-Nearest-Neighbors, classify directly
    without an intermediary model.
*** Measuring Performance
    :PROPERTIES:
    :CUSTOM_ID: sub2-ml-measure
    :END:
    #+INDEX: Accuracy
    #+INDEX: Area Under [the ROC] Curve (AUC)
    #+INDEX: AUC
    #+INDEX: AUC$_{0.01}$
    #+INDEX: AUC!bounded
    #+INDEX: closed world
    #+INDEX: confusion matrix
    #+INDEX: False Positive Rate (FPR)
    #+INDEX: open world
    #+INDEX: Receiver Operating Characteristic (ROC) curve
    #+INDEX: ROC curve
    #+INDEX: True Positive Rate (TPR)
    #+INDEX: world!closed
    #+INDEX: world!open
    To find out if \gls{wf} attacks work, and if defenses prevent them
    from working, their success can be measured. The simplest
    form of measurement is simply counting how many traces were
    classified correctly, and dividing by the total number of
    traces. This is called /accuracy/.\citep{powers} For other types
    of measurement, a /confusion matrix/ helps to
    illustrate the different cases that can occur in \gls{wf}. See
    Table [[tab:confusion_matrix]].

    #+CAPTION[Confusion matrix]: (binary) Confusion matrix. Correctly classified traces are in bold.
    #+NAME: tab:confusion_matrix
    #+ATTR_LATEX: :align |l||l | l|
    |----------------------+-----------------------+-----------------------|
    | <20>                 |                       |                       |
    |                      | real wikipedia.org    | real onclickads.net   |
    |----------------------+-----------------------+-----------------------|
    | predicted as wikipedia.org | *True Positives (TP)* | False Positives (FP)  |
    | predicted as onclickads.net | False Negatives (FN)  | *True Negatives (TN)* |
    |----------------------+-----------------------+-----------------------|


    This matrix counts the number of classifications. For example, if
    a trace that was recorded from /wikipedia.org/ is classified as
    /onclickads.net/, this increases the \gls{fn} count by 1. Each
    trace is categorized by whether it /is/ a sensitive website (here:
    wikipedia.org), and whether it is /classified/ as such. From these
    values, metrics can be derived. Apart from /accuracy/, the main
    metrics used in \gls{wf} literature are /True-/, and
    /False-Positive-Rate/. These are defined as

    #+ATTR_LATEX: :align r c l
    | Accuracy            | := | $(TP + TN) / (TP + FP + FN + TN)$ |
    | True Positive Rate  | := | $TP / (TP + FN)$                  |
    | False Positive Rate | := | $FP / (FP + TN)$                  |



    There are two scenarios of testing \gls{wf} in experiments,
    /closed-world/ and /open-world/ \citep[sec.3.2]{panchenko}. In a
    closed-world scenario, a fixed number of pages, say 100, are
    compared one to the other, the classifier only has to distinguish
    between these. Accuracy measures this classifier's success. In an
    open-world scenario, a certain number, say 4000, of background
    pages are additionally captured. The previously captured (100)
    sites are called /foreground/ pages. The classifier's task is to
    distinguish between foreground and background pages. The scenario
    is that there are certain censored sites, say wikileaks.org, which
    need to be distinguished from normal web browsing, say
    tagesschau.de.


    #+CAPTION[ROC curve example]: Example Receiver Operating Characteristic (ROC) curve. The point at the top left (0, 1) is optimal. The ROC-curve shows possible true- to false-positive ratios. Each point on the curve corresponds to one confusion matrix \citep[sec.4.0]{Fawcett:2006:IRA:1159473.1159475}.
    #+NAME: fig:roc-example
    [[./pictures/roc_example_2016-05_1_4000.pdf]]

    Some classifiers not only yield the input's class, but also output
    a certainty or probability: This indicates how sure the classifier
    is of the classification. In this dual scenario of having
    background and foreground pages, it becomes possible to weight how
    important each classification is. For example, if a nation state
    were to raid people's houses if they access a certain website, it
    better be very sure that they did in fact access the website. If
    Eve wants to check in on Alice whenever her daughter visits Bob's
    website, she might prefer some other sites to classify as Bob's.


    A /Receiver Operating Characteristic Curve/ (ROC-Curve)
    \citep{Fawcett:2006:IRA:1159473.1159475} shows the classifier
    strictness tradeoff. This diagram contrasts classifier true-, and
    false-positive-rate, see Figure [[fig:roc-example]]. The /area under/
    this /curve/ (AUC) can be measured. The closer this value is to 1,
    the better. If one is mainly interested in few false positives,
    the leftmost section of the ROC-curve is of particular
    interest. The area under the curve bounded up to a false positive
    rate of, say 1%, is called /bounded AUC/ and denoted AUC_{0.01}.
** Defending against Website Fingerprinting
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf-def
   :END:
   This section describes defenses against \gls{wf} as described
   [[#sub2-wf][previously]] in section [[#sub2-wf]]: \gls{wf} attacks have become
   increasingly better using \gls{ml}. \Gls{wf} thus effectively
   deanonymizes the traffic that users thought private: it could for
   example expose a dissident to his nation state, nullifying this
   part of Tor's protection.  The task of defenses is to confuse such
   a \gls{wf} attacker. As most \gls{ml}, \gls{wf} uses statistical
   properties of the underlying data. It could possibly be defeated by
   shuffling these properties. The total number of incoming packets
   e.g. is a feature used by almost all modern attacks.  As seen in
   Figure [[fig:total_packets_in]],
   \citet{DBLP:journals/popets/CherubinHJ17} create additional
   packets, but preserve site separation and ordering.

   # created using mplot.total_packets_in_helper(['disabled/bridge--2018-01-12--30@50', 'defense-client/bridge--2018-01-07--30@50'])
   #+CAPTION: Distribution of number of total incoming packets, once without defense, once using LLaMA \citep{DBLP:journals/popets/CherubinHJ17}.
   #+NAME: fig:total_packets_in
   #+ATTR_LATEX: :float nil
[[./pictures/total_packets_in_disabled___bridge--2018-01-12--30@50_defense-client___bridge--2018-01-07--30@50__bing.com_wordpress.com_gmw.cn_wikipedia.org__palette_colorblind.pdf]]


   Traffic analysis \citep{introta} assumes that encryption is
   unbreakable, and tries to find information from metadata:
   observable streams of traffic, e.g. radio wave origin, or IP packet
   size and timing. From inception, Tor \citep[sec.3.1]{tor-design}
   provided some defense against traffic analysis. For one, all Tor
   /data/ cells have the same size, which protects against identifying
   them by size only. Tor also multiplexes all traffic into a single
   stream, making it hard to distinguish the multiple streams that
   most websites require, let alone parallel website retrieval.
   Unavoidably, Tor also increases traffic latency
   \citep[sec.2.2]{rfc1925}, so that attacks have a harder time
   relying on packet timing \citep{challenges}. This makes \gls{wf}
   harder, to the point that it was was mentioned, but not hindered,
   in \citet[sec.7]{tor-design}.


   To protect against \gls{wf}, several early attack authors also
   mentioned possible defenses: \citet[sec.5]{Wagner96analysisof}
   proposed padding SSL \citep{sslv3} so that HTTP GET
   \citep[sec.4.3.1]{rfc7231} urls would be
   concealed. \citet[sec.3]{ssl-traffic-analysis} proposed three
   possible defenses: an additional padding layer between HTTP and
   SSL, modifying the web pages themselves, or using web proxies.
   \citet[sec.8]{hintz02} suggested padding, switching off
   e.g. images, and transferring a whole page in one connection.  The
   first website fingerprinters considered only packet lengths. This
   made it seem sensible to defend by altering the lengths of packets
   by padding, as evaluated e.g.\space{}by
   \citet[sec.5.1]{ssl-traffic-analysis}.



   The previous defenses attempted to hinder \gls{wf} in general, not
   on Tor. Several attack authors also proposed Tor-specific defenses.
   \citet[sec.6]{panchenko} add noise to traffic. They /camouflage/ by
   loading another page simultaneously in the
   background. \citet{ccs2012-fingerprinting},
   \citet{a-systematic2014}, \citet{kfingerprint},
   \citet{effective2014}, \citet{panchenko}, and \citet{wtfpad2015}
   found this simple defense to be surprisingly effective, albeit at a
   high overhead. As more and more features were used to classify the
   traces, different ways of altering the data were evaluated by
   several researchers: several ways of padding
   (\citet{Liberatore:2006}, \citet{oakland2012-peekaboo},
   \citet{a-systematic2014}, \citet{ccs2012-fingerprinting},
   \citet{wang2015walkie}), or altering traffic sizes to fit another
   web page's (\citet{morphing09}, \citet{httpos2011}).


   Prior to \citet{oakland2012-peekaboo}, most \gls{wf} defenses
   altered specific \gls{ml} features, e.g. single packet size. This
   created an arms race between attacks and defenses - the attacks
   finding new feature combinations to use, the defenses obfuscating
   these. To stop this, \citet{oakland2012-peekaboo} introduced the
   idea of a /general defense/ into the context of website
   fingerprinting. The aim is to transform groups of web retrievals so
   that all members look the same. They proposed a traffic-flow
   security \citep[ch.10.3]{applied96} solution called /BuFLO/:
   fixed-rate transmission of all data, with dummy traffic for gaps,
   for the estimated duration of web site retrieval.  This idea was
   improved on by \citep{a-systematic2014}. Both of these exhibit high
   overhead, as they send data at a fixed rate. \citet{effective2014}
   proposed the (offline) defense of morphing all traffic to
   supersequences of traffic patterns.


   The stochastic defenses of \citet{wang2015walkie} and
   \citet{wtfpad2015} have less overhead than the previous
   deterministic general defenses. \citet{wtfpad2015} aims at
   generally hiding /that traffic occurs/, not just which website is
   visited; it derives its basic mechanism from
   \citet{ShWa-Timing06}. \citet{wang2015walkie} changes the traffic
   patterns to half-duplex: It either only sends or only receives. In
   2017, LLaMA was introduced by
   \citet{DBLP:journals/popets/CherubinHJ17}. They acknowledge the
   need for a client-side application-level defense. LLaMA delays
   traffic by a uniformly distributed distribution. The authors
   provided LLaMA as a secondary defense to the server-side ALPaCA
   defense, and emphasize its prototype status \citep{LLaMA}.
** DANIEL Background [9/9]                                          :ARCHIVE:
   Knowing [[#sub2-tor][the Tor network]], [[#sub2-ml][\gls{ml} basics]] and [[#sub2-wf][previous
   attacks and defenses]] helps to understand and then counter website
   fingerprinting.
*** DANIEL Tor Website Fingerprinting
    :PROPERTIES:
    :CUSTOM_ID: sub2-wf
    :END:
    #+INDEX: traffic analysis
    #+INDEX: website fingerprinting
    #+INDEX: WF
    What does an adversary do if he cannot decrypt and the message
    traffic of a cryptographic system he is interested in? One
    alternative is to inspect the traffic itself for patterns. This
    process is called /traffic analysis/\cite{introta} and yields much
    useful information\cite[ch.10.3]{applied96}.

    \gls{wf} needs only message meta-data: who sends how much data
    when. It assumes that the system itself is computationally
    secure\cite[ch.1.1]{applied96}: there are not enough resources,
    time, or data to break it. Analysing traffic patterns can
    circumvent the system. Anyone who can see the data stream can carry
    out this attack, without anyone else learning about this. They
    simply need to capture the data stream using e.g. the
    =tcpdump=\cite{tcpdump8-manual} tool.\\

    From inception\cite{tor-design}, Tor provided defenses against
    traffic analysis. For one, all /data/ cells have the same size,
    which protects against identifying them by size only. Tor also
    multiplexes all its data traffic into a single stream, making it
    hard to distinguish the multiple streams that most websites
    require, let alone parallel retrieval. Tor also
    unavoidably\cite{rfc1925} increases traffic latency, so that
    attacks have a harder time relying on interpacket
    timing\cite{challenges}.\\


    This made \gls{wf} harder, to the point that it was was mentioned, but
    not hindered, in \cite{tor-design}. It took five years for
    \cite{ccsw09-fingerprinting} to show better than random
    classification results against Tor traffic. This evolved to
    state-of-the-art methods like \cite{panchenko2}.

    What all methods have in common is that they extract numerical
    /features/ from the raw data, which is then classified using
    \gls{ml}.
*** DANIEL Summary
    \gls{wf} can deanonymize anonymous traffic. This can pose a huge problem
    e.g. for whistleblowers. The previous sections gave a short
    introduction to the basics of Tor, \gls{wf} attacks, its basis in machine
    learning, and finally defenses against it. It also gave a first
    glimpse at this thesis' new defense.

    The next section presents the novel defense in depth.
* A New Defense
  :PROPERTIES:
  :CUSTOM_ID: ch3-newdefense
  :END:
\Gls{wf} has deanonymized Tor under laboratory conditions
\citep{panchenko}, and has continued to improve (\citet{panchenko2},
\citet{197185}, \citet{rimmerautomated}). Several approaches have been
taken to prevent \gls{wf} (\cite{panchenko},
\cite{DBLP:journals/popets/CherubinHJ17}). Yet none has made it into
mainline Tor \citep{tor-design}. This thesis presents a deployable
defense. It uses domain-specific properties of HTTP traffic
\citep{newtrafficmodel}. This is akin to \cite{newton2005preserving}
who showed good results in obfuscating faces using eigenfaces.


The question is whether this defense obfuscates effectively: Whether
HTTP-specific cover traffic can yield better results than previous
stochastic defenses. Most of these used non--subexponential
\citep{foss2011introduction} distributions. A bound on this
effectiveness are previous defenses. The state-of-the-art CUMUL
\gls{wf} attack by \citet{panchenko2} measures effectiveness of
obfuscation.


This chapter first presents the main thesis's defense design in
section [[#sub3-defense]]. Section [[#sub3-http]] goes into some detail about
the traffic model. The optional Bloom-filter based stochastic data
structure that is used for size caches is described in section [[#sub3-bloom]].

The advantages of this approach are: Firstly, the application layer is
used in \gls{wf}. The defense works at the same layer. Secondly, the
defense works inside the browser, observing requests as they occur. It
can thus send specific cover traffic fitted to each single
request. Thirdly, the defense could possibly cache element sizes, in
order to more closely tailor cover requests. Finally, the defense is
provided as a Firefox Add-on. This eases installation and
configurability.
** Main Defense
:PROPERTIES:
:CUSTOM_ID: sub3-defense
:END:
Once again, let us consider Alice. When she opens =http://bob.com= in
her browser, it first loads =bob.com='s main HTML \citep{html5}
page. This page frequently contains other resources like CSS
stylesheets, images, JavaScript \citep{ecma} files, etc. It might also
contain elements that request other elements again, like nested HTML,
CSS =@import=, or JavaScript AJAX requests. For each of these, the
browser sends a request to obtain it.


The defense obfuscates web traffic: For each load of a website,
additional cover traffic is created. To do this, the defense observes
all website requests. Algorithm \ref{algo1} describes the main idea:

 #+BEGIN_EXPORT latex
 \begin{algorithm}
 \caption{Generate Cover Traffic}\label{covertraffic}
 \label{algo1}
 \begin{algorithmic}[1]
 \Procedure{OnHttpRequest}{$\textit{url}$}
 \If {$! \textit{isRegistered}(\textit{hostnameOf}(\textit{url}))$} \Comment{unknown hostname: main HTML request}
   \State $\textit{targetHttpSize} \leftarrow \textit{randomHttpSize}()$
   \State $\textit{targetNumEmbedded} \leftarrow \textit{randomNumEmbedded}()$
   \State $\textit{urlHttpSize} \leftarrow \textit{lookupOrGuessHttpSize}(\textit{url})$
   \State $\textit{urlNumEmbedded} \leftarrow \textit{lookupOrGuessNumEmbedded}(\textit{url})$
   \State $\textit{coverHttpSize} \leftarrow \textit{targetHttpSize} - \textit{urlHttpSize}$
   \State $\textit{coverNumEmbedded} \leftarrow \textit{targetNumEmbedded} - \textit{urlNumEmbedded}$
   \State $\textit{requestCoverSized}(\textit{coverHttpSize})$
   \State $\textit{registerHost}(\textit{hostnameOf}(\textit{url}), \textit{coverNumEmbedded}, \textit{urlNumEmbedded})$
 \Else \Comment{known hostname: a resource request}
   \State $\textit{requestProbability} \leftarrow \textit{computeProbability}(\textit{hostnameOf}(\textit{url}))$
   \While {$\textit{requestProbability} > 1$} \Comment{send multiple cover requests}
     \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
     \State $\textit{updateHosts}(\textit{hostnameOf}(\textit{url}))$
     \State $\textit{requestProbability} \leftarrow \textit{requestProbability} -1$
   \EndWhile
   \If {$\textit{withProbability}(\textit{requestProbability})$} \Comment{maybe send cover request}
     \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
     \State $\textit{updateHosts}(\textit{hostnameOf}(\textit{url}))$
   \EndIf
 \EndIf
 \EndProcedure
 \end{algorithmic}
 \end{algorithm}
 #+END_EXPORT
The retrieval of the (first) HTML page provides significant
information to a \gls{wf} attacker \citep[sec.4.1]{panchenko}. To
counter this information gain, this first load is always covered by
additional traffic. Afterwards, the browser downloads the page
resources: images, style sheets, JavaScript \citep{ecma} files,
etc. Each of these transfers might be covered by additional dummy
traffic: The actual number of resources is subtracted from the target
number. This difference is randomly spread out among the actual
browser requests. The size of each cover response is adjusted to the
page's actual values, if known. The addon augments data requests to
match an imagined web page retrieval. After a timeout, the hostname
and its data is removed from the data structure used by
/registerHost/, /isRegistered/, /computeProbability/ and
/updateHosts/.



Alice's defense sees the first load from =bob.com=, and checks if it
has seen this site before. Since it is a new request, it has not. It
thus considers this to be a main HTML request, and determines the
actual, target, and cover values. Using these, it then sends a request
for HTML cover traffic concurrently to the first HTML request. Since
all traffic is encrypted and secured over Tor, only the sizes of the
requests and responses are seen. By design, on the wire this cover
request is simultaneous to the main HTML request. Bob's main website
might be simple: it contains just a stylesheet, links to further
posts, and a background image. The browser downloads the stylesheet,
and the image after the first HTML page. The defense randomly chooses
to accompany the image with two additional cover requests. It then
decrements its embedded resource cover target by two.


The created traffic is based on the HTTP model \citep{newtrafficmodel}
described in section [[#sub3-http]]. For each web page retrieval, the
defense sets target retrieval parameters. From these, the web page's
actual parameters are subtracted to set the amount of cover traffic
for this web page. If the parameters are not known, they are guessed
from the same distributions used to set the target values.
** Modelling Web Retrieval
   :PROPERTIES:
   :CUSTOM_ID: sub3-http
   :END:
This thesis' defense uses the distribution of HTTP traffic: the sizes
of HTML pages, the size of embedded resources, and the number of
embedded resources per HTML page. There are several approaches on how
to generate HTTP-shaped traffic. The naïve way, using HTTP dummy
traffic \citep{panchenko}, loads another page simultaneously in the
background. \citet{ccs2012-fingerprinting}, \citet{a-systematic2014},
\citet{kfingerprint}, \citet{effective2014}, \citet{panchenko}, and
\citet{wtfpad2015} found it to be surprisingly effective for all its
simplicity, albeit at a high overhead.


#+CAPTION[Distribution of sizes for the HTTP traffic model]: Distribution of sizes for the HTTP traffic model. While the number of embedded elements seems very low, it has mean \mu \approx 5.07 \citep{newtrafficmodel} and standard deviation \sigma \approx 15.16 (computed as of \cite[5.1.11]{compgen} and using \cite[stats.gamma]{scipy}).
#+NAME: fig:distributions
[[./pictures/fig_html_embedded.pdf]]
# see misc_gen_quantiles_numemb.py

This is why HTTP \citep{rfc7230}-shaped cover traffic might prove more
effective, as this would make it harder to separate cover and real
traffic. In addition, it would work at the layer
\citep[ch.1.7]{DBLP:books/daglib/0001977} where the problem
originates, as it mimics the HTTP/HTML \citep{html5}-specific
request-response interaction. Yet, World Wide Web \citep{rfc1945}
traffic cannot be adequately modeled using standard distributions like
normal or uniform \citep{crovella97}. This explains the many outliers
reported by
\cite[sec.5.2]{DBLP:journals/popets/CherubinHJ17}. \cite[sec.5.A]{wtfpad2015}
also distribute its \gls{wf} defense data partitions exponentially to
better fit web traffic. \cite{DBLP:conf/imc/IhmP11} repeatedly mention
a /long tail/ of traffic. Several aspects of web traffic show
subexponential distribution \citep{foss2011introduction} behavior,
where high sampling values are not as unlikely as in, say, exponential
or normal distributions\citet{newtrafficmodel}. They found that
log-normal distributions (truncated), as seen in Figure
[[fig:distributions]] fit the sizes of both HTML and embedded resources
best in their analysis. A truncated gamma function models the number
of embedded objects. This model provides the cover traffic target
sizes. It also sets the page's actual sizes, if they are not known.
** Caching Sizes using Bloom-Filters
   :PROPERTIES:
   :CUSTOM_ID: sub3-bloom
   :END:
As mentioned in the previous section, the model provides the page
retrieval's actual numbers, if these are not known. The defense might
further improve if the sizes of the webpage to be loaded are known
beforehand: Cover traffic could be tailored more exactly, increasing
obfuscation and/or reducing overhead. Knowing the exact retrieval
pattern in advance even enables new defenses \citep{effective2014}.
The problem in caching is that

1. page properties change over time, making a fixed cache increasingly
   less accurate,
2. caching visited page sizes might yield an exact log of the visited
   web pages to an attacker who gains control over the defended
   computer. \citep[sec.2.1]{tor-browser-design-impl} forbid writing
   sensitive data to disc, except on opt-in, and
3. this cache cannot store all page sizes. Even in a closed world,
   using a default mapping of string to size could take as much space
   as to preclude usage, depending on the subset of pages's sizes
   cached.

Storing the sizes of all pages as a mapping from their names to their
sizes is impractical due to size constraints. A data structure that
stochastically saves approximate sizes might solve problems /2/ and
(partially) /3/: Bloom Filters \citep{Bloom70space/timetrade-offs}
have a small error rate in exchange for a fixed size. Their otherwise
disadvantageous error probability is an advantage in this situation,
as it further confounds possible attackers. The current implementation
was initialized with fixed page sizes for the top pages. Dynamically
updating the filter might solve problem /1/ and further help with
problem /3/ (future work).


Bloom filters \citep{Bloom70space/timetrade-offs} are a stochastic
fixed-width data structure to test membership in a set. In exchange
for a small false-positive error rate, they require significantly less
space than deterministic data structures: if an element is in the set,
the filters accurately report this; if the element is not in the set,
the Bloom-filter might report that it is contained. The error rate
depends on the number of included elements in relation to the
Bloom-filter's size. Bloom-filters were developed for spell
checking. They have numerous uses in network applications, e.g. in
distributed caches, and network
analysis \citep{Broder02networkapplications}. Bloom-filters improve on
basic hash-coding by offering a tunable false positive error rate.


To approximately cache sizes of elements, this thesis uses a data
structure based on Bloom-filters. The histogram of observed values is
inspected, and split by quantiles into bins. For each bin, a Bloom
filter is created. Each site/url is then binned: An element is added
to this filter if its size is inside the bin. To approximate the size
of an element, all filters are checked. If one filter reports
containment, its size is chosen. If zero report containment, the size
is not known; if two or more report containment, it is saved
wrongly. In both of these latter cases, the default distribution is
used. If the data structure is queried, the middle quantile of each
bin is chosen to represent the bin. Each bin thus corresponds to a
Bloom-filter that saves whether the element's URL is modeled by the
bin. This data structure has the additional advantage that, even if
visited page sizes were saved, an adversary could not safely determine
that pages were visited due to the Bloom Filter's false positive
errors.

(See Appendix [[#bloom-sort]] for implementation details.)
* Evaluation
  :PROPERTIES:
  :CUSTOM_ID: ch4-evaluation
  :END:
In Evaluation, traffic data needs to first be captured and then
analysed. \citet{effective2014} and \cite{panchenko2} provide their
capture data, but it it does not contain the new defense's traces, so
new traffic data needed to be collected. The collection process is
described in section [[#sub4-setup]]. Section [[#sub4-closed]] first studies
the robustness of the attack, then compares the new defense with the
latest defense by \citet{DBLP:journals/popets/CherubinHJ17}, both in a
closed-world scenario. Section ... describes the open-world analysis,
with concluding remarks in section ...
** old intro                                                        :ARCHIVE:
This chapter compares the new defense to existing defenses. The first
section illustrates the setup. The second section [[#sub4-tools][validates the
\gls{wf} attacks used]]. The next section [[#sub4-closed][compares defenses in a
closed-world setting] where they need to only distinguish a small
fixed number of pages. The penultimate section evaluates them [[#sub4-open][in an
open-world scenario]], which has many background-page retrievals added
to the mix. Its last section [[#sub4-summary][summarizes evaluation results]].
** Capturing Web Traffic
:PROPERTIES:
:CUSTOM_ID: sub4-setup
:END:
The aim of this chapter is to test if the defense prevents \gls{wf},
and if so, it works more efficiently than other defenses. For this,
data first needs to be captured.

#+INDEX: Bridge
#+INDEX: Tor!Bridge
#+CAPTION: Setup to capture web page traffic: Tor Browser on /Client/ machine, connects to Tor server on /Bridge/ machine, connects to Tor network, connects to web servers
#+ATTR_LATEX: :float nil :width 0.5\textwidth
#+NAME: fig:setup
[[./pictures/setup-unified.pdf]]

To do this, real Tor web browsing should be closely reproduced. [[https://www.torproject.org/download/download-easy.html.en][The
current Tor Browser Bundle]] downloads the web sites traces: it is based
on the Firefox Browser, which provides the Marionette framework
\citep{marionette} for instrumentation. To be able to evaluate the
WTF-PAD \citep{wtfpad2015} defense[fn::provided at
[[https://bitbucket.org/mjuarezm/obfsproxy_wfpadtools]]], all traffic is
routed over a Tor Bridge \citep[sec.client~options]{tor-manual}, see
Figure [[fig:setup]]. This also eases defense comparability. The sites to
be downloaded come from Alexa's top million sites list[fn:: available
at [[http://s3.amazonaws.com/alexa-static/top-1m.csv.zip]]] with duplicate
sites and sites with high load error rate removed. This increases the
classification ratio, and thus makes it harder to defend. Making
\gls{wf} easier is admissible in the evaluation of a \gls{wf} defense.


Web traces are captured in batches in a round-robin fashion: each site
is captured once until all sites are done. The process then starts
again until all pages are captured the approximate number of
times. Prior to this capture process, one or no defense is enabled. In
the case of the new defense, it is configured, if necessary. The Tor
Browser Bundle clears its cache and saved data on each restart. This
takes care of resetting the circuit and deletes all cached data. This
procedure also reproduces the time gaps described in
\citet{effective2014}.


The data is analysed via a re-implemented version of CUMUL
\citep{panchenko2}, which was used as state-of-the-art in the latest
attacks by \citet{197185} and \citet{rimmerautomated}.
*** maybe                                                           :ARCHIVE:
    One host runs the Tor Browser Bundle[fn::at the current version]
    and the cover traffic server (if needed), the other runs a Tor
    server instance in bridge mode. For WTF-PAD, an additional server
    transport program is run at the bridge, and a client transport at
    the client[fn::WTF-PAD is run via the stand-alone programs. Tor's
    built-in =ServerTransportPlugin= and =ClientTransportPlugin=
    configuration naïvely failed].

    This setup utilises the same bridge for WTF-PAD and the browser
    extension.

- settings
  - no defense
  - new defense
    - factor 20
    - factor 100
  - llama
  - (wfpad)
  - (tamaraw)

    Single traces are captured via the Python script
     ~one_site.py~[fn::
     https://github.com/kreikenbaum/website-fingerprinting-thesis/blob/master/capture/one_site.py]. It
     cleans the cache between captures by restarting the Tor Browser
     Bundle.

\citet{DBLP:journals/popets/CherubinHJ17} provide aspects on
how to do this.
** KEYWORDS Analyse 1: Closed World Scenarios
   :PROPERTIES:
   :CUSTOM_ID: sub4-closed
   :END:
#+INDEX: evaluation!closed-world
#+INDEX: closed-world evaluation
As section [[#sub2-ml-measure]] mentions, there are two settings in
analysing \gls{wf}: open-world and closed-world. Closed-world analysis
distuinguishes only between the main sites, say Eve's aim is to see if
Alice visits bob.com or her uncle charlie.com [[http://etsy.com]]'s
traces, assuming both are being monitored by Eve. This section will
presents the results of closed-world analysis. It first compares the
accuracy of \citet{panchenko2}'s original to the re-implemetation on
defenseless traces.

#+CAPTION[Closed-World Results]: Results of closed-world setup. The traces captures at University of Brunswick show similar Accuracy to \cite{panchenko2}. The traces captured using Google Cloud virtual machines show higher accuracy. A similar phenomenon was reported by \citet[sec.4.7]{ccs2014-critical}.
#+NAME: tab:closed-world-accuracy
#+ATTR_LATEX: :align |c||c | c|
| Data source       | number of instances | Accuracy |
|-------------------+---------------------+----------|
| \cite{panchenko2} |                  90 |    91.38 |
| \cite{panchenko2} |                  40 |    92.03 |
| Univ. Brunswick   |                  50 |    91.61 |
| Google Cloud      |                  50 |    95.12 |


Recent studies (\citet{197185}
\citet{DBLP:journals/popets/CherubinHJ17} \citet{rimmerautomated})
consider CUMUL by \citet{panchenko2} the state-of-the-art \gls{wf}
attack. Table [[tab:closed-world-accuracy]] ensures attack accuracy by
comparing CUMUL \citep{panchenko2} to its reimplementation on
defenseless web site traces. In most cases, the original CUMUL attack
and the reimplementation had similar results. This validates that the
CUMUL-reimplementation[fn::available at
https://gitlab.com/kreikenbaum/classify.git] was used as the main
method of evaluating the extension. The kNN-attack by
\citet{effective2014} showed worse results, as validated by the above
literature.


The next step is to evaluate this thesis's defense, and to compare
them to a recent defense. The main metric in a closed-world setup is
accuracy. Table ... compares this defense




| scenario             |       date | accuracy [%] | size increase [%] | time increase [%] |
| <20>                 |            |     <7> |     <7> |     <7> |
|----------------------+------------+---------+---------+---------|
| LLaMA                | 2018-01-17 | 0.712562173548 | 77.5946240235 | 1126.55428806 |
| LLaMA                | 2018-01-17 | 0.55456425735 | 94.2469507093 | 1085.24719005 |
| no defense           | 2018-01-17 | 0.985725197071 |       0 |       0 |
| no defense           | 2018-01-21 | 0.925383200416 |       0 |       0 |
| no defense           | 2018-02-02 | 0.951234347019 |       0 |       0 |
| new defense          | 2018-02-02 | 0.252722925023 | 147.316726011 | 22.7220412822 |
| no defense           | 2018-02-04 | 0.916108218944 |       0 |       0 |
| new defense          | 2018-02-05 | 0.793416946193 | 22.8851083013 | 17.4885442924 |
| new defense          | 2018-02-05 | 0.664134930342 | 37.8220452453 | 14.6909035797 |
| defense-client-nodelay | 2018-04-19 | 0.448330253099 | 213.021412115 | 14.7926631682 |
| no defense           | 2018-04-20 | 0.945038351392 |       0 |       0 |
| defense-client-nodelay | 2018-04-19 | 0.357176576253 | 205.179048915 | 29.0409100737 |
| no defense           | 2018-04-20 | 0.92905711923 |       0 |       0 |
| new defense          | 2018-04-20 | 0.263772866075 | 142.22133021 | 28.1567120732 |
| new defense          | 2018-04-20 | 0.426254627367 | 61.8128980509 | 22.2276343619 |
# print results.to_table(sorted([r for r in results.list_all() if not r.open_world and hasattr(r.scenario, "date") and r.scenario.date > datetime.date(2018, 1, 1)]))




- does defense work (better than others)
  aka what are closed-world results
  aka what are the main results
  - does defense work
    aka does the defense work
    aka validate attack
   - accuracy
    - lowers accuracy
   - graph defense accuracy vs overhead

    The thesis' configurability is shown in Figure [[fig:oh2acc]].
    #+CAPTION: Size Overhead to Accuracy Trade-Off for Thesis' Defense, LLaMA-nodelay, and no defense on 30 sites.
    #+NAME: fig:oh2acc
    [[./pictures/oh2acc--llama-0.22-disabled.eps]]
    # code: see [[file:bin/results.py::%20scatter%20plot%20of%20accuracy%20vs%20overhead]]
     - include llama-delay @ 30
     - ?additional 0.22 data points in between groups (200--300% overheads)
     - data points with tiny overhead (?=negative factor?)

   - conclusion
  - better than others
    aka how does the other defense compare to this
   - llama lowers accuracy as well
    This shows that _original LLaMA delays web surfing by an order of
    magnitude_. This result is after removing plain load errors, of
    which there were 43.67%. After this removal, 16 of 30 sites had
    less than 30 instances. If these sites were removed, accuracy
    against LLaMA rose to 71.26%, with overheads remaining roughly the
    same.

    Compared to the modified version of LLaMA, this thesis' defense
    offers both configurability and a lower size overhead. Both LLaMA
    and the new defense have negligible time overhead. Even compared to
    non-delayed LLaMA, the new defense offers similar accuracy/size
    overhead relationship for a much lower time overhead.
   - [wtf-pad]
   - comparison accuracy vs overhead (100 sites)
    - add point for llama-nodelay

    How does the defense compare to others (\cite{wtfpad2015},
    \cite{LLaMA}, \cite{a-systematic2014})?

    - walkie-talkie theoretical
    - wtf-pad
    - llama
      - how does the defense compare to llama etc

    Table [[tab:closed_world]] compares
    LLaMAs\citep{DBLP:journals/popets/CherubinHJ17} performance with
    this thesis' addon's on 30 sites. As LLaMAs original time overhead
    seems prohibitive, the source code was edited (a constant was
    changed) to disable the delay. This version is called
    /LLaMA-nodelay/.

    #+CAPTION[Comparison of LLaMA to this thesis's defense on 30 sites]: Comparison of LLaMA to this thesis's defense on 30 sites. The top lines shows LLaMA stats, the next two lines try to match first the accuracy, then the size overhead with the new defense.
    #+NAME: tab:closed_world
    #+ATTR_LATEX: :align |l||n{2}{2}|n{3}{2}|n{4}{2}|
    | <17>              |       <10> |       <10> |       <10> |
    | scenario          | \open accuracy [%] \close | \open size overhead [%] \close | \open time overhead [%] \close |
    |-------------------+------------+------------+------------|
    | no defense (2017-12-26) |      96.46 |          0 |          0 |
    |-------------------+------------+------------+------------|
    | LLaMA             |      55.46 |      94.25 |    1085.25 |
    | 30aI--2016-07-25  |      55.75 |      81.13 |      27.80 |
    | 50aI--2016-07-26  |      52.03 |      96.25 |      15.53 |
    |-------------------+------------+------------+------------|
    | LLaMA-nodelay     |      68.22 |     163.08 |      -4.02 |
    | 5aI--2016-10-09   |      68.59 |      57.92 |       3.22 |
    | 100aI--2017-12-27 |      42.43 |     162.20 | -0.8895383 |
    # llama is from 2018-01-07
Appendix [[#appendix-accuracy]] shows more evaluation results.
*** more                                                            :ARCHIVE:
    - how is data analysed
        aka how can the analysis be reproduced
        aka how was data analysed
      - what is the original attack
      - feature extraction and classification in python
    - how is it assured
    - mention problems in capturing
    - [what about other attacks]
      - why not rimmer
        - lots (!) of data needed
        - high-power GPU needed
    - p-cumul - my cumul mean/median/t-value/...
** Analysing Open World Scenario
   :PROPERTIES:
   :CUSTOM_ID: sub4-open
   :END:
  [[./pictures/ccdf-recall-no-defense-on-2018-02-02.pdf]]
  # mplot.ccdf_curve_for_scenario( scenario.Scenario("disabled/bridge--2018-02-02--100@50"), filt=lambda x: len(x.scenario.get_traces()['background']) >= 1000)

  No defense, closely matches Panchenkos recall curve.

  [[./pictures/ccdf-recall-new-defense-2018-02-13.pdf]]
  # mplot.ccdf_curve_for_scenario( scenario.Scenario("wf-cover/bridge-20aI--2018-02-13--100@50"), filt=lambda x: len(x.scenario.get_traces()['background']) >= 1000)

  Defense: much lower values.

  [[./pictures/ccdf-recall-defense-client-nodelay-on-2018-02-25.pdf]]
  # mplot.ccdf_curve_for_scenario(...)


  Llama: also low value, but much higher overhead (200%, vs 67%)

          - validate attack
          - tpr/fpr
          - auroc
          - conclusion
- what are open-world results
- why do open-world results differ from panchenko
- what the fuck
*** removed sites
#+INDEX: cumul traces:linkedin
#+INDEX: cumul traces:removed:linkedin
#+INDEX: cumul traces:removed:cutoff
#+CAPTION: Removed traces at cutoff point of 10 FP sites (linkedin.com). These show high variability as compared to Figure [[fig:cumul-well-open]].
#+ATTR_LATEX: :float nil :width 0.5\textwidth
#+NAME: fig:traces_cumul_linkedin
[[./pictures/traces_cumul_linkedin.com_2018-02-02.pdf]]
*** CHECK Open World Accuracy
    While closed-world results are easier to compare and analyse,
    \gls{wf} is modeled more accurately by an [[#sub2-ml-measure][open-world model]]. This
    model adds additional background pages to the mix: An attacker's
    scenario is to find out if a web trace is to a sensitive site, say
    wikileaks, as opposed to normal web browsing. The background pages
    model the "normal" web browsing.


    Scikit-learn \citep{scikit-learn} has the advantage of /balanced
    class weights/: classes can have different sizes, yet are considered
    of the same importance when training classifiers. This useful
    setting was enabled in the beginning of open world classification as
    a carryover from closed-world analysis. In the open world setting,
    it reduced accuracy to 55%.



    As soon as this setting was disabled, accuracy rose to e.g. 83.55%,
    with true positive rate of 81.39% and false-positive rate of
    12.32%. This still does not match \cite{panchenko2}'s. To improve
    further, the confusion matrix in Figure [[fig:open-world-confmat]] shows
    which sites were misclassified how often:

    #+CAPTION[Open world confusion matrix]: Open world confusion matrix. Background to background classification (5928) was removed to normalize colors.
    #+NAME: fig:open-world-confmat
    [[./pictures/confmat_open_2018-02-02.pdf]]

    One site that was exclusively classified as background is [[http://t.co][t.co]] (on
    the chart between microsoftonline.com and amazonaws.com).  For
    [[http://t.co][t.co]], 48 of 48 (valid) traces were classified as background
    sites. The next-highest rates of false-negative misclassification is
    [[http://rakuten.co.jp][rakuten.co.jp]], which has 37 of 49 sites incorrectly classified as
    background pages. This might be due to their highly varied traces,
    as can be seen in Figure [[fig:t.co-rakuten-cumul-traces]]:

    #+CAPTION[CUMUL traces for t.co]: CUMUL-Traces for the websites [[http://t.co][t.co]] and [[http://rakuten.co.jp][rakuten.co.jp]]. These show high (intra-class) variability, which could explain the high misclassification rates. Compare Figure [[fig:CUMUL_traces]] and [[fig:cumul-well-open]] for lower-variability CUMUL traces.
    #+NAME: fig:t.co-rakuten-cumul-traces
    [[./pictures/cumul__t.co_rakuten.co.jp__2018-02-02.pdf]]
    # (f, a) = plt.subplots(1, 2)
    # mplot.traces_cumul(r.scenario, "rakuten.co.jp", axes=a[1])
    # mplot.traces_cumul(r.scenario, "t.co", axes=a[0])


    This can be explained by the content: t.co shows only 3 lines of
    text, and thus any variation in the download of its little content
    leads to higher variability. rakuten.co.jp has shopping results,
    which are bound to show high variation.

    #+CAPTION[CUMUL traces for well-classified open world sites]: CUMUL-Traces for well-classified website dailymotion.com.
    #+NAME: fig:cumul-well-open
    [[./pictures/cumul__dailymotion.com__2018-02-02.pdf]]
    As a contrast, the website [[http://dailymotion.com][dailymotion.com]] had 42 of 42 correctly
    classified captures. Its CUMUL-traces show low variability in Figure
    [[fig:cumul-well-open]].

    #+CAPTION: Misclassification count (open world)
    #+NAME: tab:high-error-open-world
    #+ATTR_LATEX: :align |l||r | r|
  |--------------------+-------------------------+-------------------------|
  | site               | site as background (FN) | background as site (FP) |
  |--------------------+-------------------------+-------------------------|
  | ameblo.jp          |                      19 |                      30 |
  | aliexpress.com     |                      17 |                      24 |
  | godaddy.com        |                      22 |                      23 |
  | youporn.com        |                      16 |                      21 |
  | google.com         |                      30 |                      20 |
  | aol.com            |                      32 |                      19 |
  | dropbox.com        |                      31 |                      11 |
  | netflix.com        |                      36 |                       5 |
  | rakuten.co.jp      |                      37 |                       2 |
  | t.co               |                      48 |                       0 |
  |--------------------+-------------------------+-------------------------|

    Table [[tab:high-error-open-world]] shows the misclassification rates of
    the sites that were misclassified the most often (\ge 30 false
    negatives or \ge 20 false positives). Let us examine the top-5
    false-positive sites in detail. As Figure [[fig:high-fpr-four]] shows,
    four of these show high intra-class variability:

    #+CAPTION[CUMUL traces high-false-positives sites]: CUMUL-Traces for the websites with high false-positive rates. The sites are (left-to-right) ameblo.jp, youporn.com, aliexpress.com, and godaddy.com. These also show high (intra-class) variability, which could explain the high misclassification rates.
    #+NAME: fig:high-fpr-four
    [[./pictures/traces_cumul_no_defense_on_2018-02-02_ameblo.jp_youporn.com_aliexpress.com_godaddy.com.pdf]]
    # s = scenario.Scenario("disabled/bridge--2018-02-02--100@50")
    # a = ["ameblo.jp", "youporn.com", "aliexpress.com", "godaddy.com"]
    # for (i, el) in enumerate(a): mplot.traces_cumul(s, el, axes=axess2[i]); axess2[i].set_title(" ")
    # b = axess2[0]
    # axess2[0].set_ylabel(" ")
    # axess2[1].set_xlabel("Feature Index")
    # plt.tight_layout()
    # plt.savefig("/tmp/traces_cumul_{}_{}.pdf".format(s, "_".join(a)).replace(" ", "_"))
    - ameblo.jp (and youporn.com) show a range of dynamic content
    - aliexpress.com sells different items at different times. The
      images at least are bound to have different byte sizes
    - godaddy.com had versions in various languages, serving different
      content, which explains the varied traces

    #+CAPTION[CUMUL traces high-false-positive google.com]: CUMUL-Traces for the last website with high false-positive rates: google.com. It shows low variability. Its high misclassification rate is explained by other google sites being in the background set (e.g. google.fi).
    #+NAME: fig:high-fpr-google
    [[./pictures/traces_cumul_no_defense_on_2018-02-02_google.com.pdf]] The
    fifth site, google.com also serves international content. Yet, its
    traces show low variability, as seen in Figure
    [[fig:high-fpr-google]]. While google.com sister sites, say google.fi,
    were removed from the foreground set, the background set contained
    hundreds of google sites as well. This of course made classification
    infeasible.



    Another low-res with 10 =exclude_sites=:
    [[./pictures/traces_cumul_no_defense_on_2018-02-02_wikipedia.org.pdf]]


**** Analysing defenseless results
     - correlation: _id increasing decreases fpr, tpr, score
     - increasing bg size decreases fpr, tpr, similarly for scenario.date
     - auroc: high positive correlation to fpr, high negative to tpr
     - tpr correlated to fpr
     - len of exclude_sites decreases (-0.4) fpr, (and -0.2 tpr)
     - code
     - cross-correlation table
# df = pd.DataFrame(r.to_dict() for r in results.list_all() if r.open_world and "disabled")
# df = df[df["background_size"] != 'none'][df["background_size"] != "auto"][df["background_size"].notnull()]
# df['background_size'] = df['background_size'].to_numeric()
# del df['auc_bound']
# df.corr().to_latex()
#+BEGIN_EXPORT latex
\begin{tabular}{lrrrrrrrrrrr}
\toprule
{} &      C &    \_id &  auroc &  bg\_size &    fpr &  gamma &  score &   size &  size\_overhead &  time\_overhead &    tpr \\
\midrule
C             &  1.000 &  0.050 &  0.469 &        -0.111 & -0.117 & -0.077 &  0.118 & -0.485 &        -0.033 &        -0.058 &  0.024 \\
\_id           &  0.050 &  1.000 &  0.741 &         0.285 & -0.133 &  0.035 & -0.156 &  0.152 &         0.151 &         0.126 & -0.235 \\
auroc         &  0.469 &  0.741 &  1.000 &        -0.058 & -0.005 & -0.012 & -0.451 &  0.384 &        -0.071 &        -0.098 & -0.122 \\
background... & -0.111 &  0.285 & -0.058 &         1.000 & -0.735 &  0.246 & -0.010 &  0.083 &        -0.058 &        -0.144 & -0.697 \\
fpr           & -0.117 & -0.133 & -0.005 &        -0.735 &  1.000 & -0.371 & -0.198 &  0.275 &         0.189 &         0.303 &  0.577 \\
gamma         & -0.077 &  0.035 & -0.012 &         0.246 & -0.371 &  1.000 &  0.444 &  0.003 &        -0.304 &        -0.367 &  0.223 \\
score         &  0.118 & -0.156 & -0.451 &        -0.010 & -0.198 &  0.444 &  1.000 & -0.352 &        -0.577 &        -0.763 &  0.391 \\
size          & -0.485 &  0.152 &  0.384 &         0.083 &  0.275 &  0.003 & -0.352 &  1.000 &         0.166 &         0.210 & -0.133 \\
size\_overhead & -0.033 &  0.151 & -0.071 &        -0.058 &  0.189 & -0.304 & -0.577 &  0.166 &         1.000 &         0.858 & -0.274 \\
time\_overhead & -0.058 &  0.126 & -0.098 &        -0.144 &  0.303 & -0.367 & -0.763 &  0.210 &         0.858 &         1.000 & -0.225 \\
tpr           &  0.024 & -0.235 & -0.122 &        -0.697 &  0.577 &  0.223 &  0.391 & -0.133 &        -0.274 &        -0.225 &  1.000 \\
\bottomrule
\end{tabular}

#+END_EXPORT
** Summary
   :PROPERTIES:
   :CUSTOM_ID: sub4-summary
   :END:
- what is the conclusion
- (why) is this defense better than llama
* Conclusions [0/10000]
  :PROPERTIES:
  :CUSTOM_ID: ch6-conclusion
  :END:
** Future Work [/1000]
   :PROPERTIES:
   :CUSTOM_ID: future-work
   :END:
*** purpose
*** limits
*** vision
*** brainstorm
    - addon-sdk replace by webextension
      - not that much to do
      - when/if necessary for Tor's ESR-version-based browser
      - advantage: also Google Chrome
    - seems like connection establishment leaks data, as of ch4
    - distributions new traffic model

These distributions have two drawbacks. Firstly, web traffic has
evolved since 2007 \citep{DBLP:conf/imc/IhmP11}, as also documented for
total web page size by \citet{web-is-doom}. Secondly, as mentioned in
\citet{newtrafficmodel}, the number of embedded objects are computed
per each HTML page, including frames, and possibly including
redirects. How the number of embedded elements is used in this thesis
differs from how it should be used, see next section.

- why a server
  - could easily cache sizes, reuse
    - or use list of servers
  - but this as proof of concept

- classification code at
**** How to Distinguish HTML and Embedded Objects
     :PROPERTIES:
     :CUSTOM_ID: distinguish_HTML_embedded
     :END:
     The traffic model \citep{newtrafficmodel} distinguishes between
     HTML and embedded object requests. This is usually done via HTTP's
     =content-type= header\citep[sec.3.1.1.5]{rfc7231}, yet that is only
     accessible when the content has been received, whereas this
     defense needs to distinguish at the time it is requested.

     World Wide Web URLs increasingly move away from including a file
     type suffix\cite{cooluri}, so that distinguishing HTTP elements at
     request time only by URL would not work reliably. The work-around
     in this add-on is to consider the first request to a host as the
     HTML page, while subsequent requests while the page is being
     loaded[fn::until the body's =load=
     event\cite[ch.1.6.5]{dom2-events}] are considered requests for
     embedded objects.

     This accurately distinguishes between start of a page load and the
     loading of its embedded objects, but a drawback is that is does
     not recongnize embedded iframes etc. as HTML.

     Providing an accurate estimate of embedded objects /per web page/
     would be [[#future-work][future work]].
\appendix
\part{Appendix}
* appendices (begin above this headline; this is for searching)     :ARCHIVE:
  above, as in this section cuts it out (due to ARCHIVE tag)
* CHECK Cached: Size of HTML-Documents
  :PROPERTIES:
  :CUSTOM_ID: find sizes of HTML-documents
  :END:
  The HTTP traffic model\cite{newtrafficmodel}'s statistical size
  generation is based on application-level[fn::the data sizes
  transported by TCP] sizes on the network, as its authors analysed
  logfiles of the Squid[fn:: \url{http://www.squid-cache.org}] proxy.

  These sizes could not be trivially obtained from the HTTP
  =Content-Length=-header\cite[sec.3.3.2]{rfc7230}, as it does not
  represent additional headers and size-reduction via
  compression. Thus, the sizes were determined by retrieving the
  files with =wget= via Squid, and reading the sizes from the Squid
  log.

  Retrieval is implemented in the =html_top_100.sh= script.  It
  initially empties Squid's log file and cache by restarting
  it. Afterwards, the [[#top100][top-100] files are retrieved with =wget= via
  Squid.

  From Squid's =access.log= log file, the sizes are extracted via the
  command

  #+BEGIN_SRC sh
    sudo cat /var/log/squid3/access.log | tr -s ' ' | cut -d ' ' -f 5,7 >
    HTML-sizes
  #+END_SRC

  These sizes are then converted to a JSON\cite{rfc7159}-array via
  the =htmlSizeToJSON.py=-file. It also checks for duplicate sizes
  for each file-URL, choosing the lower one. This could increase
  traffic, but the opposite might be too little traffic, thus easier
  website fingerprinting, which should be avoided.
** WAIT github link for =html_top_100= script, and htmlsizetojson.py
* WRITE Cached: Number of Embedded Objects
  :PROPERTIES:
  :CUSTOM_ID: number_embedded
  :END:
  The second parameter for generating cover traffic is the number
  of embedded objects per HTML-page.

  These are extracted via the python script ~htmlToNumEmbedded.py~
  which is called for each of the top-100's main web pages by
  ~retrieve-100-embedded.sh~.

  To extract, python's lxml module to parse the HTML's
  DOM extracts the URLs of embedded files from the features of
  several tags, f.ex.\space{}the =src= element of =img= tags.

  This approximation currently omits some possibly embedded elements,
  f.ex.\space{}those embedded in css files and =style= tags via the =@url=
  css-directive. It seems better for cover traffic to slightly
  underestimate the number of embedded elements. This might generate
  more traffic than strictly necessary, but here, safe seems better
  than sorry.
** WAIT link to lxml website
** WAIT github links
* [#D] Accuracy Results [0/1]
  :PROPERTIES:
  :CUSTOM_ID: appendix-accuracy
  :END:
In the following tables, /size/ is the (geometric) size increase in %
as related to the closest no-add-on capture, /knn/ is
\cite{effective2014}'s classifier, /p-cumul/ is \cite{panchenko2}'s CUMUL
implementation, /cumul/ this thesis's reimplementation, and
/tts-cumul/ is this thesis's CUMUL with data split into training and
testing set, parameter estimation on the training set and
cross-validation on the testing set, as opposed to vanilla cross-validation. The
classifiers trained and tested on the same scenarios, as compared to
training on data without defense, and testing on defended data.
#+CAPTION: 10-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:10-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}| :environment longtable
#+INCLUDE: "data/skip/results/alternatives.org::#10-sites-pub" :only-contents t

#+CAPTION: 30-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:30-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}| :environment longtable
#+INCLUDE: "data/skip/results/alternatives.org::#30-sites-pub" :only-contents t

#+CAPTION: 100-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:100-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|
#+INCLUDE: "data/skip/results/alternatives.org::#100-sites-pub" :only-contents t
** TODO [#D] create dynamically from database
* KEYWORDS [#C] The Base Rate Fallacy
  :PROPERTIES:
  :CUSTOM_ID: base-rate
  :END:
  - two stats-related: psych and IT/IDS
    - psych: kahneman+..., bar-hillel
    - IT/IDS: axelsson\cite{axelsson2000base}
  - bayes
    - hard for them, easy for us (?) bayes rate fallacy
      - axelsson
      - just need a few %
      - but: theoretical concept, better be a bit sceptical
        - \cite{koehler1996base} in general (original authors sceptical, too)
        - rieck\cite{rieckdiss} had success in IDS

   This knowledge helps in understanding and creating defenses. As of
   \cite{a-systematic2014}, \cite{ccs2014-critical} and \cite{panchenko2},
   [[#base-rate][the Base Rate Fallacy]] creates problems for /some/
   \gls{wf}-adversaries. This means that finding people who might have
   accessed a certain site is easier than making sure that they really
   visited the site.
* WRITE Addon: Factor to Overhead
  - addon has factor setting
  - how to estimate overhead from this
    - useful for e.g. matching llama overhead
  - from list of tuples of =(defense-factor: overhead)=
  - result: 1.47 * factor + 39.63
  - estimate to get to overhead of 295 is 175: i(175) \approx 296.9
  [[./pictures/factor_to_overhead.eps]]
  # creation see code at: [[file:~/da/da.org_archive::*factor%20to%20overhead%20at%200.22aI][factor to overhead at 0.22aI]]
* WRITE Bloom-sort
  :PROPERTIES:
  :CUSTOM_ID: bloom-sort
  :END:
   By ordering data into bins, it becomes possible to use Bloom filters
   for the estimation of sizes, using one Bloom filter for each bin.

   To achieve this, sensible separation criteria (called /splits/) for
   the bins need to be found. Afterwards, each bin needs to be assigned
   a value (called /size/) for all contained elements. See appendix
   [[#bloom-params]] on determining the sizes and splits.

   This data-structure, called /Bloom-sort/ is initialized with an
   array of splits, and an array of sizes. The sizes-array needs to
   have one more element than the splits-array, as the bins are bounded
   on the left by 0, and on the right by infinity.

   #+BEGIN_SRC js
     /**
      ,* @param {sizes Array} array of values for each bin, must be sorted
      ,* @param {splits Array} array of bin borders, must be sorted
     ,*/
     function BloomSort(sizes, splits) {
         this.sizes = sizes;
         this.splits = splits;
         this.filters = [];
         for ( let i = 0; i < sizes.length; i++ ) {
             this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
         }
     }
   #+END_SRC

   Thus, you get
   \[-\infty \le \text{size}_0 \le \text{split}_0 \le \text{size}_1 \le \text{split}_1 \le ... \le \text{split}_{n-1} \le \text{size}_n < \infty\]
   Given the splits, it becomes possible to add the elements to their
   bins:

   #+BEGIN_SRC js
     BloomSort.prototype.add = function(id, size) {
         this.filters[_.sortedIndex(this.splits, size)].add(id);
     };
   #+END_SRC

   where =_.sortedIndex()= gives the index at which =size= would be
   inserted into the sorted =this.splits= array.

   The retrieval of element sizes looks into each Bloom filter,
   checking whether it might contain the element =id=. If one Bloom
   filter reports containment, its corresponding element- =size= is
   returned. If several or no Bloom filters report containment, an
   exception is thrown. The exception is used to allow all possible
   return values, not blocking one of them, say =-1=, for the error
   condition.
   #+BEGIN_SRC js
     /** determines size of element, raises exception if unclear */
     BloomSort.prototype.query = function(id) {
         let pos = -1;
         for ( let i = 0; i < this.filters.length; i++ ) {
             if ( this.filters[i].test(id) ) {
                 if ( pos === -1 ) {
                     pos = i;
                 } else {
                     throw {
                         name: 'BloomError',
                         message: 'Contains multiple entries'
                     };
                 }
             }
         }
         if ( pos === -1 ) {
             throw {
                 name: 'BloomError',
                 message: 'Contains no entries'
             };
         }
         return this.sizes[pos];
     };
   #+END_SRC

   It is initialized with
   #+BEGIN_SRC js
   let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
   #+END_SRC

   then adding elements via =htmlSizes.add("http://google.com/", 613)=
   and querying via =htmlSizes.query("http://google.com/")=, which
   would yield =400=. See usage in =size-cache=.

   - link to main bloom-sort, create custom-id there
** WAIT github link
* CHECK Determining Bloom-Sort Parameters
  :PROPERTIES:
  :CUSTOM_ID: bloom-params
  :END:
  It is impractical to store the sizes of many URLs, as this would
  greatly increase the size of the addon. A possibility to save space
  is to use Bloom Filters to aggregate groups of URLs with similar
  values, as described in [[#bloom-sort][Bloom-sort]].

  Each group needs borders (given via the /splits/ method parameter)
  and a size to represents its contained elements.

  Determining the optimal number of groups, splits and sizes is a
  topic of [[#future-work][further work]]. Here, initially the quantiles of the
  HTTP-model (see [[#HTTP traffic model][HTTP traffic model] ) were used. When the data were
  to be inserted, it turned out that especially the numbers of
  embedded elements did not match the theoretically proposed groups:

  For three groups, the splits would be given by the 33 1/3 and 66
  2/3 quantiles, as 0.0107 and 1.481. This would create two
  single-valued groups, which only would contain the elements 0
  and 1. The next group would contain all other elements: The
  (representative) sizes of the groups were given as 7.915E-05,
  0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

  The actual data to be inserted (see [[#number_embedded][Cached: Number of Embedded
  Objects]]) had the /splits/ (quantiles) at 10 2/3 and 36 2/3 and the
  /sizes/ (middle quantile) at 6, 20, and 59 2/3.

  In addition to using the observed sizes for the Bloom filter, the
  number of groups was increased to 5.
** WAIT [#C] graphics: histogram with bins (colored bars, black bin limits)

* removed                                                           :ARCHIVE:
** DANIEL Website Fingerprinting Defenses
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf-def
   :END:
   This section describes defenses against \gls{wf} as described
   [[#sub2-wf][previously]]. As most \gls{ml}, \gls{wf} uses statistical
   properties of the underlying data. It could possibly be defeated by
   shuffling these properties. The total number of incoming packets
   e.g. is a feature used by almost all modern attacks:

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION: distribution of number of total incoming packets
   #+NAME: fig:total_packets_in
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-06_tamaraw_wtf-pad___bridge--2016-07-05__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   As seen in Figure [[fig:total_packets_in]], \cite{wtfpad2015}'s publicly
   available implementation of his own WTF-PAD and
   Tamaraw\cite{a-systematic2014} both create additional packets, but
   preserve site separation and ordering. Contrast this with this
   thesis' defense in Figure [[fig:total_packets_in_thesis]]

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION[distribution of number of total incoming packets, thesis defense]: Distribution of number of total incoming packets, thesis defense. The 5aI setting has small overhead, 30aI has average overhead.
   #+NAME: fig:total_packets_in_thesis
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-21_0.22___5aI--2016-07-19_0.22___30aI--2016-07-25__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   Each site's variances have been increased with the result of them
   overlapping. This figure hints that this thesis' defense more
   effectively munges websites fingerprinting traces, and is also
   tunable.

   While it was possible to get \cite{wtfpad2015} to run given [[https://bitbucket.org/mjuarezm/obfsproxy_wfpadtools][the
   provided software]], \cite{wang2015walkie}'s state-of-the-art defense
   was compared using his reported values. As \cite{wang2015walkie}
   uses a normal or uniform distribution in addition to a novel
   single-duplex method, it could arguably be augmented by
   HTTP-specific cover traffic distributions.

   Another point is ease-of-installation: Firefox, on which the Tor
   Browser Bundle is based, enables extensions. These already provide
   some of the Tor Browser Bundle's
   functionality\cite{tor-browser-design-impl} and were used in this
   thesis' to ease installation as compared to \cite{wtfpad2015} and
   arguably also to \cite{wang2015walkie}.\\


   In 2017, the novel LLaMA defense was introduced in
   \cite{DBLP:journals/popets/CherubinHJ17}. The paper acknowledges
   the need for a client-side application-level defense. In contrast
   to this thesis's defense, LLaMA uses a uniformly distributed delay
   until the cover request, whose URL is from the list of previously
   requested URLs at the same host.

   This mimicks another request to the same page, thus has
   approximately the correct size distributions, but does not cloak
   the high-information initial HTTP request. Also, it is highly
   deterministic for the first elements of cover traffic, until the
   list of requested elements from this host is big enough to allow
   for some randomness. The authors provided LLaMA as a secondary
   defense to the server-side ALPaCA defense, and emphasize its draft
   status\cite{LLaMA}.
** (old.org) Defenses
   :PROPERTIES:
   :CUSTOM_ID: defenses
   :END:
   As soon as it became clear that anonymized (web) traffic could be
   analysed to guess the destination, defenses were proposed.

   The general idea is to obfuscate characteristics of the
   data. Defenses progressed from obfuscating specific features
   that were used for fingerprinting, to trying to obfuscate the
   whole of traffic, including features not previously thought of.


   The next sections describe [[#def-early][early defense proposals]], then address
   [[#def-specific-vs-general][defenses tailored to specific attacks, versus /general defenses/
   that hope to defend against all possible attacks]], and the
   deterministic [[#def-tamaraw][Tamaraw]] defense. The last sections present
   [[#def-stochastic][stochastic defenses]] that add randomness to achieve meaningful
   reduction of detection strength with less bandwith and time
   overhead than deterministic ones, with the [[#def-wtf-pad][WTF-PAD]] defense.
*** Early Defenses
    :PROPERTIES:
    :CUSTOM_ID: def-early
    :END:
    #+INDEX: defense
    As defense against fingerprinting webpages when retrieved via SSL,
    \cite{ssl-traffic-analysis} mainly proposed using
    proxies[fn::as well as HTML- and protocol modification]. Addressing
    weaknesses when using proxies, \cite{hintz02} proposed
    three defenses: (1.) adding noise to traffic, (2.) reducing
    transferred data, and (3.) transferring everything in one connection.

    As for the (2.) approach of reducing transferred data: As the
    sizes and interconnection of HTML and embedded content is what
    makes a webpage easily identifyable, either using a text-only
    non-javascript browser such as Lynx[fn::available at
    \url{http://lynx.invisible-island.net/}], disabling f.ex.\space{}images,
    or reducing cross-site requests via f.ex.\space{}the
    RequestPolicy[fn::RequestPolicy
    \url{https://requestpolicycontinued.github.io/} uses domain-based
    filters, which is considered undesirable in the Tor Browser
    Bundle\cite[sec.2.3]{tor-browser-design-impl}] extension all might
    mitigate the threat of website fingerprinting for those who
    consider this trade-off acceptable. Yet, this reduces usability
    and thus conflicts with one of Tor's design
    goals\cite[sec.3]{tor-design}. It is also mostly deterministic, so
    that an adversary could simply train on modified data.

    The (3.) approach of transferring everything in one connection
    --- while valid --- would require modifying the HTTP
    protocol. This would conflict with Tor's design goal of
    deployability\cite{tor-design}.

    The (1.) approach of /adding noise to traffic/ is used by
    f.ex. \cite{panchenko}. Others --- additionally or exclusively
    --- /alter traffic/ by reducing the data rate, splitting and
    merging packets, etc, f.ex. \cite{morphing09}, \cite{httpos2011}.
*** Specific vs General Defenses
    :PROPERTIES:
    :CUSTOM_ID: def-specific-vs-general
    :END:
    #+INDEX: defense!specific
    #+INDEX: defense!general
    #+INDEX: defense!BuFLO
    #+INDEX: BuFLO
    As of \cite{wang2015walkie}, defenses designed against website
    fingerprinting can be divided into specific and general defenses.
    Both types can either add noise, modify existing traffic, or
    combine both.

    The first website fingerprinters considered only packet
    lengths. This made it seem sensible to defend by altering the
    lengths of packets by padding, as evaluated f.ex.\space{}by
    \cite{ssl-traffic-analysis}.

    As more and more features were used to classify the traces,
    different ways of altering the data were evaluated by several
    researchers: several ways of padding (\cite{Liberatore:2006},
    \cite{oakland2012-peekaboo}, \cite{a-systematic2014},
    \cite{ccs2012-fingerprinting}, \cite{wang2015walkie}), or altering
    traffic sizes to fit another web page's (\cite{morphing09},
    \cite{httpos2011}).

    Specific defenses alter specific features, mostly single packet
    size. This created an arms race between attacks and defenses - the
    attacks finding new feature combinations to use, the defenses
    obfuscating these. To stop this, the idea of a /general defense/
    was introduced by \cite{oakland2012-peekaboo} into the context of
    website fingerprinting. They proposed a traffic-flow
    security\cite[ch.10.3]{applied96} solution called /BuFLO/:
    fixed-rate transmission of all data, with dummy traffic for gaps,
    for the estimated duration of web site retrieval.

    This idea was improved on by Tamaraw \cite{a-systematic2014}[fn::see
    next section] while \cite{effective2014} proposed the (offline)
    defense of morphing all traffic to supersequences of traffic
    patterns.
*** Tamaraw
    :PROPERTIES:
    :CUSTOM_ID: def-tamaraw
    :END:
    #+INDEX: defense!Tamaraw
    #+INDEX: Tamaraw
    /Tamaraw/\cite{a-systematic2014} is an evolution of
    BuFLO\cite{oakland2012-peekaboo}[fn::a Tamaraw is a small Buffalo,
    see \url{https://en.wikipedia.org/wiki/Tamaraw}]. It also proposes
    fixed-rate sending, and receiving of data, yet with different
    parameters for each.

    To achieve this fixed rate, data is throttled if it exceeds the
    fixed rate, and dummies are sent if the data volume is too low.

    Tamaraw has a tunable overhead, with a trade-off of time vs size.
*** Stochastic Defenses
    :PROPERTIES:
    :CUSTOM_ID: def-stochastic
    :END:
    Previous general defenses were mostly deterministic. The latest
    defenses by \cite{wtfpad2015} and \cite{wang2015walkie} both use a
    stochastic approach to generate additional traffic, with the
    latter\cite{wang2015walkie} additionally modifying the browser to
    send "half-duplex", either exclusively sending or exclusively
    receiving data at the same time. The former \cite{wtfpad2015} adapted
    the ideas from \cite{ShWa-Timing06} to distinguish active and
    non-active periods, with a certain probability of sending dummy
    packets in each, omitting the sending when the browser generated
    packets itself.
*** WTF-PAD
    :PROPERTIES:
    :CUSTOM_ID: def-wtf-pad
    :END:
    #+INDEX: defense!WTF-PAD
    #+INDEX: WTF-PAD
    /Website Traffic Fingerprinting Protection with Adaptive Defense/
    (short: WTF-PAD)\cite{wtfpad2015} is a stochastic defense based on
    adaptive padding\cite{ShWa-Timing06}, which was invented to hide
    from global adversary's correlation attack.

    It samples packet distributions and creates
    distribution-histograms, one for active, called /burst/ mode,
    another for inactive mode, called /gap/ mode. When a packet is
    sent, a timer from the adequate histogram is started. If that
    timer is finished without another packet, a dummy request is sent,
    another packet restarts the timer with new values from the
    histograms.

    WTF-PAD is built using Tor's pluggable transport\cite{tor-spec-pt}
    censorship avoidance layer. This has the advantage of not
    burdening the most stressed part of the network, the exit nodes.
** see also [[*Background][Background {9/9}]]

** maybe (from a new defense:1)
The thesis's defense is implemented using Firefox's Addon-SDK. This
provides for easy deployment, and configuration.
** Defense Design
The defense obfuscates web traffic: For each load of a website,
additional cover traffic is created. The retrieval of the (first) HTML
page provides significant information as of
\citep[sec.4.1]{panchenko}. To counter this information gain, this
first retrieval is always covered by additional traffic. After the
first retrieval, the browser downloads the HTML page resources:
images, style sheets, JavaScript \citep{ecma} files, etc. Each of
these retrievals might be covered by additional dummy retrievals.


The created traffic is based on the HTTP model \citep{newtrafficmodel}
described in section [[#sub3-http]]. For each web page retrieval, the
defense sets target retrieval parameters. From these, the web page's
actual parameters are subtracted to set the amount of cover traffic
for this web page. If the parameters are not known, they are guessed
from the same distributions used to set the target values.


Each request is potentially covered by a simultaneous cover-request,
as seen in Algorithm \ref{algo1}. The algorithm randomly draws from
appropriate HTTP-related distributions \citep{newtrafficmodel}.

 /registerHost/, /isRegistered/, /computeProbability/ and
 /updateHosts/ access a mapping of the recently active hostnames to
 the number of embedded objects and cover requests that still need to
 be sent. Its elements get removed after a certain time to allow for
 new traffic.

 The /lookupOrGuess.../-functions need data structures to map urls to
 both HTTP sizes and number of embedded objects. These change over
 time, just compare the topmost plots each of Figures
 [[fig:total_packets_in]] and [[fig:total_packets_in_thesis]]. As can be seen
 in the next chapter, the amazingly good defense that was achieved
 with up-to-date cached sizes (see next chapter) decreased over time
 to that of guessing sizes. These data structures use Bloom-filter
 \citep{Bloom70space/timetrade-offs} based binning (see section
 [[#sub3-bloomsort]]) to save values related to urls in a fixed space,
 while not allowing an adversary to exactly determine which urls are
 saved. See section [[#sub3-bloomfilters]] for a short introduction to
 their underlying Bloom-filters and section [[#sub3-bloomsort]] for their
 application that saves these in a fixed-size probabilistic data
 structure.


 Ease-of-installation and configuring the amount of cover traffic is
 provided by using Firefox's Addon-SDK. Add-ons can be downloaded via
 [[https://addons.mozilla.org][Mozilla's Addon Page]]. This also provides automatic addon updates.


- why is this better
  - application layer: where problem is
  - in-browser: know which url visited
  - can send specific cover traffic
  - could even cache sizes dynamically in Bloom Filter
- solution design: augment/increase traffic
  - apply to each web page request
    1. see request
    2. use http model
       - for extra html size
       - and number of requests
    3. match requests' own size (predicted if unknown) to model
    4. request additional data over request
    5. on further requests, chance to request additional
  - secondary solution: browser extension
    - easy to install, configure, enable/disable on demand
- HTTP traffic model
  - long-tailed distributions
    - (cut off)
  - for both number of embedded resources and page/embedded sizes
- how is the defense ensured
  - code: some tests for simple modules
  - lint
  - by hand
  - good parts
  - evaluation
- quotes
  - images
    #+BEGIN_QUOTE
    As shown by \citet{newton2005preserving}, naïve privacy-enabling
    methods can be surpassed using domain-specific properties. They
    showed two methods to be effective: randomizing information
    position and adding generic aspects.
    #+END_QUOTE
  - history/stochastic
    #+BEGIN_QUOTE
    300% time\citep{a-systematic2014}). In 2015, the stochastic
    defenses of \citet{wang2015walkie} and \citet{wtfpad2015} much
    lower overhead, were developed. This validates a stochastic
    approach, yet improvements seem possible in two areas: (1) ease of
    installation, and configurability, and (2) more-closely fitting
    cover traffic generation.


    At the start of the thesis, there existed mostly deterministic
    general defenses with high overhead (e.g. over 220% bandwidth, and
    300% time\citep{a-systematic2014}). During the course of this
    thesis, the stochastic defenses of \citet{wang2015walkie} and
    \citet{wtfpad2015} much lower overhead, were developed. This
    validates a stochastic approach, yet improvements seem possible in
    two areas: (1) ease of installation, and configurability, and (2)
    more-closely fitting cover traffic generation.

    Ad 1: \citet{wang2015walkie} alters the Firefox binary, while the
    current version of \citet{wtfpad2015} needed much manual
    adjustment in our attempts.

    Ad 2: \citet{wang2015walkie} uses either a normal, or lognormal
    distribution, not adjusting to HTTP-specifics, while
    \citet{wtfpad2015} samples packets at the network layer. It aims
    at generally hiding /that traffic occurs/, not just which website
    is visited, as it derives its basic mechanism from
    \citet{ShWa-Timing06}.
         - might even help against a global adversary, timing attacks, etc
    #+END_QUOTE
** Web Retrieval Model
World Wide Web \citep{rfc1945} traffic cannot be adequately modeled
using standard distributions like normal or uniform
\citep{crovella97}. This explains the many outliers reported by
\cite[sec.5.2]{DBLP:journals/popets/CherubinHJ17}. \cite[sec.5.A]{wtfpad2015}
also distribute its \gls{wf} defense data partitions exponentially to
better fit web traffic. \cite{DBLP:conf/imc/IhmP11} repeatedly mention
a /long tail/ of traffic. This is why HTTP \citep{rfc7230}-shaped
cover traffic might prove more effective, as this would make it harder
to separate cover and real traffic. In addition, it would work at the
layer \citep[ch.1.7]{DBLP:books/daglib/0001977} where the problem
originates, as it mimics the HTTP/HTML \citep{html5}-specific
request-response interaction.


There are several approaches on how to generate HTTP-shaped
traffic. The naïve way, using HTTP dummy traffic \citep{panchenko},
loads another page simultaneously in the background. \citet{ccs2012-fingerprinting},
\citet{a-systematic2014}, \citet{kfingerprint}, \citet{effective2014},
\citet{panchenko}, and \citet{wtfpad2015} found it to be surprisingly
effective for all its simplicity, albeit at a high overhead.

#+CAPTION[Distribution of sizes for the HTTP traffic model]: Distribution of sizes for the HTTP traffic model. While the number of embedded elements seems very low, it has mean \mu \approx 5.07 \citep{newtrafficmodel} and standard deviation \sigma \approx 15.16 (computed as of \cite[5.1.11]{compgen} and using \cite[stats.gamma]{scipy}).
#+NAME: distribution
[[./pictures/fig_html_embedded.pdf]]
# see misc_gen_quantiles_numemb.py


\citet{newtrafficmodel} model web traffic. They evaluated various
distributions, such as lognormal, weibull, exponential, gamma, and the
generalized pareto distributions. This thesis applies their
distributions for the size of the HTML page, the size of embedded
elements, and the number of embedded elements. See illustration in
Figure [[distribution]].

The size of both HTML documents and embedded objects is each modeled
by truncated lognormal distributions. The number of embedded objects
is modeled by a truncated gamma function. These are implemented
following \cite{compgen}.
** Bloom Filter Basics
   :PROPERTIES:
   :CUSTOM_ID: sub3-bloomfilters
   :END:
          - set membership with false positives
          - reduced space
          - fp help here: if adynamically saves, and adversary
            captures, cannot say for sure

   Bloom filters \citep{Bloom70space/timetrade-offs} are a stochastic
   fixed-width data structure to test membership in a set. In exchange
   for a small false-positive error rate, they require significantly
   less space than deterministic data structures: if an element is in
   the set, the filters accurately report this; if the element is not
   in the set, the Bloom-filter might report that it is contained. The
   error rate depends on the number of included elements in relation
   to the Bloom-filter's size. Bloom-filters were developed for spell
   checking. They have numerous uses in network applications, e.g. in
   distributed caches, and network
   analysis.\citep{Broder02networkapplications} Bloom-filters improve
   on simple hash-coding by offering a tunable false positive error
   rate.

- why Bloom filter
  - fixed size
  - if save visited pages, problematic if computer is captured later
  - bloom filter only saves approximately
** bit older methods chapter
 As described in section [[#sub2-wf]], \gls{wf} can deanonymize Tor web
 browsing. It is a low-level passive defense that can even be deployed
 by ISPs, and company networks, as well as nation-states. This
 especially puts citizens of oppressive regimes and whistleblowers at
 risk. Yet, no defense has been included in the Tor Browser so far, and
 installing defenses has been complicated for the end-user. This
 thesis' task is to find a way to effectively obfuscate Tor traffic
 from this attack. For this, it presents a new defense that uses
 properties of HTTP \citep{rfc7230} traffic to effectively defend Tor
 web browsing against \gls{wf}.


 This thesis' defense tries to defend against \gls{wf}. It does so
 where the problem arises: at the (HTTP) application layer
 \cite[sec.1.7.1]{DBLP:books/daglib/0001977}. This approach is
 validated by the latest \gls{wf} defense by
 \citet{DBLP:journals/popets/CherubinHJ17}. They and this thesis both
 create additional traffic. In contrast to their approach, the new
 defense's data is generated by HTTP-specific, long-tailed,
 distributions. In this, it matches \citet{panchenko}'s camouflage
 traffic, but it only sends single requests, and it sends until the
 original retrieval is finished. (Additionally, page characteristics
 are cached to more accurately send cover traffic.)

 Section ([[#sub3-http]]) describes the model for web page retrieval. The
 new defense's main algorithm is described in section
 [[#sub3-defense]]. The next sections introduce Bloom filters
 ([[#sub3-bloomfilters]]) and describe their application to approximately
 save element sizes ([[#sub3-bloomsort]]).
*** Bloom Sort
    :PROPERTIES:
    :CUSTOM_ID: sub3-bloomsort
    :END:
**** Choice: Cache (approximate) sizes using Bloom Filters
     :PROPERTIES:
     :CUSTOM_ID: cache_size
     :END:
     #+INDEX: a
     #+INDEX: flavour!a
     #+INDEX: b
     #+INDEX: flavour!b
     A webpage is modeled by its HTML size and its number of embedded
     objects. In a closed world[fn::if the client can only browse to
     a limited number of URLs], it is possible to cache all
     sizes[fn::See appendices [[#find sizes of HTML-documents]] and
     [[#number_embedded]] for how the sizes were determined]
     beforehand. If a size is unknown, random variates from the
     [[#HTTP traffic model][HTTP traffic model] are used.[fn:: The
     size of each embedded dummy object is always drawn from the HTTP
     traffic model.]

     Using known sizes is called the /cache/ flavour and denoted by
     =a= in tables etc. In the /nocache/ flavour, denoted by =b=,
     sizes are always guessed.

     To cache sizes, an approximate-size data structure based on
     Bloom filters is used.
**** Application: Bloom Sort
     Sizes can be saved approximately based on Bloom filters: they
     are sorted into bins based on the target distribution. For each
     bin, a Bloom filter is created. An element is added to this
     filter if its size is inside the bin.

     To approximate the size of an element, all filters are
     checked. If one filter reports containment, its size is
     chosen. If zero report containment, the size is not known; if
     two or more report containment, it is saved wrongly. In the
     latter cases, the default distribution is used.[fn::See [[#bloom-sort]]
     for implementation details.]

     This data structure has the additional advantage that, even if
     visited page sizes were saved, an adversary could not safely
     detemine that pages were visited due to the Bloom Filter's
     false positive errors.

**** DANIEL Caching via Bloom-Filters
     :PROPERTIES:
     :CUSTOM_ID: sub3-bloom
     :END:
     The above defense might further improve if the sizes of the webpage
     to be loaded are known beforehand: Cover traffic could be tailored
     more exactly, increasing obfuscation and/or reducing
     overhead. Knowing the exact retrieval pattern in advance even
     enables new defenses\cite{effective2014}.


     The problem in caching is that
     a. page properties change over time, making this cache increasingly
        less accurate,
     b. caching visited page sizes might yield an exact log of the
        visited web pages to an attacker who gains control over the
        defended computer, and
     c. this cache could take a lot of space, depending on the subset of
        pages's sizes cached.


     A data structure that stochastically saves approximate sizes might
     solve problems /2/ and (partially) /3/: Bloom
     Filters\cite{Bloom70space/timetrade-offs} have a small error rate
     in exchange for a fixed size. Their otherwise disadvantageous error
     probability is an advantage in this situation, as it further
     confounds possible attackers.


     Dynamically updating the filter might solve problem /1/ and further
     help with problem /3/ (future work).
**** KEYWORDS Bloom-Cache
***** WRITE Error estimation of Bloom Sort
      - error both ways, and difference bin-size to real size
     - sources of error
       - filter tells that is has element when it has not
     - how does error appear
       - collision: one of several, the other might be true
       - replacement: simulates being another url
     - rates of error
       - "add" the error rates of the filters? (times population density?)
     - error estimation?
       - +: fixed size
       - -: error both ways, and difference bin-size to real size
***** WRITE Bloom usage and implementation
      - Bloom sort
        - error rate computation
      - size taken from example...
        - maybe change when altered
***** WRITE Bloom-sort
      :PROPERTIES:
      :CUSTOM_ID: bloom-sort
      :END:
       By ordering data into bins, it becomes possible to use Bloom filters
       for the estimation of sizes, using one Bloom filter for each bin.

       To achieve this, sensible separation criteria (called /splits/) for
       the bins need to be found. Afterwards, each bin needs to be assigned
       a value (called /size/) for all contained elements. See appendix
       [[#bloom-params]] on determining the sizes and splits.

       This data-structure, called /Bloom-sort/ is initialized with an
       array of splits, and an array of sizes. The sizes-array needs to
       have one more element than the splits-array, as the bins are bounded
       on the left by 0, and on the right by infinity.

       #+BEGIN_SRC js
         /**
          ,* @param {sizes Array} array of values for each bin, must be sorted
          ,* @param {splits Array} array of bin borders, must be sorted
         ,*/
         function BloomSort(sizes, splits) {
             this.sizes = sizes;
             this.splits = splits;
             this.filters = [];
             for ( let i = 0; i < sizes.length; i++ ) {
                 this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
             }
         }
       #+END_SRC

       Thus, you get
       \[-\infty \le \text{size}_0 \le \text{split}_0 \le \text{size}_1 \le \text{split}_1 \le ... \le \text{split}_{n-1} \le \text{size}_n < \infty\]
       Given the splits, it becomes possible to add the elements to their
       bins:

       #+BEGIN_SRC js
         BloomSort.prototype.add = function(id, size) {
             this.filters[_.sortedIndex(this.splits, size)].add(id);
         };
       #+END_SRC

       where =_.sortedIndex()= gives the index at which =size= would be
       inserted into the sorted =this.splits= array.

       The retrieval of element sizes looks into each Bloom filter,
       checking whether it might contain the element =id=. If one Bloom
       filter reports containment, its corresponding element- =size= is
       returned. If several or no Bloom filters report containment, an
       exception is thrown. The exception is used to allow all possible
       return values, not blocking one of them, say =-1=, for the error
       condition.
       #+BEGIN_SRC js
         /** determines size of element, raises exception if unclear */
         BloomSort.prototype.query = function(id) {
             let pos = -1;
             for ( let i = 0; i < this.filters.length; i++ ) {
                 if ( this.filters[i].test(id) ) {
                     if ( pos === -1 ) {
                         pos = i;
                     } else {
                         throw {
                             name: 'BloomError',
                             message: 'Contains multiple entries'
                         };
                     }
                 }
             }
             if ( pos === -1 ) {
                 throw {
                     name: 'BloomError',
                     message: 'Contains no entries'
                 };
             }
             return this.sizes[pos];
         };
       #+END_SRC

       It is initialized with
       #+BEGIN_SRC js
       let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
       #+END_SRC

       then adding elements via =htmlSizes.add("http://google.com/", 613)=
       and querying via =htmlSizes.query("http://google.com/")=, which
       would yield =400=. See usage in =size-cache=.

       - link to main bloom-sort, create custom-id there
****** WAIT github link
***** CHECK Determining Bloom-Sort Parameters
      :PROPERTIES:
      :CUSTOM_ID: bloom-params
      :END:
      It is impractical to store the sizes of many URLs, as this would
      greatly increase the size of the addon. A possibility to save space
      is to use Bloom Filters to aggregate groups of URLs with similar
      values, as described in [[#bloom-sort][Bloom-sort]].

      Each group needs borders (given via the /splits/ method parameter)
      and a size to represents its contained elements.

      Determining the optimal number of groups, splits and sizes is a
      topic of [[#future-work][further work]]. Here, initially the quantiles of the
      HTTP-model (see [[#HTTP traffic model][HTTP traffic model] ) were used. When the data were
      to be inserted, it turned out that especially the numbers of
      embedded elements did not match the theoretically proposed groups:

      For three groups, the splits would be given by the 33 1/3 and 66
      2/3 quantiles, as 0.0107 and 1.481. This would create two
      single-valued groups, which only would contain the elements 0
      and 1. The next group would contain all other elements: The
      (representative) sizes of the groups were given as 7.915E-05,
      0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

      The actual data to be inserted (see [[#number_embedded][Cached: Number of Embedded
      Objects]]) had the /splits/ (quantiles) at 10 2/3 and 36 2/3 and the
      /sizes/ (middle quantile) at 6, 20, and 59 2/3.

      In addition to using the observed sizes for the Bloom filter, the
      number of groups was increased to 5.
****** WAIT [#C] graphics: histogram with bins (colored bars, black bin limits)
*** rest
 - how cache sizes
   - bloom filters split by seen quantiles
   - f.ex. 5 bins
   - quantile middle as value, upper lower as limits
   - example numbers: old real-world code
   - [error probability?]
** DANIEL_BUT_NEEDS_SUBPARTS Novel Defense [2/2]
   :PROPERTIES:
   :CUSTOM_ID: ch3-newdefense
   :END:
   The last chapter showed that \gls{wf} could be a serious attack on online
   anonymity and privacy. As specific defenses could be circumvented, a
   general defense against \gls{wf} is needed. It should provide adequate
   protection for an acceptable increase in bandwidth and delay.\\


   This thesis' defense aims to tailor cover traffic to mimic web
   traffic, while coincidentally improving installation and
   configurability.


   Closely tailoring traffic has two main parts: It uses the http
   traffic model\cite{newtrafficmodel} to morph the expected real
   traffic to another size; and it caches certain page characteristics
   in order to control the amount of cover traffic.
*** DANIEL Caching via Bloom-Filters
    :PROPERTIES:
    :CUSTOM_ID: sub3-bloom
    :END:
    The above defense might further improve if the sizes of the webpage
    to be loaded are known beforehand: Cover traffic could be tailored
    more exactly, increasing obfuscation and/or reducing
    overhead. Knowing the exact retrieval pattern in advance even
    enables new defenses\cite{effective2014}.


    The problem in caching is that
    a. page properties change over time, making this cache increasingly
       less accurate,
    b. caching visited page sizes might yield an exact log of the
       visited web pages to an attacker who gains control over the
       defended computer, and
    c. this cache could take a lot of space, depending on the subset of
       pages's sizes cached.


    A data structure that stochastically saves approximate sizes might
    solve problems /2/ and (partially) /3/: Bloom
    Filters\cite{Bloom70space/timetrade-offs} have a small error rate
    in exchange for a fixed size. Their otherwise disadvantageous error
    probability is an advantage in this situation, as it further
    confounds possible attackers.


    Dynamically updating the filter might solve problem /1/ and further
    help with problem /3/ (future work).
** why blooms (aka  why Bloom filter)



  - bloom filter only saves approximately
  - size cut-off

---

This is used to approximately save element sizes.






If a size is unknown, random variates from the [[#HTTP traffic
    model][HTTP traffic model] are used.[fn:: The size of each
    embedded dummy object is always drawn from the HTTP traffic
    model.]

    Using known sizes is called the /cache/ flavour and denoted by
    =a= in tables etc. In the /nocache/ flavour, denoted by =b=,
    sizes are always guessed.

    To cache sizes, an approximate-size data structure based on
    Bloom filters is used.


#+BEGIN_QUOTE
 The /lookupOrGuess.../-functions need data structures to map urls to
 both HTTP sizes and number of embedded objects. These change over
 time, just compare the topmost plots each of Figures
 [[fig:total_packets_in]] and [[fig:total_packets_in_thesis]]. As can be seen
 in the next chapter, the amazingly good defense that was achieved
 with up-to-date cached sizes (see next chapter) decreased over time
 to that of guessing sizes. These data structures use Bloom-filter
 \citep{Bloom70space/timetrade-offs} based binning (see section
 [[#sub3-bloomsort]]) to save values related to urls in a fixed space,
 while not allowing an adversary to exactly determine which urls are
 saved. See section [[#sub3-bloomfilters]] for a short introduction to
 their underlying Bloom-filters and section [[#sub3-bloomsort]] for their
 application that saves these in a fixed-size probabilistic data
 structure.
#+END_QUOTE


    Sizes can be saved approximately based on Bloom filters
** how do bloom-filters work
- extend hash-coding
  - field of b bits
  - 1 bit whether cell is empty
- not one hash, but many


They offer a configurable false-positive error-rate: they might
erroneously state that an element is contained in a set, if it is
not.


- hash functions
- set bits
** what is the error-rate to size trade-off
    aka error rate computation
    aka [error probability?]
- Error estimation of Bloom Sort
     - error both ways, and difference bin-size to real size
    - sources of error
      - filter tells that is has element when it has not
    - how does error appear
      - collision: one of several, the other might be true
      - replacement: simulates being another url
    - rates of error
      - "add" the error rates of the filters? (times population density?)
    - error estimation?
      - +: fixed size
      - -: error both ways, and difference bin-size to real size
** how cache sizes
   aka size taken from example...
     - maybe change when altered
 - bloom filters split by seen quantiles
 - f.ex. 5 bins
 - quantile middle as value, upper lower as limits
 - example numbers: old real-world code
** Setup
   :PROPERTIES:
   :END:
   - capture
     - tbb generic
     - firefox marionette
     - script at github
   - feature extraction and classification in python

    :PROPERTIES:
    :CUSTOM_ID: capture
    :END:
    #+INDEX: Bridge
    #+INDEX: Tor!Bridge
    #+CAPTION: Setup to capture web page traffic: Tor Browser on /Client/ machine, connects to Tor server on /Bridge/ machine, connects to Tor network, connects to web servers
    #+ATTR_LATEX: :float nil :width 0.5\textwidth
    [[./pictures/Setup.eps]]

    For the first captures, a single virtual machine hosted both the
    WTF-PAD server and client. To simulate the distributed WTF-PAD
    defense's network more closely, a second server was used.

    Thus, a Tor bridge was introduced into the traffic
    flow[fn::Bridges relay Tor traffic. They act as a gateway into the
    network. Their main use is censorship avoidance\cite{tor2014}]. It
    is required for WTF-PAD anyways, but other Tor traffic also uses
    the bridge instance via Tor's =Bridge=
    directive\cite[sec.client~options]{tor-manual}, to ease
    comparability.

    One host runs the Tor Browser Bundle[fn::at the current version]
    and the cover traffic server (if needed), the other runs a Tor
    server instance in bridge mode. For WTF-PAD, an additional server
    transport program is run at the bridge, and a client transport at
    the client[fn::WTF-PAD is run via the stand-alone programs. Tor's
    built-in =ServerTransportPlugin= and =ClientTransportPlugin=
    configuration naïvely failed].

    This setup utilises the same bridge for WTF-PAD and the browser
    extension.

    Single traces are captured via the Python script
     ~one_site.py~[fn::
     https://github.com/kreikenbaum/website-fingerprinting-thesis/blob/master/capture/one_site.py]. It
     cleans the cache between captures by restarting the Tor Browser
     Bundle.
** PENDING Machine Learning Algorithms
*** Support Vector Machines
    #+INDEX: classifier!Support Vector Machine
    #+INDEX: classifier!SVM
    #+INDEX: Support Vector Machine
    #+INDEX: SVM
    #+INDEX: linear classifier
    #+INDEX: binary classification
    #+INDEX: classification!binary
    /Support Vector Machines/ (SVMs) are a linear classifier:
    they find a linear boundary between points, see Figure
    [[fig:linear_boundary]] for a simple example.

    #+CAPTION[Example binary linear classification]: Example binary linear classification from \cite[Figure 1.5]{iml}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:linear_boundary
    [[./pictures/iml_fig1.5.png]]

    [fn::This and the following parts are mostly based on
    \cite[ch.6f]{iml}] Given a set $X = \{x_1, ..., x_n\}$ with a dot
    product $\langle\cdot, \cdot\rangle: X \times X \to \mathbb R$ and tuples $(x_1, y_1), ...,
    (x_m, y_m)$, with $x_i \in X, y_i \in \{-1, 1\}$ as a /binary
    classification/ task.

    The SVM's job is to find a hyperplane[fn::as \cite[ch.4.1]{esl}
    mention, this is actually an affine set, as it need not pass
    through the origin. This thesis keeps the tradition (as long as
    those things formed by quarks are still called atoms ...).]
    #+BEGIN_EXPORT latex
      \[\{x \in X \mid \langle w, x \rangle +b = 0\}\]
    #+END_EXPORT
    such that $\langle w, x_i \rangle +b \ge 0$ whenever $y_i = 1$, and $\langle w, x_i \rangle
    +b < 0$ whenever $y_i = -1$. With added normalization, this can
    be compressed to the form \[y_i \cdot (\langle w, xi \rangle +b) \ge 1.\]
**** Soft Margin Classifiers
     :PROPERTIES:
     :CUSTOM_ID: soft-margin-svm
     :END:
     #+INDEX: margin
     #+INDEX: SVM!margin
     #+INDEX: soft-margin
     #+INDEX: SVM!soft-margin
     #+INDEX: classifier!soft-margin
     #+INDEX: C
     #+INDEX: SVM!C
     A support vector machine tries to find a hyperplane between two
     groups of points and maximize its distance to the closest points,
     called /margin/. What happens if the points lie such that a line
     cannot be found, as e.g. in Figure [[fig:non-linear-data]]?

     #+CAPTION[Example simple non-linearly separable data]: Non-linearly separable data; source: \url{https://en.wikipedia.org/wiki/File:Separability_NO.svg}
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:non-linear-data
     [[./pictures/Separability_NO.eps]]

     To solve this, a /soft-margin classifier/ introduces slack
     variables $\xi \ge 0$, which it tries to reduce while maximizing the
     margin.

     This alters the equations to $y_i( \langle w, xi \rangle +b) \ge 1 - \xi_i$ for the
     optimization problem

     \[\min_{w, b, \xi} \frac{1}{2} ||w||^2 + \frac{C}{m} \sum_{i=1..m} \xi_i\]

     The /error term/ $C$ weighs minimizing training errors against
     maximizing the margin\cite[sec.7.2.1]{iml}.
**** Multi-Class Strategies
     :PROPERTIES:
     :CUSTOM_ID: multi-class
     :END:
     #+INDEX: binary classification
     #+INDEX: multi-class classification
     #+INDEX: classification!binary
     #+INDEX: classification!multi-class
     The SVMs as described above solve the binary classification
     problem \cite[sec.1.1.3]{iml}: they propose a boundary between
     two classes of objects.

     In website fingerprinting[fn::as in e.g.\space{}handwriting
     recognition], there are most often more classes than two.

     Several strategies exist to distinguish more than two
     classes. The main are to train one classifier for each class ---
     called /One-Vs-Rest/ (OVR) --- and one for each class-class
     combination --- called /One-Vs-One/ (OVO). One-Vs-Rest trains
     fewer classifiers, while One-Vs-One trains more, but evaluates
     fewer samples per fitting.\cite[sec.4.12.3]{scikit-user-guide}.
**** Kernel Trick
     #+INDEX: kernel
     #+INDEX: kernel!radial basis function (RBF)
     #+INDEX: radial basis function (RBF) kernel
     Straight lines do not always distinguish classes correctly, as
     e.g. example in Figure [[hastie_kerneltrick]]. This would seem a
     drawback to using Support Vector Machines, yet they can compute
     these not only on the original data, but also on a projected
     space. This allows for complex decision boundaries. By using the
     kernel trick\cite[sec.2.2.2]{kernels}[fn::A kernel is a function
     with specific properties. The dot product is such a kernel. The
     kernel trick enables a algorithm with a kernel to use any other
     kernel], a SVM can not only use the dot product $\langle.,.\rangle$, but
     another kernel $k(., .)$ instead.

     #+CAPTION: Kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left side shows linear boundaries on $X$ and $Y$ --- the right side linear boundaries computed with added $X^2$, $Y^2$ and $XY$.
     #+NAME: hastie_kerneltrick
     [[./pictures/hastie.png]]

     The kernel used by default by \cite{Hsu10apractical} for SVMs is
     the (gaussian) /radial basis function/ (RBF)
     kernel\cite[sec.2.3]{kernels} \[k(x, y) = \exp \left ( - { \|x -
     y\|^2 \over 2 \gamma^2 } \right )\] This is also used by
     \cite{panchenko2}. While the algorithms still finds a straight
     line in a projected space, the resulting decision boundaries in
     the original feature space are more varied.
**** Parameter Estimation
     #+INDEX: cross-validation
     #+INDEX: grid search
     #+INDEX: $\gamma$ (gamma)
     #+INDEX: gamma
     Each [[#soft-margin-svm][soft margin classifier has an error term $C$]] which states
     how much to penalize outliers. The gaussian radial basis
     function kernel used by \cite{panchenko2} also has a $\gamma$ (gamma)
     term which varies the width of the area, see Figure
     [[fig:C-gamma-effect]].

     #+CAPTION[Example svm-rbf classification with different parameters for $C$ and \gamma]: Example svm-rbf classification with different parameters for $C$ and \gamma. Source \cite[Figure 42.328]{scikit-user-guide}, recreated for higher resolution.
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:C-gamma-effect
     [[./pictures/skl-fig-42.328.png]]
#     [[./pictures/skl-fig-42.328.eps]]

     These parameters have to be provided externally for the Support
     Vector Machine to achieve high
     accuracy. \cite[sec.3.2]{Hsu10apractical} recommend grid-search
     with cross-validation to find optimal parameters.

     In /grid-search/, several parameters of $C$ and $\gamma$ are
     evaluated, and the best one, depending on the metric, is chosen.

     In /cross-validation/, the data set is split into $k$ disjoint
     subsets, called /folds/, of equal size. Of those, $k-1$ are used
     combinedly for training the classifier, while the last is used
     for prediction evaluation. This is done $k$ times, and averaged
     for the result.

     It might be possible to evaluate these meta-parameters together
     with the main classification problem \cite[secs.2.8.3, 6.7]{esl},
     but analogously to \cite[sec.2.8.3]{esl}, this would probably
     become "combinatorially hard".

*** K-Nearest-Neighbor-Classifier
    #+INDEX: classifier!kNN
    #+INDEX: classifier!k-nearest-neighbors
    #+INDEX: kNN
    #+INDEX: k-nearest-neighbors
    The /k-nearest-neighbors/ (kNN) classifier
    (\cite[sec.1.3.2]{iml} \cite[sec.13.3]{esl}
    \cite[sec.8.2]{mitchell}) classifies data points based on the
    known class[es] of their neighbors: for each item to be
    classified, determine the (e.g.\space{}k=5) closest neighbors by a
    given metric. If all neighbors' classes agree, or based on a
    majority decision, the item's class is set to theirs. See Figure
    [[fig:knn-example]].

    It is successful "in a large number of classification and
    regression problems"\cite[sec.4.6]{scikit-user-guide}, despite its
    simplicity.

    This classifier works best if all classes have the same number of
    (training) instances. Otherwise, it is of course probable that the
    classes with the higher number of instances will be chosen as
    targets of classification more often.

     #+CAPTION[k-nearest-neighbors illustrated]: The left picture shows the five closest neighbors around the test instance $x_q$, which is then classified as =-=. The right shows the k==1-decision boundary around several training instances (the area where a test instance would be classified as the point). Source \cite[Figure 8.1]{mitchell}
     #+NAME: fig:knn-example
     #+ATTR_LATEX: :width 0.7\textwidth
     [[./pictures/mitchell-fig8.1.png]]
** PENDING (daniel) Closed World
   :PROPERTIES:
   :CUSTOM_ID: sub4-closed
   :END:
   #+INDEX: evaluation!closed-world
   #+INDEX: closed-world evaluation
   In a /closed-world/ evaluation, a fixed set of site's traces are
   captured. The aim is to distinguish between these sites alone.\\


   Table [[tab:closed_world]] compares
   LLaMAs\citep{DBLP:journals/popets/CherubinHJ17} performance with
   this thesis' addon's on 30 sites. As LLaMAs original time overhead
   seems prohibitive, the source code was edited (a constant was
   changed) to disable the delay. This version is called
   /LLaMA-nodelay/.

   #+CAPTION[Comparison of LLaMA to this thesis's defense on 30 sites]: Comparison of LLaMA to this thesis's defense on 30 sites. The top lines shows LLaMA stats, the next two lines try to match first the accuracy, then the size overhead with the new defense.
   #+NAME: tab:closed_world
   #+ATTR_LATEX: :align |l||n{2}{2}|n{3}{2}|n{4}{2}|
   | <17>              |       <10> |       <10> |       <10> |
   | scenario          | \open accuracy [%] \close | \open size overhead [%] \close | \open time overhead [%] \close |
   |-------------------+------------+------------+------------|
   | no defense (2017-12-26) |      96.46 |          0 |          0 |
   |-------------------+------------+------------+------------|
   | LLaMA             |      55.46 |      94.25 |    1085.25 |
   | 30aI--2016-07-25  |      55.75 |      81.13 |      27.80 |
   | 50aI--2016-07-26  |      52.03 |      96.25 |      15.53 |
   |-------------------+------------+------------+------------|
   | LLaMA-nodelay     |      68.22 |     163.08 |      -4.02 |
   | 5aI--2016-10-09   |      68.59 |      57.92 |       3.22 |
   | 100aI--2017-12-27 |      42.43 |     162.20 | -0.8895383 |
   # llama is from 2018-01-07


   This shows that _original LLaMA delays web surfing by an order of
   magnitude_. This result is after removing plain load errors, of
   which there were 43.67%. After removal, 16 of 30 sites had less
   than 30 instances. If these sites were removed, accuracy against
   LLaMA rose to 71.26%, with overheads remaining roughly the same.


   Compared to the modified version of LLaMA, this thesis' defense
   offers both configurability and a lower size overhead. Both LLaMA
   and the new defense have negligible time overhead. Even compared to
   non-delayed LLaMA, the new defense offers similar accuracy/size
   overhead relationship for a much lower time overhead.

   The thesis' configurability is shown in Figure [[fig:oh2acc]].
   #+CAPTION: Size Overhead to Accuracy Trade-Off for Thesis' Defense, LLaMA-nodelay, and no defense on 30 sites.
   #+NAME: fig:oh2acc
   [[./pictures/oh2acc--llama-0.22-disabled.eps]]
   # code: see [[file:bin/results.py::%20scatter%20plot%20of%20accuracy%20vs%20overhead]]
*** PENDING (llama-delay) [#C] accuracy/overhead plot    :duckstein:internet:
    :PROPERTIES:
    :Effort:   0:30
    :END:
    :LOGBOOK:
    CLOCK: [2017-12-07 Do 12:56]--[2017-12-07 Do 13:20] =>  0:24
    CLOCK: [2017-12-07 Do 11:09]--[2017-12-07 Do 11:40] =>  0:31
    :END:
    - include llama-delay @ 30
    - ?additional 0.22 data points in between groups (200--300% overheads)
    - data points with tiny overhead (?=negative factor?)
*** PENDING (daniel) [#B] accuracy/time plots: time to date
*** PENDING (daniel) Low Accuracy on my Captures
    #+CAPTION: Accuracy Decay on 30 sites.
    #+NAME: fig:date-vs-acc--30sites
    #+ATTR_LATEX: :float wrap
    [[./pictures/date-vs-acc--30sites.eps]]
    # code: [[file:bin/mplot.py::def%20date_accuracy]]()

    #+CAPTION: Accuracy Decay on 100 sites.
    #+NAME: fig:date-vs-acc--100sites
    #+ATTR_LATEX: :float wrap
    [[./pictures/date-vs-acc--100sites.eps]]
    # code: [[file:bin/mplot.py::def%20date_accuracy]](100)
    The previous data show that the thesis's defense provides better
    protection for the same overhead and/or lower overhead for the
    same protection on 30 sites. Yet, the current state-of-the-art
    compares results on at least 100 sites.\\


    Comparison of 100 sites shows a problem: Accuracy for all captures
    is markedly lower than that reported in the literature, both on 30
    and 100 sites. This is true regardless of if data was captured
    with defense or without. On 100 sites, the comparison to
    literature results shows this handicap more clearly. As both
    Panchenko's original implementation and the reimplementation yield
    the same results, the cause is probably due to capture methodology.\\


    Surprisingly, the capture's accuracies decayed over time: On
    /recent data/, neither the wf-attack-reimplementation, Panchenkos
    original, nor a student group at University of Brunswick was able
    to provide good accuracy. The accuracy on 30 sites decayed /from
    97.93%/ on 2016-08-15 /to 58.41%/ on 2017-10-16, as displayed in
    Figure [[fig:date-vs-acc--30sites]]. On 100 sites, the accuracy
    likewise fell /from 90.57%/ on 2016-06-17 /to 43.46%/ on
    2017-10-22, as displayed in Figure [[fig:date-vs-acc--100sites]].\\



    #+CAPTION: Confusion matrix for 30 sites at 2016-08-15, overall accuracy at 98%.
    #+NAME: fig:confmat-2016-08-15
    #+ATTR_LATEX: :float wrap
    [[./pictures/confmat-2016-08-15.eps]]
    # code: [[file:bin/mplot.py::def%20confusion]]

    #+CAPTION: Confusion matrix for 30 sites at 2017-10-16, overall accuracy at 58%.
    #+NAME: fig:confmat-2017-10-16
    #+ATTR_LATEX: :float wrap
    [[./pictures/confmat-2017-10-16.pdf]]
    # code: [[file:bin/mplot.py::def%20confusion]]
    Let's see if a single site or group of sites cause this low
    accuracy: A high-accuracy confusion matrix from 2016-08-15 is
    shown in Figure [[fig:confmat-2016-08-15]]. This is contrast with a
    low-accuracy confusion matrix from 2017-10-16 in Figure
    [[fig:confmat-2017-10-16]]. While there is much more class-bleed-off,
    there is no clear culprit: errors are mostly distributed.\\


    #+CAPTION: CUMUL-traces for site msn.com compared for both dates.
    #+NAME: fig:cumul-good-bad.pdf
    #+ATTR_LATEX: :float wrap
    [[./pictures/cumul-good-bad.pdf]]
    # code:
    # e = scenario.list_all("08-15")[0]; f = e.get_traces()
    # E = scenario.list_all("17-10-16")[0]; F = E.get_traces()
    # palette = sns.color_palette("colorblind", 4)
    # g = gplot.counters(f['msn.com'], label=str(e.date), color=str(palette.as_hex()[0]))
    # g = gplot.counters(F['msn.com'], g, label=str(E.date), color=str(palette.as_hex()[2]))
    A site that existed on both captures is /msn.com/. It was chosen
    to illustrate the problem. The CUMUL-features are shown in Figure
    [[fig:cumul-good-bad.pdf]].\\


    For several low-accuracy retrievals, very little data is received
    back from the server. This could possibly be due to cloudflare
    recaptcha protection etc, which can sometimes be observed when
    browsing with Tor. A possible solution as future work would be to
    filter traces with fewer bytes incoming than outgoing, e.g. Also,
    the capture code could be adjusted to check the page text for the
    recaptcha site.
**** TODO comparison table for 100 sites
**** TODO Accuracy/Overhead for 100 Sites



- how does the defense compare to llama etc


- does the attack work
  - yes, seems so (same results)
- does the defense work
  - lowers accuracy
- how does the other defense compare to this
  - validate attack
  - accuracy
  - conclusion
  - llama lowers accuracy as well
  - [wtf-pad]
  - comparison accuracy vs overhead
    - add point for llama-nodelay
- what are closed-world results
  aka what are the main results
  - result similar to CUMUL
  - see table in appendix
** Analyse 1: Closed World Scenarios
- does the attack work
  - yes, seems so (same results)
- result similar to CUMUL
* After Appendices: Bibliography and Index
\bibliography{docs/master}
\bibliographystyle{apalike}
\input{diplomarbeit.ind}
* END: /above/ this headline are INDEX, and BIBLIOGRAPHY, etc       :ARCHIVE:
