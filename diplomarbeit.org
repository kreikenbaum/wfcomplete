#+TITLE: Selective Cover Traffic
#+TODO: KEYWORDS WRITE CHECK EVA DANIEL | FINAL
#+TODO: TODO WAIT | DONE
#+TODO: INTEGRATE |
* Configuration							    :ARCHIVE:
#+LATEX_CLASS: scrartcl
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure} \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {3.1 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte % mkreik <2016-07-11 Mo>: war {5 cm}
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
* TODO look through unsorted top-level headings
* WAIT Abstract
* TODO Introduction (=basics) [0%]
** TODO Website Fingerprinting [0/34]
*** DANIEL Intro (remove this headline when reviewed)
    Website fingerprinting\cite{hintz02} aims to deduce which web page
    a user is visiting via an anonymizing proxy. It does so based on
    order, size, and timing of data packets.

    This chapter contains first [[#visual][a visual introduction to the subject]],
    then a short review of [[#http][what happens during website
    retrieval]]. Afterwards, [[#wf1.0][early methods of website fingerprinting]]
    will be presented. The next section explains [[#Hurdles][why these
    early techniques no longer work]] especially on anonymity networks
    like Tor.

    Finally, [[#wf2.0][current attacks]] will be explored.
*** DANIEL Visual Representation of Traces
    :PROPERTIES:
    :CUSTOM_ID: visual
    :END:
    To illustrate the task of a website fingerprinter, consider these
    graphical traces[fn::see appendix [[#wf-pictures]] for the creation of
    these pictures] of two web pages in figure \ref{traces}, where box
    height signifies amount of data and width the duration until the
    next packet:

#+BEGIN_LaTeX
\begin{figure}
\begin{longtable}{c c}
   craigslist.org & facebook.com \\
\endfirsthead
\includegraphics[width=0.45\textwidth]{./pictures/craigslist_org@1445352269.png}
&
\includegraphics[width=0.45 \textwidth]{./pictures/facebook_com@1445350531.png}
\\
\includegraphics[width=0.45\textwidth]{./pictures/craigslist_org@1445585277.png}
&
\includegraphics[width=0.45 \textwidth]{./pictures/facebook_com@1445422155.png}
\\
\includegraphics[width=0.45\textwidth]{./pictures/craigslist_org@1445486337.png}
&
\includegraphics[width=0.45 \textwidth]{./pictures/facebook_com@1445425799.png}
\\
\includegraphics[width=0.45\textwidth]{./pictures/craigslist_org@1445527033.png}
&
\includegraphics[width=0.45 \textwidth]{./pictures/facebook_com@1445429729.png}
\\
\end{longtable}
\caption{Example traces of craigslist.org and facebook.com}
\label{traces}
\end{figure}
#+END_LaTeX

    According to Cai et al. \cite{a-systematic}, the tuples (delay,
    packet size) visualized contain all relevant information of a
    trace.
**** WAIT when done, format in LaTeX: caption to group of images
*** DANIEL What happens during a (HTTP) website request
    :PROPERTIES:
    :CUSTOM_ID: http
    :END:
    #+CAPTION: HTTP/1.0\cite{rfc1945} example from \cite{ssl-traffic-analysis}: page with 2 images, ACKs omitted
    #+ATTR_LATEX: :float wrap :width 0.4\textwidth
    [[./pictures/cheng-http-request.pdf]]

    When a browser such as Mozilla
    Firefox[fn::\url{https://www.mozilla.org/firefox/}] retrieves a
    web page, it does many things under-the-hood.

    First, it retrieves the main object: the browser sends a
    HTTP\cite{rfc2616} request over the TCP/IP\cite{rfc793} protocol
    stack for the HTML\cite{html5} page, which contains the main page
    information. The webserver answers with the requested HTML page,
    or redirects the browser to another address, which it continues
    until the HTML is loaded.

    Afterwards, the browser parses the HTML page and requests all
    objects embedded into the page, such as images, fonts, scripts,
    videos, stylesheets, etc. These can be identified using the
    HTML-tags =<img>=, ~<link rel="stylesheet">~, =<script>=, and
    =<video>=, the CSS-property =@font-face=, etc.
**** TODO test link to firefox (and all links)
**** TODO [#C] format this in latex: left pic, right text, float, ...
**** TODO [#C] if time: link to mozilla-docs
*** CHECK Website Fingerprinting 1.0
    :PROPERTIES:
    :CUSTOM_ID: wf1.0
    :END:
    
    To make sense of the noisy data retrieved via traffic sniffing,
    the first fingerprinters distinguished by object sizes: Each
    requested file has a specific size[fn::except for
    dynamically-generated objects] and is transferrend in one or
    several (IP) packets. In the first versions[fn::up to/including
    1.0] of HTTP\cite{rfc1945}, these sizes were clearly visible, as
    each HTTP request-response pair was transmitted over a separate
    TCP connection. They could be found by splitting traffic by the
    connection's port numbers, as done by Mistry and Rahman
    \cite{quantifying}, Cheng and Avnur \cite{ssl-traffic-analysis},
    and Hintz \cite{hintz02}.

    Against HTTPS[fn::HTTP over SSL\cite{sslv3}, the attacks were
    carried out against SSL 3.0], which reveals the site being browsed
    to but hides the URLs accessed, successful attacks to determine
    the web page[fn::cryptographers talk of attempts to circumvent a
    protocol as /attack/\cite{applied96}] were carried out by the
    formers (\cite{quantifying}, \cite{ssl-traffic-analysis}) at
    Berkeley in 1998.

    The term /website fingerprinting/ was coined in the latter's
    analysis of the SafeWeb anonymizing proxy\cite{hintz02}, where he
    qualitatively classified which web site was being visited.

    All of these approaches determined which page was visited via
    object sizes. Cheng and Avnur \cite{ssl-traffic-analysis}
    additionally proposed a hidden markov model for page
    /transitions/, for hard-to-identify pages.
*** CHECK Hurdles to website fingerprinting
    :PROPERTIES:
    :CUSTOM_ID: Hurdles
    :END:
    The refinement of web protocols and anonymization systems such as
    Tor\cite{tor-design} made website fingerprinting harder:


    Building a new HTTP connection for each transferred object is
    inefficient\cite[sec.2.2.2]{DBLP:books/daglib/0001977}. Some early
    HTTP/1.0 implementations used persistent
    connections\cite{rfc2068}. These were standardized in HTTP/1.1
    \cite{rfc2616}.

    [[./pictures/HTTP_persistent_connection.png]]

    It was no longer trivial to extract the files' sizes. You had to
    determine the start and end of each request. (which was still
    possible to estimate by cutting when the client sent a new request
    data packet).

    In addition to persistent connections, HTTP/1.1 allowed pipelining
    several HTTP requests in a single connection without waiting for
    the files to arrive in between.

    [[./pictures/HTTP_pipelining2.png]]

    As this created problems with some servers, pipelining was
    disabled in
    Firefox[fn::\url{https://bugzilla.mozilla.org/show_bug.cgi?id=264354}]
    and Google
    Chrome[fn::\url{https://www.chromium.org/developers/design-documents/network-stack/http-pipelining}]
    and not implemented in Internet
    Explorer[fn::\url{http://wayback.archive.org/web/20101204053757/http://www.microsoft.com/windowsxp/expertzone/chats/transcripts/08_0814_ez_ie8.mspx}].

    After Panchenko et al.'s first successful attack\cite{panchenko},
    Firefox's built-in request pipelining was enabled with added
    request order randomization as an additional no-cost defense
    prototype in the Tor-Browser-Bundle \cite{experimental}. Yet, Cai
    et al.\cite{ccs2012-fingerprinting} found fingerprinting to be
    easier with this defense enabled than without.
**** TODO caption for pictures
**** DANIEL Tor [0/4]
     The Onion Router\cite{tor-design} (short: /Tor/) is an anonymity
     system: While encryption hides the /content of communication/,
     Tor also attempts to hide some /metadata/: Who communicates with
     whom, for how long, when, how frequent, ...?

     This metadata is important, as it can reveal "[a] lot of good
     information"\cite{applied96}.
***** CHECK History of Tor
      Tor inherits its onion design from the Onion Routing Project
      \cite{anonymous-connections}. It was originally developed by the
      Naval Research Laboratory of the US Navy with the primary purpose
      of protecting government communication.\cite{who-uses-tor}

      In recent years, Tor has also provided censorship
      circumvention\cite{tor-spec-pt}.
***** DANIEL Who uses Tor
      As more and more (internet) users wish to increase their
      anonymity for various reasons, one of Tor's main design goals is
      usability\cite[Sec.3]{tor-design}, which increases
      anonymity\cite{usability:weis2006}. This has led to a diverse
      user base\cite{who-uses-tor}: The network consists of over six
      thousand nodes and is used by about two million people
      daily[fn:metrics:\url{metrics.torproject.org}].

      As of \cite{who-uses-tor}, the groups[fn::actual or recommended]
      who increase their anonymity via Tor are: journalists and their
      audience, military, law enforcement officers, activists &
      whistleblowers, high & low profile people, business executives,
      bloggers, IT professionals, and "normal people". Actual and
      recommended use is for diverse purposes: privacy, censorship
      avoidance, covert ops, publishing, safety, online surveillance,
      anonymous tip lines, whistleblowing, blogging private opinions,
      evaluating competition, and troubleshooting IT systems.
***** CHECK How does Tor Work
      The Tor network consists of volunteer servers, called /onion
      routers/. Each connection through the network is facilitated by
      a proxy implementing the SOCKS5\cite{rfc1928} protocol called
      /onion proxy/.

      A connection is routed through three onion routers (=OR), each
      of which can only see the previous and next, due to
      encryption. Thus, no router has knowledge of both origin and
      destination of traffic.

      The messages look different from OR to OR due to same-length
      encryption.

      Tor's data cells have a fixed size of 512 bytes to prevent cell
      identification; routing through 4 globally-distributed hops
      increases latency; and Tor multiplexes all data cells through a
      single TCP-connection.
****** TODO picture from tor website
*** TODO (wf 2.0)
    :PROPERTIES:
    :CUSTOM_ID: wf2.0
    :END:
    In spite of these difficulties, website fingerprinting was shown
    to be possible in even when Tor is used.
**** TODO they used *packet sizes*
    Like the above attacks, Liberatore and
    Levine\cite{Liberatore:2006} and Herrmann et
    al\cite{ccsw09-fingerprinting} used only packet sizes, but
    employed machine learning techniques. Herrmann et al. used a Naïve
    Bayes Classifier\cite[ch.1.3.1]{intro2ir}, which gave them good
    results against one-hop anonymizing proxies, yet yielded only 3%
    accuracy against 775 distinct pages when retrieved via
    Tor\cite{tor-design}.
**** CHECK panchenko v1
     Panchenko et al.'s \cite{panchenko} attack was the first that
     successfully classifed traces for websites retrieved via
     Tor\cite{critique}.

     To achieve this, they evaluated several HTTP-specific features,
     classifying only on those they found to have the greatest impact
     on classification.

     They used both Herrmann et al.'s\cite{ccsw09-fingerprinting} and
     a new open-world dataset.

     Then, they used a Support Vector Machine with
     grid-search-parameter evaluation.

     This resulted in a closed-world recognition rates of 54.61% for
     Tor and open-world true-positive rate of up to 73%[fn::for the
     Alexa top-ranked dataset].

     The size of the open-world data set was five sites. Yet, it was
     the first to show that even open-world classification (with some
     interesting sites and many that should not trigger) could be
     achieved.
**** WRITE SVM
     Support Vector Machines (short: SVM) are a linear classifier:
     they find a linear boundary between points. While this might seem
     overly limiting, SVMs can compute the boundary not only on the
     original data, but also on a projected space. This allows for
     complex decision boundaries.

     (This section is mostly based on chapters 6 and 7 of Smola and
     Vishwanathan's book \cite{iml}). Given a set {x1, ..., xn} = X
     with a dot product $<., .>: X \times X \to \mathbb R$. Given further
     tuples (x1, y1), ..., (xm, ym), with xi \in set, yi \in {-1, 1} as a
     binary classification task.

     The SVM's job is to find a hyperplane[fn::as Hastie et
     al. mention in \cite[ch.4.1]{esl}, this is actually an affine
     set, as it need not pass through the origin. Keeping with
     tradition, it will be called hyperplane in this thesis.[fn::as
     long as those things formed by quarks are still called atoms...]]
     #+BEGIN_LaTeX
       \[\{x \in X | \langle w, x \rangle +b = 0\}\]
     #+END_LaTeX
     such that <w, xi> +b \ge 0 whenever yi = 1, and <w, xi> +b < 0
     whenever yi = -1.

     - how works
       - set with dot product
       - tuples (x1, y1), ..., (xm, ym), with xi \in set, yi \in {-1, 1}
         - "binary classification task"
       - find hyperplane {x \in X | <w, x> +b = 0} that separates
         tuples, such that
         - <w, xi> +b \ge 0 whenever yi = 1, and
         - <w, xi> +b < 0 whenever yi = -1
         - hastie et al\cite[sec.4.1]{esl}: hyperplane passes through
           origin, so strictly speaking: affine plane. Nevertheless
           hyperplane
       - hard margin classifier: assume that linearly separable
       - soft margin classifier: does not assume, tries to limit error
         in addition to maximizing margin
       - optimization problem
     - multi-class strategies aka ovr vs ovo (vs ecoc)
       - svm binary classifier
       - multi-class: train one for each class (ovr) or one for each
         class-combination (ovo)
         - ovr better efficiency, scales linearly
           - used by panchenko et al
         - ovo evaluates fewer samples per fitting
         - error-correcting codes, mention esl p.625
           - or web-dl original paper
           - used by k-forest
     - kernel trick (see figure \ref{hastie-kerneltrick})
       - instead of dot product <.,.> use =kernel= k(., .)
       - same effect as mapping each point in set to dot product
         space, and applying <.,.> there, k(x, x') = <\Phi(x), \Phi(x')>
         - but need not compute complete mapping
       #+CAPTION: kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left half shows linear boundaries, the right half  linear boundaries computed with added input data X^2, Y^2 and cross-product XY
       #+NAME fig:hastie-kerneltrick
       [[./pictures/hastie.png]]
     - parameter estimation
       - each soft margin SVM has an error term C which states how
         much to penalize outliers
       - rbf kernel also has a gamma term which gives the grade of the
         gaussian
***** TODO rbf kernel
***** TODO check page esl printed
***** TODO link to smola-book
***** TODO link to basic book (bronstein) for vector space etc
**** WRITE wang
     - improved detection
     - many features
     - weighting/learning weights
     - faster
**** WRITE KNN
     - simple
     - often effective
     - how works
       - for point, determine (f.ex. k=5) closest neighbors by metric
       - majority decision (or only if all agree), put in that group
     - as seen in table in appendix: similar results to extratrees,
       randomforest, decisiontrees
       - one or the other slightly better
**** WRITE cumul
     - better features
     - svm
     - picture
     - understandeable
     - faster than knn
**** WRITE extremely randomized trees
     - more random: (here only classifiction)
       - M trees, independent
       - split training set S into K subsets
         - split by single non-constant, randomly-selected attribute
         - return best split
     - reduce variance by randomness
     - reduce bias by several instances M
     - efficiency by basing on decision trees
***** brainstorm                                                    :ARCHIVE:
      - decision trees
        - read up
      - ensemble methods
        - read up?
      - more random: (here only classifiction)
        - M trees, independent
        - split training set S into K subsets
          - split by single non-constant, randomly-selected attribute
          - return best split
**** WRITE (move up ?) features
     :PROPERTIES:
     :CUSTOM_ID: features
     :END:
    As stressed by Perry in \cite{critique}, analysis of which feature
    contribute the most towards classification is important. Panchenko
    et al. \cite{panchenko} provided a qualitative analysis. Hayes and
    Danezis \cite{kfingerprint} used forests of randomized trees, which
    provide feature importance estimation.[fn:: f.ex. in scikit-learn
    \cite{scikit-learn} via the =_feature_importances= attribute]

    As Dyer et al \cite{oakland2012-peekaboo} noted and experiments with
    Panchenko et al's \cite{panchenko} estimator support (see
    [[different-classifiers]]), you can get good accuracy with several
    classifiers, given the right features.

    As determined by Hayes and Danezis \cite{kfingerprint}, the top-five
    features are the number (both absolute and percentage of total) of
    both incoming and outgoing packets. The standard deviation of the
    packet ordering list [fn:: Panchenko et al \cite{panchenko} call
    these features /Number Markers/] completes the top five. Each added
    feature increases accuracy, yet with nearly the same accuracy for 30 as
    for the total of 150 features.


    - patterns to features
    - why features
    - sizes, sizes, sizes
    - total_size
    - top-five
    - CUMUL
    - dyer: features count
    - feature analysis in k-forests
      #+CAPTION: CUMUL\cite{panchenko2} features example at \url{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}
      [[./pictures/cumul_aus_paper.pdf]]
***** TODO visual representation of CUMUL
     - Panchenko et al.'s recent approach allows for the visual
       comparison of website traces.
     - see images etc
     - see how it's done
     - example
       #+CAPTION: CUMUL example from {\url https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}
       #+ATTR_LATEX: :width \linewidth
       #+BEGIN_EXAMPLE
       [[./pictures/cumul_resized_aus_paper.jpg]]
       #+END_EXAMPLE
****** TODO or just link here to CUMUL
****** TODO think about order of this (at cumul, at visual, mention other...)
      then formulate correctly
***** TODO get picture to work
    - features to classes: classifiers
***** sub... of wf2.0?
*** Related Work
    - schneier
    - liberatore
    - microsoft hintz-successor(?)
    - bissias
    - wright
*** brainstorm                                                      :ARCHIVE:
    - induce pattern
      - naive
      - machine learning
      - features
    - kind of traffic analysis
      - without seeing content, deduce information
    - made harder by protocol changes and tor
      - *hope that spdy makes it harder again*
        - ref mike perry
    - no cacheing
    - current tbb (auto-update)
    - scripts etc
    - xpra for slow network connection
    - xvfb for local display
    - marionette
    - others, other tools
    - bit on tor in [[Hurdles]]
      - also that use tor to avoid wf, then tracked again (if it works)
      - maybe also on ta
    - dyer: most important are the features, similar results for naive
    - on feature importances:
      - panchenko
      - k-forest
      - wang implicit
    - not mentioned/omitted in related work?
      - schneier
      - liberatore
      - microsoft hintz-successor(?)
      - bissias
      - wright
    - classifiers
      - features important or also classifier
      - no classifier fits all
      - maps features to classes (or probabilities)
      - classifier
**** WRITE attacks
    - hintz
    - herrmann (bayes)
    - panchenko (svm)
    - cumul + k-forest + wang-knn
**** WRITE Related Work
     - mitchell
     - sklearn
     - herrmann: breakable shown via naive bayes
     - panchenko: svm
     - dyer: most important are the features, similar results for naive
       bayes and svm
     - cheng:?
     - wang: knn
     - dts-approach (?)
     - k-forest: specific classifier based on randomized trees with
       hadamard-distance on leaves
     - panchenko1 and 2: (ovr?) svm

    Schneier's seminal books /Applied Cryptography/\cite{applied96}
    and (with Ferguson) /Practical Cryptography/\cite{practical} deal
    briefly with /Traffic analysis/, of which website fingerprinting
    is a subtask. The first mention of applying it against encrypted
    internet communication dates to Wagner and Schneier's analysis of
    the SSL 3.0 protocol\cite{SSL}, and is attributed to Bennet Yee.
** TODO Defenses [0/9]
   siehe [[file:~/da/da.org::*2.%20Teil:%20Verteidigungen%20gegen%20Website-Fingerprinting%20(Welche%20Methoden%20existieren?%20Welche%20Problem%20existieren%20derzeit?%20-->%20%C3%9Cberleitung%20zum%20Hauptteil)][2. Teil: Verteidigungen gegen Website-Fingerprinting (Welche        Methoden existieren? Welche Problem existieren derzeit? -->        Überleitung zum Hauptteil)]]
*** CHECK How to defend against this
    Hintz\cite{hintz02} was the first to suggest defenses against
    website fingerprinting. He lists three ways to do so: adding noise
    to traffic, reducing the transferred data, and transferring
    everything in one connection.

    The first approach was followed by almost all researchers
    afterwards.

    As the sizes and interconnection of HTML and embedded content is
    what makes a webpage easily identifyable, using a text-only
    non-javascript browser such as Lynx, or disabling f.ex. images,
    might be a mitigation for those who consider this trade-off
    acceptable. Yet, this reduces usability and thus conflicts with
    one of Tor's design goals\cite{tor-design}.

    The third approach --- while valid --- would require modifying the
    server. This would conflict with Tor's design goal of
    deployability.\cite{tor-design}

    Later defenses can be divided as of \cite{wang2015walkie} into
    specific and general defenses.
*** CHECK defenses against specific features
    The first website fingerprinters used only packet lengths. This
    made it seem sensible to alter the lengths of packets by padding,
    as evaluated f.ex. by Liberatore et al\cite{Liberatore:2006}.

    As more and more features were used to classify the traces,
    different ways of altering the data were evaluated by several
    researchers (\cite{oakland2012-peekaboo}
    \cite{ccs2012-fingerprinting}, \cite{a-systematic},
    \cite{morphing09}, \cite{httpos}.
**** TODO bit more detail on morphing?
*** CHECK general defense
    To stop the arms race between attacks and defenses - the attacks
    finding new feature combinations to use, the defenses obfuscating
    these - the idea of a /general defense/ was presented first by
    Dyer et al.\cite{oakland2012-peekaboo}. They proposed what had
    been the mainstain of traffic-flow
    security\cite[ch.10.3]{applied96}: fixed-rate transmission of
    data, modified here to be only for the estimated duration of the
    download.

    This idea was improved on by Cai et al\cite{a-systematic} while
    Wang et al\cite{effective} proposed the (offline) defense of
    morphing all traffic to supersequences of traffic patterns.
*** CHECK deterministic vs stochastic
    The latest defenses were proposed by Juarez et al\cite{wtfpad} and
    Wang et al\cite{wang2015walkie}: These both used a stochastic
    approach to generate additional traffic, with Wang et al
    additionally modifying the browser to send "half-duplex" only,
    only exclusively sending or exclusively receiving data at the same
    time. Juarez et al. adapted the ideas from Shmatikov and
    Wang\cite{ShWa-Timing06} to distinguish active and non-active
    periods, having a certain probability of sending dummy packets in
    each, omitting the sending when the browser generated packets
    itself.
*** WRITE wtf-pad
    - Juarez\cite{wtfpad}
    - Website Traffic Fingerprinting Protection with Adaptive Defense
    - adapts adaptive padding\cite{ShWa-Timing06}
      - hide from global adversary's correlation attack
    - defense + crawler and modifier
    - packet histogram-based
      - when packet is sent, timer from (one of two) histogram is started
      - if timer is finished without another packet, send dummy request
      - else (if another packet): restart timer with new values from histogram
    - built using Tor's pluggable transport\cite{tor-spec-pt}
      censorship avoidance layer
*** WRITE walkie-talkie
    - Wang\cite{wang2015walkie}
    - half-duplex (cite kurose?)
      - send XOR receive
    - with additional traffic
    - browser modification
    - only detectable metric: number of bursts
      - (and also total data, which is a powerful metric)
*** WRITE tamaraw
    - evolution of BuFLO\cite{oakland2012-peekaboo}
    - different rate up-/downstream
    - longer sending cloaks end of transmission
    - tunable overhead
*** brainstorm                                                      :ARCHIVE:
    - obfuscate features
    - specific features
    - general obfuscation
      - deterministic
        - fixed data rate
        - supersequence if known
      - stochastic
    - [ ] traffic analysis assumes crypto is perfect
*** WRITE Related Work
    - hintz: 3 ways to do it
    - wright: morph
    - luo: also morph (HTTPOS)
    - panchenko decoy (add)
    - padding (sslv2 \to 3)
    - requestpolicy (hintz 2nd way)
    - text-only browsing (hintz 2nd way)
* TODO Methods (=methods) [0%]
** brainstorm                                                       :ARCHIVE:
  - make wf/ml harder, fudge features
    - problems wfpad: modify all of tor,
      - yet problem is browser traffic
        - and traffic is app-dependent
      - deployability: all/nothing
      - modify firefox codebase, when addon suffices
      - maybe also efficiency
        - histograms
          - not fitting: no need to hide *that* traffic occurs, just where to,
          - compare to real fingerprints
          - less efficient
      - not tunable, bridge-dependent
    - problems walkie-talkie: also modify all
      - bit slower
      - not preferred method
      - TD: compare to wfpad accuracy/efficiency
    - conversely:
      - addon: easier to modify/implement/test
        - *easy to use* if not default (currently needs server, but
          others need too, does not need by default)
        - HTTP traffic properties used
        - "general defense": not trying to modify specific settings
  - design
    - different versions
    - different factors
** TODO Motivation [0/3]
*** CHECK remove this header when done
   When this thesis was started, there existed mostly deterministic
   defenses, with both Walkie-Talkie\cite{wang2015walkie} and
   wtf-pad\cite{wtfpad} not having been published yet.

   As acknowledged both in \cite{wang2015walkie} and \cite{wtfpad},
   these deterministic approaches had the major shortcoming of
   introducing additional delay into the traffic, which conflicted
   with Tor's design goal of usability\cite{tor-design}, increasing
   f.ex. the sometimes bothersome delay of using Tor for browsing the
   web.[fn::As for the positive side of higher latency, see
   \cite[sec.4.2]{challenges}.]

   While also providing this functionality through an easy-to-add
   browser extension, keeping the Tor Browser code as-is, this
   thesis's approach uses properties of web traffic to determine when
   and how much traffic to send. This stands in contrast to both Wang
   et al.'s Walkie-Talkie\cite{wang2015walkie}, which offers sampling
   from both uniform and normal distributions, and Juarez et al.'s
   Wtfpad\cite{wtfpad}, which creates histogram-based traffic, but
   rather works at Tor's cell level, and, critically, adapts a method
   that tries to do more (hiding from a global adversary), instead of
   hiding which site was browsed to from a local passive
   observer.[fn::which is included in Tor's design goald]
*** WRITE Aim: selective cover traffic
   As detailed in section [[#features]], there are key features that are
   hard to cloak except by extra traffic, f.ex. total bytes up-/ and
   downstream.

   Given that, the next question is how to shape traffic in order to
   effectively cloak the fingerprint.

    - based on target web site
    - simultaneous to real traffic

    - make wf harder such that it is impossible
** TODO Design and Implementation (=Implementation) [0/20]
*** CHECK description of add-on
    The add-on tries to defend against website fingerprinting by
    adding HTTP-distributed extra traffic.

    To do so, it detects the start of each web site request. With the
    start of the /HTML-request/, a dummy request is sent. Of which
    size and how it continues depends on the version of the add-on.

    With the user's first request, the host is marked active. At the
    end of the page load, indicated either by a load
    event\cite[ch.1.6.5]{dom2-events}, or the end of a timeout, the
    host goes back to inactive status.

    For implementation details, see appendix ... .
**** TODO appendix for addon
*** CHECK HTTP traffic model
    :PROPERTIES:
    :CUSTOM_ID: HTTP traffic model
    :END:

    The work by Lee and Gupta\cite{newtrafficmodel} has not been cited
    in a later paper by Ihm and Pai \cite{tumwt}. It matches web
    traffic to statistical distributions.

    The size of HTML documents is characterized by a lognormal
    distribution with parameters \mu = 7.90272, \sigma = 1.7643, truncated to
    the maximum of 2 MB. This yields a mean size of 11872.

    The size of objects embedded in a HTML document is also
    characterized by a lognormal distribution with parameters
    \mu = 7.51384, and \sigma = 2.17454, truncated to the maximum of 6 MB. This
    yields a mean of 12460.

    The number of embedded objects is characterized by a gamma
    function with parameters \kappa = 0.141385, and \theta = 40.3257. It is
    truncated to the maximum of 300, which yields a mean of 5.07
    embedded objects per page.

    They offer further parameters to fully model web browsing, which
    do not apply to the problem at hand.

    There are two problems with the distributions given
    above. Firstly, web traffic has evolved since 2007, when the paper
    was written, as documented for total web page size in
    \cite{web-is-doom}. Secondly, as mentioned in
    \cite{newtrafficmodel}, the number of embedded objects are
    computed per each HTML page, including frames, and possibly
    including redirects. This lowers this number artificially.

    Providing an accurate estimate of embedded objects /per web page/
    is further work.
**** TODO some distribution pictures
**** TODO further work link + mention
*** TODO Addon-Versions
     While just adding random traffic to each page might enhance
     anonymity, always adding in the same way might be wasteful and
     easier to detect. Per-page values could provide better cover.
**** CHECK 0.15: base version
     :PROPERTIES:
     :CUSTOM_ID: addon0.15
     :END:
     This is the base version which was tested. Despite its alpha
     status, it achieved an accuracy drop from 99.2% to 54.0--44.0%
     (depending on flavor) when tested on the top-10 sites. This drop
     came an a bandwidth overhead of 49--85%.

     A simulated webpage is specified by its HTML size and its number
     of embedded objects. In a closed world, it is possible to always
     know the page sizes beforehand. If unknown, the random variates
     from the [[#HTTP traffic model][HTTP traffic model]] are used. [fn:: The size of each
     embedded element is always drawn from the HTTP traffic model.]
     Using known sizes is called the /cache/ flavor. In the /nocache/
     flavor, sizes are always guessed.

     Once the /page's values/ are thus set, there are two tactics on
     how to set /target/ values.

     One tactic is to group the webpages by their values into bins and
     to set the bin border as the target value, as all webpages in the
     bin must have a size less than or equal the border. This approach
     approximates that taken by Wang et al. in \cite{effective} with
     the bins being equivalent to the anonymity sets / partitions. As
     the biggest bin does not have a maximum size, its median value is
     chosen.[fn:: The optimal size for the biggest bin is a parameter
     that should be evaluated as well.]

     The other tactic is to have a single target distribution from
     which values are sampled each time, once again the [[#HTTP traffic model][HTTP traffic
     model]]. This is, again, also the fallback approach if the web
     page's values are not known.

     The web page's own values --- known or guessed, as described
     [[#addon0.15][above]] --- are subtracted from the target values.

     At the same time as the HTML-query, another query for the target
     HTML-size (or a token amount if too small) is sent. Concerning
     the embedded objects, the ratio of
     (target-embedded)/(site-embedded) is computed. For each embedded
     object retrieved, this ratio determines the number of
     embedded-sized requests, once again from the [[#HTTP traffic model][HTTP traffic model]].

          - 15.3 first results
          - buggy: did not match spec (only did html requests)
**** TODO improvements
***** TODO 0.17 bursts at end - bursts on addon site load finish
      - wang: burst distinguishing feature left with
        w/t\cite{wang2015walkie}
      - solution: count how many embedded, add those as bursts at the
        end

      One characteristic which identified sites well as per Dyer et
      al.\cite{oakland2012-peekaboo} and Wang and Goldberg
      \cite{wang2015walkie} is the number of bursts.

      As the addon would conceptually only increase burst sizes, and not
      alter their number, this should be covered as well. To address this,
      the per-site traffic module [[CoverTraffic]] remembers the number of
      unsent requests for embedded elements. When the page loading is
      finished, this number (which should be 0 or less in more than half
      the cases) of embedded objects is requested. As the cover traffic
      currently comes from a single server, the multiple connection limit
      (compare [[#Hurdles]]) should automatically lead to multiple bursts if
      the number of embedded objects is high enough.
***** WRITE 0.18: configurability
      - options choosing which tactic:
        - known/guess sizes
        - bins/target
        - bursts

      - much more traffic
        - try to fix at 19 (and backport to 15.3, codename retro)
***** WRITE 0.19: negative values in distribution
      :PROPERTIES:
      :CUSTOM_ID: addon0.19
      :END:
      - negative values for requests are saved and randomly subtracted
      - occur with real size > target size
      - solution
        - if small /negative request value:
          - save value (min size is 160, thus =160 - requested_size=)
        - else:
          - get value at random up to min(request size, saved values)
          - subtract from request size, and from saved value

      The improvements described in this section were backported to
      version 0.15.3, with version name 0.15.3-retrofixed. This
      greatly reduced the amount of overhead, but had the same
      problem: the factor was not correlated to the overhead: it
      lacked control on how much traffic to generate.
***** WRITE 0.20: bounds for probability
      - buggy html model: counts many more URLs as HTML than expected
        - fix would be: use only absolute numbers, not probabilities,
          detect HTML (by suffix as approximation, and by content-type
          when found), increase counter when found
        - workaround: bound probability
        - 20 limits number of embedded requests
***** WRITE 0.21 bounds absolute number of retrieved objects
       - better workaround
         - stricter bounds on retrieval of embedded objects
         - and stop when limit reached
**** CHECK simple 1.1
     The previous versions had become quite complex for a Firefox
     extension: they had more than 500 lines of code. A
     simplified[ch.7.2]\cite{xp} algorithm triggers a FACTOR-sized
     HTML-sized request at the beginning and an embedded-sized request
     with probability FACTOR for each embedded object.

     This halved the number of lines of code, allowing for better
     refactoring.
*** WRITE Server
    - where there are several possibilities how to generate cover
      traffic,
    - here: simplest: server, GET-query with size=bytes parameter
      returns this many bytes random data
*** brainstorm                                                      :ARCHIVE:
    - aim: selective cover traffic
      - select based on web site
      - and target
      - simultaneous to real traffic
    - firefox browser extension / addon
      - addon sdk
      - maybe mention next generation
    - good code
      - tests
        - unit tests
        - by hand
      - good parts
      - js garden
      - style guide
      - version control
    - algorithm
    - implementation
      - classes
    - server
      - later: .onion (link to related work)
    - http traffic distribution
*** TODO Bloom Filters
**** WRITE General Idea
     - stochastic fixed-width data structure
     - works flawlessly if element is inside
       - might fail if not
**** WRITE Application: Bloom Sort
     - sort into bins
       - based on target distribution
       - one bloom filter per bin
     - check size: check all filters
       - if one returns: fine
       - if none returns: ok: clear that not inserted, default value
       - if two return: error, fall back to default value
**** MAYBE Error estimation of Bloom Sort
     - error both ways, and difference bin-size to real size
**** brainstorm                                                     :ARCHIVE:
     - stochastic fixed-width data structure
     - works flawlessly if element is inside
       - might fail if not
     - based on this: bloomsort: combine filters
       - sort into bins
         - based on target distribution
         - one bloom filter per bin
       - check size: check all filters
         - if one returns: fine
         - if none returns: ok: clear that not inserted, default value
         - if two return: error, fall back to default value
       - error estimation?
       - +: fixed size
       - -: error both ways, and difference bin-size to real size
*** WRITE Related Work?
    - bloom paper
    - network applications
* TODO Results and Evaluation [0%]
** TODO setup
*** TODO sites
**** CHECK modified top-100
     The files for retrieval were from the alexa-top-1m[fn:: Current
     version available  at \url{
     http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}], from
     September 30, 2015. Akin to \cite{wpes13-fingerprinting}, similar
     sites were removed. Also removed were those sites which failed to
     respond to python's =urllib=. The list of sites with their Alexa
     index can be found in appendix [[#top-100]].
**** WRITE 30 sites
     - top-10
       - tried first
       - have other size-distributions than f.ex. top-100 sites (see f.ex.
         \cite{web-is-doom})
     - find sites with high variance
       - use also addon data on top-100 (version 0.18.2) to check
       - quantiles of (total incoming) sizes
         - panchenko: relevant metric
     - link to code: top30() in analyse.py
*** WRITE capture
      #+CAPTION: setup to capture web page traffic
      #+ATTR_LATEX: :float nil :width 0.5\textwidth
      [[./pictures/Setup.eps]]
    - evolved
      - first version: all on same host
        - bug: wfpad had overhead of \pm 1%
        - and had to filtered (also cover traffic server)
    - now: two virtual machines
      - tbb, ct-server on one host
      - tor bridge on other, connects to Tor network
      - same for wfpad and addon
      - capture always via bridge
    - code:
      - see one_site
      - start_xvfb
      - retrieve ...

     If the Browser Bundle runs when started manually, webpages can be
     retrieved automatically. This is done via the [[one-site.py][one-site.py]] script.

     The script

     1. starts the Tor Browser Bundle's =firefox= binary, enabling
        remote-control via the [[Marionette][=-marionette=]] command-line argument,
        waiting up to 60 seconds for its initialization
     2. starts the =tshark= capture
     3. loads the page (given as first parameter) via Marionette
     4. waits up to 600 seconds for the page load to finish
     5. waits 3 more seconds (for the last cover traffic to finish)
     6. ends the capture
     7. ends Firefox

     This setup (restart after each trace) avoids caching issues with
     website fingerprinting, as the Tor Browser Bundle cleans the
     cache between restarts (as mentioned f.ex. in \cite{critique}). If a
     browsing-session scenario is desired, the script could be
     modified to omit terminating the browser instance.
*** TODO classification
**** WRITE counter.py: represent trace files
     Once the website traces are stored in pcap-files, feature vectors
     need to be extracted. A feature vector is represented by a Python
     class =Counter=, which can be created from a pcap file, or
     persisted to a JSON\cite{rfc7159} file containing timing and
     packet size information (to save time and space).

     To create a counter, you can use =counter.Counter.from(filename1,
     filename2, ...)=. This is also called indirectly when using
     =counter.py= from the command line, as in

     python /path/to/counter.py

     This extracts data from all pcap files in the current directory and
     subdirectories (excluding Address Resolution Protocol messages and
     ACKs). The filename of the pcap files needs to be =url@timestamp=,
     for example =craigslist.org@1445352269=. The part up to the
     separator =@= is treated as the URL. If JSON-files of the name
     =url.json= (for example =craigslist.com.json=) exist, those are
     preferred instead of the pcap files.

     In the interactive shell, there is a dictionary called
     =COUNTERS=, with the domain names as keys and an array with
     =Counter=s as values. If there were no JSON files in the
     directory, these are created automatically via the =save()=-method.

     To extract the features from a single =Counter=, they
     =panchenko()= or =cumul()=-methods can be used, to inspect single
     features of Panchenko et al.'s first attack\cite{panchenko} you
     can call =get('feature_name')=[fn::for example
     =COUNTERS['cnn.com'][0].get('duration')')=].

     =panchenko()= yields a feature vector with default padding of
     Panchenko's variable-length features. Since Panchenko et
     al\cite{panchenko} gave explicit size conversions, the sizes have
     not been normalized further. The default padding is computed to
     be large enough for all traces.
***** TODO cumul
**** WRITE analyse.py: code to classify etc: transform features to vector
     Once the =Counter=s data is obtained, it needs to be transformed
     to input for scikit-learn's\cite{scikit-learn} classifiers.

     The code to convert these features to classification input can be
     found in =analyse.py= (see appendix [[#analyse]]). This determines the maximum
     length of all variable-length features, 0-pads Panchenko's features
     with zeroes to the same length, and converts them to an array fit
     for input into scikit-learn's classifiers. When called from the
     command line, as

     python -i /path/to/analyse.py

     , it will extract the feature vectors from JSON or pcap files in
     the current directory, and run 5-fold cross-validated classifiers
     against the data.

     =Counter= input features are transformed into scikit-learn input
     in the =to_features()= function, which normalizes all vectors to
     have the same size (padding with 0s), and creates the feature
     matrix =X= with numeric class labels =y= (and class names in
     =y_domain=).

     If you wish to run LibSVM on the command-line, there is also
     =to_libsvm(X, y, fname='libsvm_in')=, which can be called with the
     output of =to_features=. It writes lines in X with labels in y to the
     file 'libsvm_in' (by default).
***** TODO ref stackoverflow why 0 padding
****** TODO or better, some statistics text
***** TODO see also =to_features_cumul=

** WRITE outlier removal
   As described by Panchenko et al. \cite{panchenko2}, CUMUL is
   enhanced by outlier removal. Their
   software[fn::\url{http://lorre.uni.lu/~andriy/zwiebelfreunde/}] has
   different settings, with a median-based[fn::in addition to the
   median-based original by Wang and
   Goldberg\cite{wpes13-fingerprinting}] as well as a
   25%/75%-quantile-based approach. The paper mentions only the
   quantile-based filtering, so that was also used in the later tests
   here.

   If parameter estimation for the support vector machine is done on
   both training and test data, the results

   - implemented in [[file:bin/extract_attribute.py::def%20remove_quantiles_panchenko_2(counter_list):][ex-att]]
   - quantiles:
     - numpy instead of his original code for code clarity
     - just take quantiles, use his limits
   - how many? 5%?
   - evaluation of outlier removal steps
     - cumul-print of outlier removal steps? (maybe, ask d if enough time ;-)
   - train on or, test without
   - maybe show outlier in CUMUL graphics
   - daniel arp\cite{arp-personal}
     - better approach: only OR on training data, or keep values from
       there to apply later
     - other approach makes little sense (except for minimal-OR)
       - do not know in advance
   - results
     - use [[file:data/results/alternatives.org::*or%201-3,-1-3][or 1-3,-1-3]]
     - test -1 (use max-min), no advantage over just removing smallest/largest
*** TODO run test, include results
*** TODO link to panchenko's software
   - after cross-validation split
   - but can do that before features
     - list of (url, list-id) tuples, one for each counter
     - randomly choose subset
   - link to file
   - document effect of different schemes: is there any accuracy effect?
     - tables
     - scheme: all, only quantile, none, only minimial
   - document number of filtered traces
     - both panchenko's own and mine
** KEYWORDS panchenko v1 different classifiers
   <<different-classifiers>>
   - more features than cumul default
     - grows with more classes as it has to pad to maximum
     - takes a long time to do param-eval, and partly not as accurate
   - experiment
   - different classifiers, different results
   - much easier to just use knn, ....
     - and slightly better results
   - more work for svm parameter estimation
** WRITE panchenko v1 vs cumul
   Panchenko et al proposed two methods for analysing traces.

   They both use support vector machines as classifier, but differ in
   the features they select.

   Since CUMUL\cite{panchenko2} is Panchenko et al.'s newer approach
   after their first classifier\cite{panchenko} (called /version 1/
   from now on), better accuracy of the former is to expected.

   - both: similar results for different classifiers
   - panchenko v1:
     - takes longer
     - is less accurate
     - is more work: vector length normalisation [for cross-test]
     - first to really work
   - accuracy hit of about 20%?
   - best parameters sometimes outside of panchenko's range
   - same classifier
   - state of the art:
     - wang-knn
       - knn with parameter weighting step
       - first to 92% accuracy (current limit)
     - cumul:
       - faster
       - easier to see
       - bigger dataset
     - k-fingerprinting
       - accuracy
     - all similar accuracies (as of k-fingerprinting)
** TODO Evaluation of Defenses [0/5]
*** WRITE Evaluation of Addon
    #+CAPTION: different defense versions with CUMUL, svc classifier
    #+ATTR_LATEX: :float nil :width \textwidth
    [[./pictures/svc_oh_vs_acc.eps]]

    #+CAPTION: different defense versions with CUMUL, extratrees classifier
    #+ATTR_LATEX: :float nil :width \textwidth
    [[./pictures/extratrees_oh_vs_acc.eps]]
    
    - problem: tunable: factor correlation to overhead
      - not given for 0.15, retro, 0.18-0.21
      - old graph: cluster for retro, etc
      - given in 0.22? (td: tests)
      - graph: accuracy vs overhead
        - mention (somewhere else that tamaraw fell through)
    - problem: variants
      - a.k.a. scenarios (a/b I/II bursts)
      - reevaluate if make a difference
    - problem: bursts
      - at simple
      - at normal
    - factor at simple
      - 0.18 over-engineered?
    - optimal defenses: 22 and simple2@10
      - td: battle those
      - td: pix vs disabled
    - different factors
    - all around same curve
    - which classifier classifies which page well?
    - 5aII higher accuracy at svc than at et
*** WRITE sota (practical): wtfpad
    - overhead of wfpad depends on client-bridge network connection
      - show different results
      - insert value from paper?
    - (maybe show both graphs, at least mention values)
    - all of mine so far add additional data for each request, wtfpad
      adds additional data over time, less with more requests, more
      with less
    - graph
      - disabled vs wfpad
        - on each page: how much correctly classified?
        - google.com
#+BEGIN_LaTeX
\begin{table}[H]
\begin{longtable}{c c c}
   Page: google.com & Page: tumblr.com & Page: netflix.com \\
\endfirsthead
   Page: google.com & Page: tumblr.com & Page: netflix.com \\
\endhead
   \hline
   \multicolumn{3}{c}{WTF-PAD} \\
  \includegraphics[width=0.3 \textwidth]{./pictures/google.com__wfpad.eps}
  & \includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__wfpad.eps}
  & \includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__wfpad.eps}
  \\
   \multicolumn{3}{c}{Addon Version Simple.1, Factor 10\%} \\
\includegraphics[width=0.3 \textwidth]{./pictures/google.com__simple1@10.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__simple1@10.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__simple1@10.eps}
\\
   \multicolumn{3}{c}{Addon Version 0.22, Factor 10\%} \\
\includegraphics[width=0.3 \textwidth]{./pictures/google.com__22.0@10aI.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/tumblr.com__22.0@10aI.eps}
&\includegraphics[width=0.3 \textwidth]{./pictures/netflix.com__22.0@10aI.eps}
\end{longtable}
\caption{CUMUL traces of different defenses}
\end{table}
#+END_LaTeX
*** WAIT sota (theoretical): walkie-talkie
    - as of paper: 32%bw \to 5% fpr and 55% bw \to 10% fpr
    - how to translate to closed-world?
    - wait for [[file:~/da/da.org::*open-world?%20(vs%20erst%20mal%20fertig?)][open-world? (vs erst mal fertig?)]]
*** WAIT (maybe) vs optimal attacker
    - show just traces of single html retrieval:
      - small page, small page with addon, bigger page
      - does with addon look like bigger page?
    - wait for [[file:~/da/da.org::*experimente][experimente]] plan 3
*** KEYWORDS svc vs extratrees
    - svc very good on unaddoned data, (if trained and or on all)
      - extratrees lacked ca 5% behind, but shines on addons
        - similar, but bit less good for knn, randomforest, even decisiontrees
      - others not tested
      - exception 22.0/5aII
        - continue here: inspect which pages, etc
** WAIT Websites
   - which websites classify well with which classifier, which badly
     - algorithm
       1. clf.train on whole disabled set, with or level 2
       2. clf.predict on addon set
       3. for each class (number) in y2
          1. create list of classes it was mapped to
          2. compute score of how much it was mapped to itself
          3. compute score of top three other classes
             1. count occurrence number
             2. sort
       4. look up names
     - implemented up to 3.2.
   - google.com
     - check that not a robot
     - td: estimate probability if matches traces
   - aliexpress.com
     - https of akamai
     - td: check with recapture both
   - wait for [[file:~/da/da.org::*klassifikator][klassifikator {0/12}.plan.2]] and
   - results:
    #+CAPTION: classification accuracy on 30 classes, different classifiers
    #+ATTR_LATEX: :align l||p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}|p{0.08\textwidth}
| id   | disabled with ExtraTreesClassifier | wfpad with ExtraTreesClassifier | simple2/5 with ExtraTreesClassifier | 0.22/5aI with ExtraTreesClassifier | disabled with OneVsRestClassifier | wfpad with OneVsRestClassifier | simple2/5 with OneVsRestClassifier | 0.22/5aI with OneVsRestClassifier |
| <4>  |    <6> |    <6> |    <6> |    <6> |    <6> |    <6> |    <6> |    <6> |
|------+--------+--------+--------+--------+--------+--------+--------+--------|
| msn.com |    1.0 |  0.875 |  0.525 | 0.1230 |   0.95 |    0.3 |  0.025 |    0.0 |
| sina.com.cn |    1.0 |  0.925 |   0.95 | 0.9538 |    1.0 |  0.925 |  0.925 | 0.9538 |
| buzzfeed.com |  0.975 |  0.875 |   0.85 | 0.5230 |   0.85 |    0.0 |    0.0 |    0.0 |
| youtube.com |  0.825 |  0.575 |  0.725 | 0.3281 |  0.375 |  0.025 |    0.0 |    0.0 |
| xvideos.com |   0.85 |    0.7 |  0.675 | 0.0923 |    0.5 |  0.025 |    0.0 | 0.1538 |
| bankofamerica.com |  0.775 |   0.85 |    0.8 | 0.0307 |  0.775 |  0.125 |  0.025 | 0.1846 |
| blogspot.com |    1.0 | 0.9487 |  0.825 | 0.0307 |    1.0 |    0.0 |  0.225 | 0.0307 |
| jd.com |   0.95 |    0.7 |    0.6 | 0.0769 |   0.75 |    0.2 |    0.0 | 0.1384 |
| weibo.com |    1.0 |  0.975 |  0.875 | 0.4153 |    1.0 |    1.0 |  0.925 | 0.7846 |
| ettoday.net |    0.9 |   0.85 | 0.3333 | 0.1384 |  0.625 |  0.425 | 0.1794 | 0.3692 |
| huffingtonpost.com |  0.975 |    0.3 |  0.175 | 0.1692 |  0.425 |  0.075 |    0.0 | 0.0461 |
| microsoftonline.com |    1.0 |  0.625 |  0.675 | 0.3692 |   0.75 |  0.125 |  0.125 | 0.1076 |
| twitter.com |   0.75 |  0.825 |  0.625 | 0.2307 |  0.475 |  0.025 |    0.0 | 0.1384 |
| facebook.com |  0.925 |  0.825 |  0.925 | 0.5846 |    0.1 |    0.0 |    0.0 | 0.0307 |
| netflix.com |    0.9 |    0.9 |    0.5 |    0.0 |  0.875 |    0.0 |  0.025 | 0.0307 |
| reddit.com |    1.0 |    0.9 |  0.625 | 0.1230 |  0.975 |  0.475 |   0.35 | 0.1230 |
| github.com |   0.95 |    0.9 |  0.675 |    0.2 |   0.85 |    0.0 |    0.0 | 0.0615 |
| coccoc.com |   0.95 |  0.975 |  0.775 | 0.0461 |  0.125 |    0.0 |    0.0 | 0.0769 |
| apple.com |  0.975 |    0.9 |    0.0 | 0.0307 |  0.825 |    0.0 |    0.0 | 0.0461 |
| go.com |  0.875 |  0.825 |   0.55 |    0.0 |   0.55 |    0.0 |    0.0 |    0.0 |
| xnxx.com |    0.9 |   0.95 |  0.725 | 0.1230 |  0.925 |    0.4 |    0.0 | 0.0153 |
| imgur.com |   0.95 |  0.925 |    0.6 | 0.1692 |  0.675 |   0.05 |    0.0 |    0.0 |
| pornhub.com |    1.0 |  0.625 |  0.525 | 0.2461 |  0.975 |    0.8 |   0.55 | 0.4461 |
| yahoo.com |  0.975 |   0.75 |  0.325 | 0.3230 |    0.5 |    0.1 |    0.0 |    0.0 |
| wordpress.com |  0.775 |  0.725 |  0.325 | 0.0156 |  0.775 |  0.075 |  0.175 | 0.0625 |
| tumblr.com |   0.95 |  0.075 |  0.925 | 0.8461 |    0.3 |    0.2 |    0.0 |    0.0 |
| google.com |    1.0 |    0.0 |  0.475 |    0.2 |  0.975 |  0.975 |    0.0 | 0.1230 |
| qq.com |   0.85 |   0.75 |    0.5 |    0.0 |  0.825 |   0.15 |   0.15 | 0.4531 |
| cntv.cn |   0.95 |  0.975 |  0.725 | 0.2812 |   0.95 |   0.95 |  0.425 | 0.5156 |
| soso.com |  0.975 |  0.975 |   0.95 |   0.25 |    0.9 |  0.025 |  0.075 | 0.5312 |

- CUMUL-traces for buzzfeed.com (svc fails) and weibo.com (svc wins)

#+BEGIN_LaTeX
\begin{table}[H]
\begin{longtable}{c c c}
   WTF-PAD & Simple Addon & Addon 0.22 \\
\endfirsthead
   WTF-PAD & Simple Addon & Addon 0.22 \\
\endhead
   \hline
   \multicolumn{3}{c}{buzzfeed.com} \\
\includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__wfpad.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__simple2@5.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/buzzfeed.com__0.22@5aI.eps}
\\
   \multicolumn{3}{c}{weibo.com} \\
\includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__wfpad.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__simple2@5.eps}
& \includegraphics[width=0.3 \textwidth]{./pictures/weibo.com__0.22@5aI.eps}
\end{longtable}
\caption{CUMUL traces of defenses with different classifier results}
\end{table}
#+END_LaTeX

similar, little to see

- 

** brainstorm                                                       :ARCHIVE:
   - describe setup
     - which sites, why
       - some with great variance
       - top-10 did not work
     - how to capture
       - tools + scripts
       - bridge
     - how to analyse
       - json
       - script: reimplement
         - version 1: problems to achieve panchenko's accuracy
           - classifiers
         - cumul: problems to achieve panchenko's accuracy
           - outlier removal
   - addon
   - does it work?
   - does it work better?
   - which variant works?
   - difference svm others
     - other grouped
     - svm alone
       - but better for fitting original data
       - "overfitting"
     - review trace pictures
   - panchenko worse?
     - do pictures/comparisons
     - timing comparison on disabled
   - plots
     - accuracy vs overhead
       - all methods at 30
       - and vs version 1 for comparison
         - which parts?
           - unaddoned
     - cumul
       - disabled vs wfpad, tamaraw, simple10, simple30, 22@best
   - compare bursts to nobursts
   - wfpad pads small sites much, larger sites little
     - addon-simple does the opposite
   - have a look at [[*practical wf: analyzing traces][practical wf: analyzing traces]]
* WAIT Conclusion
* TODO topics [0/108]
** TODO Website Fingerprinting [0/50]
*** tools
**** TODO capture alternatives [0/1]
     Several applications can capture network traffic to files. The most
     well-known and oldest of these is tcpdump
     [fn:: \url{http://tcpdump.org}] It is a command-line utility, which is
     available on many UNIX-like systems and Windows.

     A modern contender with a GUI is wireshark. It also sports a
     command-line version, tshark. As it offers TLS packet reassembly,
     tshark was used in this thesis.

     Both programs rely on the libpcap library for access to network
     packets.
***** TODO subsect to [[*by-hand initialization to retrieve websites][by-hand initialization to retrieve websites]]
**** shell script
     Simply calling =firefox website= loads the website in Firefox. This
     is the approach Wang recommended(\cite{wang-personal}.
***** TODO how to check that page has loaded
**** Selenium
     Selenium is the de-facto standard for testing web applications. It
     has drivers for several browsers, allowing it to control them, and
     evaluate the retrieved page. Its documentation is currently
     transferring from Version 1 to Version 2.
**** Chickenfoot
     Chickenfoot was a Firefox addon which allowed browser scripting. It
     was developed at MIT\cite{chickenfoot}. The most recent GitHub
     release[fn:: \url{https://github.com/bolinfest/chickenfoot}] is for
     Firefox 4.
**** CHECK Marionette
     <<Marionette>> Marionette is the next generation mozilla testing
     framework. It is works just like Selenium and was designed to be
     integrated into it. It was chosen for this thesis, as it made the
     Tor Browser Bundle easily accessible.

     After installation of the library (see below), controlling the browser
     takes two easy steps:

     1. start the Tor Browser Bundle with the `-marionette` switch

        #+BEGIN_SRC sh
          cd tor-browser_en-US/Browser
          ./firefox -marionette
        #+END_SRC

     2. attach to a running browser in Python

        #+BEGIN_SRC python
          from marionette import Marionette
          client = Marionette('localhost', port=2828);
          client.start_session()
          client.navigate('http://cnn.com'); # navigate loads a website
        #+END_SRC

     Marionette has the benefit that the =client.navigate()= call
     returns only after the page has loaded, (and throws an error if
     the page could not be loaded). This obsoletes f.ex. Panchenko et al.'s
     \cite{panchenko} need to test whether a page loaded completely.
**** CHECK Marionette installation
     Marionette exists as a Python Package. It is thus easily installed
     via

     pip install marionette_client

     After installation pip via =sudo apt-get install python-pip=). Using
     a virtualenv is highly recommended in the documentation. If using
     only Marionette, it proved to be unnecessary. The combined
     installation of Marionette with Mozmill broke Marionette.
***** TODO merge with above and split out pip install (also needed for wsgi)
**** criteria for tool to retrieve websites
     - script tor browser: load new page
     - easy set-up
     - should
       - register page load or error
     - might
       - set tor's paranoia slider
       - install extra addon
**** TODO who used which retrieval method
     - who did sth
       - p: 
         1. chickenfoot only
         2. Chickenfoot, iMacros, and Scriptish
       - h
       - ll
       - w
       - c
       - d
       - j
     - what did they use
       - list
       - chickenfoot
       - modified browser
       - selenium: daniel
       - plain tor bundle
*** setup
**** TODO by-hand initialization to retrieve websites
     After installation, the tor browser bundle performs some
     initialization steps. To complete these easily, start the tor
     browser bundle-firefox by hand once, set the connection type and
     have it load any website via Tor. All this also downloads Tor
     metadata, which allows to connect more quickly later on.
**** tshark installation
     You also need to install =tshark= [fn:: via f.ex. =sudo apt-get
     install tshark= on Debian-based systems] and enable the user to
     capture packets [fn:: via (Debian-based) =sudo dpkg-reconfigure
     wireshark-common= and adding the user to the =wireshark= group
     (in =/etc/groups=)].
**** TODO how to get tor browser bundle to work
     In order to start the tor browser bundle via the =./firefox=
     command, you need libraries, which are bundled with the binary.
     They can be found inside the =/TorBrowser/Tor= directory.

     The library path environment variable can be set on the command-line via
     #+BEGIN_SRC sh
     export LD_LIBRARY_PATH=/lib:/usr/lib:/path/to/bundle/Browser/TorBrowser/Tor
     #+END_SRC
     The script [[one-site.py][one-site.py]] uses this internally.

     - install xpra
***** TODO is old, still use, or remove?
**** Avoiding safe mode on restart
     If Firefox was killed via a signal (as opposed to closing the
     window), it prompts to start in Safe Mode afterwards.

     This behavior can be avoided in three ways:

     You can set the firefox preference
     =toolkit.startup.max_resumed_crashes= to -1, you can set the
     environment variable =MOZ_DISABLE_AUTO_SAFE_MODE= (did not work
     in Tor Browser Bundle version ...), or --- as a last resort ---
     you can remove the =toolkit.startup.recent_crashes= line in the
     =prefs.js= config file which saves the number of consecutive
     kills via =sed -i '/toolkit\.startup\.recent_crashes/d'
     /path/to/prefs.js=.
***** TODO TBB current version
**** headless configuration
     If you want to capture on a headless server, you can use the
     =xvfb=-package. which is installed via =sudo apt-get install xvfb=.

     Then, you can run the X Virtual Framebuffer via

     =Xvfb :1=

     tell the browser to use it via

     =export DISPLAY:1=

     and start the retrieval as mentioned above.
**** TODO thoughts on size of data set
     - computable (n^2 for svm with good results)
     - number of instances negligible for computation
       - check this
     - stable results
     - recent papers
       - Panchenko: 775 a 20
       - Wang:
         - 100 a 90 of sensitive pages
         - 5000 a 1 of non-monitored pages
       - Cai: 400 samples of bbc.co.uk
         - 100 \to 800 once a 20 \to 40 twice
     - (currently closed world)
***** TODO more complete list?
**** TODO filtering tshark files [0/4]
     Although this requirement might later be removed (see [[*Further%20work][further
     work]]), the addon currently needs a generator of cover traffic to
     work. While it can be set in the add-on's preferences, this
     generator ran on the same host as the tor client. Thus, the
     capture files also contained traffic of the cover traffic
     server. As they do not belong to the Tor traffic, are not what
     the adversary sees, and might distort the result, they were
     filtered. (Even though the accuracy results were not greatly
     changed by this).

     Fortunately, =tshark= offers a way to filter these files as
     mentioned in \cite{splitcap}. The (read) filter commands are
     described in the manual \cite{wireshark-filter}, with the tcp
     protocol specific fields as given in \cite{tcp-filter-fields}.

     The script to solve this is in the appendix [[7777]]. As the server
     ran on port 7777, which was allowed only as an incoming port by
     the firewall, it suffices to filter by port name. (Otherwise, the
     read filter would need to be modified).
***** TODO implementation
     - summary approach: file 7777.sh takes each (pcap) file in
       current directory, filters the port 7777 out
     - apply this to each subdirectory
     - then move all files to a common directory
****** TODO include script from duckstein
***** TODO link to man tshark
**** overview
     - for the sake of comparability, also bridge for addon tests
       - and easier to filter
*** TODO example: single files of a website
    The complete data of google.com can be retrieved via

    =mkdir site; cd site; wget -p -H google.com=

    which yields (in germany) the files (=find . -type f -ls=, formatted)

    |  size | url                                                               |
    |-------+-------------------------------------------------------------------|
    |       | <65>                                                              |
    | 18979 | google.com/index.html                                             |
    | 17284 | www.google.de/images/nav_logo229.png                              |
    |  1834 | www.google.de/images/icons/product/chrome-48.png                  |
    |  5482 | =www.google.de/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png= |
    |  5430 | =www.google.de/images/branding/product/ico/googleg_lodp.ico=      |
    |  8080 | www.google.de/robots.txt                                          |

    thus, there should be 5-6 (depending on robots.txt) requests
**** TODO tshark for normal (non-tor) retrieval
**** TODO mention redirects
*** practical wf: analyzing traces
**** TODO how to process the data
     The aim of processing is to extract features relevant for machine
     learning from the original trace files, which are in =pcap= format.

     Of the several tools available for reading =pcap=, =tshark= was
     chosen. It is the command-line version of the Wireshark protocol
     analyzer[fn:: \url{http://www.wireshark.org}].

     - tshark internally
     - python triggers
     - collects,
     - sums in the end
     - displays
     - =Counter=-class
***** TODO why filtering allowed
***** TODO see if merge/unify with [[transform to panchenko-features]]
***** TODO and if include stuff from [[*from%20%5B%5B*transform%20to%20panchenko-features%5D%5D][from {{*transform to panchenko-features}}]]
**** transform to panchenko-features
     In encoding packet sizes, this thesis follows Panchenko et al.'s
     approach, who recorded "incoming packets as positive, outgoing ones
     as negative numbers."\cite{panchenko}
**** WAIT and MAYBE how to get wang/goldberg to work
     As the =notes= file says:

     "svm-train and svm-predict come from the libSVM package."
***** maybe to unused
**** TODO libsvm (short)
     LibSVM is a library for support vector machine classification and
     regression. It is used under-the-hood for scikit-learn, yet one part
     of functionality required a specific module which was not
     integrated.

     Its input format is very simple: First a number determining the
     class of the data, then a colon, finally all the data for an
     instance, separated by whitespace.
***** TODO link to code to generate
**** WAIT effect of panchenko's weighting schema
     Currently, fixed attributes are weighted heavily in favor of total
     incoming/outgoing bytes.
***** maybe
**** TODO scikit-learn
     The python module scikit-learn\cite{scikit-learn} is described as a
     collection of "tools for data mining and data analysis".

     It combines python's ease-of-use with the efficiency of libraries
     written in C, such as LibSVM. It offers many different classifiers
     and regressors, such as K-NN, SVM, decision trees, linear
     approximation, random trees, etc.
***** TODO regressor? wording
*** INTEGRATE History of Website Fingerprinting
    The idea of using traffic analysis to gather information about
    encrypted traffic was mentioned in \cite[10.3]{applied96} and
    applied in the analysis of SSL 3.0 by Wagner and
    Schneier\cite{SSL}

    - quantifying etc

    The term /website fingerprinting/ was coined by Hintz in 2002. A
    successful attack against single-hop proxies was carried out by
    Herrmann et al. in 2009.

    The website fingerprinting attack scenario is already described in
    the original Tor design paper\cite{tor-design}. Previous to
    Panchenko et al.\cite{panchenko}, it was considered "less
    effective"\cite{tor-design} against Tor, due to stream/circuit
    multiplexing and fixed cell sizes.
**** index? traffic analysis
*** TODO defenses
    - walkie-talkie
    - wtfpad
    - supersequence
    - tamaraw
    - buflo

    There are other methods of defense, which might help mitigate
    website fingerprinting. A certain browser extension and text-only
    browsing might reduce the fingerprint.
**** CHECK Additional Plugin: requestpolicy
     In addition to the security-centric addons deployed with the
     Tor-Browser-Bundle, there is an additional addon with orthogonal
     protection:
     RequestPolicy[fn::\url{https://requestpolicycontinued.github.io/}]
     controls which third-party content to load on a given page. Every
     query to the original domain is allowed, while requests to other
     domains must be temporarily or permanently approved. It comes
     with a restrictive set of pre-defined rules (for example google
     pages are allowed to access gstatic). Both a blacklist and a
     whitelist mode exist.

     This could easily (and individually) alter the request/response
     characteristic of a website. More study might shed some light.

     RequestPolicy hindered early versions of the Addon, as it blocked
     [[page-worker]]s. If both are deployed alongside, it should be
     carefully checked.
***** TODO move below tbb
***** MAYBE also cite requestpolicy (orthogonal)
**** CHECK write new plugins
     Instead of inserting dummy traffic into the connection, one could
     throttle the "data rate" of request and responses (or only
     requests or the ratio) --- optionally padding with dummies up to
     the maximum rate.

     This approach has been used by f.ex. \cite{effective}, and has
     been proven to work, albeit requiring higher latency, it has not
     been explored further, as
     - it might be hard to implement in a plug-in, and
     - randomized defenses seem offer adequate defense at reduced
       latency and bandwith
***** TODO move to description of other defenses
**** CHECK tor browser bundle defense
     After the attack by Panchenko et al. \cite{panchenko}, the Tor
     Project deployed an experimental defense \cite{experimental} in
     the Tor Browser Bundle.

     This defense enables HTTP pipelining and randomizes both the
     number of concurrent requests and their order.  It was shown to
     be ineffective by \cite{ccs2012-fingerprinting}, and confirmed by
     \cite{wpes13-fingerprinting} and \cite{effective}.
***** TODO HTTP pipelining refer to/elaborate, make own show subsubsection
**** TODO running an OR
     - hinted by ...
     - extra traffic
     - depends on data rate: if all is easily decorrelatable, maybe no
       extra protection
**** CHECK text-only
***** TODO lynx link
*** distribution of (main) features
    These distribution histograms show how Panchenko's main features
    are distributed. They are stacked histograms with classes
    separated by colors. They are compared (visually) to the HTTP
    Traffic Model\cite{newtrafficmodel}.

    [[file:pictures/all_count_in.png]]
    shows the number of downstream/incoming packets.

    The general form of a gamma distribution may be
    fitting. Conceptually, this should be approximately

    num_embedded (gamma) * size_embedded (lognormal) / packet_size

    [[file:pictures/all_count_out.png]]
    shows the number of upstream/outgoing packets.

    Conceptually, the

    [[file:pictures/all_length_0.png]]
    the length of the Size Marker feature vector.

    [[file:pictures/all_num_sizes_in.png]]
    number of different packet sizes downstream/incoming.

    [[file:pictures/all_num_sizes_out.png]]
    number of different packet sizes upstream/outgoing.

    [[file:pictures/all_percentage_in.png]]
    percentage of incoming bytes (of total).

    [[file:pictures/all_total_in.png]]
    total bytes downstream/incoming.

    [[file:pictures/all_total_out.png]]
    total bytes upstream/outgoing.
**** TODO compare to HTTP model
*** Who could attack via WF
    As website fingerprinting requires very litte resources, a specific
    attacker could be a WLAN sniffer, an ISP, up to maybe even a nation
    state.
*** CHECK Panchenko et al.'s Attack via Website Fingerprinting
    The first website fingerprinting
    attack\cite{ccsw09-fingerprinting} to also target Tor had yielded
    little accuracy. This was due\cite{experimental} to Tor's
    multiplexing and fixed cell-size.

    Panchenko et al\cite{panchenko} were the first to publish a
    successful website fingerprinting attack on Tor. They extracted
    HTTP-specific features from the packet trace and used those in a
    hand-tuned support vector machine with a radial basis function
    kernel.
**** practical wf: Capturing traces
**** TODO move to subsection related work
** TODO Addon Design and Implementation [0/55]
*** [[*description of add-on][description of add-on]]
*** Defenses
*** TODO Variations of Cover Traffic
    There are two variations how to generate Cover Traffic.
    2. Given a webpage and its size, how much traffic should be generated?

    This leads to the following variations:

    1) bloom binning (I) with known sizes (A)
    2) bloom binning (I) with random sizes (B)
    3) one target distribution (II) with original size from bloom (A),
    4) one target distribution (II) with random sizes (B)

    | SIZES \ TARGETS | I: bloom binning | II: one distribution |
    |-----------------+------------------+----------------------|
    | A: known sizes  |                  |                      |
    | B: random sizes |                  |                      |
*** TODO Mozilla Add-On Sdk [0/12]
**** CHECK Introduction to the Mozilla Add-On Sdk
     #INDEX: XUL
     #INDEX: XML User-interface Langage
     The Add-on SDK by Mozilla facilitates the development of
     Firefox-Addons.

     It allows users to create addons using HTML and Javascript only, as
     opposed to the previous use of
     XUL[fn:: \url{https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL}],
     the XML User-interface Language.

     The addon execution entry point (like =main= in C and Java) can be
     configured via the =preferences.json= file. By default, the main
     addon-script is called =index.js=.

     The SDK contains many tools to interact with the browser. URLs can
     be loaded in the background via the =page-worker= module; the
     =page-mod= module injects JavaScript code into the page the user is
     browsing to. User-created code can be tested via unit tests.

     If none of the easily accessible high-, or low-level modules
     suffice, much of the browser's functionality is accessible via
     the Components
     object[fn:: \url{https://developer.mozilla.org/en/Components_object}],
     which can be accessed as =require("chrome")=.
**** TODO Debugger
**** TODO Available Data
     Firefox offers several ways for an add-on to listen for web activity.

     - contents of main page
       \to links to each domain
     - page-mod
       - problems: only when page is loaded, problem for cover traffic
       - but +: ends of all the loading (and processing)
     [[file:docs/lit.org::*%5B%5B./Intercepting%20Page%20Loads%20-%20Mozilla%20|%20MDN.html%5D%5BIntercepting%20Page%20Loads%5D%5D][Intercepting Page Loads*]] lists several
     - load events
     - http observer
     - webprogersslistener
     - xpcom
       - policymanager
       - documentloader
***** each load of page
***** end of page load
***** TODO as references or as footnote?
      ref, as completely read?
**** separation of scripts
     As a security measure, there is a separation between

     1) /add-on scripts/, which are run in the browser context, but
	cannot access the web page, and
     2) /content scripts/, which are run in the page context. They can
	access the DOM, but not add-on scripts. nor
     3) /page scripts/, which are those included in the website via
	f.ex. =<script>= tags

     Bridging this separation, f.ex. accessing page scripts (and vice
     versa) is possible, but needs some extra work.
***** WAIT index: page scripts, content scripts, add-on scripts
**** CHECK message-passing
     There is a mechanism to pass content from the add-on to the
     content scripts, as shown in the example.

     A single string can be passed. As this string can be any serialized
     JSON\cite{rfc7159} object, this is not much of a limitation. (It
     effectively disallows the passing of functions and circular
     objects).

     In a content-script, a message can be sent via
     =self.port.emit('message_type', param)= and received via
     =self.port.on('message_type', function(param))=.

     In the Addon-Context, a =worker= object is used and the
     content-script's =self= is replaced by a =worker=. The worker is
     initialized via the =onAttach= parameter of f.ex. the page-mod.
**** TODO collect/list all addon sections
**** CHECK page-worker
     <<page-worker>>
     A =page-worker= creates "a permanent, invisible page and access[es]
     its
     DOM."[fn:: \url{developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-worker.html}]

     New pages can be loaded in the background, which would allow for the
     retrieval of camouflage traffic, as described by \cite{panchenko}.

     A minimal new page-worker is created via

     #+BEGIN_SRC js
       var pageWorker = require("sdk/page-worker").Page({});
     #+END_SRC

     The page-worker's page can be set dynamically via

     pageWorker.contentURL = "http://en.wikipedia.org/wiki/Cheese"

     This fetches only the file pointed to. The retrieval of included
     images, stylesheets, etc, is not automatic.

     A page-worker was used in the initial prototype. The RequestPolicy
     addon blocked this method of retrieval.

**** TODO page-mod
     <<page-mod>>
     The
     page-mod[fn:: \url{http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html}]
     module injects "scripts in the context of web pages whose URL
     matches a given pattern."

     The pattern can be given as ="*"= or =/.*/= to run on every
     user-visited page.

     It thus offers the possibility to check for the end of a web page
     load by the user.

     A page-mod example is

     #+BEGIN_SRC js
       const pageMod = require("sdk/page-mod");
       pageMod.PageMod({
           include: /.*/,
           contentScriptFile: "./getLinks.js",
           onAttach: function(worker) {
               worker.port.on("links", function(JSONlinks) {
                   addToCandidates(JSON.parse(JSONlinks));
               });
           }
       });
     #+END_SRC

     , which is run on every page, applies the =getLinks.js= script and
     listens for its feedback, which is then used via
     =addToCandidates()=.

     The page-mod has a =contentScriptWhen= parameter, which specifies
     when to attach the script to the page. Valid values are =start=,
     =ready=, and =end=, the last of which triggers at the
     =window.onload= event, when the complete page, including
     JavaScript, CSS, and images has loaded.

     A page-mod offers many other options such as f.ex. stylesheets,
     script parameters, etc.
***** link page-mod
      http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html
**** CHECK Installation and Use of Jpm (the build tool)
     (SDK-)addons can be built via the =jpm=-tool. It is available as a
     NodeJS-Module via the built-in NodeJS Package Manager =npm=.

     Installing =jpm= is a two-step process. Firstly, install NodeJS
     either via built-in tools[fn:: for example =apt-get install
     nodejs-legacy= in Debian and Ubuntu] or via
     download[fn:: \url{https://nodejs.org}] then, do a

     npm install jpm

     to install jpm[fn:: for the current user, global installation is done
     via =npm install -g jpm=].

     Once =jpm= is installed, new addons can be created via =jpm init=,
     unit-tested via =jpm test=, live-tested via =jpm run=, the addon
     package built via =jpm xpi=.

     Another command that may be of use is =jpm sign=: as of Firefox
     version 47, Mozilla enforces that all addons be
     signed\cite{addon-signing}. If they are distributed via Mozilla's
     Addon Marketplace[fn:: \url{https://addons.mozilla.org}], they are
     checked and signed automatically. Otherwise, you can request an
     API key for signing and sign via the command
     [fn:: \url{https://developer.mozilla.org/en-US/Add-ons/SDK/Tools/jpm\#jpm_sign}]
     =jpm sign --api-key $SIGNING_KEY --api-secret $SIGNING_SECRET=.
**** TODO interacting with page-scripts
     By default, content-scripts are isolated from the modifications
     done by page-scripts.[[Interacting with page scripts]]

     To access object inside the page-scripts context, you can use
     =unsafeWindow=.

     The reverse is only true for primitive values. If page-scripts
     need to see altered behavior, it is possible to override
     functionality of the page by using =exportFunction=, as in

     exportFunction(open,
		    unsafeWindow.XMLHttpRequest.prototype,
		    {defineAs: "open"});

     This exports the (previously-defined) function =open()= to the
     XMLHttpRequest.prototype, where it replaces the built-in
     functionality.
***** Interacting with page scripts
=developer.mozilla.org/en-US/Add-ons/SDK/Guides/Content_Scripts/Interacting_with_page_scripts.html=
**** TODO [#C] <<<DOM>>>
     domain-object-model
**** TODO unit tests jpm
     JPM also offers the ability to write unit-tests.
*** WAIT [#B] Design
    #+BEGIN_LATEX
    \begin{adjustbox}{max width=\textwidth}
    \input{pictures/model.tex}
    \end{adjustbox}
    #+END_LATEX
**** by
     - generating cover traffic
**** modules [0/17]                                     :appendix:move_later:
***** TODO replace with pic [[shell:dia pictures/model.dia &]]
***** TODO how to show (singleton)-module in jUML
***** TODO Watcher
      - notifies when user loads sth, and when finished
      - implements nsIObserverService
****** TODO link nsIObserverService
****** methods
       - loads
       - endsLoad
***** CHECK CoverTraffic(Loader=default)
      <<CoverTraffic>> The =CoverTraffic= module provides requests for a
      single host contacted. This is the only module used with a
      constructor, as it requires several instances, one for each host.

      The cover traffic provided tries to mimic the [[#HTTP traffic model][HTTP traffic
      models]] parameters.

      There are two strategies implemented which have to be set by
      modifying the source code.

      One strategy deals with size estimation: for each page, the size
      of its HTML request and the number of embedded elements are
      stored in a statistic data structure depending on bloom filters,
      called [[SizeCache]]. If the size is known, it can be used or
      randomly guessed from the HTTP traffic model.

      Another strategy deals with target sizes. The size-cache stores
      approximate sizes, due to binning of values. One strategy is to
      pad both the HTML request size and the number of embedded
      elements up to the bin border. The other strategy determines a
      target distribution for each parameter, multiplies by the
      overhead parameter, and tries to attain that.

      Thus, on creation the site's and a target HTML size and number
      of embedded elements are determined. As creation is synchronous
      with the first HTML request, another request to pad up to the
      target HTML size is sent. As the target number of requests for
      embedded elements is a certain multiple of the actual number of
      requests, on each such request[fn:: signaled by the =loadNext=
      call], a probability is sampled, potentially resulting in a
      request for a cover embedded element. The cover element sizes
      are once again drawn from the HTTP traffic model.

      After the page has finished loading, the =CoverTraffic='s
      =finish()= method is called. If the number of embedded elements
      requests has been to low, the remainder are then dispatched.
****** WAIT check if still two strategies
****** TODO link to number of embedded elements and HTML request
****** TODO link to sizecache
***** TODO Loader(Source=default2)
      loads new cover page (mockable)
***** TODO Stats - Static functions
      statistical distributions (html, embedded, etc)
***** TODO CoverUrl
      source for cover traffic
      fixed domain, size as parameter
***** TODO BloomSort
      <<bloomsort>>
      sorts elements by size using Bloom Filters
      +add(id, value)
      +query(id): value
      +save
      +restore
***** TODO Random
      provides randomization methods
      +string(length:number) pseudo-random string
      +uniform01() secure random float in the range [0,1)
***** CHECK SizeCache
      <<SizeCache>> The SizeCache element stores approximations for
      both the HTMLsizes (=htmlSize()=) and number of embedded objects
      (=numberEmbeddedObject()=) per URL, using a [[bloomsort][BloomSort]] data
      structure for each.

      Exceptions from the BloomSort are passed on. This module is a
      facade \cite{gof} that initializes the bloom filters and
      simplifies access.
***** CHECK User
      The [[user.js][User]] module handles user action. It is the main controller.

      On each loading of a object via HTTP(S), it receives a message
      from the =Watcher= module via =loads()=, with the loaded URL as
      parameter.

      If it is a new request to the host, loading of an HTML page is
      assumed and a new =CoverTraffic=-Object is generated.

      If the host is known (as defined below), an embedded page is
      assumed and the (existant) related =CoverTraffic=-Object is told
      that an embedded element was loaded.
**** TODO browser caching
     - browsers cache
     - only helps in cover traffic, (unless warm/cold site model is used)
***** WAIT where to put this?
**** TODO Parameter: Sizes of HTML-Documents                       :appendix:
     :PROPERTIES:
     :CUSTOM_ID: find sizes of HTML-documents
     :END:
     The statistical size generation works with application-level
     sizes on the network, as the authors of the HTTP traffic
     model\cite{newtrafficmodel} analysed logfiles of the Squid
     proxy[fn:: \url{http://www.squid-cache.org}].

     The HTML-sizes could not be trivially obtained from the
     =Content-Length= in the browser, as there are additional headers
     and size-reduction via compression. The sizes were determined by
     retrieving the files with =wget= via squid. This is implemented
     via the [[./bin/html_top_100.sh]] script (see appendix).

     It empties the =access.log= file and the squid cache by
     restarting. Afterwards, the top-100 files are retrieved with
     =wget= via squid.

     From the log file =access.log=, the sizes are extracted via the
     command sequence

     #+BEGIN_SRC sh
       sudo cat /var/log/squid3/access.log | tr -s ' ' | cut -d ' ' -f 5,7 > /mnt/data/HTML-sizes
     #+END_SRC

     These sizes are then converted to a JSON-array via the
     [[./htmlSizeToJSON.py]]-file. It also does a check for duplicate
     values, choosing the lower one. This increases traffic, but the
     opposite might be too little traffic, thus easier website
     fingerprinting, which should be avoided.
***** TODO mention somewhere (cache)
**** TODO Estimate Parameter: Number of Embedded Objects
     <<number_embedded>>
     The second parameter for generating cover traffic is the number
     of embedded objects per HTML-page.

     These are extracted via the python script [[htmlToNumEmbedded.py][htmlToNumEmbedded.py]]
     which is called for each of the top-100's main web pages by
     [[retrieve-100-embedded.sh][retrieve-100-embedded.sh]].

     To extract, python's lxml module to parse the HTML's
     DOM extracts the URLs of embedded files from the attributes of
     several tags, f.ex. the =src= element of =img= tags.

     This implementation currently omits some possibly embedded
     elements, f.ex. those embedded in css files and =style= tags via
     the =@url= css-directive. It seems better for cover traffic to
     slightly underestimate the number of embedded elements. This
     might generate more traffic than strictly necessary, but here,
     safe seems better than sorry. Extracting just the right URLs is a
     matter of [[*Further%20work][further research]].
***** TODO read dom reference
***** TODO link to lxml website
**** TODO bloom-sort usage
     It is impractical to store the sizes of all URLs. Another
     possibility is to use Bloom Filters to aggregate groups of URLs
     with similar values, as described in [[*bloom-sort][bloom-sort]].

     Each groups gets borders (/splits/) and a size which represents each
     contained element.

     Determining the optimal number of groups, splits and sizes is a
     topic of [[*Further%20work][Further work]]. Here, initially the quantiles of the
     HTTP-model (see [[#HTTP traffic model][HTTP traffic model]]) were used. When the data were
     to be inserted, it turned out that especially the numbers of
     embedded elements did not match the theoretically proposed groups:

     For three groups, the splits would be given by the 33 1/3 and 66
     2/3 quantiles, as 0.0107 and 1.481. As the number of embedded
     elements is a whole number, two thirds of the information would
     be if an element is 0, the next group would contain all other
     elements: The (representative) sizes of the groups were given as
     7.915E-05, 0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

     The data to be inserted (see previous section) had the splits
     (quantiles) at 10 2/3 and 36 2/3 and the sizes at 6, 20, and 59
     2/3.

     In addition to using the observed sizes for the bloom filter, the
     number of groups was increased to 5.
***** TODO error rate computing
      - sources of error
        - filter tells that is has element when it has not
      - how does error appear
        - collision: one of several, the other might be true
        - replacement: simulates being another url
      - rates of error
        - "add" the error rates of the filters? (times population density?)
***** maybe graphics?
***** WAIT check "see previous section"
*** TODO Implementation [0/5]
**** TODO js coding best practices
     JavaScript\cite{ecma} is arguably a language with some great
     parts, but also several bad ones\cite{javascript}. Approaches to
     mitigate these include

     - "use strict";
     - unit tests
     - mention "good parts"?
       - for what exactly?
       - and javascript garden
     - jshint
***** mention bad parts?
**** Unit Testing
**** WAIT Cover add-on
     Defends against website fingerprinting by injecting artificial
     cover traffic into the communication.
***** when stable
      also cover against website fingerprinting by injecting really
      artificial cover traffic

      for every request, do one as well,
***** why as an add-on
      This is one of the few low-latency communication methods, Instead
      of burdening all of Tor with extra bells and whistles, this solves
      this deanonymization problem at the application layer, where its
      origins are. (Separation of Concerns)
**** TODO http server for testing
**** TODO Apache mod_wsgi
     =mod_wsgi= is a module for the Apache web
     server[fn:: \url{https://httpd.apache.org/}]. It executes python
     scripts which implement the WSGI standard\cite{pep3333}. An
     apache httpd serving only WSGI is easily set up via the
     =mod_wsgi-express= command, which is included in the =mod_wsgi=
     python package[fn:: \url{https://pypi.python.org/pypi/mod_wsgi}].

     Installation (Ubuntu Server Edition and Linux Mint 17.1 Rebecca)

     - apt-get install apache2-bin apache2-dev python-dev
     - pip install mod_wsgi

     start via

     - ~/.local/bin/mod_wsgi-express start-server wsgi.py

     (here, also --port 7777), as for the script wsgi.py see appendix [[wsgi.py][appendix]].
**** CHECK python web server nichol.as
     The naïve implementation based on Python's BaseHTTPServer did not
     perform flawlessly (see [[*Non-parallelized-based web server for cover traffic][Non-parallelized-based web server for
     cover traffic {0/1}]]), even for the queries of a single
     addon. This prompted the search for a python-based,
     adequately-performing technology stack.

     Luckily, an evaluation of Python web server performance had been
     performed by Nicholas Piël \cite{nicholas}. It shows the apache
     server with the mod_wsgi module as well-performing. As it was noted
     to be very easy to set up, it was chosen for this evaluation.
*** Evaluation
**** add-on
***** TODO differences to adaptive padding/wtfpad
- delay of some possible (f.ex. images)
- knowledge of packets
- end of transmission detectable
- different target distributions
- multiple distributions
- optionally no cooperator necessary
    dummy packets chosen as response to real request (as in web traffic)
- add evaluation values
- similarities: no delay
  - also has app_hint
- currently uses exit nodes
- this has no gap traffic, aims less at global adversary, more at ISP
****** TODO understand adaptive padding histogram
***** TODO differences to walkie-talkie
***** TODO differences to panchenkos
      - feature extraction via python class directly from pcap
        - packet data saveable to JSON
***** TODO why several covers
      - competition
      - when this started, walkie-talkie and juarez had not yet published
      - harder to break
        - more effort: one classifier for each cover scheme
** TODO Bloom Filters
*** TODO what is a bloom filter
    A Bloom Filter is a data structure to test membership in a set. It
    has a fixed size and a certain one-way error rate. If an item is in
    the set, the Bloom Filter is guaranteed to report this. If an item
    is not in the set, there is a certain probability, the /error rate/,
    of reporting that it belongs.

    This error rate is dependent on the size of the bloom filter and the
    number of inserted elements.
*** TODO bloom usage and implementation
    - bloom sort
      - error rate computation
    - size taken from example...
      - maybe change when altered
*** CHECK bloom-sort
    By ordering data into bins, it becomes possible to use bloom filters
    for the estimation of sizes, using one bloom filter for each bin.

    To achieve this, sensible separation criteria (called /splits/) for
    the bins need to be found. Afterwards, each bin needs to be assigned
    a value (called /size/) for all contained elements. See section
    [[*bloom-sort%20usage][bloom-sort usage]] on determining the sizes and splits.

    This data-structure, called /bloom-sort/ is initialized with an
    array of splits, and an array of sizes. The sizes-array needs to
    have one more element than the splits-array, as the bins are bounded
    on the left by 0, and on the right by infinity.

    #+BEGIN_SRC js
      /**
       ,* @param {sizes Array} array of values for each bin, must be sorted
       ,* @param {splits Array} array of bin borders, must be sorted
      ,*/
      function BloomSort(sizes, splits) {
          this.sizes = sizes;
          this.splits = splits;
          this.filters = [];
          for ( let i = 0; i < sizes.length; i++ ) {
              this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
          }
      }
    #+END_SRC

    Thus, you get

    -\infty \le size0 \le split0 \le size1 \le split1 \le ... \le split(n-1) \le sizen < \infty

    Given the splits, it becomes possible to add the elements to their
    bins:

    #+BEGIN_SRC js
      BloomSort.prototype.add = function(id, size) {
          this.filters[_.sortedIndex(this.splits, size)].add(id);
      };
    #+END_SRC

    where =_.sortedIndex()= gives the index at which =size= would be
    inserted into the sorted =this.splits= array.

    The retrieval of element sizes looks into each bloom filter,
    checking whether it might contain the element =id=. If one bloom
    filter reports containment, its corresponding element- =size= is
    returned. If several or no bloom filters report containment, an
    exception is thrown. The exception is used to allow all possible
    return values, not blocking one of them, say =-1=, for the error
    condition.
    #+BEGIN_SRC js
      /** determines size of element, raises exception if unclear */
      BloomSort.prototype.query = function(id) {
          let pos = -1;
          for ( let i = 0; i < this.filters.length; i++ ) {
              if ( this.filters[i].test(id) ) {
                  if ( pos === -1 ) {
                      pos = i;
                  } else {
                      throw {
                          name: 'BloomError',
                          message: 'Contains multiple entries'
                      };
                  }
              }
          }
          if ( pos === -1 ) {
              throw {
                  name: 'BloomError',
                  message: 'Contains no entries'
              };
          }
          return this.sizes[pos];
      };
    #+END_SRC

    It can be used by initializing with
    #+BEGIN_SRC js
    let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
    #+END_SRC

    then adding elements via =htmlSizes.add("http://google.com/", 613)=
    and querying via =htmlSizes.query("http://google.com/")=, which
    would yield =400=. (see usage in [[file:cover/js/size-cache.js::let%20numEmbeddeds%20%3D%20new%20BloomSort.BloomSort(NUM_EMBEDDED_SIZES,][size-cache]])
* MAYBE-then-WAIT torben
  Torben is a deanonymization attack based on injected website content
  in combination with Pattern Recognition. The authors show that when
  the user's browser sends requests of certain sizes for responses of
  certain sizes, this can be recognized in the encrypted TLS-Traffic
  from the Guard Node to the Onion Proxy.

  Each request/response pair corresponds to a certain amount of
  information (the authors show their approach with four request and
  response sizes, yielding a four-bit side-channel per request). This
  channel is used to encode a hash of the currently visited page.

  The requests are performed via XMLHttpRequest, but they authors also
  mention using HTTP redirects for the same effect.



  inject additional traffic into communication via JS XMLHttpRequest
  fixed request/response sizes of 2k, 4k, 6k, 8k bytes
  \to quad bits, concatenate, data transfer rate rate
  after 30 or 120 ms (tor latency bigger)
  detect via svm (how)
  setzt auf tcp an statt auf ip, (weil tor ja tcp ! yeah!)
** WAIT talk to daniel whether mention or not
* MAYBE why privacy
  - fundamental human need
  - concentration camp:
    "solitude in a Camp is more precious and rare than bread." -- primo levi
* TODO extract dom tags python
  - diveintopython
  - see code
* CHECK wsgi.py cover traffic server and generator :appendix:addon:move_later:
  With the technology stack to implement the cover traffic generator
  being settled, implementation becomes a single-page file, see
  [[wsgi.py]].

  One detail is that the length of the content gets inflated by the
  content-headers. To decrease this again, the length (which in turn
  depends on the required length) needs to be calculated and
  subtracted from the body-length. Some uncertainty arises because the
  =Proxy-Connection: keep-alive= header is added in some
  circumstances. The implementation errs on the side of returning too
  much data.

  Once the size is computed, a pseudo-random choice from the list of
  all printable characters is returned to the HTML query.

  To test this algorithm, the first 1000 sizes are retrieved via
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          curl -D /tmp/curlheaders/$i.head 127.0.0.1:8000/?size=$i > /tmp/curlheaders/$i.body; 
      done
    #+END_SRC
  which outputs the header and body of each query to the files,
  f.ex. =134.head= and =134.body=.

  This data is then evaluated by hand to check the sizes:
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          echo "$i: $(cat ${i}.* | wc -c)";
      done
    #+END_SRC
* TODO differences theoretical HTML-num embedded and observed
  - redirects
    - html had 176 elements, embedded only 100
    - the others were redirects (f.ex. from google.com to
      www.google.de)
    - these could be counted as having 0 embedded elements,
      - yet still a difference remains
  - it fits better if you enlarge the sizes by 0 for each redirected
    element (there are 176 elements in the html filter, including
    redirects, and only 99 in the embedded filter, if you pad the
    embedded filter by 0 for each of those, it is not a perfect fit,
    but better)
  - growth of websites
  - [[*Further%20work][Further work]]
* TODO truncated distributions
  - html: truncated lognormal instead of lognormal
    truncated at 0.999918739 quantile
  - embeddedSize: ebd
  - numEmbeddedObjects: truncated gamma instead of gamma
* TODO does this hide bursts?
  - meta-bursts as described in walkie-talkie
  - are those hidden, too, or can the number of bursts be found out
  - easy to implement, maybe do this
  - see that higher overhead per protection
** maybe see cumul-graphics
* TODO mention tor browser bundle version etc
* TODO why defense better
* TODO [#A] which sites well-protected, which less
* rest of bursts on addon site load finish                    :evaluation:mb:
  This should emulate normal browser traffic better than the proposed
  probabilistic schemes by Wang and Goldberg (normal and uniform
  distribution). It might be that Wang and Goldberg's deterministic
  padding to common values performs better, but that seems require a
  priori knowledge of website burst sizes.
* TODO machine learning
* TODO addon weaknesses/uncertainties
  - all HTTP gets treated the same
    - redirects
    - iframes
    - normal pages
  - request sizes not altered
    - can clearly see each cover request (as each should have size < 500)
  - sizes have grown since 2007
* TODO strong assumptions on feasibility
  - as critiqued in \cite{critique}
  - if protects against this, should also protect against worse
  - additional (?defense?) as proposed in critique
** TODO follow critique at all?
* TODO npm short installation/description               :appendix:move_later:
* WRITE how to set up wfpad                             :appendix:move_later:
  - helpful notes at scramblesuit
  - modify for wfpad
  - failed to work if called from Tor
  - thus 3/4 separate parts
    - tor server: listen on ORPort X
    - wfpad server script: send to X, listen on Y
    - wfpad client script: send to Y, listen on Z
    - in tbb/on 2nd tor (a.k.a. client): send traffic to bridge
      =Bridge 127.0.0.1 Z=
      - here: =Bridge 134.169.109.51:40300=
      - or in tbb without the Bridge: =134.169.109.51:40300=
  - modify capture: localhost
    - =-i lo=
    - =port Y=
  - alternative: client/server on separate hosts
    - because localhost did not yield much
      - because histogram based on arrival times, which are small on
        the same host (loopback interface)
    - ohne =-i=
    - =host server_host=
  - start capture
  - bug on multiple uses:
    #+BEGIN_EXAMPLE
    exceptions.IOError: [Errno 24] Too many open files: '/proc/23634/stat'
    #+END_EXAMPLE
    - try temporary fix: increase number of file descriptors, set
      #+BEGIN_SRC sh
        username        -    nofile  10000
      #+END_SRC
      in =/etc/security/limits.conf=. (Alter 10000 to something else
      if you retrieve more than 4000 at once. It seems to be about
      1 per domain-request).
    - bug report in appendix, needs some code to mitigate
  - without wfpad
    - at bridge
      tor -f torrc.no_wfpad.server (just orport)
    - at client
      configure tbb with
      #+BEGIN_EXAMPLE
        Bridge bridge.ip:40200
      #+END_EXAMPLE
** TODO scripts
** TODO below [[*host install list][host install list]]
* WRITE sampling web sites                                             :eval:
  - top 10 found wanting: different results from top-100
  - top-100 represented all as of ...
  - needed: representation with fewer classes
  - solutions: percentiles of both addon and non-addon sizes joined
  - link to code
  - solution: 30 sites
    - tables of sites with average non-addon size and average addon-size
      - write which addon version (top-100, so 0.18.2@bI?)
    - easier to distinguish than average, as intentionally very different sites
      - harder to conceal
* misc: tex bibliography
\bibliography{docs/master}
\bibliographystyle{plain}
* TODO host install list
  1. apt update
  2. apt upgrade
  3. apt install emacs tmux unison
  4. download tbb
     1. apt install firefox
     2. starting failed with error
     3. apt install xpra
        - installs needed x-libraries
        - and is faster
     4. test via (local)
        #+BEGIN_SRC sh
          xpra start ssh:mkreik@duckstein:37 --start=firefox
        #+END_SRC
        and kill by hand (on duckstein) via
        #+BEGIN_SRC sh
          xpra stop
        #+END_SRC
     5. download tbb, gpg check, cp to hosts, test
  5. apt install python-pip; pip install --upgrade pip
  6. pip install marionette_client
  7. apt install xvfb
  8. apt install tshark
     - add user to wireshark group in /etc/group
     - log out, log back in
  9. mkdir mnt/data, chown to current user/group
  10. mod_wsgi: see [[*Apache%20mod_wsgi][Apache mod_wsgi]]
  11. apt install tor
  12. apt install python-pyptlib python-crypto python-yaml
      python-psutil
  13. apt install unison
** TODO mention unison (in bib)
* WAIT Discussion
  intel model: interdependences (html bigger \to more embedded) not mentioned
* TODO Acknowledgements
  - Dr. med. Dr. phil. Eva
  - Daniel Arp
  - Prof. Dr. Konrad Rieck
  - Tao Wang
  - ...
  - Elena
* TODO Further work
** eval
  - bigger world sizes
  - open world
  - more elaborate tests with different world sizes / open world / etc
** addon
  - how to distinguish HTML/embedded
  - redirects + iframes included in model's number of embedded objects
  - source cover traffic: user gives domain as starting point
  - how to generate
    - how often, which parameters
    - just triggered by start and until end, or for each load
  - background if non-active (IPP self-similar)
    - 802.16 model
  - does a new connection to another site create a measurable tor-response
    (with variable-length packets)?
  - provable protection
  - size of bloom filter
  - number of bloom filters,
  - which and how many items to prepopulate
    - country-specific f.ex. google.com
    - leave out redirect from prepopulation
  - automatic update of bloom-filter
    - with currently visited sites
  - loading further items
  - The choice of cover traffic domains was explicitly taken out of
    the research focus. Currently, all cover traffic is dynamically
    generated by a web server written in Python.

    There exists basic code to use a list of webpages, given their
    sizes. It could be augmented by following links.
    - update from visited URLs
  - no morphing (delay, segmentation)
    - justify why good idea
  - bloomsort save/restore
  - number of embedded elements lacks <style> tags and some in <link>
    - does not honor reloads/cacheing
      - or does it? (maybe only called on cacheing)
    - but better than too many?
      - some approaches yes, binning no
  - elaborate on [[number_embedded]]
  - how to set splits and sizes
  - [[differences theoretical HTML-num embedded and observed]]
  - improve code to include css, (iframes?), js in number of embedded elements
  - web pages got bigger. See if \cite{newtrafficmodel}'s values are
    still accurate.
    - or only rely on quantiles of observed data
      - but these are hard to gather
	- use networkmanager code to do that
    - cite web-doom
  - User class: should aggregate smarter, not by-host, but by-page
    with every page-embedded element as just that.
    - indexed by host as workaround, can do better later
      - hard to find out which is HTML, which is non-HTML-traffic
      - so all is lumped together per domain
	- first request seen as HTML
	- other requests as non-HTML
    - == determine if HTML page by suffix (not clear as of ... and
      ... (link to SO))
  - bursts maybe less hidden (number of)
  - time not hidden (no delays of single files)
  - firefox e10n multiprocess
  - delay some requests (f.ex. images)
** TODO also helps against global observer if .onion generator is used
   - murdoch/danezis: correlation
   - this creates additional traffic which might hinder correlation attacks
   - further work
   - if cover traffic server is used by enough clients at once
   - or is unobservable (hidden service)
   - information-theoretical / stochastical analysis
   - quote perry critique
*** TODO first read murdoch/danezis paper
** onion host for cover traffic
   As indicated f.ex. by Wang and Goldberg,
   \cite{wpes13-fingerprinting}, network load already is a bottleneck
   on Tor, with the key bottleneck being exit nodes\cite{wtfpad}. The
   exit nodes might be spared the extra traffic by using =.onion=
   traffic generators (or, alternatively, hosts). A traffic generator
   could be further optimized by using tor proposals ... (see todo) to
   reduce latency, if this does not reduce privacy.
*** TODO tor proposals as of tor.sx
*** TODO read/skim and cite "on performance..."
** more thorough evaluation
   - only two panchenko approaches
   - assumption: can split traces
** TODO always also link in text
*** TODO check with darp
** TODO links to original, back to further work
** Exactly distinguishing HTML and embedded requests
   The current version of the [[user.js][User module]] separates
   CoverTraffic by DNS-domainname. As it often happens that one HTML
   page has embedded elements from different domains, this does not
   perfectly represent reality. It would be more exact to analyse the
   HTML page and at least return the domains of all embedded elements.
* TODO appendices [0/2]
\appendix
** Script =one-site.py=: capture pcap traces
   <<one-site.py>>
   #+INCLUDE: "./bin/one_site.py" src python
** Script =analyse.py=: classify the data
   :PROPERTIES:
   :CUSTOM_ID: analyse
   :END:
   #+INCLUDE: "./bin/analyse.py" src python
** Script =counter.py=: parse pcap files
   #+INCLUDE: "./bin/counter.py" src python
** Cover Traffic Server: =wsgi.py=
   <<wsgi.py>>
   #+INCLUDE: "./bin/wsgi.py" src python
** Script =htmlToNumEmbedded.py=: extract embedded objects
   <<htmlToNumEmbedded.py>>
   #+INCLUDE: "./bin/htmlToNumEmbedded.py" src python
** Script =html-top-100.sh= to retrieve html pages via squid
   #+INCLUDE: "./bin/html_top_100.sh" src sh
** Script retrieve-100-embedded.sh run htmlToNumEmbedded
   <<retrieve-100-embedded.sh>>
   #+INCLUDE: "./bin/retrieve_100_embedded.sh" src python
** modified top-100
   :PROPERTIES:
   :CUSTOM_ID: top-100
   :END:
   #+INCLUDE: "./sw/top-100-modified.csv" example
** TODO Remove same-host cover traffic server from traces: =7777.sh=
   <<7777>>
   #+INCLUDE: "./bin/7777.sh" src sh
** Addon
*** Control module User
    <<user.js>>
    #+INCLUDE: "./cover/js/user.js" src js
** WF-Trace Pictures
   :PROPERTIES:
   :CUSTOM_ID: wf-pictures
   :END:
   The pictures were created by the commands

   #+BEGIN_EXAMPLE
    for fb in $(ls | grep facebook); do
      python ~/da/bin/counter.py ./$fb  | tail -1 | sed 's/),/\n/g' | \
          tr -d "'()][" > /tmp/times;
      gnuplot -e "set terminal png size 1024,680; \
              set output \"/tmp/${fb}.png\"; plot '/tmp/times' with boxes;";
    done
   #+END_EXAMPLE

   and respectively for craigslist, in the directories containing the
   pcap files.

   These commands first extract each =Counter=s =timing= attribute
   (the last line of the output of counter.py at git commit 791af76),
   format it for gnuplot (inserting appropriate newlines via =sed= and
   removing extra characters via =tr=), and =gnuplot= s it to a png
   file with the name of the trace file as prefix.
** KEYWORDS Tor Browser despite --sync problems
   - broke with error (quote)
   - solution1: --sync
     - but verrry slow
   - solution 2: xpra
     - install via instructions at ...
       - use their repo
       - quote fp
   - use as
     #+BEGIN_SRC sh
       xpra start ssh:user@host:37 --start=path/to/tor-browser_en-US/Browser/start-tor-browser     
     #+END_SRC
* unused
** from [[*transform to panchenko-features]]
  The code to examine a single trace file is in =analyze_file()=
  It
  - opens the filename in tshark
  - splits the output by tokens
  - gives the relevant values (source IP, size, timestamp) (with the
    timestamp not used by Panchenko) to a =Counts=-object, which
    aggregates it

  [...]
  For a single line, a =Counter=-object aggregates bytes (incoming,
  outgoing), packets (incoming/outgoing), distills into a size/packets
  array and (size+timestamp)/packets array.
  [...]
  This is used in =postprocess()= to determine
  - size markers, (via the =_sum_stream()=-function),
  - the html marker as the first of those
  - the total transmitted bytes incoming and outgoing
  - number marker (via the =_sum_numbers()=-function)
    - slightly extended, as the number 16 was occuring
      everything above 14 was mapped to the same as 14
    - a bit unclear, currently, 3-5 \to 3, 6-8 \to 4, 9-13 \to 5, 14-\infty \to 6
  - occurring packet sizes incoming and outgoing (binned in steps of 2)
  - percentage of outgoing packets
  - number of packets incoming and outgoing.
** start browser with -marionette parameter
   Each modern Firefox, and thus also the tor-browser-bundle, has
   marionette-support built-in. It needs to be enabled on the
   command-line via the =-marionette= switch, for example


   This starts the Tor browser with marionette enabled.
*** marionette support page link
** Sally installation
   Sally is a tool to transfer text into points in a vector space.

   It is installed on Ubuntu Vivid Vervet by following the official
   instructions, then changing =vivid= in the file
   =/etc/apt/sources.list.d/mlsec-ubuntu-sally-vivid.list= to
   =devel=.
** from getting tbb to work
  One external repository is required, which can be installed via

  =add-apt-repository ppa:ubuntu-toolchain-r/test=
  =apt-get update=
  =apt-get dist-upgrade=

  Furthermore, the binary needs some firefox libraries, which can be
  retrieved most easily via =apt-get install firefox=.

  Afterwards, the binary can be started by typing =./firefox=.
** throttling
   As especially outgoing web requests are often quite small, and this
   paper has at the moment a 1:1 rate of outgoing vs incoming for the
   requests, throttling the amount of data leaving the end user might
   well suffice for reducing the bandwidth of the side-channel enough
   to make it insignificant.
** in-browser vs tcp-level ( ???) (generation?)
** how sally works
   - configuration file
     - input
     - features
     - output

** problematic websites
   The above setup worked on most websites.
   The websites sina.com.cn and xinhuanet.com both did not terminate loading.
   This might need further looking into.

   - do they load completely when not Tor, repeat necessary
   - is this by design?
*** exclude
    "scheint sonst zu klappen"
** Plugins: noscript and requestpolicy
   There exist two plugins, which should both allow mitigation of this
   attack. Used in parallel, they may hinder normal browsing somewhat
   (which is why they are not enabled/installed by default in the Tor
   Browser Bundle).

   The first is NoScript, which selects which Javascript sources to
   run and which to block. This is installed by default in the Tor
   Browser Bundle for the additional security benefits it brings (XSS
   defense etc), but not fully enabled. It is recommended by Edward
   Snowden and many others\cite{noscript}.

** what sets Tor apart / other anonymity networks
   There are other anonymity networks, such as JonDonym, I2P, MixNet
   and freedom.

   Tor is an anonymity service.
   - decentralized
   - biggest
   - high throughput
   - rather low latency, usable for web browsing
   - also hidden services

   Using a client called /Onion Proxy/ on the local computer, almost all
*** TODO ref onion routing
*** TODO onion routing
** TODO Non-parallelized-based web server for cover traffic [0/1]
   This approach did not scale to several parallel connections, so it
   was not used. It is included as a reference of what seems to work,
   but did not.

   The python module =TrafficHTTPServer= can be started on the
   command-line via

   python TrafficHTTPServer.py portname

   with portname set to 8000 by default. It generates cover traffic of the
   size given by the =size= parameter, for example the command

   wget 'http://localhost:8000/?size=10'

   retrieves a document with 10 bytes content from a TrafficHTTPServer
   running at localhost port 8000.
*** Problems
    - did not scale: did not respond immediately for parallel
      connections
      - obviously delays problematic as they in effect create less
        cover traffic
    - maybe further work: test that really works worse
*** Script
    #+INCLUDE: "./bin/unused/TrafficHTTPServer.py" src python
** CHECK Why could website fingerprinting be a problem
   As a typical scenario, consider the government of some state. A
   whistleblower posts something very critical of the regime on a
   well-known critical website. The whistleblower uses Tor or some
   other anonymity service to protect his identity. The government
   monitors and records all Tor connections. Even though Tor
   obfuscates the user's traffic, the specific data-pattern of the
   website allows the government to limit its search to, say three,
   subjects. This gives the whistleblower away.[fn:: Such has not been
   observed.]

** from [[#http][What happens during a (HTTP) website request]]
  Thus, the objects embedded within a page could allow a local
  passive observer to infer which web page from a set of pages the
  user requested.
** from [[*Hurdles%20to%20website%20fingerprinting][Hurdles to website fingerprinting]]
  Originally, a browser should open at most two connections per host
  \cite{rfc2616} to retrieve the files one-by-one. An update
  \cite{rfc7230} removed this fixed limit, but encouraged clients
  "to be conservative when opening multiple connections".
