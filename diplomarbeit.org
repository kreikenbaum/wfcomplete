#+TITLE: Selective Cover Traffic
#+PRIORITIES: A D B
#+TODO: KEYWORDS WRITE CHECK | DANIEL FINAL
#+TODO: RECHECK | DANIEL FINAL
#+TODO: | DANIEL_BUT_NEEDS_SUBPARTS
#+TODO: TODO | PENDING
\listoffigures
\listoftables
\printnoidxglossaries
* Configuration							    :ARCHIVE:
#+BIBLIOGRAPHY: master plain option:-d
#+LATEX_CLASS: scrreprt
#+LATEX_CLASS_OPTIONS: [a4paper,12pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{algorithm}
#+LATEX_HEADER: \usepackage[noend]{algpseudocode}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{numprint}
#+LATEX_HEADER: \usepackage{pgf}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usepackage{times}
# glossaries after hyperref
#+LATEX_HEADER: \usepackage{glossaries}
#+LATEX_HEADER: \makenoidxglossaries
#+LATEX_HEADER: \setacronymstyle{long-short}
#+LATEX_HEADER: \newacronym{ml}{ML}{machine learning}
#+LATEX_HEADER: \newacronym{wf}{WF}{website fingerprinting}
# end glossaries
#+LATEX_HEADER: \pagenumbering{roman}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \restylefloat{table}
#+LATEX_HEADER: \setlongtables
#+LATEX_HEADER: \npdecimalsign{.}
#+LATEX_HEADER: \nprounddigits{2}
#+LATEX_HEADER: \npthousandthpartsep{}
#+LATEX_HEADER: \makeindex
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure} \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.1 cm} % WAR: \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\textbf{Defending against Tor Website Fingerprinting with Selective Cover Traffic}} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.1cm} % WAR: \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \textbf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {1.9 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte % mkreik <2016-07-11 Mo>: war {5 cm}
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ %
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\%
#+LATEX_HEADER: 				 %
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:                                     %
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
#+LATEX_HEADER: \makeatletter
#+LATEX_HEADER: \def\BState{\State\hskip-\ALG@thistlm}
#+LATEX_HEADER: \makeatother
#+OPTIONS: H:6
* Exposé
#+BEGIN_EXPORT latex
\pagenumbering{arabic}
#+END_EXPORT
  #+INDEX: Tor
  #+INDEX: website fingerprinting
  #+INDEX: fingerprint
  Imagine you are talking in private to a good friend. You feel that
  you can talk about things you would avoid otherwise, as you are in a
  private space. How would you feel if the topics of conversation
  became known to others? Maybe it was due to your friend, maybe
  someone eavesdropped. Maybe you would stop trusting, either only the
  feeling of privacy, or also your good friend.


  Many parties like to observe users' online browsing: Shops and
  advertisers want to tailor advertisement to focus groups; people who
  share your WLAN might just be curious; Governments could aim for
  dragnet crime prevention, steering public discourse, and sometimes
  censorship; criminals want to create better scams.  Against all
  these, Tor\citep{tor-design} helps individuals protect their online
  privacy. They can browse censored information\citep{jardine2016tor},
  and publish e.g. leaks without fear of retaliation. Several
  constitutions implicitly \citep{katz} or explicitly
  \citep[Art.10]{grundgesetz} acknowledge that communication privacy
  is fundamental to an open society \citep[ch.10]{popper2012open}. Tor
  protects many and diverse parts of society, such as journalists,
  businessmen, and military units deployed abroad.


  Tor\cite{tor-design} works by creating an encrypted path through its
  network of servers. These servers are run by volunteers. They are
  located all over the world. A local software on a client's computer
  negotiates this path step-by-step: It uses encryption to ensure that
  none of the forwarding servers can know the full path of the
  message. A response from, say, a web server is sent back to the
  local software along the same path. This is how Tor makes it
  possible for two parties to communicate anonymously over the
  internet.


  People need to trust that Tor truly protects their online
  privacy. The more they doubt this, the less they would use it, or
  the more they would self-censor their online activities. The Tor
  project encourages research into its vulnerabilities. They believe
  that this makes the system stronger in the long run. The main group
  to discover vulnerabilities has been academia. Many of these
  vulnerabilities have been fixed very soon. One as-of-yet-unfixed
  vulnerability is \gls{wf}: the sizes and timing of each data packet
  for a web page retrieval are collected to form a /fingerprint/. This
  is matched to other observed retrievals to predict which page was
  visited. Originally, \Gls{wf} was briefly mentioned by
  \citet{SSL}. \citet{panchenko} were the first to successfully attack
  Tor by \gls{wf} under laboratory conditions.


  Various defenses have been proposed so far (\citet{hintz02},
  \citet{morphing09}, \citet{httpos2011}, \citet{oakland2012-peekaboo},
  \citet{effective2014}, \citet{a-systematic2014}, \citet{wtfpad2015}
  \citet{wang2015walkie},
  \citet{DBLP:journals/popets/CherubinHJ17}). None have been deployed
  with the Tor Browser. The reasons are manifold: Defenses delayed web
  browsing too much, required tuning by hand or changes to the Tor
  code base, complicated setup, were only presented conceptually,
  etc. Still, \gls{wf} attacks have continued to improve
  (\citet{quantifying}, \citet{ssl-traffic-analysis}, \citet{hintz02},
  \citet{ccsw09-fingerprinting}, \citet{panchenko},
  \citet{ccs2012-fingerprinting}, \citet{effective2014},
  \citet{panchenko2}, \citet{197185}).
** Contributions: a New Website Fingerprinting Defense and Tools
   The thesis tries to find if a HTTP-specific defense increases
   protection for the same overhead.


   A new defense should be easy to use, easy to deploy, and hinder
   users as little as possible. It should use resources responsibly,
   that is: it should try to get the best possible defense from a
   given resource level. Such a defense is attempted in this
   thesis. Ideally, it should also be easy to configure.


   The new defense tightly integrates into the Tor Browser; it tries
   to make each fetch of a webpage look like another random webpage;
   it also uses the Tor Browser's extension system for easy
   configurability, and deployability. The new defense sets sensible
   defaults, allowing users to adjust its resource use to account for
   a their personal security needs.


   The new defense needs to be compared to existing defenses: As a
   secondary benefit, this thesis reimplements and validates a
   state-of-the art attack by \citet{panchenko2}; it also provides a
   way to record web page browsing via Tor. The assorted data in trace
   format includes more than 150 captures. A capture took on average
   more than a day. More than 50 captures are without defense. This
   data takes more than 10GB of disk space, in a format similar to
   those of \cite{effective2014} and \cite{panchenko2}. The included
   code can convert from and to their formats.
** Structure: Foundations, Design, Evaluation
The organization of this thesis follows thesis best practices: [[#ch2-background][a
foundational chapter]] is followed by [[#ch3-newdefense][the problem statement and design]],
which is [[#ch4-evaluation][evaluated in the following chapter]]. The final chapter
contains [[#ch6-conclusion][a conclusion with future work]].


Chapter [[#ch2-background]] introduces \gls{wf}, Tor, and defenses against
\gls{wf} on a need-to-know basis. Section [[#sub2-tor]] gives a gentle
introduction to Tor, with an example how Tor achieves privacy. Section
[[#sub2-wf]] explains how \gls{wf} works in general and against Tor in
particular. A big part of \gls{wf} is \gls{ml}, which is also
explained here. How to defend against \gls{wf} is presented in section
[[#sub2-wf-def]].


This thesis' new \gls{wf} defense is introduced in chapter
[[#ch3-newdefense]]. It also presents the aspects the defense is based
upon: section [[#sub3-http]] justifies the chosen HTTP traffic model;
whereas the stochastic size cache based on
Bloom-filters\citep{Bloom70space/timetrade-offs} is described in
section [[#sub3-bloom]]. Finally, section [[#sub3-defense]] explains the
defense's itself.


Chapter [[#ch4-evaluation]] puts the defense to the test, analysing how it
compares to the most recent \gls{wf} defense by
\citet{DBLP:journals/popets/CherubinHJ17}. The analysis follows
\gls{wf} literature best practices: it first evaluates how a subset of
sites are distinguished one from the other in section [[#sub4-closed]]; it
then tries to distinguish these sites from a bigger set of background
sites in section [[#sub4-open]]. All results are summarized in section
[[#sub4-summary]].


Chapter [[#ch6-conclusion]] summarizes the findings and presents courses
of future work in section [[#future-work]].
* Need-To-Know: Tor, Website Fingerprinting and Defenses
  :PROPERTIES:
  :CUSTOM_ID: ch2-background
  :END:
** How Tor Works
   :PROPERTIES:
   :CUSTOM_ID: sub2-tor
   :END:
  In the wake of both the Snowden revelations in the western world,
  and increased internet censorship in countries such as Iran,
  Saudi-Arabia, and China\citep{china}, more and more Internet users
  search for ways to keep online communication and web browsing both
  private and free of censorship. The /Tor/ project\citep{tor-design}
  provides this. It protects whistleblowers, journalists, the people
  in oppressive regimes\citep{jardine2016tor}, even the military, and
  regular internet users, against e.g.\space{}nation-states or businesses
  which want to follow user's online steps. It routes encrypted data
  traffic via intermediaries, obscuring who connects to whom.



  Let us conceive of the internet as a series of tubes. Each internet
  message sent from Alice\citep{rivest1978method} to Bob passes many
  of their joinings. At each joint, there are many paths in and
  out. The message needs to find a way to Bob, so it contains Bob's
  address on the envelope. In case Bob wants to answer, the envelope
  also contains Alice's address.

  This is why the internet is not anonymous by design. To provide
  partial anonymity, a group of tube intersections can join, wrap each
  data packet in layers (of encryption), and bounce it along the group
  randomly, unwrapping a layer at each bounce. After several bounces,
  say to Carol, Dave, and Frank, the data packet is completely
  unwrapped again. Its destination is Bob, but Alice's name is blotted
  out. Frank sends the packet to Bob. To answer the packet, Bob sends
  the packet back to Frank, who sends it via Dave and Carol back to
  Alice. Because encryption, Alice knows the full path, but Carol only
  knows Alice and Dave, Dave only knows Carol and Frank, and Frank
  only knows Dave and Bob.\\


  #+CAPTION[Tor Network]: Tor Network. Source: \href{https://upload.wikimedia.org/wikipedia/commons/a/a1/How_Tor_Works_3.svg}{wikipedia}, modified
  #+NAME: fig:tor-network
  #+ATTR_LATEX: :float wrap
  [[./pictures/How_Tor_Works_3.pdf]]

  This closely models the Internet: Each Internet
  Protocol\citep{rfc791} packet lists the sender and destination. This
  makes it easy to identify communication partners. To achieve
  anonymity, the Tor software forms a path to the destination along
  multiple hops, establishing separate encryption with each hop. The
  hops are globally-distributed volunteer servers. Each intermediary
  hop only knows its predecessor and successor. Only Alice knows the
  full path.


  The local Tor software selects three globally-distributed hops to
  initialize a connection. It makes a connection to the first,
  establishes encryption, asks the first hop to make a connection to
  the second, sets up encryption to this, and from there to the
  third. The third hop establishes a connection to its destination.

  Each message is encrypted three times using same-length encryption
  and sent along this path. The first router decrypts the first layer,
  and so on, like layers of an onion. As a result of this setup, each
  hop can only see its direct neighbors along the path. Even if one
  hop of a three-hop setup is compromised, directly linking source and
  destination becomes pretty hard.
** Website Fingerprinting Attack
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf
   :END:
   Some groups dislike other people's privacy. It's too
   resource-intensive to protect against all of them. Tor, as any
   privacy system, has elected to protect against certain
   threats\cite{tor-design}. E.g., Tor does not protect against an
   adversary that can see all Tor network traffic; this level of
   observation would make correlation attacks
   (\citet{flow-correlation04}, \citet{ccs2013-usersrouted}) easy. Tor
   is designed to defend e.g., against a local passive adversary:
   someone who can see the traffic from a client to the network.


   # no re-flow in this paragraph (breaks link)
   Let us assume that Eve is a concerned mother who wants to find out
   whether Alice visits Bob's website. If Alice is just using the
   normal internet, Eve can see where Alice connects. Alice needs to
   use a middle-man. She
   \href{https://www.torproject.org/download/download-easy.html.en}{downloads   the Tor Browser Bundle}. This program routes all traffic via three
   Tor servers, say Carol, Dave, and Frank. These three are regularly
   replaced with other random nodes. Eve is frustrated: she can not
   see whom Alice connects to. Also, all traffic is encrypted, so she
   can only see that packets are sent, not their content.


   Eve is not to be thwarted: \Gls{wf} to the rescue! She uses Tor on
   her own computer to both connect to Bob's website, recording the
   network traffic of Bob's site, as well as other random web browsing
   traffic. (She could of course just block Tor, but then Alice might
   sneak off in the middle of the night to see Bob.) This traffic
   trains a \gls{ml} classifier. When Alice uses Tor, her traffic is
   input to this classifier, which can decide in almost real-time
   whether the site Alice visits is Bob's. If so, Eve can devise
   targeted ways to keep her daughter occupied with other things.


# ## code, last trace does not fit well
# wiki = scenario.list_all("17-12-26")[0].get_traces()["wikipedia.org"]
# mplot.traces([wiki[x-1] for x in [5, 26, 45, 35, 32, 24, 44, 1]]) #
    #+CAPTION[Trace Visualization Example]: Example of traces of wikipedia.org. Box width is the time to the next packet, box height the size of the packet (positive incoming, negative outgoing). The top traces seem similar to the naked eye (modulo time dilation). The bottom trace does not fit this pattern. The whole of Wikipedia's traces were recognized with 93% accuracy.
    #+NAME: fix:traces
    [[./pictures/example_traces_wiki.pdf]]

   In \gls{wf}, the time and size of users' data packets (called
   /traces/) are recorded. A visualization of this data is in Figure
   \ref{fig:traces}. There are some similarities in the packets. These
   can be expected, as every retrieval retrieves similar
   content. \Gls{wf} distinguishes between 30 and many more sites with
   high accuracy.(\citet{panchenko2}, \cite{effective2014}
   \cite{197185})
** Website Fingerprinting Defense
   \Gls{wf} effectively deanonymizes the traffic that users thought
   secure: it could for example expose a dissident to his nation
   state, nullifying this part of Tor's protection.



   To protect against \gls{wf}, most early attacks
   (\cite{Wagner96analysisof}, \cite{hintz02},
   \cite{ssl-traffic-analysis}) also proposed defenses. These evolved
   from /specific defenses/, e.g. packet-size-altering methods
   (\cite{httpos2011}, \cite{morphing09}) to /general defenses/, which
   transform groups of web retrievals so that all members look the
   same.

   At the start of the thesis, there existed mostly deterministic
   general defenses with high overhead (e.g. over 220% bandwidth, and
   300% time\cite{a-systematic2014}). During the course of this thesis, the
   stochastic defenses of \cite{wang2015walkie} and \cite{wtfpad2015}
   much lower overhead, were developed. This validates a stochastic
   approach, yet improvements seem possible in two areas: (1) ease of
   installation, and configurability, and (2) more-closely fitting
   cover traffic generation.

   Ad 1: \cite{wang2015walkie} alters the Firefox binary, while the
   current version of \cite{wtfpad2015} needed much manual adjustment in
   our attempts.

   Ad 2: \cite{wang2015walkie} uses either a normal, or lognormal
   distribution, not adjusting to HTTP-specifics, while \cite{wtfpad2015}
   samples packets at the network layer. It aims at generally hiding
   /that traffic occurs/, not just which website is visited, as it
   derives its basic mechanism from \cite{ShWa-Timing06}.
*** Thesis Contribution
    This thesis presents and tests a new defense against \gls{wf}. This
    new defense mimics HTTP\cite{rfc7230}-shaped cover traffic: Each
    web page retrieval is augmented by stochastically-drawn dummy HTTP
    traffic\cite{newtrafficmodel}. This could optimize the protection
    offered for given bandwidth overhead. It is implemented in a
    browser extension, which makes the defense easier to install,
    configure, and maintain.
*** Thesis Structure
    The following chapters try to solve the question whether the new
    defense works more effectively than existing ones.

    Chapter [[#ch2-background]] provides basic background for the IT-savvy
    who have not yet encountered Tor, machine learning, or website
    fingerprinting. For the Tor network, we treat its basic structure
    and why \gls{wf} might be a credible threat. \Gls{ml} basic steps
    and algorithms are briefly skimmed. Finally, \gls{wf} on Tor is
    presented. These parts can safely be skipped given previous
    knowledge.

    The defense's why and how (motivation and design) is described in
    chapter [[#ch3-newdefense]]. This also describes the bloom sort data
    structure for stochastically saving object sizes.

    Chapter [[#ch4-evaluation]] evaluates the defense. It first describes
    the data-gathering process. Next, the website fingerprinting
    attacks of \cite{panchenko2}, and \cite{ccsw09-fingerprinting} are
    validated on defenseless data. This is followed by the evaluation
    on data with cloaking.

    Chapter [[#ch6-conclusion]] summarizes the results, shows a path to
    implementation, with both included and additional further work.
* DANIEL Background [9/9]
  Knowing [[#sub2-tor][the Tor network]], [[#sub2-ml][\gls{ml} basics]] and [[#sub2-wf][previous
  attacks and defenses]] helps to understand and then counter website
  fingerprinting.
** DANIEL Tor Website Fingerprinting
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf
   :END:
   #+INDEX: traffic analysis
   #+INDEX: website fingerprinting
   #+INDEX: WF
   What does an adversary do if he cannot decrypt and the message
   traffic of a cryptographic system he is interested in? One
   alternative is to inspect the traffic itself for patterns. This
   process is called /traffic analysis/\cite{introta} and yields much
   useful information\cite[ch.10.3]{applied96}.

   \gls{wf} needs only message meta-data: who sends how much data
   when. It assumes that the system itself is computationally
   secure\cite[ch.1.1]{applied96}: there are not enough resources,
   time, or data to break it. Analysing traffic patterns can
   circumvent the system. Anyone who can see the data stream can carry
   out this attack, without anyone else learning about this. They
   simply need to capture the data stream using e.g. the
   =tcpdump=\cite{tcpdump8-manual} tool.\\

   From inception\cite{tor-design}, Tor provided defenses against
   traffic analysis. For one, all /data/ cells have the same size,
   which protects against identifying them by size only. Tor also
   multiplexes all its data traffic into a single stream, making it
   hard to distinguish the multiple streams that most websites
   require, let alone parallel retrieval. Tor also
   unavoidably\cite{rfc1925} increases traffic latency, so that
   attacks have a harder time relying on interpacket
   timing\cite{challenges}.\\


   This made \gls{wf} harder, to the point that it was was mentioned, but
   not hindered, in \cite{tor-design}. It took five years for
   \cite{ccsw09-fingerprinting} to show better than random
   classification results against Tor traffic. This evolved to
   state-of-the-art methods like \cite{panchenko2}.

   What all methods have in common is that they extract numerical
   /features/ from the raw data, which is then classified using
   \gls{ml}.
** DANIEL Machine Learning
   :PROPERTIES:
   :CUSTOM_ID: sub2-ml
   :END:
   #+INDEX: machine learning
   #+INDEX: ML
   Let us review basics of \gls{ml}: a
   computer\cite{turing1936a} algorithm extracts and generalizes
   patterns from learning data.\cite[ch.1.2]{rieckdiss} This is then used to
   classify further patterns (e.g. for handwriting recognition),
   or to act on the generalizations (say, for self-driving cars).

   The \gls{ml} process consists of at least two separate
   steps: feature extraction and classification. Domain-specific
   [[#ml-features][feature extraction]] transforms the raw input data --- in our case,
   website traces --- into /features/ --- in our case, numbers,
   e.g. the number of outgoing packets. [[#ml-class][Classification]] then
   generalizes and assigns these characteristics into categories.

   A last section presents [[#ml-measure][measures to evaluate \gls{ml}
   performance]].
*** DANIEL Feature Extraction
    :PROPERTIES:
    :CUSTOM_ID: ml-features
    :END:
    #+INDEX: feature extraction
    #+INDEX: machine learning!feature extraction
    \gls{wf} tries to analyse web traces. To be able to defend against
    \gls{wf}, it should first be understood. To be understood, it
    should be reproduced.

    \gls{wf} input data needs to be wrangled for the classification to work:
    extra information that might change from request to request ---
    such as the hosts IP address, or the absolute time of the
    retrieval --- needs to be removed or unified to a common
    format. The trick is as always: keeping the signals and discarding
    the noise, (also called "reducing intra-class variability while
    increasing inter-class variability").

    The source data in website fingerprinting are traces (e.g. in the
    =pcap=\cite{pcap-manual} format). From this, only the size,
    direction and timing of each packet is extracted. The size of
    files is hidden by the traffic's encryption; the closest
    approximation is the size of each TLS\cite{rfc5246} record.

    Feature extraction\cite[sec.1.3.1]{duda} transforms (preprocessed)
    input data into features/characteristics suitable for
    classification.


    \cite{ccsw09-fingerprinting} follows \cite{hintz02}, and uses
    packet sizes for features. They use a jaccard metric as
    classifier, but as seen in chapter [[#ch4-evaluation]], nothing but
    sizes can yield surprisingly good results in combination with
    support vector machines, despite Tor's fixed data cell size.\\

    #+CAPTION[CUMUL features example]: CUMUL\cite{panchenko2} \href{https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}{features example}
    #+ATTR_LATEX: :float wrap :width 0.5\textwidth :placement {r}{0.55\textwidth}
    #+NAME: CUMUL_traces
    [[./pictures/cumul_aus_paper.pdf]]

    \cite{panchenko2} uses a cumulative size metric, CUMUL. As seen in
    Figure [[CUMUL_traces]], these provide a graphical representation of
    traces, while still allowing for computer-based comparison after
    normalization. This is also evaluated with support vector
    machines.\\

    There are numerous \gls{wf} attacks. \cite{effective2014} achieved
    accuracy improvements using over 3000 features and a
    KNN-classification with weighted metrics while
    \cite{kfingerprint}'s approach uses approximately 150 features
    with Random Forest\cite{DBLP:journals/ml/Breiman01} classification.\\

    The boundary of feature extraction to classification is "somewhat
    arbitrary"\cite[sec.1.3.1]{duda}: in the figure above e.g., a
    feature could be computed that distinguishes whether the last
    feature's value is above 100 kByte. This would effectively
    classify the data. In general, the /decision boundary/ between
    classes is more complicated and more elaborate classification
    techniques need to be used.
*** DANIEL Classification
    :PROPERTIES:
    :CUSTOM_ID: ml-class
    :END:
    #+INDEX: classification
    #+INDEX: training
    #+INDEX: classification!training
    #+INDEX: classification!test
    #+INDEX: machine learning!classification
    The feature extraction step transformed raw input data to a
    (numerical) feature vector. In classification, some of these
    vectors are for training the classifier. Others for testing.

    In classifier /training/, the classifier gets as input several
    feature vectors and their respective classes and tries to
    generalize a relationship.

    In actual /classification/, the classifier only receives an input
    feature vector, and needs to predict the class label: the web
    page.\\


    Most classifiers, such as [[*Support Vector Machines][support vector machines]], form an
    internal model from which further input data is
    classified. Others, notably [[*K-Nearest-Neighbor-Classifier][k-Nearest-Neighbors]], classify directly
    without an intermediary model.
*** DANIEL Measuring Performance
    :PROPERTIES:
    :CUSTOM_ID: ml-measure
    :END:
    #+INDEX: Accuracy (acc)
    #+INDEX: Area Under Curve
    #+INDEX: AUC
    #+INDEX: AUC$_{0.01}$
    #+INDEX: AUC!bounded
    #+INDEX: confusion matrix
    #+INDEX: False Positive Rate
    #+INDEX: fpr
    #+INDEX: Receiver Operating Characteristic curve
    #+INDEX: ROC curve
    #+INDEX: True Positive Rate
    #+INDEX: tpr
    To find out if \gls{wf} attacks work, and if defenses prevent this,
    their success needs to be measured.

    A /confusion matrix/\cite{powers} helps to illustrate the
    different cases that can occur in \gls{wf}. Each trace is categorized by
    whether it /is/ a sensitive website, and whether it is
    /classified/ as such. See Table [[tab:confusion_matrix]].

    #+CAPTION: Confusion matrix. Correctly classified traces are in bold.
    #+NAME: tab:confusion_matrix
    #+ATTR_LATEX: :align |l||l | l|
    |----------------------+-----------------------+-----------------------|
    | <20>                 |                       |                       |
    |                      | real wikileaks.org    | real facebook.com     |
    |----------------------+-----------------------+-----------------------|
    | predicted as wikileaks.org | *True Positives (TP)* | False Positives (FP)  |
    | predicted as facebook.com | False Negatives (FN)  | *True Negatives (TN)* |
    |----------------------+-----------------------+-----------------------|

    From these counts, other metrics can be derived. The main metrics
    used in \gls{wf} literature are /Accuracy/ (acc), and /True-/ and
    /False-Positive-Rate/ (tpr and fpr). These are defined as

    #+ATTR_LATEX: :align r c l
    | True Positive Rate  | := | $TP / (TP + FN)$                  |
    | False Positive Rate | := | $FP / (FP + TN)$                  |
    | Accuracy            | := | $(TP + TN) / (TP + FP + FN + TN)$ |

    To show the classifier strictness tradeoff, a /Receiver
    Operating Characteristic Curve/ (ROC-Curve) can be used.
    This diagram contrasts classifier tpr vs fpr, see Figure
    [[fig:roc-example]]. The /area under/ the /curve/ (AUC) can
    be measured. The closer this value is to 1, the better. If one
    is mainly interested in low fpr, the leftmost section of the
    ROC-curve is of particular interest. The area under the curve
    bounded up to a fpr value of 0.01 is denoted AUC_{0.01}.

    #+CAPTION[ROC curve example]: Example Receiver Operating Characteristic (ROC) curve \cite[sec.11.18.8]{scikit-user-guide}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:roc-example
    [[./pictures/plot_roc.png]]
** PENDING Machine Learning Algorithms
*** Support Vector Machines
    #+INDEX: classifier!Support Vector Machine
    #+INDEX: classifier!SVM
    #+INDEX: Support Vector Machine
    #+INDEX: SVM
    #+INDEX: linear classifier
    #+INDEX: binary classification
    #+INDEX: classification!binary
    /Support Vector Machines/ (SVMs) are a linear classifier:
    they find a linear boundary between points, see Figure
    [[fig:linear_boundary]] for a simple example.

    #+CAPTION[Example binary linear classification]: Example binary linear classification from \cite[Figure 1.5]{iml}.
    #+ATTR_LATEX: :width 0.4\textwidth
    #+NAME: fig:linear_boundary
    [[./pictures/iml_fig1.5.png]]

    [fn::This and the following parts are mostly based on
    \cite[ch.6f]{iml}] Given a set $X = \{x_1, ..., x_n\}$ with a dot
    product $\langle\cdot, \cdot\rangle: X \times X \to \mathbb R$ and tuples $(x_1, y_1), ...,
    (x_m, y_m)$, with $x_i \in X, y_i \in \{-1, 1\}$ as a /binary
    classification/ task.

    The SVM's job is to find a hyperplane[fn::as \cite[ch.4.1]{esl}
    mention, this is actually an affine set, as it need not pass
    through the origin. This thesis keeps the tradition (as long as
    those things formed by quarks are still called atoms ...).]
    #+BEGIN_EXPORT latex
      \[\{x \in X \mid \langle w, x \rangle +b = 0\}\]
    #+END_EXPORT
    such that $\langle w, x_i \rangle +b \ge 0$ whenever $y_i = 1$, and $\langle w, x_i \rangle
    +b < 0$ whenever $y_i = -1$. With added normalization, this can
    be compressed to the form \[y_i \cdot (\langle w, xi \rangle +b) \ge 1.\]
**** Soft Margin Classifiers
     :PROPERTIES:
     :CUSTOM_ID: soft-margin-svm
     :END:
     #+INDEX: margin
     #+INDEX: SVM!margin
     #+INDEX: soft-margin
     #+INDEX: SVM!soft-margin
     #+INDEX: classifier!soft-margin
     #+INDEX: C
     #+INDEX: SVM!C
     A support vector machine tries to find a hyperplane between two
     groups of points and maximize its distance to the closest points,
     called /margin/. What happens if the points lie such that a line
     cannot be found, as e.g. in Figure [[fig:non-linear-data]]?

     #+CAPTION[Example simple non-linearly separable data]: Non-linearly separable data; source: \url{https://en.wikipedia.org/wiki/File:Separability_NO.svg}
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:non-linear-data
     [[./pictures/Separability_NO.eps]]

     To solve this, a /soft-margin classifier/ introduces slack
     variables $\xi \ge 0$, which it tries to reduce while maximizing the
     margin.

     This alters the equations to $y_i( \langle w, xi \rangle +b) \ge 1 - \xi_i$ for the
     optimization problem

     \[\min_{w, b, \xi} \frac{1}{2} ||w||^2 + \frac{C}{m} \sum_{i=1..m} \xi_i\]

     The /error term/ $C$ weighs minimizing training errors against
     maximizing the margin\cite[sec.7.2.1]{iml}.
**** Multi-Class Strategies
     :PROPERTIES:
     :CUSTOM_ID: multi-class
     :END:
     #+INDEX: binary classification
     #+INDEX: multi-class classification
     #+INDEX: classification!binary
     #+INDEX: classification!multi-class
     The SVMs as described above solve the binary classification
     problem \cite[sec.1.1.3]{iml}: they propose a boundary between
     two classes of objects.

     In website fingerprinting[fn::as in e.g.\space{}handwriting
     recognition], there are most often more classes than two.

     Several strategies exist to distinguish more than two
     classes. The main are to train one classifier for each class ---
     called /One-Vs-Rest/ (OVR) --- and one for each class-class
     combination --- called /One-Vs-One/ (OVO). One-Vs-Rest trains
     fewer classifiers, while One-Vs-One trains more, but evaluates
     fewer samples per fitting.\cite[sec.4.12.3]{scikit-user-guide}.
**** Kernel Trick
     #+INDEX: kernel
     #+INDEX: kernel!radial basis function
     #+INDEX: kernel!RBF
     #+INDEX: radial basis function kernel
     #+INDEX: RBF kernel
     Straight lines do not always distinguish classes correctly, as
     e.g. example in Figure [[hastie_kerneltrick]]. This would seem a
     drawback to using Support Vector Machines, yet they can compute
     these not only on the original data, but also on a projected
     space. This allows for complex decision boundaries. By using the
     kernel trick\cite[sec.2.2.2]{kernels}[fn::A kernel is a function
     with specific properties. The dot product is such a kernel. The
     kernel trick enables a algorithm with a kernel to use any other
     kernel], a SVM can not only use the dot product $\langle.,.\rangle$, but
     another kernel $k(., .)$ instead.

     #+CAPTION: Kernel trick application example from Hastie et al.\cite[ch.4.1]{esl}. The left side shows linear boundaries on $X$ and $Y$ --- the right side linear boundaries computed with added $X^2$, $Y^2$ and $XY$.
     #+NAME: hastie_kerneltrick
     [[./pictures/hastie.png]]

     The kernel used by default by \cite{Hsu10apractical} for SVMs is
     the (gaussian) /radial basis function/ (RBF)
     kernel\cite[sec.2.3]{kernels} \[k(x, y) = \exp \left ( - { \|x -
     y\|^2 \over 2 \gamma^2 } \right )\] This is also used by
     \cite{panchenko2}. While the algorithms still finds a straight
     line in a projected space, the resulting decision boundaries in
     the original feature space are more varied.
**** Parameter Estimation
     #+INDEX: cross-validation
     #+INDEX: grid search
     #+INDEX: $\gamma$ (gamma)
     #+INDEX: gamma
     Each [[#soft-margin-svm][soft margin classifier has an error term $C$]] which states
     how much to penalize outliers. The gaussian radial basis
     function kernel used by \cite{panchenko2} also has a $\gamma$ (gamma)
     term which varies the width of the area, see Figure
     [[fig:C-gamma-effect]].

     #+CAPTION[Example svm-rbf classification with different parameters for $C$ and \gamma]: Example svm-rbf classification with different parameters for $C$ and \gamma. Source \cite[Figure 42.328]{scikit-user-guide}, recreated for higher resolution.
     #+ATTR_LATEX: :width 0.4\textwidth
     #+NAME: fig:C-gamma-effect
     [[./pictures/skl-fig-42.328.png]]
#     [[./pictures/skl-fig-42.328.eps]]

     These parameters have to be provided externally for the Support
     Vector Machine to achieve high
     accuracy. \cite[sec.3.2]{Hsu10apractical} recommend grid-search
     with cross-validation to find optimal parameters.

     In /grid-search/, several parameters of $C$ and $\gamma$ are
     evaluated, and the best one, depending on the metric, is chosen.

     In /cross-validation/, the data set is split into $k$ disjoint
     subsets, called /folds/, of equal size. Of those, $k-1$ are used
     combinedly for training the classifier, while the last is used
     for prediction evaluation. This is done $k$ times, and averaged
     for the result.

     It might be possible to evaluate these meta-parameters together
     with the main classification problem \cite[secs.2.8.3, 6.7]{esl},
     but analogously to \cite[sec.2.8.3]{esl}, this would probably
     become "combinatorially hard".

*** K-Nearest-Neighbor-Classifier
    #+INDEX: classifier!kNN
    #+INDEX: classifier!k-nearest-neighbors
    #+INDEX: kNN
    #+INDEX: k-nearest-neighbors
    The /k-nearest-neighbors/ (kNN) classifier
    (\cite[sec.1.3.2]{iml} \cite[sec.13.3]{esl}
    \cite[sec.8.2]{mitchell}) classifies data points based on the
    known class[es] of their neighbors: for each item to be
    classified, determine the (e.g.\space{}k=5) closest neighbors by a
    given metric. If all neighbors' classes agree, or based on a
    majority decision, the item's class is set to theirs. See Figure
    [[fig:knn-example]].

    It is successful "in a large number of classification and
    regression problems"\cite[sec.4.6]{scikit-user-guide}, despite its
    simplicity.

    This classifier works best if all classes have the same number of
    (training) instances. Otherwise, it is of course probable that the
    classes with the higher number of instances will be chosen as
    targets of classification more often.

     #+CAPTION[k-nearest-neighbors illustrated]: The left picture shows the five closest neighbors around the test instance $x_q$, which is then classified as =-=. The right shows the k==1-decision boundary around several training instances (the area where a test instance would be classified as the point). Source \cite[Figure 8.1]{mitchell}
     #+NAME: fig:knn-example
     #+ATTR_LATEX: :width 0.7\textwidth
     [[./pictures/mitchell-fig8.1.png]]
** DANIEL Website Fingerprinting Defenses
   :PROPERTIES:
   :CUSTOM_ID: sub2-wf-def
   :END:
   This section describes defenses against \gls{wf} as described
   [[#sub2-wf][previously]]. As most \gls{ml}, \gls{wf} uses statistical
   properties of the underlying data. It could possibly be defeated by
   shuffling these properties. The total number of incoming packets
   e.g. is a feature used by almost all modern attacks:

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION: distribution of number of total incoming packets
   #+NAME: fig:total_packets_in
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-06_tamaraw_wtf-pad___bridge--2016-07-05__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   As seen in Figure [[fig:total_packets_in]], \cite{wtfpad2015}'s publicly
   available implementation of his own WTF-PAD and
   Tamaraw\cite{a-systematic2014} both create additional packets, but
   preserve site separation and ordering. Contrast this with this
   thesis' defense in Figure [[fig:total_packets_in_thesis]]

   # created using mplot.py:total_packets_in, see name for scenarios+sites
   #+CAPTION[distribution of number of total incoming packets, thesis defense]: Distribution of number of total incoming packets, thesis defense. The 5aI setting has small overhead, 30aI has average overhead.
   #+NAME: fig:total_packets_in_thesis
   #+ATTR_LATEX: :float nil
   [[./pictures/total_packets_in_disabled___bridge--2016-07-21_0.22___5aI--2016-07-19_0.22___30aI--2016-07-25__msn.com_buzzfeed.com_youtube.com_xvideos.com__palette_colorblind.pdf]]

   Each site's variances have been increased with the result of them
   overlapping. This figure hints that this thesis' defense more
   effectively munges websites fingerprinting traces, and is also
   tunable.

   While it was possible to get \cite{wtfpad2015} to run given [[https://bitbucket.org/mjuarezm/obfsproxy_wfpadtools][the
   provided software]], \cite{wang2015walkie}'s state-of-the-art defense
   was compared using his reported values. As \cite{wang2015walkie}
   uses simple statistical distributions in addition to a novel
   single-duplex method, it could arguably be augmented by
   HTTP-specific cover traffic distributions.

   Another point is ease-of-installation: Firefox, on which the Tor
   Browser Bundle is based, enables extensions. These already provide
   some of the Tor Browser Bundle's
   functionality\cite{tor-browser-design-impl} and were used in this
   thesis' to ease installation as compared to \cite{wtfpad2015} and
   arguably also to \cite{wang2015walkie}.\\


   In 2017, the novel LLaMA defense was introduced in
   \cite{DBLP:journals/popets/CherubinHJ17}. The paper acknowledges
   the need for a client-side application-level defense. In contrast
   to this thesis's defense, LLaMA uses a uniformly distributed delay
   until the cover request, whose URL is from the list of previously
   requested URLs at the same host.

   This mimicks another request to the same page, thus has
   approximately the correct size distributions, but does not cloak
   the high-information initial HTTP request. Also, it is highly
   deterministic for the first elements of cover traffic, until the
   list of requested elements from this host is big enough to allow
   for some randomness. The authors provided LLaMA as a secondary
   defense to the server-side ALPaCA defense, and emphasize its draft
   status\cite{LLaMA}.
** DANIEL Summary
   \gls{wf} can deanonymize anonymous traffic. This can pose a huge problem
   e.g. for whistleblowers. The previous sections gave a short
   introduction to the basics of Tor, \gls{wf} attacks, its basis in machine
   learning, and finally defenses against it. It also gave a first
   glimpse at this thesis' new defense.

   The next section presents the novel defense in depth.
* DANIEL_BUT_NEEDS_SUBPARTS Novel Defense [2/2]
  :PROPERTIES:
  :CUSTOM_ID: ch3-newdefense
  :END:
  The last chapter showed that \gls{wf} could be a serious attack on online
  anonymity and privacy. As specific defenses could be circumvented, a
  general defense against \gls{wf} is needed. It should provide adequate
  protection for an acceptable increase in bandwidth and delay.\\


  This thesis' defense aims to tailor cover traffic to mimic web
  traffic, while coincidentally improving installation and
  configurability.


  Closely tailoring traffic has two main parts: It uses the http
  traffic model\cite{newtrafficmodel} to morph the expected real
  traffic to another size; and it caches certain page characteristics
  in order to control the amount of cover traffic.
** Simulating HTTP Traffic
   :PROPERTIES:
   :CUSTOM_ID: sub3-http
   :END:
** Traffic Enhancement
   :PROPERTIES:
   :CUSTOM_ID: sub3-defense
   :END:
   The created traffic is based on the HTTP
   model\cite{newtrafficmodel}: for each web page retrieval, the
   defense sets target retrieval sizes. From these, the retrieval's
   actual parameters are subtracted to set the amount of cover traffic
   for this web page.

   Each request is potentially covered by a simultaneous
   cover-request, as seen in Algorithm \ref{algo1}. The algorithm
   randomly draws from appropriate HTTP-related
   distributions.\cite{newtrafficmodel}

 #+BEGIN_EXPORT latex
 \begin{algorithm}
 \caption{Generate Cover Traffic}\label{covertraffic}
 \label{algo1}
 \begin{algorithmic}[1]
 \Procedure{OnHttpRequest}{$\textit{url}$}
 \If {$! \textit{isRegistered}(\textit{hostnameOf}(\textit{url}))$}
   \State $\textit{targetHttpSize} \leftarrow \textit{randomHttpSize}()$
   \State $\textit{targetNumEmbedded} \leftarrow \textit{randomNumEmbedded}()$
   \State $\textit{urlHttpSize} \leftarrow \textit{lookupOrGuessHttpSize}(\textit{url})$
   \State $\textit{urlNumEmbedded} \leftarrow \textit{lookupOrGuessNumEmbedded}(\textit{url})$
   \State $\textit{coverHttpSize} \leftarrow \textit{targetHttpSize} - \textit{urlHttpSize}$
   \State $\textit{coverNumEmbedded} \leftarrow \textit{targetNumEmbedded} - \textit{urlNumEmbedded}$
   \State $\textit{requestCoverSized}(\textit{coverHttpSize})$
   \State $\textit{registerHost}(\textit{hostnameOf}(\textit{url}), \textit{coverNumEmbedded}, \textit{urlNumEmbedded})$
 \Else
   \State $\textit{requestProbability} \leftarrow \textit{computeProbability}(\textit{hostnameOf}(\textit{url}))$
   \While {$\textit{requestProbability} > 1$}
     \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
     \State $\textit{updateHosts}(\textit{hostnameOf}(\textit{url}))$
     \State $\textit{requestProbability} \leftarrow \textit{requestProbability} -1$
   \EndWhile
   \If {$\textit{withProbability}(\textit{requestProbability})$}
     \State $\textit{requestCoverSized}(\textit{randomEmbeddedSize}())$
     \State $\textit{updateHosts}(\textit{hostnameOf}(\textit{url}))$
   \EndIf
 \EndIf
 \EndProcedure
 \end{algorithmic}
 \end{algorithm}
 #+END_EXPORT

 /registerHost/, /isRegistered/, /computeProbability/ and /updateHosts/
 access a data structure that saves the recently active hosts with
 their number of embedded objects and the cover requests that still
 need to be sent.

 The /lookupOrGuess.../-functions need data structures to map urls to
 both HTTP sizes and number of embedded objects. These change over
 time, just compare the topmost plots each of Figures
 [[fig:total_packets_in]] and [[fig:total_packets_in_thesis]]. As can be seen
 in the next chapter, the amazingly good defense that was achieved
 with up-to-date cached sizes (see next chapter) decreased over time
 to that of guessing sizes. These data structures use
 Bloom-filter\cite{Bloom70space/timetrade-offs} based binning to save
 values related to urls in a fixed space, while not allowing an
 adversary to exactly determine which urls are saved.


 Ease-of-installation and configuring the amount of cover traffic is
 provided by using Firefox's Addon-SDK. Add-ons can be downloaded via
 [[https://addons.mozilla.org][Mozilla's Addon Page]] with automatic updates.
** DANIEL Caching via Bloom-Filters
   :PROPERTIES:
   :CUSTOM_ID: sub3-bloom
   :END:
   The above defense might further improve if the sizes of the webpage
   to be loaded are known beforehand: Cover traffic could be tailored
   more exactly, increasing obfuscation and/or reducing
   overhead. Knowing the exact retrieval pattern in advance even
   enables new defenses\cite{effective2014}.


   The problem in caching is that
   a. page properties change over time, making this cache increasingly
      less accurate,
   b. caching visited page sizes might yield an exact log of the
      visited web pages to an attacker who gains control over the
      defended computer, and
   c. this cache could take a lot of space, depending on the subset of
      pages's sizes cached.


   A data structure that stochastically saves approximate sizes might
   solve problems /2/ and (partially) /3/: Bloom
   Filters\cite{Bloom70space/timetrade-offs} have a small error rate
   in exchange for a fixed size. Their otherwise disadvantageous error
   probability is an advantage in this situation, as it further
   confounds possible attackers.


   Dynamically updating the filter might solve problem /1/ and further
   help with problem /3/ (future work).
* WRITE Evaluation [1/10]
  :PROPERTIES:
  :CUSTOM_ID: ch4-evaluation
  :END:
  [[file:~/da/da.org::*Evaluation][gtd]]
  - overhead section
  - wording

  This chapter compares the new defense to existing defenses.  The
  first section illustrates the setup. The second section [[#sub4-tools][validates
  the \gls{wf} attacks used]]. The next section [[#sub4-closed][compares defenses in a
  closed-world setting]], where they need to only distinguish a small
  fixed number of pages. The penultimate section evaluates them [[#sub4-open][in an
  open-world scenario]], which has many background-page retrievals added
  to the mix. Its last section [[#sub4-summary][summarizes evaluation results]].
** KEYWORDS Setup
   :PROPERTIES:
   :END:
   - capture
     - tbb generic
     - firefox marionette
     - script at github
   - feature extraction and classification in python

    :PROPERTIES:
    :CUSTOM_ID: capture
    :END:
    #+INDEX: Bridge
    #+INDEX: Tor!Bridge
    #+CAPTION: Setup to capture web page traffic: Tor Browser on /Client/ machine, connects to Tor server on /Bridge/ machine, connects to Tor network, connects to web servers
    #+ATTR_LATEX: :float nil :width 0.5\textwidth
    [[./pictures/Setup.eps]] 

    For the first captures, a single virtual machine hosted both the
    WTF-PAD server and client. To simulate the distributed WTF-PAD
    defense's network more closely, a second server was used.

    Thus, a Tor bridge was introduced into the traffic
    flow[fn::Bridges relay Tor traffic. They act as a gateway into the
    network. Their main use is censorship avoidance\cite{tor2014}]. It
    is required for WTF-PAD anyways, but other Tor traffic also uses
    the bridge instance via Tor's =Bridge=
    directive\cite[sec.client~options]{tor-manual}, to ease
    comparability.

    One host runs the Tor Browser Bundle[fn::at the current version]
    and the cover traffic server (if needed), the other runs a Tor
    server instance in bridge mode. For WTF-PAD, an additional server
    transport program is run at the bridge, and a client transport at
    the client[fn::WTF-PAD is run via the stand-alone programs. Tor's
    built-in =ServerTransportPlugin= and =ClientTransportPlugin=
    configuration naïvely failed].

    This setup utilises the same bridge for WTF-PAD and the browser
    extension.

    Single traces are captured via the Python script
     ~one_site.py~[fn::
     https://github.com/kreikenbaum/website-fingerprinting-thesis/blob/master/capture/one_site.py]. It
     cleans the cache between captures by restarting the Tor Browser
     Bundle.
   
** CHECK Website Fingerprinting Attack Evaluation
   :PROPERTIES:
   :CUSTOM_ID: sub4-tools
   :END:
   How does the defense compare to others (\cite{wtfpad2015},
   \cite{LLaMA}, \cite{a-systematic2014})? To examine this, \gls{wf} attacks are
   executed against web site traces collected with disabled defenses.

   The evaluation uses the attacks CUMUL by \cite{panchenko2} --- both
   as a re-implementation and its original form --- and the first
   approach to show better-than-random accuracy by
   \cite{ccsw09-fingerprinting} --- in a reimplementation. KNN by
   \cite{effective2014} was evaluated as well.

   Appendix [[#appendix-accuracy]] shows evaluation results. In most
   cases, the original CUMUL attack and the reimplementation had very
   similar results. This validates that the CUMUL-reimplementation was
   used as the main method of evaluating the extension.
** PENDING (daniel) Closed World
   :PROPERTIES:
   :CUSTOM_ID: sub4-closed
   :END:
   #+INDEX: evaluation!closed-world
   #+INDEX: closed-world evaluation
   In a /closed-world/ evaluation, a fixed set of site's traces are
   captured. The aim is to distinguish between these sites alone.\\


   This thesis evaluates its addon on the top 30 and 100 sites from
   Alexa's top million sites list[fn::
   \url{http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}] with
   duplicate sites and sites with many load errors
   removed[fn::Original from 2016-06-30 available at
   https://github.com/kreikenbaum/website-fingerprinting-thesis/blob/master/data/top-1m.csv.zip,
   modified version at
   https://github.com/kreikenbaum/website-fingerprinting-thesis/blob/master/data/top-1m.csv.modified].


   Table [[tab:closed_world]] compares
   LLaMAs\citep{DBLP:journals/popets/CherubinHJ17} performance with
   this thesis' addon's on 30 sites. As LLaMAs original time overhead
   seems prohibitive, the source code was edited (a constant was
   changed) to disable the delay. This version is called
   /LLaMA-nodelay/.

   #+CAPTION: comparison of LLaMA to this thesis's defense on 30 sites
   #+NAME: tab:closed_world
   #+ATTR_LATEX: :align |l||n{2}{2}|n{3}{2}|n{4}{2}|
   | <17>              |       <10> |       <10> |       <10> |
   | scenario          | \open accuracy [%] \close | \open size overhead [%] \close | \open time overhead [%] \close |
   |-------------------+------------+------------+------------|
   | no defense (2017-12-26) |      96.46 |          0 |          0 |
   |-------------------+------------+------------+------------|
   | llama (redo!)     |         26 |        295 |       2981 |
   | 100aI--2017-12-14 |      45.64 |     284.35 |      65.92 |
   |-------------------+------------+------------+------------|
   | llama-nodelay     |      68.22 |     163.08 |      -4.02 |
   | 20aI--2016-10-07  |         63 |      72.47 |       1.50 |
   | 100aI--2017-12-27 |      42.43 |     162.20 |      -0.88 |



   This shows that _original LLaMA delays web surfing by an order of
   magnitude_. Compared to the modified version of LLaMA, this thesis'
   defense offers both configurability and lower time overhead, as
   well as lower size overhead as compared to non-delayed LLaMA.

   The thesis' configurability is shown in Figure [[fig:oh2acc]].
   #+CAPTION: Size Overhead to Accuracy Trade-Off for Thesis' Defense, LLaMA-nodelay, and no defense on 30 sites.
   #+NAME: fig:oh2acc
   [[./pictures/oh2acc--llama-0.22-disabled.eps]]
   # code: see [[file:bin/results.py::%20scatter%20plot%20of%20accuracy%20vs%20overhead]]
*** PENDING (llama-delay) [#C] accuracy/overhead plot    :duckstein:internet:
    :PROPERTIES:
    :Effort:   0:30
    :END:
    :LOGBOOK:
    CLOCK: [2017-12-07 Do 12:56]--[2017-12-07 Do 13:20] =>  0:24
    CLOCK: [2017-12-07 Do 11:09]--[2017-12-07 Do 11:40] =>  0:31
    :END:
    - include llama-delay @ 30
    - ?additional 0.22 data points in between groups (200--300% overheads)
    - data points with tiny overhead (?=negative factor?)
*** PENDING (daniel) [#B] accuracy/time plots: time to date
*** PENDING (daniel) Low Accuracy on my Captures
    #+CAPTION: Accuracy Decay on 30 sites.
    #+NAME: fig:date-vs-acc--30sites
    #+ATTR_LATEX: :float wrap
    [[./pictures/date-vs-acc--30sites.eps]]
    # code: [[file:bin/mplot.py::def%20date_accuracy]]()

    #+CAPTION: Accuracy Decay on 100 sites.
    #+NAME: fig:date-vs-acc--100sites
    #+ATTR_LATEX: :float wrap
    [[./pictures/date-vs-acc--100sites.eps]]
    # code: [[file:bin/mplot.py::def%20date_accuracy]](100)
    The previous data show that the thesis's defense provides better
    protection for the same overhead and/or lower overhead for the
    same protection on 30 sites. Yet, the current state-of-the-art
    compares results on at least 100 sites.\\


    Comparison of 100 sites shows a problem: Accuracy for all captures
    is markedly lower than that reported in the literature, both on 30
    and 100 sites. This is true regardless of if data was captured
    with defense or without. On 100 sites, the comparison to
    literature results shows this handicap more clearly. As both
    Panchenko's original implementation and the reimplementation yield
    the same results, the cause is probably due to capture methodology.\\


    Surprisingly, the capture's accuracies decayed over time: On
    /recent data/, neither the wf-attack-reimplementation, Panchenkos
    original, nor a student group at University of Brunswick was able
    to provide good accuracy. The accuracy on 30 sites decayed /from
    97.93%/ on 2016-08-15 /to 58.41%/ on 2017-10-16, as displayed in
    Figure [[fig:date-vs-acc--30sites]]. On 100 sites, the accuracy
    likewise fell /from 90.57%/ on 2016-06-17 /to 43.46%/ on
    2017-10-22, as displayed in Figure [[fig:date-vs-acc--100sites]].\\



    #+CAPTION: Confusion matrix for 30 sites at 2016-08-15, overall accuracy at 98%.
    #+NAME: fig:confmat-2016-08-15
    #+ATTR_LATEX: :float wrap
    [[./pictures/confmat-2016-08-15.eps]]
    # code: [[file:bin/mplot.py::def%20confusion]]

    #+CAPTION: Confusion matrix for 30 sites at 2017-10-16, overall accuracy at 58%.
    #+NAME: fig:confmat-2017-10-16
    #+ATTR_LATEX: :float wrap
    [[./pictures/confmat-2017-10-16.pdf]]
    # code: [[file:bin/mplot.py::def%20confusion]]
    Let's see if a single site or group of sites cause this low
    accuracy: A high-accuracy confusion matrix from 2016-08-15 is
    shown in Figure [[fig:confmat-2016-08-15]]. This is contrast with a
    low-accuracy confusion matrix from 2017-10-16 in Figure
    [[fig:confmat-2017-10-16]]. While there is much more class-bleed-off,
    there is no clear culprit: errors are mostly distributed.\\


    #+CAPTION: CUMUL-traces for site msn.com compared for both dates.
    #+NAME: fig:cumul-good-bad.pdf
    #+ATTR_LATEX: :float wrap
    [[./pictures/cumul-good-bad.pdf]]
    # code:
    # e = scenario.list_all("08-15")[0]; f = e.get_traces()
    # E = scenario.list_all("17-10-16")[0]; F = E.get_traces()
    # palette = sns.color_palette("colorblind", 4)
    # g = gplot.counters(f['msn.com'], label=str(e.date), color=str(palette.as_hex()[0]))
    # g = gplot.counters(F['msn.com'], g, label=str(E.date), color=str(palette.as_hex()[2]))
    A site that existed on both captures is /msn.com/. It was chosen
    to illustrate the problem. The CUMUL-features are shown in Figure
    [[fig:cumul-good-bad.pdf]].\\


    For several low-accuracy retrievals, very little data is received
    back from the server. This could possibly be due to cloudflare
    recaptcha protection etc, which can sometimes be observed when
    browsing with Tor. A possible solution as future work would be to
    filter traces with fewer bytes incoming than outgoing, e.g. Also,
    the capture code could be adjusted to check the page text for the
    recaptcha site.
**** TODO comparison table for 100 sites
**** TODO Accuracy/Overhead for 100 Sites
** KEYWORDS Open World
   :PROPERTIES:
   :CUSTOM_ID: sub4-open
   :END:
** KEYWORDS Summary
   :PROPERTIES:
   :CUSTOM_ID: sub4-summary
   :END:
* Conclusions [0/10000]
  :PROPERTIES:
  :CUSTOM_ID: ch6-conclusion
  :END:
** Future Work [/1000]
   :PROPERTIES:
   :CUSTOM_ID: future-work
   :END:
*** purpose
*** limits
*** vision
*** brainstorm
    - addon-sdk replace by webextension
      - not that much to do
      - when/if necessary for Tor's ESR-version-based browser
      - advantage: also Google Chrome
    - seems like connection establishment leaks data, as of ch4





\appendix
\part{Appendix}
* appendices (begin above this headline; this is for searching)     :ARCHIVE:
  above, as in this section cuts it out (due to ARCHIVE tag)
* [#D] Accuracy Results [0/1]
  :PROPERTIES:
  :CUSTOM_ID: appendix-accuracy
  :END:
In the following tables, /size/ is the (geometric) size increase in %
as related to the closest no-add-on capture, /knn/ is
\cite{effective2014}'s classifier, /p-cumul/ is \cite{panchenko2}'s CUMUL
implementation, /cumul/ this thesis's reimplementation, and
/tts-cumul/ is this thesis's CUMUL with data split into training and
testing set, parameter estimation on the training set and
cross-validation on the testing set, not simple cross-validation. The
classifiers trained and tested on the same scenarios, as compared to
training on data without defense, and testing on defended data.
#+CAPTION: 10-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:10-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}| :environment longtable
#+INCLUDE: "data/skip/results/alternatives.org::#10-sites-pub" :only-contents t

#+CAPTION: 30-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:30-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}| :environment longtable
#+INCLUDE: "data/skip/results/alternatives.org::#30-sites-pub" :only-contents t

#+CAPTION: 100-class data sets's accuracy with state-of-the-art classifiers.
#+NAME: tab:100-sites
#+ATTR_LATEX: :align |r||n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|n{3}{2}|
#+INCLUDE: "data/skip/results/alternatives.org::#100-sites-pub" :only-contents t
** TODO [#D] create dynamically from database
* KEYWORDS [#C] The Base Rate Fallacy
  :PROPERTIES:
  :CUSTOM_ID: base-rate
  :END:
  - two stats-related: psych and IT/IDS
    - psych: kahneman+..., bar-hillel
    - IT/IDS: axelsson
  - bayes
    - hard for them, easy for us (?) bayes rate fallacy
      - axelsson
      - just need a few %
      - but: theoretical concept, better be a bit sceptical
        - \cite{koehler1996base} in general (original authors sceptical, too)
        - rieck\cite{rieckdiss} had success in IDS

   This knowledge helps in understanding and creating defenses. As of
   \cite{a-systematic}, \cite{ccs2014-critical} and \cite{panchenko2},
   [[#base-rate][the Base Rate Fallacy]] creates problems for /some/
   \gls{wf}-adversaries. This means that finding people who might have
   accessed a certain site is easier than making sure that they really
   visited the site.
* WRITE Addon: Factor to Overhead
  - addon has factor setting
  - how to estimate overhead from this
    - useful for e.g. matching llama overhead
  - from list of tuples of =(defense-factor: overhead)=
  - result: 1.47 * factor + 39.63
  - estimate to get to overhead of 295 is 175: i(175) \approx 296.9
  [[./pictures/factor_to_overhead.eps]]
  # creation see code at: [[file:~/da/da.org_archive::*factor%20to%20overhead%20at%200.22aI][factor to overhead at 0.22aI]]
* After Appendices: Bibliography and Index
\bibliography{docs/master}
\bibliographystyle{apalike}
\input{diplomarbeit.ind}
* END: /above/ this headline are INDEX, and BIBLIOGRAPHY, etc       :ARCHIVE:
