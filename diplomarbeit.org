#+TITLE: Selective Cover Traffic
#+TODO: TODO CHECK | DONE
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{tikz}
* final sections
** LATER Abstract
*** Teaser
    #+BEGIN_QUOTE
    "They who can give up essential liberty to obtain a little temporary
    safety, deserve neither liberty nor safety" - Benjamin
    Franklin\cite{franklin}
    #+END_QUOTE

    In Germany, the basis of all laws is the "Grundgesetz", which
    ensures free speech (Art. 5 GG) and protects private communication
    (Art. 10 GG).

    The Fourth Amendmend to the United States Constitution is
    interpreted as providing similar protections to Art. 10 GG, as of
    \cite{katz}.

    With new technologies for communication and information emerge new
    challenges to secure these rights.

    The internet has offered many new ways to communicate and,
    conversely, wiretap, of which website fingerprinting and page-marker
    detection are examples.

    Yet, there may exist user-friendly ways to hamper, or even to deter,
    this surveillance.
**** evtl rein
     Auch in der DDR gab es das Recht auf freie Meinungsäußerung, nur
     hat es niemand genutzt, da durch die Ueberwachung die Angst vor
     Repressalien zu groß war. (td: quote)
**** WAIT may \to (ohne) (falls richtig)
** Introduction
** Materials and Methods
* TODO topics [0/16]
** TODO Website Fingerprinting [0/3]
*** What is Website Fingerprinting
    Website fingerprinting\cite{hintz02} can be used to discover which
    webpages or websites a user visits via an anonymizing proxy. It is a
    type of traffic analysis\cite{applied96}, where characteristics of
    traffic are used to infer conjectures about content and metadata.
*** TODO What happens during a websiet request
    When a user visits a webpage, the browser first sends a request for
    the HTML page. The Webserver answers with that HTML page. Then, the
    browser requests the objects embedded into that page, such as
    stylesheets (\verb|<link rel="stylesheet">|), fonts, images
    (=<img>=), scripts(=<script>=), etc.  Each of these files has a
    specific size, which might be detected in the TCP-flow. Thus, the
    objects embedded within a page could allow a local passive observer
    to infer whether the user requests a specific web page from a set of
    pages.
**** TODO how are fonts included
*** CHECK Why could website fingerprinting be a problem
    As a typical scenario, consider the government of some state. A
    whistleblower posts something very critical of the regime on a
    well-known critical website. The whistleblower uses Tor or some
    other anonymity service to protect his identity. The government
    monitors and records all Tor connections. Even though Tor obfuscates
    the user's traffic, the specific data-pattern of the website gives
    the whistleblower away.[fn::This is, as of now, a theoretical
    threat. It has not been observed in the wild.]
*** Panchenko's Attack via Website Fingerprinting
    For a long time, traffic analysis attacks against Tor had not
    yielded results. Prior to Panchenko et al\cite{panchenko}, it was
    assumed that Tor introduced multiplexing, enough padding and
    latency into website retrieval that this attack was inapplicable.
*** Hurdles to website fingerprinting
    The progress of web protocols made website fingerprinting
    harder. In the original HTTP/1.0\cite{rfc1945} protocol, each
    request used a separate TCP-connection. This facilitated the
    original attacks against HTTPS browsing\cite{quantifying} and the
    anonymizing web proxy SafeWeb\cite{hintz02}, which both extracted
    the exact file size of each embedded object.

    Building a new connection for each transferred object proved to be
    inefficient. Some HTTP implementations \cite{rfc2068} used
    persistent connections. These were included HTTP/1.1
    \cite{rfc2616}. Due to this, it was no longer trivial to extract
    the files' sizes. You had to determine the start and end of each
    request. (which was still possible by seeing when the client sent
    a new request).

    [[./pictures/HTTP_persistent_connection.svg.png]]

    In addition to persistent connections, HTTP/1.1 allowed pipelining
    several HTTP requests in a single connection without waiting for
    the files to arrive in between.

    [[./pictures/HTTP_pipelining2.svg.png]]

    As this created problems with some servers, pipelining was disabled
    by default in Firefox \cite{firefox-pipelining} and Chrome
    \cite{chromium-pipelining}, and not implemented in Internet
    Explorer \cite{ie-pipelining}.

    After the Panchenko paper\cite{panchenko} an additional no-cost
    defense prototype was implemented in the Tor-Browser-Bundle
    \cite{experimental}: Firefox's built-in request pipelining was
    enabled with added request order randomization.  Cai et al. found
    fingerprinting to be easier with this defense enabled than
    without. \cite{ccs2012-fingerprinting}

    Originally, a browser should open at most two connections per host
    \cite{rfc2616} to retrieve the files one-by-one. An update
    \cite{rfc7230} removed this fixed limit, but encouraged clients
    "to be conservative when opening multiple connections".
*** Who could attack via WF
    As website fingerprinting requires very litte resources, a specific
    attacker could be a WLAN sniffer, an ISP, up to maybe even a nation
    state.
*** practical wf: Capturing traces
**** [[capture alternatives]]
**** [[shell script]]
**** [[Selenium]]
**** [[Chickenfoot]]
**** [[Marionette]]
**** [[Marionette installation]]
**** [[criteria for tool to retrieve websites]]
*** practical wf: analyzing traces
*** Related Work
**** [[History of Website Fingerprinting]]
**** [[MERGE history of website fingerprinting in Tor]]
*** [[History of website fingerprinting]]
** TODO Tor [0/8]
*** How to use Tor
    Tor offers a SOCKS5\cite{rfc1928}-(TCP\cite{rfc793}-)proxy for
    users. (If an application has no proxy settings, the
    =tsocks=-program provides transparent proxying). After setting this,
    all traffic is routed through the Tor network, anonymizing the IP
    address, with a fixed message length to hinder traffic analysis.

    (Tor also offers the possibility to "hide" a (TCP-)internet service
    via "hidden services" so that noone can see its location).
*** CHECK how Tor works internally
    A TCP stream is triply-encrypted and sent along a path of three
    intermediaries, none of whom can link its origin to its destination.

    After the third hop (the /exit node/), the message most often leaves
    the Tor network to any server on the Internet. It could also be
    forwarded to a location-hidden server via Tor's /hidden services/.

    To build a TCP channel, called /circuit/ in Tor, the Tor client
    software (/Onion Proxy/) uses a telescoping approach:

    First, Tor builds a cryptographically secure connection to its first
    hop (each hop is called /Onion Router (OR)/). This connection is on
    top of TLS, using additional, same-length, Tor encryption.

    Through this connection, the onion proxy establishes a connection to
    a second hop, and through the second, to a third hop. (The actual
    TCP connection is from the first hop to the second, and from the
    second to the third). Each message to be sent is encrypted three
    times, sent to the first hop, which decrypts it once, and sends it
    on. The second and third hop do likewise, with the third hop sending
    the plain-text-message in to the desired recipient.

    Tor sends data traffic in 512-byte cells.
**** TODO pictures with attribution
*** TODO Tor's Cryptography
    In Tor, each Onion Router has three distinct keys. One is an
    /identity key/ of the onion router, a long-term key used for signing
    only. This signs a medium-term /onion key/, which is kept at least
    one week after advertising.

    - each router has 3 keys, two of which are rotated, one identity key
    - fixed minimum messages size of 512 Byte
      - extensions allow other sizes
    - directory of all onion routers in directory servers
    - encrypted from client to "guard node"
    - passed to two other hosts, chosen by client "at random"
    - exits at "exit node" as normal tcp connection (except for hidden services)
*** TODO Who uses Tor
    From the beginning, the Tor Project envisoned a broad user base.

    Tor is a development of the Onion Routing Project
    \cite{anonymous-connections}. It was originally developed by the
    Naval Research Laboratory of the US Navy with the primary purpose
    to protect government communication.\cite{who-uses-tor}

    Nowadays, it is an anonymization network with a diverse user
    base. It has 6'000 nodes and is accessed by more than 3 million
    daily clients.

    The main user groups as listed by the Tor Project
    \cite{who-uses-tor}, in order, are:

    - normal people,
    - military,
    - journalists and their audience,
    - law enforcement officers,
    - activists & whistleblwers,
    - high & low profile people,
    - business executives,
    - bloggers, and
    - IT professionals.

    They use it for diverse purposes, such as

    - privacy,
    - censorship avoidance,
    - covert ops,
    - publishing,
    - safety,
    - online surveillance,
    - anonymous tip lines,
    - whistleblowing,
    - blogging private opinions,
    - evaluating competition, and
    - troubleshooting IT systems.
**** TODO "nowadays" used like this and here?
**** TODO client numbers with link (footnote?)
*** [[Tor Threat Model]]
*** Tor Browser
    There are many technologies based on the Tor protocol. Top of the
    list ([[file:docs/lit.org::*%5B%5B./projects.html%5D%5BSoftware%20&%20Services%5D%5D][Software & Services*]]) is the Tor Browser Bundle. This is a
    modified version of Firefox which uses Tor and comes with built-in
    privacy and security enhancements and Add-ons.

    Among the additional privacy features the Tor-Browser-Bundle
    provides are added request randomization and enabled pipelining.
*** [[tbb anti-wf modification]]
*** TODO history of website fingerprinting in Tor
**** with  [[History of Website Fingerprinting]]
    The website fingerprinting attack scenario is already described in
    the original Tor design\cite{tor-design} paper. Previous to
    Panchenko et al.\cite{panchenko}, it was considered "less
    effective"\cite{tor-design} against Tor, due to stream/circuit
    multiplexing and fixed cell sizes.
*** TODO sort subtopics
** TODO MOVE TO [[Addon Design and Implementation]] Developing Firefox Addons [0/2]
*** [[Mozilla Add-On Sdk]]
*** Debugger
*** [[Available Data]]
*** separation of scripts
    As a security measure, there is a separation between

    1) /add-on scripts/, which are run in the browser context, but
       cannot access the web page, and
    2) /content scripts/, which are run in the page context. They can
       access the DOM, but not add-on scripts. nor
    3) /page scripts/, which are those included in the website via
       f.ex. =<script>= tags

    Bridging this separation, f.ex. accessing page scripts (and vice
    versa) is possible, but needs some extra work.
**** LATER index: page scripts, content scripts, add-on scripts
*** [[message-passing]]
*** TODO collect/list all addon sections
*** TODO organize all addon sections
** Addon Design and Implementation
*** [[Design]]
*** Implementation
**** Unit Testing
* MAYBE_AND_MOVE How to not use Tor
* MAYBE_AND_MOVE what sets Tor apart / other anonymity networks
  There are other anonymity networks, such as JonDonym, I2P, MixNet
  and freedom.

  Tor is an anonymity service.
  - decentralized
  - biggest
  - high throughput
  - rather low latency, usable for web browsing
  - also hidden services

  Using a client called /Onion Proxy/ on the local computer, almost all
** TODO ref onion routing
** TODO onion routing
* TODO Tor Threat Model [0/2]
  Tor does not attempt to protect against a global passive adversary,
  who can observe all connections. A Tor adversary can\cite{tor-design}

  - observe a fraction of all traffic,
  - generate, modify, and delete traffic,
  - operate its own Onion Routers, and
  - compromise a fraction of the other ORs

  This adversary is not purely passive, but lacks global information.

  Tor is not intended to protect you
  - if someone can monitor a big amount of internet traffic (td: quote)
  - if someone can exploit your browser
  - if someone can exploit your computer
  - if you enter identifying information while using tor
** TODO maybe schneier re adversaries
** TODO quote big amount internet traffic, read paper
* TODO tbb anti-wf modification
  - pipelining enabled
  - request order randomization
* TODO History of Website Fingerprinting
  The idea of using traffic analysis to gather information about
  encrypted traffic was mentioned in \cite{applied96} and applied
  in the analysis of SSL 3.0 by Schneier and Wagner in
  1997\cite{SSL}.

  - quantifying etc

  The term /website fingerprinting/ was coined by
  Hintz in 2002. A successful attack against single-hop proxies was
  carried out by Herrmann et al. in 2009.
** SSL
   [[file:~/da/docs/master.bib::SSL][Wagner & Schneier 1997: Analysis SSL]]
* MAYBE-then-TODO torben
  Torben is a deanonymization attack based on injected website content
  in combination with Pattern Recognition. The authors show that when
  the user's browser sends requests of certain sizes for responses of
  certain sizes, this can be recognized in the encrypted TLS-Traffic
  from the Guard Node to the Onion Proxy.

  Each request/response pair corresponds to a certain amount of
  information (the authors show their approach with four request and
  response sizes, yielding a four-bit side-channel per request). This
  channel is used to encode a hash of the currently visited page.

  The requests are performed via XMLHttpRequest, but they authors also
  mention using HTTP redirects for the same effect.



  inject additional traffic into communication via JS XMLHttpRequest
  fixed request/response sizes of 2k, 4k, 6k, 8k bytes
  \to quad bits, concatenate, data transfer rate rate
  after 30 or 120 ms (tor latency bigger)
  detect via svm (how)
  setzt auf tcp an statt auf ip, (weil tor ja tcp ! yeah!)
* MAYBE why privacy
  - fundamental human need
  - concentration camp:
    "solitude in a Camp is more precious and rare than bread." -- primo levi
* TODO Mozilla Add-On Sdk
  The Add-on SDK by Mozilla supports the development of
  Firefox-Addons.

  It allows users to create addons using HTML and Javascript only, as
  opposed to the previous use of
  XUL[fn::\url{https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL}],
  the XML User-interface Language.

  The addon execution entry point (like `main` in C and Java) can be
  configured via the =preferences.json= file. By default, the main
  addon-script is called =index.js=.

  The SDK contains many tools to interact with the browser. URLs can
  be loaded in the background via the =page-worker= module; the
  =page-mod= module injects Javascript code into the page the user is
  browsing to. User-created code can be tested via unit tests.

  If none of the easily accessible high-, or low-level modules
  suffice, much of the browser's functionality is accessible via the
  XPCOM module.
** TODO link XUL
* CHECK page-worker
  A =page-worker= creates "a permanent, invisible page and access[es]
  its
  DOM."[fn::\url{developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-worker.html}]

  New pages can be loaded in the background, which would allow for the
  retrieval of camouflage traffic, as described by \cite{panchenko}.

  A minimal new page-worker is created via

  #+BEGIN_SRC js
    var pageWorker = require("sdk/page-worker").Page({});
  #+END_SRC

  The page-worker's page can be set dynamically via

  pageWorker.contentURL = "http://en.wikipedia.org/wiki/Cheese"

  This fetches only the file pointed to. The retrieval of included
  images, stylesheets, etc, is not automatic.

  A page-worker was used in the initial prototype. The RequestPolicy
  addon blocked this method of retrieval.

* TODO Installation and Use of Jpm (the build tool)
  (SDK-)addons can be built via the =jpm=-tool. It is available as a
  NodeJS-Module via the built-in NodeJS Package Manager =npm=.

  Installing =jpm= is thus a two-step process. Firstly, install
  NodeJS either via built-in tools (for example =apt-get install
  nodejs-legacy= in Debian and Ubuntu) or via [[link npm][download]], then, do a

  npm install jpm

  to install jpm (for the current user, global installation is done
  via =npm install -g jpm=).

  Once =jpm= is installed, new addons can be created via =jpm init=,
  unit-tested via =jpm test=, live-tested via =jpm run=, the addon
  package built via =jpm xpi= and signed via =jpm xpi= (or online).
** link npm
   nodejs.org
** TODO link online signing.
** jpm xpi date/time problem
* TODO page-mod
  The
  page-mod[fn::\url{http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html}]
  module injects "scripts in the context of web pages whose URL
  matches a given pattern."

  The pattern can be given as ="*"= or =/.*/= to run on every
  user-visited page.

  It thus offers the possibility to check for the end of a web page
  load by the user.

  A page-mod example is

  #+BEGIN_SRC js
    pageMod.PageMod({
        include: /.*/,
        contentScriptFile: "./getLinks.js",
        onAttach: function(worker) {
            worker.port.on("links", function(JSONlinks) {
                addToCandidates(JSON.parse(JSONlinks));
            });
        }
    });
  #+END_SRC

  , which is run on every page, applies the =getLinks.js= script and
  listens for its feedback, which is then used via
  `addToCandidates()`.

  A page-mod offers other parameters, such as the moment of the script
  execution, stylesheet modification, etc.
** link page-mod
   http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html
* CHECK message-passing
  There is a mechanism to pass content from the add-on to the
  content scripts, as shown in the example.

  A single string can be passed. As this string can be any serialized
  JSON\cite{rfc7159} object, this is not much of a limitation. (It
  effectively disallows the passing of functions and circular
  objects).

  In a content-script, a message can be sent via
  =self.port.emit('message_type', param)= and received via
  =self.port.on('message_type', function(param))=.

  In the Addon-Context, a =worker= object is used and the
  content-script's =self= is replaced by a =worker=. The worker is
  initialized via the =onAttach= parameter of f.ex. the page-mod.
* TODO interacting with page-scripts
  By default, content-scripts are isolated from the modifications
  done by page-scripts.[[Interacting with page scripts]]

  To access object inside the page-scripts context, you can use
  =unsafeWindow=.

  The reverse is only true for primitive values. If page-scripts
  need to see altered behavior, it is possible to override
  functionality of the page by using =exportFunction=, as in

  exportFunction(open,
		 unsafeWindow.XMLHttpRequest.prototype,
		 {defineAs: "open"});

  This exports the (previously-defined) function =open()= to the
  XMLHttpRequest.prototype, where it replaces the built-in
  functionality.
** Interacting with page scripts
developer.mozilla.org/en-US/Add-ons/SDK/Guides/Content_Scripts/Interacting_with_page_scripts.html
* TODO [#C] DOM
  domain-object-model
* TODO unit tests jpm
  JPM also offers the ability to write unit-tests.
* TODO js coding best practices
  - "use strict";
  - unit tests
  - mention subset of ...
* TODO other defenses
  There are other methods of defense, which might help mitigate
  website fingerprinting. One is based on reducing the fingerprint by
  browser plugins and text-only browsing, another on rate limiting.
** TODO Plugins
   There exist two plugins, which should both allow mitigation of this
   attack. Used in parallel, they may hinder normal browsing somewhat
   (which is why they are not enabled/installed by default in the Tor
   Browser Bundle).

   The first is NoScript, which selects which Javascript sources to
   run and which to block. This is installed by default in the Tor
   Browser Bundle for the additional security benefits it brings (XSS
   defense etc). It is considered as a one of the most
   security-enhancing plugins by ... o'reilly:

   QUOTE

   A second Add-On with a different set of protective measures is
   RequestPolicy: This add-on controls, which third-party content to
   load on a given page. Every query to the original domain is passed,
   while requests to other domains must be temporarily or permanently
   approved. It comes with a restrictive set of pre-defined rules (for
   example google pages are allowed to access gstatic), and a
   blacklist and whitelist mode.
*** TODO quote o'reilly
** write new plugins
   Instead of inserting dummy traffic into the connection, one could
   throttle the "data rate" of request and responses (or only requests
   or the ratio). As this approach has been used by
   f.ex. \cite{effective}, and has been proven to work, albeit
   requiring higher latency, it has not been explored further.

** TODO link tor browser bundle
*** or writeup something
** TODO link Requestpolicy
** TODO running an OR
** TODO text-only
   As the sizes and interconnection of HTML and embedded content is
   what makes a webpage easily identifyable, using a text-only
   non-javascript browser such as Lynx might be a mitigation for those
   who consider this trade-off acceptable.
*** TODO lynx link
* TODO capture alternatives [0/1]
  In order to retrieve sites inside the browser, there are several
  approaches. (OR)

  There exist several applications to capture network traffic. The
  most well-known and oldest of these is [[file:docs/lit.org::*%5B%5B./tcpdump%20-%20Wikipedia,%20the%20free%20encyclopedia.html%5D%5D][tcpdump]]. It is available on
  many UNIX-like systems and Windows. It is a command-line utility.

  A modern contender with a GUI is wireshark. It also offers a
  command-line version, tshark. tshark was used in this thesis, as it
  offers TLS packet reassembly.

  Both programs rely on the libpcap library for access to network
  packets.
** TODO subsubsect parts onto here, then subsect to [[*SECTION%20HOW%20TO%20CAPTURE%20TRACE%20FILES][SECTION HOW TO CAPTURE TRACE FILES]]
* WAIT Cover add-on
  Defends against website fingerprinting by injecting artificial
  cover traffic into the communication.
** when stable
   also cover against website fingerprinting by injecting really
   artificial cover traffic

   for every request, do one as well,
** why as an add-on
   This is one of the few low-latency communication methods, Instead
   of burdening all of Tor with extra bells and whistles, this solves
   this deanonymization problem at the application layer, where its
   origins are. (Separation of Concerns)
* Selenium
  Selenium is the de-facto standard for testing web applications. It
  has drivers for several browsers, allowing it to control them, and
  evaluate the retrieved page. Its documentation is currently
  transferring from Version 1 to Version 2.
* Marionette
  Marionette is the next generation mozilla testing framework. It is
  made to work just like Selenium. It was chosen for this thesis, as
  it made the Tor Browser Bundle easily accessible:

  After installation (see below), controlling the browser takes two
  easy steps:

  1. start the Tor Browser Bundle with the `-marionette` switch

     #+BEGIN_SRC sh
       cd tor-browser_en-US/Browser
       ./firefox -marionette
     #+END_SRC

  2. attach to a running browser in Python

     #+BEGIN_SRC python
       from marionette import Marionette
       client = Marionette('localhost', port=2828);
       client.start_session()
       client.navigate('http://cnn.com'); # navigate loads a website
     #+END_SRC

  Marionette has the benefit that the =client.navigate()= call returns
  only after the page has loaded, (and throws an error if the page
  could not be loaded). This obsoletes the need to test whether a page
  loaded completely([[Panchenko]]).
* TODO Marionette installation
  Marionette exists as a Python Package. It is thus easily installed
  via

  pip install marionette_client

  After installation pip via =sudo apt-get install python-pip=). Using
  a virtualenv is highly recommended in the documentation.
* shell script
  Simply calling =firefox website= loads the website in Firefox. This
  is the approach Wang recommended(\cite{wang-scripting}.
** TODO how to check that page has loaded
* Chickenfoot
  Chickenfoot was a Firefox addon which allowed browser scripting. It
  was developed at MIT\cite{chickenfoot}. The most recent GitHub
  release[fn::\url{https://github.com/bolinfest/chickenfoot}] is for
  Firefox 4.
* TODO http server for testing
* criteria for tool to retrieve websites
  - script tor browser: load new page
  - easy set-up
  - should
    - register page load or error
  - might
    - set tor's paranoia slider
    - install extra addon
* by-hand initialization to retrieve websites
  After installation, the tor browser bundle performs some
  initialization steps. To complete these easily, start the tor
  browser bundle-firefox by hand once.

  Set the connection type and have it load its first website via
  Tor. This also downloads descriptors to connect quicker later on.
* TODO one_site.py: retrieval of a single page
  Once you ensured that the Browser Bundle is working by starting it
  manually once, webpages can be retrieved automatically. This is done
  via the [[file:../bin/one_site.py][one_site.py]] script.

  It

  1. starts Firefox, waiting up to 60 seconds for its initialization
  2. starts the capture
  3. loads the page (given as first parameter)
  4. waits up to 600 seconds for the load to finish
  5. ends the capture
  6. ends Firefox

  This setup (restart after each trace) avoids caching issues with
  website fingerprinting, as the Tor Browser Bundle cleans the cache
  between restarts. If a more realistic scenario is desired, the
  script could to be modified to omit terminating the browser
  instance.
* tshark installation
  You also need to install =tshark= via =sudo apt-get install tshark=
  and set the current user to be able to capture packets via =sudo
  dpkg-reconfigure wireshark-common= and adding the user to the
  =wireshark= group (in =/etc/groups=).
** TODO scripts source + doc
* TODO how to get wang/goldberg to work
  As the =notes= file says:

  "svm-train and svm-predict come from the libSVM package."
* how to get tor browser bundle to work
  In order to start the tor browser bundle via the =./firefox=
  command, you need libraries, which are bundled with the binary.
  They can be found inside the =/TorBrowser/Tor= directory.

  The library path environment variable can be set on the command-line via

  export LD_LIBRARY_PATH=/lib:/usr/lib:/path/to/bundle/Browser/TorBrowser/Tor

  The script [[*%5B%5Bfile:./bin/one_site.py%5D%5D][one_site.py]] uses this internally.

** TODO where exactly is the torrc: directory
* how to avoid the safe mode error on multiple restarts
  If Firefox was killed via a signal (as opposed to closing the
  window), it prompts to start in Safe Mode afterwards.

  This behavior can be avoided in two ways([[disable-safe]]):

  You can set the firefox preference
  =toolkit.startup.max_resumed_crashes= to -1, or you can set the
  environment variable =MOZ_DISABLE_AUTO_SAFE_MODE=.
* headless configuration
  If you want to capture on a headless server, you can use the
  =xvfb=-package. which is installed via =sudo apt-get install xvfb=.

  Then, you can run the X Virtual Framebuffer via

  =Xvfb :1=

  tell the browser to use it via

  =export DISPLAY:1=

  and start the retrieval as mentioned above.
* TODO how to process the data
  - tshark internally
  - python triggers
  - collects,
  - sums in the end
  - displays
  - =Counts=-class
** TODO why filtering allowed
* TODO how sally works
  - configuration file
    - input
    - features
    - output
* TODO libsvm (short)
  LibSVM is a library for support vector machine classification and
  regression. It is used under-the-hood for scikit-learn, yet one part
  of functionality required a specific module which was not
  integrated.

  Its input format is very simple: First a number determining the
  class of the data, then a colon, finally all the data for an
  instance, separated by whitespace.
** TODO link to code to generate
* TODO problematic websites
  The above setup worked on most websites.
  The websites sina.com.cn and xinhuanet.com both did not terminate loading.
  This might need further looking into.

  - do they load completely when not Tor, repeat necessary
  - is this by design?
** MAYBE exclude
* transform to panchenko-features
  In encoding packet sizes, this thesis follows Panchenko et al.'s
  approach, who recorded ([[Panchenko]]) "incoming packets as positive,
  outgoing ones as negative numbers."

  Once the website traces are stored in pcap-files, feature vectors
  need to be extracted. A feature vector is represented by a Python
  class `Counter`, which can be created from a pcap file, or persisted
  to a json file containing timing and packet size information (to
  save time and space).

  To create a counter, you can use `counter.Counter.from(filename1,
  filename2, ...)`. This is also called indirectly when using
  `counter.py` from the command line, as in

  python -i /path/to/counter.py

  This extracts data from all pcap files in the current directory and
  subdirectories (excluding Address Resolution Protocol messages and
  ACKs). The filename of the pcap files needs to be `domain@tstamp`,
  for example `craigslist.org@1445352269`. The part up to the
  separator `@` is treated as the URL. If JSON-files of the name
  `domain.json` (for example `craigslist.com.json`) exist, those are
  preferred instead of the pcap files.

  In the interactive shell, there is a dictionary called `counters`,
  with the domain names as keys and an array of `Counter`s as
  values. To persist these to JSON, you can use `save` in the
  python interactive shell, for example

  >>> Counter.save(counters)

  To distill the features from a single `Counter`, call its
  `panchenko()`, to inspect single features, call
  `get('feature_name')` (for example
  `counters['cnn.com'][0].get('duration')').

  `panchenko()` yields a feature vector with default padding of
  Panchenko's variable-length features. Since [[Panchenko]] et al gave
  explicit size conversions, the sizes have not been normalized
  further. The default padding (300 per feature) might not be large
  enough for some traces.
** TODO maybe rename counter.py to trace.py
* TODO transform features to vector
  Once the `Counter`s are obtained, they need to be modified to fit as
  input for scikit-learn's\cite{scikit-learn} classifiers.

  The code to convert these features to classification input can be
  found in `extract_attribute.py`. This determines the maximum length
  of the variable-length features, gets Panchenko's features
  appropriately padded, and converts them to an array fit for input
  into scikit-learn's classifiers. When called from the command line, as

  python -i /path/to/extract_attribute.py

  , it will extract the feature vectors from JSON or pcap files in the
  current directory, and run some (5-fold) cross-validated classifiers
  against the data.

  This is transformed into scikit-learn input in the `to_features()`
  function, which normalizes all vectors to have the same size
  (padding with 0s), and creates the feature matrix `X` with numeric
  class labels `y` (and class names in `y_domain`).

  If you wish to run LibSVM on the command-line, there is also
  `to_libsvm(X, y, fname='libsvm_in')`, which can be called with the
  output of `to_features`. It writes lines in X with labels in y to the
  file 'libsvm_in' (by default).
** TODO maybe rename extract_attribute.py? To what?
** TODO ref stackoverflow why 0 padding
*** TODO better:
* MAYBE effect of panchenko's weighting schema
  Currently, fixed attributes are weighted heavily in favor of total
  incoming/outgoing bytes.
* TODO what happens when retrieving a website
  The complete data of google.com can be retrieved via

  =mkdir site; cd site; wget -p -H google.com=

  which yields (in germany) the files (=find . -type f -ls=, formatted)

  |  size | url                                                               |
  |-------+-------------------------------------------------------------------|
  |       | <65>                                                              |
  | 18979 | google.com/index.html                                             |
  | 17284 | www.google.de/images/nav_logo229.png                              |
  |  1834 | www.google.de/images/icons/product/chrome-48.png                  |
  |  5482 | www.google.de/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png |
  |  5430 | www.google.de/images/branding/product/ico/googleg_lodp.ico        |
  |  8080 | www.google.de/robots.txt                                          |

  thus, there should be 5-6 (depending on robots.txt) requests
** TODO tshark for normal (non-tor) retrieval
* TODO what did panchenko do (wf)?
  [[file:docs/lit.org::*%5B%5B./acmccs-wpes11-fingerprinting.pdf%5D%5BPanchenko%20-%20Website%20Fingerprinting%20in%20Onion%20Routing%20Based%20Anonymization%20Networks%5D%5D][Panchenko]] first published a successful website fingerprinting attack
  on Tor. He extracted HTTP-specific features from the packet trace
  and used those in a hand-tuned support vector machine with a radial
  basis function kernel.
* TODO visual inspection of data
  to exemplify the problems a wf'er has, consider the following
  pictures which represent complete (considered to contain all
  relevant information([[file:docs/lit.org::*%5B%5B./ccs14.pdf%5D%5BCai%20-%20A%20Systematic%20Approach%20to%20Developing%20and%20Evaluating%20Website%20Fingerprinting%20Defenses%5D%5D][a-systematic]]) packet trace data in the form of
  (delay, packet size), which is

  | facebook.com                              |                                          |
  |-------------------------------------------+------------------------------------------|
  | <35>                                      | <35>                                     |
  | [[file:pictures/facebook_com@1445350531.png]] | [[./pictures/craigslist_org@1445352269.png]] |
  |                                           |                                          |
  
  | [[file:pictures/facebook_com@1445422155.png]] |
  | [[file:pictures/facebook_com@1445425799.png]] |
  | [[file:pictures/facebook_com@1445429729.png]] |

  | craigslist.org                           |
  |------------------------------------------|
  | [[./pictures/craigslist_org@1445352269.png]] |
  | [[./pictures/craigslist_org@1445428146.png]] |
  | [[./pictures/craigslist_org@1445435476.png]] |
  | [[./pictures/craigslist_org@1445442917.png]] |

  They were created by the commands

  #+BEGIN_EXAMPLE
    for fb in $(ls | grep facebook); do
      python ~/da/bin/counter.py ./$fb  | tail -1 | sed 's/),/\n/g' | \
          tr -d "'()][" > /tmp/times;
      gnuplot -e "set terminal png size 1024,680; \
              set output \"/tmp/${fb}.png\"; plot '/tmp/times' with boxes;";
    done
  #+END_EXAMPLE

  and respectively for `s/facebook/craigslist/g`, in the directories
  containing the pcap files.

  These commands first extract the timing attributes (at git commit
  791af76 the last line of the output of counter.py), format it for
  gnuplot (inserting appropriate newlines via =sed= and removing extra
  characters via =tr=), and =gnuplot= s it to a png file with the name of
  the trace file as prefix.
** TODO 4th is cut off, better without table
* distribution of (main) features
  These distribution histograms show how Panchenko's main features are
  distributed. They are stacked histograms with classes separated by
  colors. They are compared (visually) to the [[file:docs/lit.org::*%5B%5B./HTTP%20Traffic%20Model_v1%201%20white%20paper.pdf%5D%5D][HTTP Traffic Model]].

  [[file:pictures/all_count_in.png]]
  shows the number of downstream/incoming packets.

  The general form of a gamma distribution may be
  fitting. Conceptually, this should be approximately

  num_embedded (gamma) * size_embedded (lognormal) / packet_size

  [[file:pictures/all_count_out.png]]
  shows the number of upstream/outgoing packets.

  Conceptually, the

  [[file:pictures/all_length_0.png]]
  the length of the Size Marker feature vector.

  [[file:pictures/all_num_sizes_in.png]]
  number of different packet sizes downstream/incoming.

  [[file:pictures/all_num_sizes_out.png]]
  number of different packet sizes upstream/outgoing.

  [[file:pictures/all_percentage_in.png]]
  percentage of incoming bytes (of total).

  [[file:pictures/all_total_in.png]]
  total bytes downstream/incoming.

  [[file:pictures/all_total_out.png]]
  total bytes upstream/outgoing.
** TODO compare to HTTP model
* TODO scikit-learn
  The python module scikit-learn\cite{scikit-learn} is described as a
  collection of "tools for data mining and data analysis".

  It combines python's ease-of-use with the efficiency of libraries
  written in C, such as LibSVM. It offers many different classifiers
  and regressors, such as K-NN, SVM, decision trees, linear
  approximation, random trees, etc.
** TODO regressor? wording
* TODO thoughts on size of data set
  - computable (n^2 for svm with good results)
  - number of instances negligible for computation
    - check this
  - stable results
  - recent papers
    - Panchenko: 775 a 20
    - Wang:
      - 100 a 90 of sensitive pages
      - 5000 a 1 of non-monitored pages
    - Cai: 400 samples of bbc.co.uk
      - 100 \to 800 once a 20 \to 40 twice
  - (currently closed world)
* TODO in-browser vs tcp-level
* TODO description of add-on
  The add-on is an experimental defense against website
  fingerprinting.

  - detect start of transmission
    - request extra HTML doc to obfuscate that
    - maybe do something to IPP-model (trigger off/on-state on some)
  - always send dummy traffic
    - on each request
    - better: leave some out
  - better: delay some requests (f.ex. images)
  - detect end of page load
    - maybe do something to IPP-model (trigger on/off-state on some)
  - request size uniform [0, 300)
    - except if request.len > 300
* TODO add-on caveats
  - The choice of cover traffic domains was explicitly taken out of
    the research focus. Currently, all cover traffic is dynamically
    generated by a web server written in Python.

    There exists basic code to use a list of webpages, given their
    sizes. It could be augmented by following links.
  - As Juarez implicated that the first tab is harder to conceal, and
    it would require additional work to distinguish between websites,
    frames, etc, it only foreground-covers while the first page from a
    domain is active.
  - no morphing (delay, segmentation)
* TODO differences to adaptive padding/wtfpad
** no cooperator necessary
   dummy packets chosen as response to real request (as in web traffic)
** knowledge of packets
** delay of some possible (f.ex. images)
** end of transmission detectable
* TODO differences to walkie-talkie
* TODO why several covers
** competition
** harder to break
* TODO HTML traffic model
  - intel
    - html object lognormal with params \mu = 7.90272, \sigma = 1.7643
    - embedded objects
  - test
    - download html top 10000
    - analysis
* TODO cacheing
  - browsers cache
  - only helps in cover traffic, (unless warm/cold site model is used)
* TODO assumptions
  split traffic traces
* TODO [#B] Design
  #+BEGIN_LATEX
  \begin{adjustbox}{max width=\textwidth}
  \input{pictures/model.tex}
  \end{adjustbox}
  #+END_LATEX
** tmp [[shell:dia pictures/model.dia &]]
** procedural
*** check which urls user loads
**** aggregate by domain
*** on finish load: tick off domains which were included
**** else still active (?)
*** for each loaded url, maybe load something else
** objects
*** UserWatcher
    notifies when user loads sth
**** maybe change name
**** methods
     - loads
     - endsLoad
*** Cover(Loader=default)
    provides cover traffic
*** Loader(Source=default2)
    loads new cover page (mockable)
*** Stats - Static functions
    statistical distributions (html, embedded, etc)
*** Source
    source for cover traffic
    1. fixed domain, fixed elements
    2. fixed domain, size as parameter
    3. LATER MAYBE get new ones from cover traffic
* TODO who used which retrieval method
  - list
  - chickenfoot
  - modified browser
  - selenium: daniel
  - plain tor bundle
* TODO Available Data
  Firefox offers several ways for an add-on to listen for web activity.

  - contents of main page
    \to links to each domain
  - page-mod
    - problems: only when page is loaded, problem for cover traffic
    - but +: ends of all the loading (and processing)
  [[file:git/docs/lit.org::*%5B%5B./Intercepting%20Page%20Loads%20-%20Mozilla%20|%20MDN.html%5D%5BIntercepting%20Page%20Loads%5D%5D][Intercepting Page Loads*]] lists several
  - load events
  - http observer
  - webprogersslistener
  - xpcom
    - policymanager
    - documentloader
** each load of page
** end of page load
* MOVE appendix Python web server for cover traffic
  This approach did not scale, so it was not used. It is included as a
  reference of what seems to work, but did not.

  The python module =TrafficHTTPServer= can be started on the command-line via 

  python TrafficHTTPServer.py portname

  with portname set to 8000 by default. It generates cover traffic of the
  size given by the =size= parameter, for example the command

  wget 'http://localhost:8000/?size=10'

  retrieves a document with 10 bytes content from a TrafficHTTPServer
  running at localhost port 8000.
** TODO include file
* TODO mod_wsgi
  =mod_wsgi= module is a module for the apache web server. It provides
  functionality to execute python scripts implementing the WSGI
  standard. Contrary to normal apache installation, a server using
  only wsgi is easily set up via the =mod_wsgi-express= command, which
  is included in the =mod_wsgi= python package.

  installation

  - apt-get install apache2-bin apache2-dev
  - pip install mod_wsgi

  start via

  - ~/.local/bin/mod_wsgi-express start-server wsgi.py

  (here, also --port 7777), as for the script wsgi.py see appendix
** TODO WSGI lit
** TODO link to pypi mod_wsgi
** TODO link to comparison via nichol.as
* TODO Tor overview
* misc: tex bibliography
\bibliography{docs/master}
\bibliographystyle{plain}
* UPTO HERE +BIB, TEXING WORKS -------------------------------------------
* WAIT Results
** classifiers (ordered by accuracy)
*** 20 classes (~78%)
**** scikit-learn *knn* (~78%)
***** scikit-learn *knn* distance metrics (git:0603b7) (~78%)
****** call
       from scipy.spatial import distance
       for dist in [distance.braycurtis, distance.canberra,
                 distance.chebyshev, distance.cityblock, distance.correlation,
                 distance.cosine, distance.euclidean, distance.sqeuclidean]:
		 test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc', func=dist))
****** result (~78%)
******* braycurtis (~78%)
[ 0.78809524  0.79        0.7625      0.76        0.7875    ]
mean = 0.77761904799999992
******* canberra
[ 0.63809524  0.625       0.5975      0.6         0.6       ]
******* chebyshev
[ 0.71190476  0.69        0.6625      0.6775      0.715     ]
******* cityblock (~76%)
[ 0.7452381  0.7975     0.75       0.7525     0.775    ]
mean = 0.76404762000000004
******* correlation
[ 0.6     0.6225  0.6     0.62    0.61  ]
******* cosine
[ 0.6     0.6225  0.6     0.62    0.61  ]
******* euclidean
[ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
******* sqeuclidean
[ 0.75    0.7525  0.715   0.7425  0.7475]

***** scikit-learn *knn* n_neighbors parameter (git: 516d56) (~74%)
     1) [ 0.71666667  0.7325      0.7         0.73        0.7075    ]
     2) [ 0.69047619  0.7275      0.6925      0.7225      0.7425    ]
     3) [ 0.73095238  0.73        0.6975      0.735       0.7325    ]
     4) [ 0.73809524  0.74        0.7025      0.7475      0.7325    ]
     5) [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
     6) [ 0.74761905  0.755       0.7125      0.7525      0.7475    ]
     7) [ 0.7547619  0.75       0.71       0.7425     0.75     ]
     8) [ 0.7547619  0.7425     0.7        0.7425     0.745    ]
     9) [ 0.7547619  0.7525     0.705      0.735      0.7475   ]
****** result: default = 5 is best: 0.74445238
**** scikit-learn *decision tree* (~75%)
***** default (git: 05ed6f0) (~75%)
****** call
       from sklearn import tree
       test(X, y, tree.DecisionTreeClassifier())
****** result
       [ 0.78333333  0.7525      0.735       0.7225      0.735     ]
       mean = 0.74566666599999998
***** max_features="auto" (git: 05ed6f0) (~47%)
****** call
       test(X, y, tree.DecisionTreeClassifier(max_features="auto"))
****** result (~47%)
       [ 0.47142857  0.5025      0.465       0.4475      0.485     ]
       mean = 0.47428571399999997
**** scikit-learn *extratrees* (~72%)
***** scikit-learn *extratrees* parameter n_estimators 200-300 (git:05ed6f0)  (~72% \uparrow)
'***** call
      for num in range(200, 300, 10):
          test(X, y, ensemble.ExtraTreesClassifier(n_estimators=num))
****** result  (~72% \uparrow)
******* n_estimators=200
[ 0.7452381  0.7175     0.7075     0.6875     0.735    ]
0.71854762000000005
******* n_estimators=210
[ 0.74761905  0.7175      0.7125      0.695       0.7375    ]
0.72202380999999993
******* n_estimators=220
[ 0.74047619  0.7175      0.7175      0.695       0.735     ]
0.72109523799999997
******* n_estimators=230
[ 0.74285714  0.72        0.7175      0.695       0.735     ]
0.7220714279999999
******* n_estimators=240 (~72% \uparrow)
[ 0.7452381  0.72       0.7275     0.69       0.74     ]
0.72454762000000006
******* n_estimators=250
[ 0.7452381  0.72       0.715      0.685      0.73     ]
0.71904762
******* n_estimators=260
[ 0.75238095  0.7175      0.7175      0.69        0.73      ]
0.72147618999999996
******* n_estimators=270
[ 0.75238095  0.715       0.7125      0.6875      0.73      ]
0.71947618999999996
******* n_estimators=280
[ 0.7452381  0.7125     0.715      0.695      0.72     ]
0.71754762000000005
******* n_estimators=290
[ 0.74761905  0.715       0.7175      0.7         0.7225    ]
0.7205238100000001
***** scikit-learn *extratrees* parameter n_estimators 50-400 (git:05ed6f0) (~72%)
****** call
       test(X, y, ensemble.ExtraTreesClassifier()) #n_estimators == 10
       for num in range(50, 400, 50):
           test(X, y, ensemble.ExtraTreesClassifier(n_estimators=num))
****** result
******* n_estimators=10
[ 0.59047619  0.61        0.585       0.5375      0.585     ]
******* n_estimators=50
[ 0.69761905  0.675       0.6625      0.665       0.695     ]
******* n_estimators=100
[ 0.74047619  0.6975      0.68        0.6925      0.7025    ]
mean = 0.70259523800000001
******* n_estimators=150
[ 0.73333333  0.71        0.7075      0.685       0.7225    ]
mean = 0.71166666600000006
******* n_estimators=200
[ 0.7452381  0.7175     0.7075     0.6875     0.735    ]
mean = 0.71854762000000005
******* n_estimators=250 (~72% \uparrow)
[ 0.7452381  0.72       0.715      0.685      0.73     ]
mean = 0.71904762
******* n_estimators=300
[ 0.74285714  0.7125      0.7175      0.6975      0.72      ]
mean = 0.71807142800000001
******* n_estimators=350
[ 0.74047619  0.7125      0.715       0.7025      0.7075    ]
mean = 0.71559523800000002
***** scikit-learn *extratrees* (git:05ed6f0) (~72%)
****** call
       forest = ensemble.ExtraTreesClassifier(n_estimators=250)
       test(X, y, forest)
****** result
       [ 0.7452381  0.72       0.715      0.685      0.73     ]
       mean: 0.71904762
**** scikit-learn *svc linear*  (~66%)
***** default (git: 516d56) (~66%)
****** call
       test(X, y, svm.SVC(kernel='linear'))
****** result (~66%)
      [ 0.64285714  0.66        0.6725      0.6675      0.64      ]
      mean = 0.65657142800000001
***** parameter C 10e-5 to 10e5 (git: 05ed6f0) (~66%)
****** call
****** results (~66%)
******* C=1.0000000000000001e-05
[ 0.64761905  0.6525      0.6725      0.675       0.6425    ]
******* C=0.0001
[ 0.6452381  0.6575     0.6725     0.6675     0.64     ]
******* C=0.001
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=0.01
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=0.10000000000000001
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=1.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=10.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=100.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=1000.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=10000.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
******* C=100000.0
[ 0.64285714  0.66        0.6725      0.6675      0.64      ]
****** same for every C
**** scikit-learn *randomforest* (git: 05ed6f0) (~62%)
***** call
      from sklearn import ensemble
      test(X, y, ensemble.RandomForestClassifier())
***** result (~62%)
      [ 0.6047619  0.6175     0.6375     0.59       0.6475   ]
      mean = 0.61945238000000002
**** scikit-learn *svc rbf* (~58%)
***** scikit-learn *svc rbf* parameter C,gamma 1e-20\to1e-11 (git: bacd2e9) (~58%)
****** call
     Cs = np.logspace(-20, -11, base=10, num=10)
     Gs = np.logspace(-20, -11, base=10, num=10)
     for c in Cs:
         for gamma in Gs:
             test(X, y, svm.SVC(C=c, gamma=gamma))
****** result (~58%)
******* C=9.9999999999999995e-21 (~58%)
******** gamma=9.9999999999999995e-21 (~47%)
	 [ 0.45952381  0.465       0.47        0.475       0.495     ]
	 mean = 0.47290476200000003
******** gamma=9.9999999999999998e-20 (~47%)
	 [ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-18 (~47%)
	 [ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-17 (~47%)
	 [ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-15 (~47% \uparrow)
	 [ 0.46190476  0.465       0.47        0.4775      0.495     ]
	 mean = 0.47388095200000002
******** gamma=1e-14 (~48%)
	 [ 0.46428571  0.47        0.48        0.4825      0.5075    ]
	 mean = 0.48085714199999996
******** gamma=1e-13
	 [ 0.47380952  0.4875      0.5         0.5125      0.525     ]
	 mean = 0.49976190399999998
******** gamma=9.9999999999999998e-13
	 [ 0.50238095  0.5125      0.5225      0.5425      0.5575    ]
	 mean = 0.52747619000000001
******** gamma=9.9999999999999994e-12 (~58%)
	 [ 0.57142857  0.6025      0.575       0.575       0.5875    ]
	 mean = 0.58228571399999995
******* C=9.9999999999999998e-20 (~58%)
******** gamma=9.9999999999999995e-21
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-20
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-18
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-15
[ 0.46190476  0.465       0.47        0.4775      0.495     ]
******** gamma=1e-14
[ 0.46428571  0.47        0.48        0.4825      0.5075    ]
******** gamma=1e-13
[ 0.47380952  0.4875      0.5         0.5125      0.525     ]
******** gamma=9.9999999999999998e-13
[ 0.50238095  0.5125      0.5225      0.5425      0.5575    ]
******** gamma=9.9999999999999994e-12 (~58%)
[ 0.57142857  0.6025      0.575       0.575       0.5875    ]
mean = 0.58228571399999995
******* C=1.0000000000000001e-18 (~58%)
******** gamma=9.9999999999999995e-21
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-20
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-18
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-15
[ 0.46190476  0.465       0.47        0.4775      0.495     ]
******** gamma=1e-14
[ 0.46428571  0.47        0.48        0.4825      0.5075    ]
******** gamma=1e-13
[ 0.47380952  0.4875      0.5         0.5125      0.525     ]
******** gamma=9.9999999999999998e-13
[ 0.50238095  0.5125      0.5225      0.5425      0.5575    ]
******** gamma=9.9999999999999994e-12 (~58%)
[ 0.57142857  0.6025      0.575       0.575       0.5875    ]
mean = 0.58228571399999995
******* C=1.0000000000000001e-17 (~58%)
******** gamma=9.9999999999999995e-21
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-20
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-18
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-15
[ 0.46190476  0.465       0.47        0.4775      0.495     ]
******** gamma=1e-14
[ 0.46428571  0.47        0.48        0.4825      0.5075    ]
******** gamma=1e-13
[ 0.47380952  0.4875      0.5         0.5125      0.525     ]
******** gamma=9.9999999999999998e-13
[ 0.50238095  0.5125      0.5225      0.5425      0.5575    ]
******** gamma=9.9999999999999994e-12 (~58%)
[ 0.57142857  0.6025      0.575       0.575       0.5875    ]
mean = 0.58228571399999995
******* C=9.9999999999999998e-17 (~55%)
******** gamma=9.9999999999999995e-21
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-20
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-18
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=9.9999999999999998e-17
[ 0.45952381  0.465       0.47        0.475       0.495     ]
******** gamma=1.0000000000000001e-15
[ 0.46190476  0.465       0.47        0.4775      0.495     ]
******** gamma=1e-14
[ 0.46428571  0.4725      0.48        0.4825      0.5075    ]
******** gamma=1e-13
[ 0.47380952  0.49        0.5025      0.52        0.545     ]
******** gamma=9.9999999999999998e-13
[ 0.50952381  0.52        0.5275      0.55        0.5625    ]
******** gamma=9.9999999999999994e-12 (~55%)
[ 0.52619048  0.5375      0.555       0.5675      0.5575    ]
mean = 0.54873809600000001
******* abgebrochen, wurde schlechter
***** scikit-learn *svc rbf* parameter C gamma 1e-10\to1e0 (git: bacd2e9) (~38%)
****** call
     Cs = np.logspace(-10, 0, base=10, num=10)
     Gs = np.logspace(-10, 0, base=10, num=10)
     for c in Cs:
         for gamma in Gs:
             test(X, y, svm.SVC(C=c, gamma=gamma))
****** result (~38%)
******* C=1e-10 (~38%)
******** gamma = 1e-10 (~38%)
	 0.37142857  0.3875      0.4075      0.3575      0.36      ]
	 mean = 0.37678571399999994
******** gamma = 1.29e-9 (~36%)
	 [ 0.3452381  0.3625     0.3675     0.3475     0.36     ]
	 mean = 0.35654762000000001
******** gamma=1.66e-8 (~32%)
	 [ 0.30714286  0.3425      0.35        0.2925      0.315     ]
	 mean = 0.32142857199999997
******** gamma=2.1544346900318867e-07
	 [ 0.21190476 0.2 0.205 0.2075 0.2275 ]
******** gamma=2.782559402207126e-06
	 [ 0.08333333  0.0775      0.08        0.075       0.0825    ]
******** gamma=3.5938136638046256e-05
	 [ 0.05238095  0.05        0.0525      0.05        0.0525    ]
******** gamma=0.00046415888336127822
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=0.0059948425031894209
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=0.077426368268112777
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=1.0
	 [ 0.05  0.05  0.05  0.05  0.05]
******* C=1.2915496650148826e-09 (~38%)
******** gamma=1e-10 (~38%)
	 [ 0.37142857  0.3875      0.4075      0.3575      0.36      ]
	 mean = 0.37678571399999994
******** gamma=1.2915496650148826e-09 (~32%)
	 [ 0.31666667  0.3625      0.3275      0.305       0.3125    ]
	 mean = 0.32483333399999997
******** gamma=1.6681005372000592e-08
	 [ 0.25714286  0.3425      0.315       0.2575      0.2675    ]
******** gamma=2.1544346900318867e-07
	 [ 0.21190476  0.2         0.205       0.2075      0.2275    ]
******** gamma=2.782559402207126e-06
	 [ 0.08333333  0.0775      0.08        0.075       0.0825    ]
******** gamma=3.5938136638046256e-05
	 [ 0.05238095  0.05        0.0525      0.05        0.0525    ]
******** gamma=0.00046415888336127822
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=0.0059948425031894209
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=0.077426368268112777
	 [ 0.05  0.05  0.05  0.05  0.05]
******** gamma=1.0
	 [ 0.05  0.05  0.05  0.05  0.05]
******* C=1.6681005372000592e-08 (~38%)
******** gamma=1e-10 (~38%)
	 [ 0.37142857  0.3875      0.4075      0.3575      0.36      ]
	 mean = 0.37678571399999994
******** gamma=1.2915496650148826e-09 (~30%)
	 [ 0.29285714  0.3         0.3075      0.305       0.3125    ]
	 mean = 0.30357142800000003
******** gamma=1.6681005372000592e-08
	 [ 0.25714286  0.3025      0.275       0.2575      0.2675    ]
******* aborted as it got worse
******* next time num=11 will make cleaner params
***** scikit-learn *svc rbf* C=131072, gamma=1.9e-06 (git: 516d56) (~7%)
****** call
       test(X, y, svm.SVC(C=2**17, gamma=2**(-19)))
****** result
      [ 0.06904762  0.0625      0.065       0.0825      0.07      ]
      mean = 0.069809523999999998
**** scikit-learn *svc* liblinear (git: bacd2e9 (= 516d56+1)) (~33%)
***** call
      test(X, y, svm.LinearSVC())
***** result
     [ 0.37619048  0.345       0.385       0.3425      0.195     ]
     mean = 0.32873809600000004
**** scikit-learn *adaboost* (git: 05ed6f0) (~11%)
***** call
      test(X, y, ensemble.AdaBoostClassifier())
***** result
      [ 0.11904762  0.1025      0.1         0.1         0.145     ]
      mean = 0.11330952400000001
*** 76 classes (~63%)
**** scikit-learn *knn* (~63%)
***** bray-curtis (git: 09beeeb-1) (~63%)
****** call
       test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc',
       func=distance.braycurtis))
****** result
       [ 0.63550816  0.64539474  0.63552632  0.63684211  0.61710526]
       mean = 0.63407531800000005
***** metrics test (git: ca 09beeeb) (~63%)
****** call
       from scipy.spatial import distance
       for dist in [distance.braycurtis, distance.canberra,
                 distance.chebyshev, distance.cityblock, distance.correlation,
                 distance.cosine, distance.euclidean, distance.sqeuclidean]:
		 test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc', func=dist))
****** result (~63%)
******* braycurtis (~63%)
[ 0.63550816  0.64539474  0.63552632  0.63684211  0.61710526]
mean = 0.634075315327
******* cityblock (~62%)
[ 0.62233375  0.61710526  0.63157895  0.62434211  0.60921053]
mean = 0.620914118735
******* euclidean (~59%)
[ 0.61731493  0.61184211  0.58289474  0.58289474  0.57105263]
mean = 0.593199828304
******* sqeuclidean (~59%)
[ 0.61731493  0.6125      0.58092105  0.58026316  0.56973684]
mean = 0.592147196725
******* chebyshev (~49%)
[ 0.48557089  0.50460526  0.48552632  0.48618421  0.49539474]
mean = 0.491456283431
******* correlation (~38%)
[ 0.38268507  0.38947368  0.38289474  0.37763158  0.36907895]
mean = 0.380352803275
******* cosine
[ 0.38268507  0.38947368  0.38289474  0.37763158  0.36907895]
mean = 0.380352803275
******* canberra (~38%)
[ 0.37139272  0.37368421  0.39144737  0.38421053  0.37105263]
mean = 0.37835749191
***** default (git: f956a6) (~59%)
****** call
       test(X, y, neighbors.KNeighborsClassifier())
****** result
       [ 0.61731493  0.61184211  0.58289474  0.58289474  0.57105263]
       mean = 0.593199828304
**** scikit-learn *svc rbf* (~60%)
***** parameter search c: -35..-15 (git: abbf) (~60%)
****** call
    cstart, cstop = -35, -15
    Cs = np.logspace(cstart, cstop, base=10, num=(abs(cstart - cstop)+1))
    gamma = 4.175318936560409e-10
    for c in Cs:
        test(X, y, svm.SVC(C=c, gamma=gamma))
****** results
******* C=1e-35
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684]
mean = 0.599016542297
******* TODO
***** parameter search c: -45..35 (git: abbf) (~60%)
****** call
       cstart, cstop = -45, -35
       Cs = np.logspace(cstart, cstop, base=10, num=(abs(cstart - cstop)+1))
       gamma = 4.175318936560409e-10
       for c in Cs:
           test(X, y, svm.SVC(C=c, gamma=gamma))
****** results
******* C=9.9999999999999998e-46 (~60%)
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=9.9999999999999995e-45
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=1.0000000000000001e-43
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=1e-42
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=1e-41
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=9.9999999999999993e-41
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=9.9999999999999993e-40
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=9.9999999999999996e-39
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=1.0000000000000001e-37
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
******* C=9.9999999999999994e-37
[ 0.60100376  0.6125      0.59342105  0.59342105  0.59473684], mean = 0.599016542297
***** parameter search c: -28..-16, gamma: -10..-8 (git: f95a6) (~58%)
****** call
    cstart, cstop = -28, -16
    Cs = np.logspace(cstart, cstop, base=10, num=(abs(cstart - cstop)+1))
    gstart, gstop = -10, -8
    Gs = np.logspace(gstart, gstop, base=10, num=10*(abs(gstart - gstop)+1))
    for c in Cs:
        for gamma in Gs:
            test(X, y, svm.SVC(C=c, gamma=gamma))
****** result
******* C=9.9999999999999997e-29 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55921053  0.55065789]
mean = 0.568098296242
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58155583  0.56644737  0.55197368  0.55065789  0.54013158]
mean = 0.558153272139
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.55986842  0.54671053  0.55065789  0.53618421]
mean = 0.5532387902
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55592105  0.54144737  0.54407895  0.53355263]
mean = 0.547923462986
******** gamma=3.2903445623126709e-09
[ 0.55457967  0.54671053  0.5375      0.53815789  0.525     ]
mean = 0.540389618966
******** gamma=3.8566204211634722e-09
[ 0.54956085  0.5375      0.53026316  0.52565789  0.51513158]
mean = 0.531622696956
******** gamma=4.5203536563602409e-09
[ 0.53764115  0.53092105  0.52302632  0.52105263  0.50789474]
mean = 0.524107178234
******** gamma=5.2983169062837021e-09
[ 0.52760351  0.52368421  0.51118421  0.50921053  0.49868421]
mean = 0.514073334214
******** gamma=6.2101694189156032e-09
[ 0.51442911  0.51907895  0.50394737  0.50131579  0.49078947]
mean = 0.505912137621
******** gamma=7.2789538439831614e-09
[ 0.50878294  0.50723684  0.49407895  0.49210526  0.48289474]
mean = 0.497019745097
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49144737  0.48355263  0.48092105  0.46447368]
mean = 0.483953476854
******** gamma=1e-08
[ 0.48494354  0.48092105  0.475       0.47434211  0.45460526]
mean = 0.473962391864
******* C=1e-27 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55921053  0.55065789]
mean = 0.568098296242
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58155583  0.56710526  0.55197368  0.55065789  0.54013158]
mean = 0.558284851086
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56052632  0.54671053  0.55065789  0.53618421]
mean = 0.553370369147
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55592105  0.54144737  0.54407895  0.53355263]
mean = 0.547923462986
******** gamma=3.2903445623126709e-09
[ 0.55520703  0.54671053  0.5375      0.53815789  0.525     ]
mean = 0.54051508948
******** gamma=3.8566204211634722e-09
[ 0.54956085  0.53815789  0.53026316  0.52565789  0.51513158]
mean = 0.531754275903
******** gamma=4.5203536563602409e-09
[ 0.53764115  0.53092105  0.52368421  0.52105263  0.50855263]
mean = 0.524370336129
******** gamma=5.2983169062837021e-09
[ 0.52760351  0.52368421  0.51184211  0.50986842  0.49934211]
mean = 0.514468071056
******** gamma=6.2101694189156032e-09
[ 0.51442911  0.51907895  0.50460526  0.50131579  0.49078947]
mean = 0.506043716569
******** gamma=7.2789538439831614e-09
[ 0.50878294  0.50723684  0.49407895  0.49210526  0.48289474]
mean = 0.497019745097
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49144737  0.48421053  0.48157895  0.46513158]
mean = 0.484348213696
******** gamma=1e-08
[ 0.48494354  0.48092105  0.475       0.47434211  0.45460526]
mean = 0.473962391864
******* C=1e-26 (~58% \downarrow)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55921053  0.55065789]
mean = 0.568098296242
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58218319  0.56710526  0.55197368  0.55065789  0.54013158]
mean = 0.558410321601
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56118421  0.54671053  0.55065789  0.53618421]
mean = 0.553501948095
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55592105  0.54144737  0.54407895  0.53355263]
mean = 0.547923462986
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54671053  0.5375      0.53815789  0.525     ]
mean = 0.540640559995
******** gamma=3.8566204211634722e-09
[ 0.55018821  0.53881579  0.53026316  0.52565789  0.51513158]
mean = 0.532011325365
******** gamma=4.5203536563602409e-09
[ 0.53826851  0.53223684  0.52368421  0.52105263  0.50855263]
mean = 0.524758964538
******** gamma=5.2983169062837021e-09
[ 0.52760351  0.52368421  0.5125      0.50986842  0.49934211]
mean = 0.514599650003
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51907895  0.50460526  0.50131579  0.49078947]
mean = 0.506169187083
******** gamma=7.2789538439831614e-09
[ 0.50878294  0.50723684  0.49407895  0.49210526  0.48289474]
mean = 0.497019745097
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49210526  0.48421053  0.48157895  0.46513158]
mean = 0.484479792643
******** gamma=1e-08
[ 0.48557089  0.48092105  0.47565789  0.47434211  0.45526316]
mean = 0.474351020273
******* C=1e-25 (~58% \downarrow)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55921053  0.55065789]
mean = 0.568098296242
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58218319  0.56710526  0.55197368  0.55065789  0.54013158]
mean = 0.558410321601
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56118421  0.54671053  0.55065789  0.53684211]
mean = 0.553633527042
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55592105  0.54144737  0.54473684  0.53355263]
mean = 0.548055041934
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54671053  0.5375      0.53815789  0.525     ]
mean = 0.540640559995
******** gamma=3.8566204211634722e-09
[ 0.55018821  0.53881579  0.53026316  0.52565789  0.51513158]
mean = 0.532011325365
******** gamma=4.5203536563602409e-09
[ 0.53826851  0.53223684  0.52368421  0.52105263  0.50855263]
mean = 0.524758964538
******** gamma=5.2983169062837021e-09
[ 0.52760351  0.52368421  0.5125      0.50986842  0.50065789]
mean = 0.514862807898
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51907895  0.50460526  0.50131579  0.49078947]
mean = 0.506169187083
******** gamma=7.2789538439831614e-09
[ 0.50878294  0.50723684  0.49407895  0.49210526  0.48289474]
mean = 0.497019745097
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49210526  0.48421053  0.48157895  0.46578947]
mean = 0.484611371591
******** gamma=1e-08
[ 0.48619824  0.48092105  0.47565789  0.47434211  0.45526316]
mean = 0.474476490788
******* C=9.9999999999999992e-25 (~58% \downarrow)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55921053  0.55065789]
mean = 0.568098296242
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58281054  0.56710526  0.55197368  0.55065789  0.54078947]
mean = 0.558667371063
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56184211  0.54671053  0.55065789  0.53684211]
mean = 0.55376510599
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55592105  0.54210526  0.54473684  0.53355263]
mean = 0.548186620881
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54736842  0.5375      0.53815789  0.52565789]
mean = 0.540903717889
******** gamma=3.8566204211634722e-09
[ 0.55018821  0.53881579  0.53157895  0.52565789  0.51578947]
mean = 0.532406062207
******** gamma=4.5203536563602409e-09
[ 0.53826851  0.53223684  0.52368421  0.52171053  0.50855263]
mean = 0.524890543485
******** gamma=5.2983169062837021e-09
[ 0.52823087  0.52368421  0.5125      0.50986842  0.50131579]
mean = 0.51511985736
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51973684  0.50526316  0.50131579  0.49078947]
mean = 0.506432344978
******** gamma=7.2789538439831614e-09
[ 0.50878294  0.50855263  0.49407895  0.49276316  0.48289474]
mean = 0.497414481939
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49210526  0.48421053  0.48223684  0.46578947]
mean = 0.484742950538
******** gamma=1e-08
[ 0.4868256   0.48092105  0.47565789  0.47434211  0.45526316]
mean = 0.474601961302
******* C=9.9999999999999996e-24 (~58% \downarrow)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842]
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474]
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105]
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053]
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211]
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316]
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842]
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368]
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842]
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316]
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158]
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842]
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263]
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684]
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158]
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789]
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421]
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684]
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58281054  0.56710526  0.55197368  0.55065789  0.54078947]
mean = 0.558667371063
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56184211  0.54671053  0.55065789  0.5375    ]
mean = 0.553896684937
******** gamma=2.8072162039411812e-09
[ 0.56461731  0.55657895  0.54210526  0.54539474  0.53355263]
mean = 0.548449778776
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54736842  0.5375      0.53881579  0.52565789]
mean = 0.541035296837
******** gamma=3.8566204211634722e-09
[ 0.55018821  0.53947368  0.53157895  0.52631579  0.51776316]
mean = 0.533063956944
******** gamma=4.5203536563602409e-09
[ 0.53826851  0.53223684  0.52368421  0.52171053  0.50855263]
mean = 0.524890543485
******** gamma=5.2983169062837021e-09
[ 0.52948557  0.525       0.51315789  0.50986842  0.50131579]
mean = 0.515765535231
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51973684  0.50526316  0.50131579  0.49078947]
mean = 0.506432344978
******** gamma=7.2789538439831614e-09
[ 0.50941029  0.50921053  0.49407895  0.49342105  0.48289474]
mean = 0.497803110348
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49210526  0.48421053  0.48223684  0.46578947]
mean = 0.484742950538
******** gamma=1e-08
[ 0.48745295  0.48157895  0.47565789  0.47434211  0.45592105]
mean = 0.474990589711
******* C=1e-22 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ]
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158],
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58281054  0.56710526  0.55197368  0.55065789  0.54078947],
mean = 0.558667371063
******** gamma=2.3950266199874909e-09
[ 0.5727729   0.56184211  0.54671053  0.55131579  0.5375    ],
mean = 0.554028263884
******** gamma=2.8072162039411812e-09
[ 0.56524467  0.55657895  0.54342105  0.54605263  0.53355263],
mean = 0.548969986132
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54736842  0.5375      0.53881579  0.52565789],
mean = 0.541035296837
******** gamma=3.8566204211634722e-09
[ 0.55081556  0.53947368  0.53157895  0.52763158  0.51776316],
mean = 0.533452585353
******** gamma=4.5203536563602409e-09
[ 0.53826851  0.53223684  0.52434211  0.52236842  0.50921053],
mean = 0.525285280328
******** gamma=5.2983169062837021e-09
[ 0.52948557  0.525       0.51315789  0.50986842  0.50131579],
mean = 0.515765535231
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51973684  0.50526316  0.50131579  0.49078947],
mean = 0.506432344978
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.50986842  0.49407895  0.49342105  0.48421053],
mean = 0.498323317705
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49276316  0.48486842  0.48223684  0.46710526],
mean = 0.485269266328
******** gamma=1e-08
[ 0.4880803   0.48223684  0.47631579  0.47434211  0.45592105],
mean = 0.475379218121
******* C=9.9999999999999991e-22 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ],
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158],
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58281054  0.56710526  0.55197368  0.55065789  0.54078947],
mean = 0.558667371063
******** gamma=2.3950266199874909e-09
[ 0.57340025  0.56184211  0.54736842  0.55131579  0.53815789],
mean = 0.554416892293
******** gamma=2.8072162039411812e-09
[ 0.56524467  0.55657895  0.54342105  0.54605263  0.53355263],
mean = 0.548969986132
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54736842  0.5375      0.53881579  0.52631579],
mean = 0.541166875784
******** gamma=3.8566204211634722e-09
[ 0.55081556  0.53947368  0.53223684  0.52894737  0.51776316],
mean = 0.533847322195
******** gamma=4.5203536563602409e-09
[ 0.53889586  0.53223684  0.52434211  0.52236842  0.50921053],
mean = 0.525410750842
******** gamma=5.2983169062837021e-09
[ 0.53011292  0.525       0.51315789  0.51052632  0.50131579],
mean = 0.516022584693
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51973684  0.50526316  0.50197368  0.49144737],
mean = 0.506695502873
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.51052632  0.49407895  0.49407895  0.48421053],
mean = 0.498586475599
******** gamma=8.5316785241728148e-09
[ 0.49937265  0.49276316  0.48552632  0.48289474  0.46710526],
mean = 0.485532424222
******** gamma=1e-08
[ 0.4880803   0.48289474  0.47631579  0.47434211  0.45592105],
mean = 0.475510797068
******* C=9.9999999999999995e-21 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ],
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158],
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58343789  0.56776316  0.55197368  0.55065789  0.54078947],
mean = 0.558924420524
******** gamma=2.3950266199874909e-09
[ 0.57340025  0.56184211  0.54736842  0.55131579  0.53815789],
mean = 0.554416892293
******** gamma=2.8072162039411812e-09
[ 0.56524467  0.55657895  0.54342105  0.54671053  0.53355263],
mean = 0.54910156508
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54736842  0.5375      0.53881579  0.52631579],
mean = 0.541166875784
******** gamma=3.8566204211634722e-09
[ 0.55081556  0.53947368  0.53223684  0.52894737  0.51776316],
mean = 0.533847322195
******** gamma=4.5203536563602409e-09
[ 0.53889586  0.53223684  0.52434211  0.52236842  0.50921053],
mean = 0.525410750842
******** gamma=5.2983169062837021e-09
[ 0.53011292  0.525       0.51315789  0.51052632  0.50131579],
mean = 0.516022584693
******** gamma=6.2101694189156032e-09
[ 0.51505646  0.51973684  0.50526316  0.50197368  0.49144737],
mean = 0.506695502873
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.51052632  0.49407895  0.49473684  0.48421053],
mean = 0.498718054547
******** gamma=8.5316785241728148e-09
[ 0.5         0.49276316  0.48618421  0.48289474  0.46776316],
mean = 0.485921052632
******** gamma=1e-08
[ 0.4880803   0.48355263  0.47697368  0.475       0.45592105],
mean = 0.47590553391
******* C=9.9999999999999998e-20 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ],
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56776316  0.56118421  0.55263158],
mean = 0.569570098395
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58343789  0.56842105  0.55263158  0.55197368  0.54078947],
mean = 0.559450736314
******** gamma=2.3950266199874909e-09
[ 0.57340025  0.56184211  0.54736842  0.55131579  0.53815789],
mean = 0.554416892293
******** gamma=2.8072162039411812e-09
[ 0.56587202  0.55657895  0.54342105  0.54671053  0.53355263],
mean = 0.549227035594
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54802632  0.53815789  0.53881579  0.52631579],
mean = 0.541430033679
******** gamma=3.8566204211634722e-09
[ 0.55081556  0.53947368  0.53289474  0.52960526  0.51776316],
mean = 0.53411048009
******** gamma=4.5203536563602409e-09
[ 0.53889586  0.53223684  0.52434211  0.52236842  0.50921053],
mean = 0.525410750842
******** gamma=5.2983169062837021e-09
[ 0.53074028  0.52565789  0.51315789  0.51118421  0.50131579],
mean = 0.516411213102
******** gamma=6.2101694189156032e-09
[ 0.51568381  0.51973684  0.50526316  0.50197368  0.49144737],
mean = 0.506820973387
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.51052632  0.49407895  0.49473684  0.48421053],
mean = 0.498718054547
******** gamma=8.5316785241728148e-09
[ 0.5         0.49276316  0.48618421  0.48289474  0.46842105],
mean = 0.486052631579
******** gamma=1e-08
[ 0.48870765  0.48355263  0.47697368  0.475       0.45657895],
mean = 0.476162583372
******* C=1.0000000000000001e-18 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ],
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58157895  0.56842105  0.56118421  0.55263158],
mean = 0.569701677343
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58343789  0.56842105  0.55263158  0.55197368  0.54078947],
mean = 0.559450736314
******** gamma=2.3950266199874909e-09
[ 0.57340025  0.56184211  0.54736842  0.55131579  0.53881579],
mean = 0.554548471241
******** gamma=2.8072162039411812e-09
[ 0.56587202  0.55657895  0.54342105  0.54671053  0.53421053],
mean = 0.549358614541
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54802632  0.53815789  0.53881579  0.52631579],
mean = 0.541430033679
******** gamma=3.8566204211634722e-09
[ 0.55144291  0.54013158  0.53355263  0.53026316  0.51776316],
mean = 0.534630687446
******** gamma=4.5203536563602409e-09
[ 0.53889586  0.53223684  0.52434211  0.52302632  0.50921053],
mean = 0.525542329789
******** gamma=5.2983169062837021e-09
[ 0.53074028  0.52565789  0.51315789  0.51118421  0.50131579],
mean = 0.516411213102
******** gamma=6.2101694189156032e-09
[ 0.51693852  0.51973684  0.50526316  0.50263158  0.49276316],
mean = 0.507466651258
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.5125      0.49539474  0.49539474  0.48421053],
mean = 0.499507528231
******** gamma=8.5316785241728148e-09
[ 0.5         0.49276316  0.48618421  0.48355263  0.46842105],
mean = 0.486184210526
******** gamma=1e-08
[ 0.48996236  0.48355263  0.47697368  0.475       0.45657895],
mean = 0.476413524401
******* C=1.0000000000000001e-17 (~58%)
******** gamma=1e-10
[ 0.52258469  0.53486842  0.49934211  0.5125      0.525     ],
mean = 0.518859043783
******** gamma=1.1721022975334794e-10
[ 0.53764115  0.54276316  0.51315789  0.52368421  0.53486842],
mean = 0.530422967708
******** gamma=1.3738237958832609e-10
[ 0.54642409  0.55131579  0.525       0.53092105  0.54539474],
mean = 0.539811133857
******** gamma=1.6102620275609427e-10
[ 0.55959849  0.55723684  0.53552632  0.54144737  0.55592105],
mean = 0.54994601466
******** gamma=1.8873918221350996e-10
[ 0.5727729   0.5625      0.53684211  0.54802632  0.55921053],
mean = 0.555870369147
******** gamma=2.2122162910704502e-10
[ 0.58030113  0.57368421  0.54736842  0.55197368  0.56184211],
mean = 0.563033910057
******** gamma=2.5929437974046672e-10
[ 0.58343789  0.57565789  0.55394737  0.56052632  0.56776316],
mean = 0.568266525787
******** gamma=3.0391953823131951e-10
[ 0.5859473   0.58421053  0.5625      0.56381579  0.57236842],
mean = 0.573768407845
******** gamma=3.5622478902624368e-10
[ 0.59222083  0.58289474  0.56118421  0.56578947  0.57697368],
mean = 0.575812586674
******** gamma=4.175318936560409e-10 (~58%)
[ 0.59222083  0.57894737  0.56447368  0.57171053  0.57236842],
mean = 0.575944165621
******** gamma=4.8939009184774994e-10
[ 0.59033877  0.57828947  0.56447368  0.57039474  0.56776316],
mean = 0.574251964604
******** gamma=5.7361525104486815e-10
[ 0.58782936  0.57894737  0.56710526  0.56513158  0.56513158],
mean = 0.572829029915
******** gamma=6.7233575364993349e-10
[ 0.58281054  0.57763158  0.57302632  0.5625      0.55986842],
mean = 0.571167371063
******** gamma=7.8804628156699043e-10
[ 0.58406524  0.57894737  0.57236842  0.56315789  0.55855263],
mean = 0.571418312091
******** gamma=9.2367085718738469e-10
[ 0.58657465  0.58092105  0.57039474  0.56184211  0.55723684],
mean = 0.57139387836
******** gamma=1.0826367338740564e-09
[ 0.5846926   0.58223684  0.56842105  0.56118421  0.55328947],
mean = 0.569964835237
******** gamma=1.2689610031679233e-09
[ 0.58720201  0.57960526  0.56381579  0.55986842  0.55065789],
mean = 0.56822987519
******** gamma=1.4873521072935119e-09
[ 0.58908407  0.57434211  0.55855263  0.55592105  0.54868421],
mean = 0.565316813049
******** gamma=1.7433288221999873e-09
[ 0.58908407  0.56973684  0.55460526  0.55394737  0.54473684],
mean = 0.562422076207
******** gamma=2.0433597178569395e-09
[ 0.58406524  0.56907895  0.55263158  0.55197368  0.54144737],
mean = 0.559839364723
******** gamma=2.3950266199874909e-09
[ 0.57340025  0.56184211  0.54736842  0.55131579  0.53881579],
mean = 0.554548471241
******** gamma=2.8072162039411812e-09
[ 0.56587202  0.55657895  0.54407895  0.54671053  0.53486842],
mean = 0.549621772436
******** gamma=3.2903445623126709e-09
[ 0.55583438  0.54802632  0.53815789  0.53947368  0.52631579],
mean = 0.541561612626
******** gamma=3.8566204211634722e-09
[ 0.55144291  0.54078947  0.53355263  0.53026316  0.51776316],
mean = 0.534762266394
******** gamma=4.5203536563602409e-09
[ 0.53952321  0.53223684  0.525       0.52302632  0.50921053],
mean = 0.525799379251
******** gamma=5.2983169062837021e-09
[ 0.53136763  0.52631579  0.51381579  0.51184211  0.50131579],
mean = 0.516931420458
******** gamma=6.2101694189156032e-09
[ 0.51819322  0.51973684  0.50526316  0.50263158  0.49276316],
mean = 0.507717592287
******** gamma=7.2789538439831614e-09
[ 0.51003764  0.5125      0.49605263  0.49605263  0.48421053],
mean = 0.499770686126
******** gamma=8.5316785241728148e-09
[ 0.5         0.49276316  0.48618421  0.48355263  0.46907895],
mean = 0.486315789474
******** gamma=1e-08
[ 0.48996236  0.48355263  0.47763158  0.475       0.45657895],
mean = 0.476545103348
******* C=9.9999999999999998e-17 (~54%)
******** gamma=1e-10
[ 0.48180678  0.47302632  0.45460526  0.46315789  0.47105263],
mean = 0.468729776134
******** gamma=1.1721022975334794e-10
[ 0.49435383  0.48157895  0.47171053  0.47236842  0.47302632],
mean = 0.478607607475
******** gamma=1.3738237958832609e-10
[ 0.49749059  0.4875      0.48421053  0.4875      0.47894737],
mean = 0.48712969689
******** gamma=1.6102620275609427e-10
[ 0.50627353  0.50460526  0.49934211  0.49539474  0.50328947],
mean = 0.501781020934
******** gamma=1.8873918221350996e-10
[ 0.51756587  0.51052632  0.49802632  0.50131579  0.50789474],
mean = 0.507065805983
******** gamma=2.2122162910704502e-10
[ 0.52823087  0.51973684  0.50460526  0.50197368  0.51644737],
mean = 0.514198804728
******** gamma=2.5929437974046672e-10
[ 0.53011292  0.52434211  0.50921053  0.51118421  0.52763158],
mean = 0.520496268903
******** gamma=3.0391953823131951e-10
[ 0.53324969  0.52763158  0.51907895  0.51842105  0.53026316],
mean = 0.525728884633
******** gamma=3.5622478902624368e-10
[ 0.53826851  0.52631579  0.51513158  0.52171053  0.5375    ],
mean = 0.527785280328
******** gamma=4.175318936560409e-10 (~54%)
[ 0.54265997  0.53552632  0.52039474  0.525       0.53289474],
mean = 0.531295152876
******** gamma=4.8939009184774994e-10
[ 0.53826851  0.53881579  0.51776316  0.52302632  0.52960526],
mean = 0.529495806643
******** gamma=5.7361525104486815e-10
[ 0.52948557  0.52697368  0.51513158  0.52039474  0.52960526],
mean = 0.52431816681
******** gamma=6.7233575364993349e-10
[ 0.5370138   0.52039474  0.51842105  0.50657895  0.52171053],
mean = 0.520823812983
******** gamma=7.8804628156699043e-10
[ 0.53262233  0.5125      0.50592105  0.50855263  0.52039474],
mean = 0.515998150961
******** gamma=9.2367085718738469e-10
[ 0.53513174  0.52039474  0.50723684  0.50723684  0.51644737],
mean = 0.517289506703
******** gamma=1.0826367338740564e-09
[ 0.52823087  0.525       0.51578947  0.51842105  0.51118421],
mean = 0.519725120518
******** gamma=1.2689610031679233e-09
[ 0.5370138   0.52565789  0.51513158  0.51644737  0.5125    ],
mean = 0.521350128772
******** gamma=1.4873521072935119e-09
[ 0.54956085  0.52434211  0.51710526  0.51052632  0.51644737],
mean = 0.523596381166
******** gamma=1.7433288221999873e-09
[ 0.5489335   0.51842105  0.51513158  0.50855263  0.51184211],
mean = 0.52057617381
******** gamma=2.0433597178569395e-09
[ 0.5489335   0.51184211  0.50855263  0.50921053  0.51184211],
mean = 0.51807617381
******** gamma=2.3950266199874909e-09
[ 0.5476788   0.53486842  0.51907895  0.51776316  0.51776316],
mean = 0.527430495939
******** gamma=2.8072162039411812e-09
[ 0.53826851  0.525       0.5125      0.51381579  0.51052632],
mean = 0.520022122433
******** gamma=3.2903445623126709e-09
[ 0.53952321  0.52763158  0.51315789  0.52105263  0.51381579],
mean = 0.523036221356
******** gamma=3.8566204211634722e-09
[ 0.5370138   0.52368421  0.51513158  0.51842105  0.50394737],
mean = 0.519639602457
******** gamma=4.5203536563602409e-09
[ 0.5250941   0.51776316  0.50263158  0.51513158  0.49736842],
mean = 0.511597767946
******** gamma=5.2983169062837021e-09
[ 0.51882058  0.50921053  0.49210526  0.49342105  0.48881579],
mean = 0.500474641749
******** gamma=6.2101694189156032e-09
[ 0.51066499  0.50263158  0.49210526  0.48223684  0.47894737],
mean = 0.493317209272
******** gamma=7.2789538439831614e-09
[ 0.50313676  0.49407895  0.47763158  0.47171053  0.46381579],
mean = 0.482074720993
******** gamma=8.5316785241728148e-09
[ 0.48745295  0.47828947  0.475       0.46644737  0.44539474],
mean = 0.470516905501
******** gamma=1e-08
[ 0.47553325  0.4625      0.46447368  0.45855263  0.43026316],
mean = 0.458264544674
***** parameter search c: -25..-15, gamma: -14..-7 (git: 05ed6f0) (~48%)
****** call
       #+BEGIN_SRC python
         cstart, cstop = -25, -15
         Cs = np.logspace(cstart, cstop, base=10, num=(abs(cstart - cstop)+1))
         gstart, gstop = -14, -7
         Gs = np.logspace(gstart, gstop, base=10, num=(abs(gstart - gstop)+1))
         for c in Cs:
             for gamma in Gs:
                 test(X, y, svm.SVC(C=c, gamma=gamma))
       #+END_SRC
****** result (~48%)
******* C=1e-25 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38371336  0.40521173  0.4         0.41172638  0.40156454]
******** gamma=1e-07
[ 0.15114007  0.16938111  0.15960912  0.18306189  0.16818774]
******* C=1e-24 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38436482  0.40521173  0.4         0.41172638  0.40156454]
******** gamma=1e-07
[ 0.15179153  0.16938111  0.15960912  0.18306189  0.16883963]
******* C=1e-23 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38436482  0.40521173  0.4         0.41172638  0.40221643]
******** gamma=1e-07
[ 0.152443    0.17003257  0.16026059  0.18371336  0.16883963]
******* C=1e-22 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38436482  0.40456026  0.4         0.41172638  0.40221643]
******** gamma=1e-07
[ 0.15309446  0.17003257  0.16156352  0.18371336  0.16883963]
******* C=1e-21 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38436482  0.40456026  0.4         0.41172638  0.40221643]
******** gamma=1e-07
[ 0.15309446  0.17003257  0.16221498  0.18371336  0.16949153]
******* C=1e-20 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38436482  0.40456026  0.4         0.41237785  0.40221643]
******** gamma=1e-07
[ 0.15309446  0.17068404  0.16221498  0.18371336  0.16949153]
******* C=1e-19 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38501629  0.40456026  0.4         0.41237785  0.40221643]
******** gamma=1e-07
[ 0.15374593  0.1713355   0.16221498  0.18371336  0.16949153]
******* C=1e-18 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38501629  0.40456026  0.40065147  0.41237785  0.40286832]
******** gamma=1e-07
[ 0.15439739  0.1713355   0.16221498  0.18566775  0.17014342]
******* C=1e-17 (~48%)
******** gamma=1e-14
[ 0.28534202  0.29511401  0.27296417  0.25863192  0.25684485]
******** gamma=1e-13
[ 0.29250814  0.29967427  0.27361564  0.26319218  0.26857888]
******** gamma=1e-12
[ 0.30879479  0.30488599  0.28859935  0.28208469  0.28552803]
******** gamma=1e-11
[ 0.35830619  0.34723127  0.34006515  0.32964169  0.34680574]
******** gamma=1e-10
[ 0.44820847  0.41693811  0.44429967  0.42801303  0.45632334]
******** gamma=1e-09 (~48%)
[ 0.47687296  0.47491857  0.48078176  0.4723127   0.48696219]
mean = 0.47622149749999998
******** gamma=1e-08
[ 0.38501629  0.40456026  0.40065147  0.41237785  0.40286832]
******** gamma=1e-07
[ 0.15439739  0.1713355   0.16221498  0.18566775  0.17014342]
******* C=1e-16
******** gamma=1e-14
[ 0.28664495  0.29511401  0.27296417  0.25798046  0.25814863]
******** gamma=1e-13
[ 0.28859935  0.30032573  0.27035831  0.26188925  0.26857888]
******** gamma=1e-12
[ 0.31661238  0.30423453  0.28990228  0.28338762  0.29074316]
******** gamma=1e-11
[ 0.3465798   0.34918567  0.34136808  0.33745928  0.34028683]
******** gamma=1e-10
[ 0.40260586  0.36286645  0.39674267  0.38827362  0.41395046]
******** gamma=1e-09
[ 0.44429967  0.42149837  0.41824104  0.44364821  0.46936115]
******** gamma=1e-08
[ 0.3752443   0.40325733  0.37459283  0.40065147  0.38591917]
******** gamma=1e-07
[ 0.1504886   0.15960912  0.15570033  0.17915309  0.16101695]
******* C=1e-15
******** gamma=1e-14
[ 0.27687296  0.26384365  0.24234528  0.23713355  0.23207301]
******** gamma=1e-13
[ 0.25472313  0.26775244  0.23778502  0.2228013   0.24902216]
******** gamma=1e-12
[ 0.27491857  0.247557    0.2762215   0.23648208  0.29595828]
******** gamma=1e-11
[ 0.34462541  0.33159609  0.36351792  0.32052117  0.33376793]
******** gamma=1e-10
[ 0.31661238  0.30944625  0.32703583  0.31726384  0.35658409]
******** gamma=1e-09
[ 0.39543974  0.36156352  0.38762215  0.37198697  0.40482399]
******** gamma=1e-08
[ 0.35765472  0.35700326  0.33029316  0.36872964  0.34615385]
******** gamma=1e-07
[ 0.14267101  0.15765472  0.13745928  0.16482085  0.15319426]
***** grid (~1-2%)
****** call
       #+BEGIN_SRC python
         >>> tester=svm.SVC()
         >>> Cs=np.logspace(-2, 20, 11, base=2)
         >>> Gs=np.logspace(20, -2, 11, base=2)
         >>> from sklearn import grid_search
         >>> clf=grid_search.GridSearchCV(estimator=tester, param_grid=dict(gamma=Gs, C=Cs))
         >>> clf.fit(X, y)
       #+END_SRC
****** result
       gamma = 1
       C = 1
       score \approx 1-2%
****** result
[ 0.5         0.51973684  0.52236842  0.52039474  0.48947368]
mean = 0.510394736842
**** scikit-learn DecisionTreeClassifier
***** (git: 6cea) (~57%)
****** result
       [ 0.57841907  0.57434211  0.56578947  0.57697368  0.54473684]
       mean = 0.568052235356
***** (git: f95a6) (~55%)
****** call
       from sklearn import tree
       test(X, y, tree.DecisionTreeClassifier())
****** result
       [ 0.54516939  0.56513158  0.55592105  0.54671053  0.52828947]
       mean = 0.548244403355
**** scikit-learn *svc linear* (~51%) (git: f956a6)
***** default (C=1.0)
****** call
       test(X, y, svm.SVC(kernel='linear'))
**** sh-liblinear-ovo (multiclass one versus one) (~37%)
***** built via (Xubuntu 15.10)
      make
***** called via
      - liblinear-ovo-1.96/train -M 1 train
      - liblinear-ovo-1.96/predict test train.model output
***** result
      Accuracy = 36.7188% (282/768)
**** scikit-learn RandomForestClassifier (git: f95a6) (~35%)
***** call
      from sklearn import ensemble
      test(X, y, ensemble.RandomForestClassifier())
***** result
      [ 0.34629862  0.35        0.35526316  0.33092105  0.34868421]
      mean = 0.346233408175
**** scikit-learn ExtraTreesClassifier (~32%)
***** (git: f95a6)
****** call
       test(X, y, ensemble.ExtraTreesClassifier())
****** result (~32%)
       [ 0.33249686  0.31315789  0.32105263  0.32039474  0.30855263]
       mean = 0.319130951595
***** (git: ~abbf)
****** call
       test(X, y, ensemble.ExtraTreesClassifier(n_estimators=250))
****** result (~54%)
[ 0.5476788   0.55065789  0.53552632  0.52434211  0.52828947]
mean = 0.537298916991
***** feature-importances (git: ~abbf)
****** call
       "  ".join([str(x) for x in forest.feature_importances_])
****** result
'0.00683596157478  0.00539915275747  0.00536343497146  0.00370537302315  0.00810352527418  0.00791920372113  0.0109320373658  0.00646527293151  0.00521576109468  0.00541254699919  0.00363142605961  0.00337930694992  0.00332704182245  0.00270043179681  0.00341258392872  0.00254278820784  0.00334809099516  0.00249162772281  0.00337707949373  0.00256136978974  0.00347062565093  0.00266079382551  0.00352567796399  0.00277128335441  0.00361878763893  0.0029360297592  0.00364739231526  0.00285518510725  0.00363547520583  0.00291570107366  0.00380223621404  0.00285494850251  0.00372564280077  0.00275106311148  0.00369242327453  0.0027751070071  0.00350139577878  0.00274490672909  0.00364153715698  0.00274209320444  0.00343063702193  0.00264032264471  0.00351467836469  0.00256766329318  0.00330031324307  0.00237734972875  0.0032485434477  0.00217093611847  0.00326633059935  0.00213706375827  0.00338989352383  0.00216119833527  0.00302231645638  0.0021130634216  0.00296010672967  0.00199757399201  0.00298668860863  0.00187268417788  0.00288589533914  0.00191935184926  0.00291142054056  0.00185816628035  0.00268684066617  0.00193997978734  0.00298067475761  0.00185392149603  0.00287234395349  0.0017215913057  0.00264629528068  0.00183673851413  0.00273327542232  0.00178997204709  0.00275713993232  0.00175147391636  0.00271617756057  0.00166749452803  0.00250388728694  0.00163031250361  0.00260858123179  0.001560915367  0.00261124902203  0.00146224168766  0.0024996221264  0.00156015628036  0.00243885828532  0.00159571493534  0.00243971248059  0.00144396942517  0.00231813178187  0.00154054809927  0.00240678811972  0.00152506515174  0.00219794209913  0.00142888470998  0.00215825257466  0.00140424889301  0.00213969148774  0.00132294019919  0.00202920822102  0.0014732062013  0.00201362591929  0.00131072419475  0.00203850977012  0.00138014367942  0.00198378405096  0.00133215658227  0.00196624491884  0.00116311917632  0.00182539951029  0.00128132350593  0.00192153743354  0.00118008271147  0.00181065693104  0.00117304986898  0.00196596439841  0.00122975695532  0.00183733820541  0.00117527805074  0.00177258339721  0.00113313557285  0.00175511755128  0.00105157602983  0.00170263271952  0.00117191614269  0.00150355279566  0.000927552441905  0.0015772083313  0.001159638692  0.00160540267089  0.00112417239525  0.00158287767377  0.00098301986792  0.00157147787863  0.000862690706039  0.0017292593086  0.000972508784363  0.00155631742167  0.000966273612249  0.00154767918684  0.00099338404107  0.00156366438393  0.0009125579239  0.00149720717038  0.000935560674736  0.00143818856733  0.000827659659671  0.00144116655672  0.00102017716737  0.00138939190743  0.000877082087554  0.00130927201931  0.000820036519723  0.00126799569115  0.00082988814078  0.00126825886023  0.000781572697729  0.00123984391104  0.00083842864115  0.00120401150887  0.000817127987821  0.0012828278105  0.000759010430418  0.00120931135108  0.000743018893792  0.00110615110686  0.000774787044428  0.00107131311918  0.000775552186316  0.00127780145709  0.000684345478777  0.00116340412226  0.000683569817428  0.00106182166573  0.000792809159979  0.00109859051382  0.000651637911226  0.00103190473112  0.000711126891505  0.00102238913887  0.000696421825283  0.000987401938491  0.000727220793108  0.000979504568292  0.000691421259876  0.00100392053014  0.000695385541553  0.000919525864677  0.000646032090978  0.000840553974056  0.000571463696785  0.000989353556259  0.000648218908211  0.000907904929434  0.000536627796134  0.000830547027022  0.000475397210044  0.000877370002038  0.00053582808959  0.000942060649153  0.00057830457309  0.000844107761116  0.000519417829549  0.000753536692068  0.000494558162108  0.000726331062459  0.000541650213358  0.000779674530203  0.000457251111513  0.000816877245747  0.000495074551661  0.000805225181882  0.00046927400624  0.000718607829543  0.000495936050764  0.000701368234825  0.000477917678917  0.000666007845823  0.000486825565799  0.000689230413461  0.00048113220602  0.000762338359295  0.000534156701216  0.000766689427167  0.000443998201937  0.000699524139698  0.000437076026749  0.000592945813656  0.00042699029707  0.000664458132209  0.00046650926102  0.000698956957299  0.000467055590083  0.000555608165937  0.000416577822479  0.000658655195279  0.000424323998044  0.000604504682292  0.000410778332687  0.000683326514518  0.000374734042478  0.000617795112919  0.000392865167741  0.00062076314639  0.000401448212822  0.000564727085912  0.000412281188479  0.000568414364784  0.000416389602955  0.00063359698489  0.000373243819007  0.000565239096761  0.000356907671349  0.000562012661275  0.000397414556718  0.000544243776143  0.000329763423912  0.000559185272438  0.000354466285971  0.000594281799171  0.00034701666615  0.000517875943097  0.000425354935037  0.000552409266679  0.000340404591703  0.000580467410725  0.000394950758255  0.000539123313463  0.000330613194144  0.000527575444344  0.00038812229983  0.000510729247167  0.000330042193247  0.000516152396994  0.0003421973512  0.000535210784331  0.000346398581341  0.00049280280407  0.000302759818344  0.00044996165864  0.000316990360834  0.000536977294011  0.000322442530132  0.000500143402495  0.000346969860723  0.000484914963222  0.000266517717285  0.000607984642939  0.000286282546285  0.000433800941055  0.000318286226926  0.000448644958879  0.000350810168286  0.000467324640736  0.000309185432478  0.000463773281055  0.000273963036874  0.000440204457716  0.000260141671121  0.000469389859607  0.000253763125905  0.000427067895776  0.000277469833646  0.000419921201362  0.000271722929453  0.000424971670651  0.00020502424034  0.000436735423326  0.000265967847942  0.000350745692751  0.000241263009507  0.000336197391493  0.000222818148835  0.000369001909987  0.000205991059891  0.000391543319361  0.000250527264312  0.000385704300018  0.00023685358054  0.000353566386087  0.000185505835459  0.000332073619202  0.00022608842817  0.000447355797089  0.000214783406408  0.00036567575375  0.000257420991535  0.000437293742104  0.000209426598142  0.000369582747822  0.00016688278738  0.0002926733719  0.000222309225362  0.000301704177829  0.000256654496805  0.000304428258933  0.000257667789989  0.000387933395556  0.000307592128203  0.000418468044704  0.000251407352294  0.000362514557189  0.00020104162821  0.000398517064585  0.000205188504979  0.000377387677501  0.000198325964499  0.0003137486988  0.000189252809116  0.000267966416821  0.000193410147899  0.000236884549905  0.00021920925849  0.000449946810462  0.000150022523574  0.000267818594364  0.00017208373496  0.000278748279494  0.000198746463525  0.000228367268287  0.000172863956876  0.000307031019232  0.000211067214564  0.000281661007556  0.000235250145173  0.000209301394252  0.00019906951802  0.000295754541323  0.000183285553432  0.000241492268459  0.000137773697609  0.000276378688915  0.000152882882559  0.00022768869637  0.000167671711915  0.000258002948669  0.000225433885459  0.000277587053363  0.000140245485613  0.00022442709501  0.000161948990483  0.000242634677707  0.000130147373689  0.000219019625731  0.000123529899972  0.000290132978729  0.000200581626926  0.000260887575695  0.000148948951677  0.00025388868728  0.000156575418419  0.00018783236659  0.000224408359979  0.00019577093087  0.000138442021571  0.000163230101179  0.000147282298678  0.0001971867223  0.000112359617878  0.000221750464788  0.000112532415516  0.00023112372726  0.000109952229308  0.000260066547713  0.000109513502876  0.000203119144649  0.000178781494797  0.000169664594564  0.000138374650602  0.000181932601964  9.78433751798e-05  0.00012687061382  0.00013193470466  0.000189820756698  0.0001251786774  0.000182269823226  0.000187175838937  0.000144612506247  0.000132008723381  0.000174060117399  8.34409555778e-05  0.000144268131801  9.44051649662e-05  0.000190079669099  0.000101259795734  0.000139259062047  0.000121936898596  0.000228702777197  6.90016044956e-05  0.000140996549891  9.42916350566e-05  0.000110942601413  0.000107603263727  0.000154010855397  0.00013365092352  0.00018413862868  0.000159682129399  0.00015515344055  0.000143714820252  0.000212374868677  0.000150842919512  0.00018516523543  7.17628481383e-05  0.000136700210715  0.000178946000697  0.0001756114903  0.000118244157792  0.000160579852988  7.7713214969e-05  0.000133712653591  9.40968372812e-05  0.000219605654389  8.45154994051e-05  0.000195499806125  9.13539119429e-05  0.000120202811111  7.76933090018e-05  0.000189363115029  8.43303494491e-05  0.000126870660962  0.000141165942965  0.000101213473215  8.75736454698e-05  0.000127526082095  9.74179433782e-05  0.000104637623596  0.000141087951193  0.000179575367732  7.66387648161e-05  0.000165498836558  0.000101500856886  0.000184499366679  0.000112903456413  9.57838271185e-05  6.14759390899e-05  0.000118709090716  5.88238638295e-05  7.85464232918e-05  4.31329269864e-05  0.000140839432549  5.15288809964e-05  0.000161609463152  9.25177070252e-05  8.15359396994e-05  8.43088294356e-05  0.00013973780664  5.71953378573e-05  0.000100518620298  6.36334949634e-05  0.000126937140738  8.56335694063e-05  9.55640567025e-05  6.3376706999e-05  0.000154799170239  7.40330628214e-05  0.000126125311693  6.41001186216e-05  0.000145673742415  5.76778926182e-05  9.89458299001e-05  6.15720965438e-05  0.000104038513771  8.59468075618e-05  0.000117181631488  7.63222032805e-05  0.000126049430852  7.04896601695e-05  8.2453134371e-05  4.0776919831e-05  9.73071546527e-05  9.1939424112e-05  8.24269347599e-05  5.48429474981e-05  8.17883165618e-05  5.94953065332e-05  0.000103153355504  9.5342758554e-05  0.000137983270857  5.98502792822e-05  0.000128298809041  5.12740872022e-05  9.48204575584e-05  8.87355581863e-05  9.03879288115e-05  6.46063043976e-05  8.77825628792e-05  0.000119874503039  0.000115365615861  5.11388441054e-05  9.08887958532e-05  3.50951146606e-05  0.000167016939566  4.94347223134e-05  0.00013939898298  4.55039631158e-05  9.74853617132e-05  4.6455197588e-05  0.000146942049606  5.63427470897e-05  0.000162314120315  4.9516700456e-05  0.000131659222559  8.4676802924e-05  0.000102142420997  3.52088979232e-05  7.2051109842e-05  6.64431156626e-05  5.99351229285e-05  4.60899942092e-05  0.000122978034438  5.679528886e-05  0.000123212858697  5.82810638781e-05  0.000177532554485  4.07946662149e-05  7.88688524327e-05  7.07588142909e-05  6.9993959275e-05  4.61707634165e-05  6.86602847839e-05  6.24682376004e-05  4.34319199597e-05  4.82793386952e-05  0.000103719084143  4.85366050329e-05  8.01566904531e-05  0.000108129690673  7.39546225638e-05  3.78046743151e-05  7.27572732082e-05  7.29279902965e-05  0.00015573943495  0.000100985148653  7.68578629711e-05  3.57850415184e-05  0.000137684504259  4.88331254784e-05  6.87282765314e-05  5.56001614916e-05  0.000108106540132  3.84361503368e-05  0.000104482904604  2.97043664593e-05  6.06338870209e-05  4.91984439627e-05  0.000125887840728  4.26869645629e-05  7.8743361797e-05  7.84890051534e-05  7.78588827356e-05  4.20095538403e-05  0.000109164384009  8.96700912707e-05  0.000107394702448  5.43606236764e-05  7.40432669406e-05  7.57194590776e-05  0.000102701316466  2.88756562524e-05  8.08731337628e-05  2.54562181735e-05  7.63143906039e-05  5.72485652223e-05  9.15716418693e-05  7.10351734485e-05  5.42139332846e-05  2.63353742521e-05  4.59403595481e-05  0.000110841035378  4.53702042913e-05  7.03649990024e-05  6.46288845893e-05  4.85588112271e-05  7.79329330249e-05  4.78089302531e-05  9.07170764022e-05  5.30066323488e-05  6.89311182594e-05  3.18998441814e-05  5.04541902247e-05  2.85950729691e-05  8.65175617098e-05  5.56643329949e-05  9.46948495924e-05  5.20871595634e-05  8.59917661586e-05  3.09014535051e-05  4.44820074334e-05  1.65471380494e-05  3.74454134508e-05  3.61073948675e-05  6.78198380301e-05  3.7745126862e-05  6.4594086635e-05  2.2050898567e-05  4.56309799149e-05  4.04844883371e-05  3.57920768534e-05  6.83830248272e-05  4.3775187167e-05  2.94119692175e-05  8.00107655561e-05  5.45612889713e-05  3.85150681029e-05  3.00622403562e-05  3.03471101992e-05  1.98137677164e-05  3.34337483308e-05  2.12849293276e-05  3.58288582704e-05  2.81291416841e-05  4.92567355444e-05  9.09689112289e-05  2.88172884431e-05  5.7976826946e-05  2.57448963449e-05  2.73091665177e-05  3.33636187552e-05  1.97260022262e-05  4.55457806625e-05  1.19151046625e-05  2.81587673921e-05  3.55788222309e-05  5.37218566819e-05  3.31555639258e-05  4.29304265442e-05  3.08428338131e-05  4.80561851577e-05  2.50637237043e-05  3.37550489423e-05  3.14198940154e-05  5.065341886e-05  4.97810954623e-05  3.29601535799e-05  4.31043037177e-05  3.15824709994e-05  1.56882235729e-05  3.17077583936e-05  2.16935875581e-05  6.03246584761e-05  2.02785491428e-05  2.92740592309e-05  3.04902470864e-05  4.07442310783e-05  2.55394660109e-05  4.15877560739e-05  3.81107050181e-05  4.37327749946e-05  1.14085736117e-05  4.93238747818e-05  5.04559478883e-05  5.87991235027e-05  1.55491089479e-05  2.96284813718e-05  1.73775252742e-05  3.39990274794e-05  2.59101871298e-05  3.61292952947e-05  1.54258609023e-05  3.63791681993e-05  2.38871290001e-05  6.60864129852e-05  3.3723216547e-05  2.04821146426e-05  5.67333375442e-05  2.6240805589e-05  1.98104689555e-05  3.42635155466e-05  1.73542995012e-05  3.86189548595e-05  5.7413962489e-05  3.78051566584e-05  1.17802211482e-05  7.20135225959e-05  2.64724026839e-05  4.21310816637e-05  7.52094764891e-06  2.59114921357e-05  1.31853635524e-05  0.000113100230909  2.80414254741e-05  4.48576440034e-05  6.4019995417e-05  5.86739666396e-05  3.16877361567e-05  2.93735673133e-05  5.58378599552e-05  9.54195311501e-05  1.46997762453e-05  2.87345985751e-05  1.68624623133e-05  4.46997630803e-05  2.21244797426e-05  4.5204555409e-05  4.66321353246e-05  3.40601854428e-05  1.42709280492e-05  2.45135879039e-05  2.70595826385e-05  2.95081340283e-05  1.42061771245e-05  7.72363177889e-05  5.21977918546e-06  1.56913406301e-05  1.73219100535e-05  2.03712778984e-05  5.50490476101e-05  2.40779178113e-05  2.2542747399e-05  2.80376189526e-05  5.23379202273e-05  3.37119706511e-05  1.85459198639e-05  5.98903731124e-05  1.30584696271e-05  1.63289548547e-05  1.60186378628e-05  2.41103586071e-05  1.85961444055e-05  3.4394649786e-05  1.32575465416e-05  2.83132613592e-05  8.37507399127e-06  3.56448982367e-05  1.47753126862e-05  2.60577828416e-05  1.41335576175e-05  2.79110422755e-05  1.16421398037e-05  3.15521598256e-05  1.12300320556e-05  2.87317640089e-05  2.48694867673e-05  0.000115317430297  9.31933380362e-06  2.53429591468e-05  1.06252540844e-05  5.53409412347e-05  1.72889541412e-05  2.74716438159e-05  1.24192301182e-05  1.76918349824e-05  1.15769073852e-05  2.20387395504e-05  1.28507789529e-05  3.77897854094e-05  1.61809659149e-05  9.44765781092e-05  2.42950876925e-05  5.50346875153e-05  1.49990522574e-05  6.3819834337e-05  1.72780543387e-05  7.57647678073e-06  4.60141423024e-05  8.73368580822e-06  5.57139858495e-05  5.67186984671e-05  3.28062865314e-05  4.76436807542e-05  5.55390033753e-05  2.21066006387e-05  1.46964556961e-05  2.37813304538e-05  1.51216688074e-05  5.04467667069e-05  1.25714370429e-05  1.65928434872e-05  1.666225728e-05  5.74696218779e-05  5.1823564305e-05  5.81536164342e-05  1.29819970606e-05  3.65466873562e-05  1.41911193415e-05  7.2943458737e-05  2.0920040804e-05  2.88673438403e-05  1.67105148012e-05  2.16469745899e-05  1.31097211243e-05  3.18491018824e-05  1.46696399426e-05  5.39765553994e-05  5.64869983153e-05  1.27964462016e-05  1.58931829108e-05  0.000117362838475  1.59009250992e-05  1.91203459939e-05  4.40174145388e-05  5.97982479768e-05  2.92501933752e-05  1.38991033166e-05  6.15612657281e-05  2.03133289719e-05  1.61613556644e-05  6.410274655e-05  2.3545577357e-05  3.96974556159e-05  1.28339540139e-05  1.64396922087e-05  4.30260353043e-05  3.52228216228e-05  1.31636136693e-05  2.5389716428e-05  5.76621282013e-05  5.89187804231e-05  6.20633445808e-06  1.33577776077e-05  1.62502182026e-05  4.10695632677e-05  2.7482676225e-05  4.79841436447e-06  1.09531961271e-05  5.8828238393e-05  1.39167170848e-05  2.2610396181e-05  1.6454372196e-05  1.68904444045e-05  1.4553503335e-05  6.85570292876e-05  5.00398986052e-05  6.41234519252e-05  6.49214402627e-06  9.33069348121e-05  4.9836455494e-05  1.85856516843e-05  1.11851492375e-05  6.97516988157e-05  4.7430066898e-05  1.96444127444e-05  8.83283401588e-05  9.64524726661e-05  5.64568348771e-05  8.19092504091e-06  1.08060027796e-05  1.27553917751e-05  1.20525201563e-05  2.23103651182e-05  4.79608531223e-05  6.30941272926e-05  1.30661860442e-05  2.4352628685e-05  0.000122162009992  3.35325182147e-05  1.39184152027e-05  2.17167780732e-05  6.44210808894e-05  4.62565323957e-05  1.77311904963e-05  2.57185936889e-05  0.000122232690884  1.84346977828e-05  1.70844048698e-05  5.689754677e-05  8.44610408028e-06  2.20506585123e-05  1.199282151e-05  6.19262131564e-05  4.91569995072e-05  1.6525241711e-05  7.69923317693e-06  6.17829893382e-05  5.36161400204e-05  1.39477546724e-05  1.3584019796e-05  3.73115240894e-05  9.19425287164e-06  1.38322573596e-05  9.59122443998e-05  5.46267397202e-05  3.80125005136e-05  6.1706026425e-05  3.01980004178e-05  0.00010840053838  4.91762543121e-05  3.2707640705e-05  1.55530324255e-05  0.000113352825642  5.08103939761e-05  1.84982459324e-05  7.6935690219e-05  5.45521434117e-05  1.66392542454e-05  1.22852758106e-05  1.13745307964e-05  1.49308231381e-05  1.52681256379e-05  8.62139238731e-05  5.99040426253e-05  6.5548268692e-05  8.83950099994e-05  1.33849246535e-05  8.7910173066e-06  7.20084648713e-05  6.65459436378e-06  3.81883806806e-05  7.50755629917e-06  5.40174261367e-05  9.06970303954e-05  9.07179166709e-05  1.03253788491e-05  5.77948841666e-05  9.61330987709e-05  9.53897483179e-05  8.90946344971e-06  5.58094448067e-05  1.77357210544e-05  0.000143352924503  1.64245404934e-05  2.27262800684e-05  2.35239632162e-05  0.000100804636823  7.2286937053e-05  7.17020742147e-05  4.9657530849e-05  5.4008954634e-05  1.75540572213e-05  6.09694882882e-05  5.55012512402e-05  6.3295717624e-05  8.80983301574e-06  5.28578973738e-05  5.34215258061e-05  1.74626173608e-05  5.49528558505e-05  0.000102069229937  5.23515579373e-05  7.94891694889e-06  8.51994728227e-05  5.73711414526e-05  4.7299468232e-05  2.7972542058e-05  2.46779760766e-05  9.53842668221e-05  2.48204838033e-05  0.000138197635873  4.58601869083e-06  1.41677305975e-05  0.000134404219527  5.58062430911e-05  3.53859266093e-05  5.84953091357e-05  1.58596401758e-05  1.48215450878e-05  5.39180287986e-05  0.000100049344975  5.40174991418e-05  1.80851748144e-05  4.9741723164e-05  1.19286070519e-05  1.2857867595e-05  0.000108228882366  5.82546546496e-05  2.15867031368e-05  7.42612500085e-05  5.93243204499e-05  5.33493882263e-05  0.000143752151214  0.000103854962213  0.000112779521797  1.42590239556e-05  7.62420592391e-06  1.14048665312e-05  5.59852750965e-05  5.87460463904e-05  3.70646417877e-05  5.69635965054e-05  7.48670500719e-06  2.16558082141e-06  2.80718910628e-05  5.78313553885e-05  9.81646419998e-05  1.87896689234e-05  4.33624198568e-05  4.93357431839e-05  1.64727275869e-05  9.4625724066e-06  2.07302479051e-05  1.31524649825e-05  2.56877593086e-05  5.19915873125e-05  2.1207823129e-05  6.18873358861e-05  6.64101598873e-05  1.30764785243e-05  5.09815589308e-05  4.33973043202e-05  3.97134596552e-05  6.5661293701e-06  3.23407659123e-05  2.2204462483e-05  1.89705054484e-05  7.25538523111e-06  5.07491154266e-05  1.90801939379e-05  1.29369265018e-05  2.32992285293e-05  1.94137444742e-05  5.1832088765e-05  4.43244738843e-05  1.70631460845e-05  1.76778484533e-05  1.56605077069e-05  1.82751061766e-05  8.34984968402e-05  5.21469881938e-05  4.29694801999e-05  1.37659881376e-05  4.23429464446e-05  1.58613613741e-05  4.63801175393e-06  2.43066566303e-05  1.37088694495e-05  0.000129496006856  1.11517379548e-05  2.37855352106e-05  1.73444152262e-05  8.63960685582e-05  8.41489911472e-06  1.93015794152e-05  2.73779411809e-05  1.02190843688e-05  2.84757471107e-05  0.000101876713167  1.36858235053e-05  1.21084022481e-05  6.69707880275e-05  9.03410957325e-05  4.09287681949e-05  2.46878484584e-05  4.97130726036e-05  1.5259273419e-05  1.06885852484e-05  8.88787086563e-06  1.08242636967e-05  4.59676992315e-05  6.84326608615e-06  1.40530085877e-05  2.39425860097e-05  5.32269523519e-05  5.19328385356e-05  5.27959656491e-05  4.12138995573e-05  6.50348775347e-06  7.08319916876e-06  1.72822909829e-05  1.0380410475e-05  5.76242960371e-06  6.562145922e-06  1.42807782988e-05  3.65650143605e-05  7.44247712331e-06  8.00093955305e-06  1.47814371077e-05  5.98204324545e-06  5.94097041241e-06  5.6431226931e-06  5.08456511281e-05  6.48733533021e-06  1.15775213917e-05  1.23769337689e-05  3.69733310973e-06  6.98620355563e-06  9.81712475383e-06  1.32879688365e-05  1.80845967843e-05  8.31055025199e-06  8.47058230369e-06  1.19377616014e-05  1.88246098093e-05  1.25259130224e-05  1.61565654959e-05  1.07325093108e-05  1.11155700275e-05  4.22108014809e-05  2.44572677888e-05  1.13036785826e-05  1.20245896997e-05  1.63093651831e-05  1.25285790587e-05  8.90712012099e-06  1.80720894208e-05  1.76272659053e-05  8.52486982821e-06  7.71500553047e-06  1.24658541098e-05  1.67812613244e-05  1.90611062422e-05  4.08171349437e-05  1.56384829209e-05  8.27995346539e-06  8.83906158096e-06  1.78698571641e-05  1.9421982657e-05  4.17984027809e-05  8.38407245884e-06  6.17356695149e-06  1.32175971862e-05  1.99984124105e-05  9.60114165973e-06  4.04931695859e-05  2.17129555662e-05  1.31379715815e-06  9.57697798067e-06  8.56723190551e-06  1.32149770233e-05  3.79416802451e-06  5.78596882779e-06  5.80589448207e-06  2.27128989102e-05  1.16457327741e-05  5.45993053629e-06  7.68698078633e-06  6.49484947402e-06  3.76871385979e-05  8.06504836576e-06  4.05663220641e-05  1.26157178048e-05  1.17647069897e-05  5.29639025156e-06  4.93462835148e-06  5.94134220923e-06  3.96030635869e-06  8.1282801535e-06  8.23988471266e-06  3.75831010284e-06  3.61517015174e-06  3.55061354189e-06  5.08446182403e-06  8.4487132724e-06  2.28781918137e-06  1.60776618194e-05  5.54446441717e-06  3.93228344568e-05  7.33035479946e-06  8.26551936312e-06  4.53328754539e-06  5.51509621436e-06  2.32906833987e-06  9.08425395101e-06  3.60941913724e-06  9.96392590001e-06  2.67982072071e-06  8.48485879784e-06  2.64874897944e-05  1.72511908205e-05  4.10387969409e-06  1.2557457221e-05  1.11958852541e-05  1.05647428125e-05  2.8305139029e-06  3.5831644948e-05  3.19178988594e-05  9.8441646714e-06  7.51214964249e-06  1.05930403979e-05  5.13219457038e-06  7.44181164561e-06  5.89275280035e-06  5.82758195066e-06  5.44555319123e-06  9.42187136942e-06  3.71242426528e-06  5.5042555811e-06  4.52885578053e-06  1.45455522339e-06  4.98291264892e-06  6.6068393036e-06  3.12580396237e-06  1.51705420945e-05  2.02473003628e-06  7.2346780994e-06  7.95919229534e-06  3.13832415861e-06  3.68035116255e-06  8.50961859722e-06  5.65770852148e-06  5.62922967399e-06  6.99073596306e-06  7.79443388903e-06  5.89012374191e-06  5.1176430186e-06  2.95139356567e-06  1.64180974928e-05  5.42824139108e-06  6.69800396974e-06  6.35895602875e-06  1.02226635631e-05  8.86711294732e-06  2.57161109058e-06  5.28114600432e-06  1.65135587857e-05  3.76819097885e-06  2.52001289445e-05  5.49412064165e-06  6.85378996873e-06  7.10240861537e-06  1.22644644368e-05  4.43061711032e-06  8.22060752948e-06  1.05638088849e-06  4.59590785598e-06  5.21960505674e-06  6.18306187735e-06  8.47718929132e-06  3.25282388924e-05  7.65429207718e-06  4.86224702208e-06  6.57217342515e-06  8.1584671725e-06  3.37929611953e-06  6.98711543134e-06  8.00851265834e-06  1.24255720199e-05  1.79584751044e-06  3.48605693203e-06  7.60523666413e-06  6.30731602515e-06  6.03322700537e-06  6.92280377714e-06  3.29948531398e-06  2.26986805521e-05  4.91971670928e-06  1.53197288471e-05  2.99971397895e-06  7.25194004033e-06  9.79148082984e-06  1.99026180994e-05  3.43601783732e-06  1.16961946609e-05  1.4084038707e-05  9.7098416466e-06  8.55781494472e-06  1.5033112644e-06  9.8458443945e-06  1.25643013475e-05  0.0  9.35996406243e-06  8.30936331971e-06  9.75197701542e-06  1.03763062447e-05  5.22693077185e-06  8.77599674847e-06  9.83467461781e-06  2.1949892271e-06  8.30311402141e-06  5.02873807659e-06  1.42715919535e-05  2.47897381833e-06  4.73130591877e-06  8.91196593655e-06  4.83671202615e-06  3.70524445898e-06  8.85487127198e-06  4.98587268321e-06  1.2447284269e-05  2.52101818314e-06  8.72414963972e-06  5.44501398002e-06  4.71527215435e-06  9.53694407823e-06  4.05860622744e-06  1.76653167717e-06  3.66742489013e-06  5.49991711502e-06  7.93234383956e-06  3.26709006929e-06  4.9566443096e-06  8.42774458836e-07  1.3502216444e-05  2.80645189377e-06  7.72995216596e-06  5.307972728e-06  1.61450078631e-05  6.26454445726e-06  7.10110499781e-06  5.28092484556e-06  1.24898677486e-05  2.21839986584e-06  4.41845388417e-06  7.04253925663e-07  6.16114796865e-06  5.71829035713e-06  1.22153289487e-05  3.53810207105e-06  3.55299937075e-06  9.26942914015e-07  9.47743704886e-06  5.33202219731e-06  5.08165752884e-06  6.9469977771e-06  2.86895824057e-06  0.0  4.28114360834e-06  4.31943759994e-06  8.18595047087e-06  1.27618933902e-05  8.9371515398e-06  0.0  1.60657926792e-06  3.15770606327e-06  1.32047611062e-06  1.69461100863e-06  5.81617417858e-06  6.77506285418e-06  1.51424098119e-06  1.11045752922e-06  1.2488775505e-05  4.75210831882e-06  2.0311323447e-06  1.87451015875e-06  4.95618700186e-06  1.05638088849e-06  4.04273271439e-06  4.2967492351e-06  3.99954713203e-06  3.75171048e-06  1.14119277541e-05  1.34636779906e-06  1.31165681874e-05  2.66781708004e-06  1.10538079715e-05  5.82101594052e-06  5.43571370192e-06  2.22784005889e-06  2.16357893958e-05  4.29287145253e-06  7.82039946443e-06  8.92455571126e-06  1.50210552368e-05  4.61830209646e-06  1.04528885394e-05  3.41940432835e-06  9.82674184592e-06  1.47201646427e-06  1.82945962962e-06  2.02473003628e-06  1.44864781937e-05  5.28190444247e-07  1.62847364306e-06  1.8249657016e-06  5.71049359952e-06  3.21711219777e-06  1.18483236747e-05  1.58457133274e-06  3.35043380209e-06  5.85888647516e-06  5.96492877647e-06  1.88094485979e-06  4.86732805706e-06  1.89508151048e-06  4.98027480783e-06  2.73778713602e-06  8.05039846993e-06  1.92551806306e-06  7.35391629508e-06  2.95392593655e-06  3.45249732047e-06  2.34311149851e-06  7.00323416134e-06  0.0  4.45988546116e-06  3.97277979316e-06  4.25387632553e-06  3.39002230581e-06  2.30559320902e-06  2.79320397186e-06  5.27758916107e-06  5.14764104739e-06  4.5730172673e-06  2.48799044051e-06  3.09605912622e-06  3.83371082308e-06  5.43482114377e-06  4.37076881305e-06  1.27767621709e-05  0.0  8.80583327093e-06  2.48249508796e-06  8.11149610809e-07  2.14875122393e-06  9.17995878375e-06  1.82463249232e-06  5.57977172869e-06  3.72545209184e-06  6.0769643919e-06  1.6285872031e-06  4.36373307906e-06  1.97191099186e-06  9.92262981723e-06  1.86669453841e-06  6.47438548583e-06  4.33179044098e-06  1.1278861801e-05  6.97394309504e-07  4.12077024652e-06  4.98005425404e-06  1.94866880884e-06  2.52042876532e-06  6.50728336761e-06  5.76546804339e-06  1.23155611159e-05  4.25362478346e-06  1.30333725132e-05  3.05014803667e-06  9.11476302926e-06  7.82649680578e-06  4.75526680695e-06  5.28190444247e-07  4.52567869515e-06  2.11276177699e-06  9.68224761813e-06  1.68580783456e-06  0.0  1.57702575497e-06  0.0  4.8172729993e-06  4.4185789205e-06  3.12135400624e-06  6.00259543951e-06  2.02473003628e-06  6.45429071842e-06  3.61115031154e-06  4.33678899863e-06  1.03437295332e-06  1.84866655487e-06  9.73239800049e-07  4.26574206978e-06  3.26940252337e-06  9.21282520732e-06  5.17233511162e-06  5.70682972844e-06  2.06920958272e-06  2.75954783477e-06  0.0  2.82656116412e-06  3.73311542316e-06  4.97836071109e-06  4.81622375478e-06  2.26323456163e-06  3.53887597646e-06  6.57329597416e-06  2.17210574133e-06  9.11273623592e-07  3.7740216301e-06  3.10139823354e-06  7.04253925663e-07  2.26205915193e-06  5.28190444247e-07  3.48696032343e-06  5.28190444247e-07  4.03226022019e-06  1.70404298085e-06  1.12556015061e-05  0.0  2.93125647921e-06  3.14902112475e-06  8.45104710796e-07  2.79815175822e-06  4.41913992297e-06  4.68636256951e-06  5.14945668714e-06  1.30776564538e-06  0.0  1.76063481416e-06  9.6130660853e-07  2.86964622695e-06  8.66449218795e-06  2.30598016172e-06  3.51968663997e-06  3.6470292579e-06  5.60946538051e-06  3.82917112141e-06  3.19194788061e-06  9.80925110745e-07  0.0  7.04253925663e-07  1.92944862281e-06  1.43617496983e-06  2.86845544791e-06  2.81301425989e-06  4.90195792522e-06  0.0  5.90563776526e-06  5.28190444247e-07  8.88320292598e-07  5.04486413656e-06  1.66924948333e-06  1.40445862486e-06  9.60346262268e-07  0.0  1.06008182606e-05  8.36301536725e-07  2.63141834892e-06  1.68517903641e-06  1.06392646627e-06  4.15045783284e-06  5.32658336141e-06  3.63822608383e-06  5.33813175512e-06  3.10458605563e-06  4.10350153522e-06  5.7733002247e-06  9.07588716898e-06  2.01215407332e-06  5.40363755118e-06  1.84866655487e-06  0.0  3.50229135694e-06  2.63428314997e-06  7.04253925663e-07  6.3483419085e-06  7.47180726437e-06  5.67236291303e-06  7.92285666371e-07  8.73915098664e-07  0.0  3.96802011693e-06  1.37329515504e-06  5.03345091694e-06  1.89268242522e-06  7.02291570872e-06  2.54343335058e-06  5.09225892106e-06  3.82697985514e-06  2.12163892731e-06  6.17689380634e-06  7.08822869522e-06  2.63810408409e-06  2.78685577369e-06  7.18925882448e-07  3.35531931711e-06  1.50763602994e-06  1.0211714958e-05  6.97698092506e-06  8.42890362771e-06  5.93156884086e-06  1.13969808734e-05  1.18842849956e-06  5.73501061189e-06  1.8219902698e-06  5.91445072052e-06  1.97616857249e-06  8.80317407079e-07  1.34448476718e-06  7.92285666371e-07  3.83378230783e-06  3.58537305031e-06  2.43119237974e-06  4.86694684773e-06  1.63739037717e-06  3.84764662563e-06  1.8172874989e-06  1.83013355682e-06  9.68349147787e-07  1.95079948974e-06  0.0  1.10702510022e-05  4.0056347471e-06  9.24333277433e-07  9.15626841539e-07  3.74208305213e-06  5.29007028268e-06  2.34814076284e-06  1.97125485466e-06  0.0  0.0  8.0994421891e-06  4.76336150483e-06  1.3768710144e-05  0.0  2.69628645825e-06  7.48050765733e-06  1.47893324389e-06  0.0  6.37849181134e-06  5.28190444247e-07  8.80317407079e-07  1.5033112644e-06  3.2720237163e-06  6.79101999747e-07  2.41458488799e-06  1.67260307345e-06  3.76184285584e-06  1.24502033287e-06  0.0  1.45590955786e-06  2.81701570265e-07  9.39005234218e-07  4.71724276924e-06  0.0  1.85006177491e-06  7.04253925663e-07  6.10082097874e-06  1.7166189438e-06  3.96667208033e-06  1.53523486268e-06  1.3763372029e-06  1.4099719967e-06  2.5617236546e-06  4.12022926502e-06  9.78542296711e-07  0.0  8.80317407079e-07  1.99614830229e-06  2.81399747154e-06  0.0  7.24192073264e-06  0.0  1.39090150318e-06  0.0  1.99538612271e-06  0.0  0.0  1.47893324389e-06  1.70194698702e-06  9.50742799645e-07  0.0  6.77167236215e-07  0.0  1.82691753657e-06  1.1808167325e-06  0.0  0.0  5.28190444247e-07  1.37329515504e-06  1.3351701305e-06  0.0  1.14557094158e-06  0.0  2.33343683227e-06  4.88080520036e-06  1.58457133274e-06  5.35516310172e-06  7.79709703413e-07  1.81093866599e-06  0.0  3.61034469655e-06  4.0489444805e-06  0.0  8.45104710796e-07  8.32915700544e-07  2.0531710602e-06  0.0  7.04253925663e-07  0.0  9.24333277433e-07  5.28190444247e-07  1.96950409833e-06  0.0  7.78123169107e-06  0.0  8.45104710796e-07  2.63779026484e-06  9.39005234218e-07  9.35553009092e-07  0.0  0.0  0.0  0.0  5.28190444247e-07  1.49653959203e-06  0.0  7.04253925663e-07  8.85906723949e-07  1.72677645235e-06  0.0  0.0  0.0  0.0  7.92285666371e-07  1.51414594018e-06  0.0  5.28190444247e-07  0.0  7.77880472437e-07  1.7166189438e-06  8.84890484518e-07  0.0  0.0  9.39005234218e-07  0.0  8.80317407079e-07  1.78809425621e-06  0.0  2.78246489916e-06  1.26765706619e-06  0.0  0.0  0.0  0.0  8.36301536725e-07  7.73643650692e-07  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  3.30563416261e-06  0.0  0.0  1.56958677986e-06  1.37518154949e-06  0.0  0.0  8.45104710796e-07  1.0043903781e-06  0.0  7.92285666371e-07  0.0  0.0  5.28190444247e-07  7.04253925663e-07  9.05469332996e-07  7.79709703413e-07  1.84866655487e-06  8.80317407079e-07  0.0  0.0  1.69849476189e-06  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  7.04253925663e-07  0.0  7.04253925663e-07  1.66002711049e-06  0.0  0.0  1.32436322644e-06  8.45104710796e-07  0.0  0.0  2.37449599457e-06  8.88320292598e-07  0.0  0.0  0.0  9.00632424166e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.24142355187e-06  0.0  0.0  0.0  1.61978402903e-06  0.0  1.39245174768e-06  1.49546790128e-06  0.0  0.0  0.0  1.62520136692e-06  0.0  0.0  0.0  2.140379578e-06  0.0  0.0  0.0  8.45104710796e-07  8.56842276224e-07  0.0  0.0  0.0  0.0  0.0  8.80317407079e-07  0.0  2.49229216878e-06  1.2885407865e-06  0.0  7.04253925663e-07  9.00632424166e-07  0.0  7.92285666371e-07  0.0  2.00195259288e-06  0.0  0.0  0.0  1.05638088849e-06  1.49653959203e-06  0.0  0.0  0.0  6.86647577522e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.92160714002e-06  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  8.80317407079e-07  7.79709703413e-07  0.0  0.0  0.0  0.0  0.0  1.40850785133e-06  0.0  7.04253925663e-07  9.85955495929e-07  0.0  0.0  8.41346834118e-07  7.92285666371e-07  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  0.0  0.0  0.0  0.0  7.92285666371e-07  7.92285666371e-07  0.0  0.0  0.0  5.28190444247e-07  0.0  8.45104710796e-07  0.0  1.40850785133e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  5.28190444247e-07  0.0  0.0  0.0  7.04253925663e-07  0.0  0.0  0.0  7.04253925663e-07  0.0  1.50911555499e-06  0.0  0.0  0.0  0.0  9.28734864468e-07  0.0  5.28190444247e-07  0.0  8.31986255318e-07  0.0  0.0  5.28190444247e-07  9.24333277433e-07  0.0  1.25759629583e-06  8.45104710796e-07  8.45104710796e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.79709703413e-07  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  9.94240836231e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  5.28190444247e-07  0.0  0.0  8.79233939501e-07  0.0  0.0  1.55350130661e-06  0.0  0.0  0.0  0.0  0.0  9.8620173856e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.16294322928e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.96169419638e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.64564358742e-07  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.05469332996e-07  0.0  0.0  0.0  0.0  0.0  0.0  9.24333277433e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.34897086318e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.00007748889e-07  0.0  0.0  0.0  0.0  1.35820399949e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.25821567593e-07  0.0  5.28190444247e-07  0.0  7.92285666371e-07  1.58031485297e-06  0.0  7.76750653305e-07  0.0  1.07364201412e-06  0.0  0.0  0.0  0.0  0.0  9.11273623592e-07  9.99160257035e-07  0.0  0.0  0.0  0.0  0.0  1.24327904569e-06  0.0  0.0  8.11149610809e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.79709703413e-07  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  1.23244436991e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.51204187374e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  6.65651771984e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.50742799645e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.94126307582e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.97693061356e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.35817845842e-07  0.0  0.0  0.0  0.0  0.0  0.0  9.60346262268e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.88320292598e-07  7.79709703413e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.05469332996e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.73915098664e-07  0.0  0.0  0.0  7.39466621946e-07  0.003395191075  0.00382180721597  0.00288990454529  0.00393241434009  0.00278838771624  0.00374188073699  0.00272765745754  0.00378530618064  0.00271443381816  0.00404321752902  0.00274062838231  0.00393058473077  0.00282765471362  0.00418895486695  0.00281065947062  0.00415879803982  0.00289360727792  0.00427680005526  0.00298521787007  0.00407324365751  0.00303176516596  0.0039991645545  0.0028767214467  0.00404597786673  0.00278003076957  0.0040679903775  0.00270209647272  0.0038821189498  0.00266024568825  0.00380435575679  0.0026835062811  0.00359651871595  0.00260158744155  0.00361729121221  0.00238430099792  0.00359030916715  0.00233126043531  0.00340057101125  0.00221293609511  0.0033141628467  0.00233122930836  0.00315414282383  0.00219294491284  0.00304997396226  0.00221313733048  0.00305884534388  0.00225984650821  0.00316731837692  0.00212967007294  0.00298095261418  0.00198671298118  0.00290546861123  0.00205546282623  0.00292744422739  0.00201994122513  0.00289829394879  0.00197452770185  0.00280764710326  0.00200755690956  0.00278401487724  0.00190852750537  0.0027920827029  0.00192297260951  0.0026689736484  0.00189247363134  0.00260745383284  0.0017784256177  0.00261107954103  0.00176621283074  0.00254556368749  0.00171301429968  0.00250625799517  0.00176051455085  0.00260769518359  0.00170359780726  0.00243078655511  0.00177459380939  0.00232828798324  0.00166187954596  0.0024055180849  0.00171712200674  0.0022500366952  0.00164370812533  0.00224705482733  0.00163646807723  0.00216197938846  0.00161666598356  0.0021959495493  0.00158992160299  0.0020961058756  0.00151504520409  0.00197683477419  0.00144706872423  0.00197173400951  0.0014300523153  0.00193006678809  0.00139875600365  0.0019030286959  0.00137886011361  0.0019771679121  0.0013178673954  0.00179764719108  0.00131958071765  0.00173344560405  0.00127804378634  0.00181673029653  0.0012410114265  0.0017896798374  0.00130159869768  0.00175377650102  0.00125168760277  0.00175142331485  0.00127216810056  0.00163445163168  0.00113654899937  0.00155923354568  0.00116069356348  0.00166341665384  0.00112524153675  0.00152244748166  0.00116423692802  0.00158172766113  0.0011193007147  0.00146815275171  0.0010685118178  0.00153578622817  0.0010356490934  0.00145578162162  0.0010263985432  0.00151249178677  0.0010392803257  0.00146504392679  0.000958013510837  0.00140820371677  0.000966415533049  0.00138676934197  0.00100087446003  0.00134053611969  0.000965499552817  0.00138910916479  0.000933666964861  0.00132296747292  0.000932329665797  0.00128217065889  0.000896284597099  0.00126649107904  0.000901434224435  0.00126086081485  0.000868156262745  0.00122532571019  0.000921957981498  0.00115745150725  0.000840040943835  0.00120468744517  0.00086735915571  0.00112649629952  0.000868574302696  0.00113541222991  0.000832620557091  0.00104804612981  0.000813142965476  0.00111728939839  0.000765484768013  0.00106284075875  0.000797950896754  0.00104012100289  0.000776953782715  0.000986360187982  0.000771561488169  0.00101906661536  0.000774592233366  0.00102098322279  0.000714732516069  0.000968259245583  0.000689826076623  0.00088175652268  0.000690933086808  0.000972025266188  0.000700586434249  0.000873305554491  0.000669310237212  0.000942165218588  0.000678571596894  0.000838709999493  0.000631859159267  0.000840228028639  0.000602837996517  0.000895203788951  0.000572941031326  0.000765489243975  0.000586177321689  0.000721608296367  0.000554393883519  0.000836635599211  0.000568096583546  0.000774323835969  0.000544517017281  0.000777908475906  0.000563869776108  0.000732530435033  0.000530236555681  0.000721648343316  0.000545087474813  0.000720844891843  0.000476222431076  0.000744564033684  0.000530337474934  0.000697885657798  0.000525298038492  0.000619410919673  0.00046920192225  0.000648540601366  0.000467744425448  0.000652925897669  0.000488246814291  0.000560712041528  0.000462722432784  0.000685044949518  0.000460647560395  0.000620796943297  0.000445605868583  0.000663297651755  0.000426854961857  0.000601923817267  0.000439159195039  0.000594999979236  0.000455579537759  0.000605478978106  0.000389744378219  0.000591629506567  0.000427480581431  0.000663054836754  0.00042863592177  0.000576663477352  0.000394522938489  0.000600518770471  0.000361145705166  0.000547706732054  0.000385118740463  0.000535071178516  0.000372674650755  0.00061595508191  0.000376459411512  0.000515216523506  0.000353756555912  0.000497262153698  0.000400139196803  0.000554365926209  0.000330125649788  0.00049899332459  0.00037372790264  0.000517914203778  0.000380821354692  0.000526874243201  0.000416662348878  0.000516785591574  0.000374190368899  0.00050552165762  0.000338135232977  0.000517135414763  0.000352232421831  0.000526124889091  0.000357388027459  0.000484045353705  0.000307115920505  0.000464725786189  0.000338055877223  0.000447458874182  0.000316880984432  0.000517012235233  0.000326084527389  0.00052223137925  0.000318848481272  0.000507971226934  0.000314264042154  0.000423780768138  0.00034000493768  0.000435399948321  0.000299161427397  0.000401443458591  0.000289622691564  0.00045961510521  0.000320333242965  0.000459662090545  0.000263881122657  0.000426874919779  0.00027140220201  0.000390017355048  0.000253870322176  0.00037826435816  0.000236223339356  0.000424095240801  0.000239915431624  0.000380076537378  0.000199887145395  0.000397160902254  0.000311590571685  0.0003790236946  0.000265842032223  0.000349147151923  0.000226961416696  0.000341375504301  0.000210965631577  0.000339555779538  0.000227165041458  0.000316087988551  0.000225051918528  0.000345737905543  0.000232176591856  0.00034906046631  0.00020110739261  0.00038818474139  0.000204223853122  0.000337644436892  0.000161812851656  0.000319606266894  0.000210607448155  0.000289806837568  0.000226170660669  0.000331616914179  0.000210678696321  0.000292294098099  0.000212928855349  0.000302521277979  0.000212946748583  0.00029944811307  0.000243556688312  0.000310355127007  0.000203060135297  0.000323909037194  0.000217892935953  0.000364637164145  0.000177388411399  0.000325913936818  0.000213232176154  0.000276193163664  0.000185117412162  0.000244483430122  0.00018709951419  0.000310021345889  0.000168216906458  0.000316833577081  0.00018092769157  0.000261250420833  0.000149790818714  0.000274039957457  0.000140660099334  0.000247823153277  0.000175634112075  0.000217791636724  0.000165688335582  0.000258075217948  0.000177469910288  0.000231558994474  0.000176312660809  0.000277497852187  0.000150009469219  0.000230349234528  0.000149037927557  0.000216155520255  0.000135793979603  0.000254328754336  0.000144127295386  0.00021920157632  0.000119956206904  0.000192395428931  0.000145388622266  0.000206860355434  0.000140223891105  0.000231782195219  0.000122196492935  0.000195208691232  0.000109514716801  0.000263230027036  0.000149325519642  0.000211060860832  0.000150120242926  0.000238107582417  0.000138146416676  0.000229777477848  0.000126991273877  0.000184102578995  0.000123510769026  0.000185686893779  0.000118089622978  0.000261266526876  0.000117479608607  0.000227638739852  0.000108725595369  0.000177246369977  0.000149723197591  0.000180115015113  0.000121429825911  0.000162477913745  0.000114836975001  0.000225970388885  0.000105568532326  0.000231397614936  0.000113623091001  0.000226106313162  0.000106820337953  0.000185187336532  0.000118587180127  0.000156615594839  9.34102155653e-05  0.000184874729928  0.000126867868252  0.000193961584461  0.000120158223148  0.00015286314066  9.13565123044e-05  0.000157492220604  0.00011360794804  0.000173163790745  0.000113515295798  0.000157986706917  9.05942242251e-05  0.000158389459363  9.94373347608e-05  0.00013625673942  8.87357442966e-05  0.000156405385226  0.000125466524721  0.000173764741416  0.000121772766059  0.000178939020798  0.000101520687549  0.000177147095166  0.000104770762542  0.000187220442383  8.01527756247e-05  0.000132815452499  9.05166400775e-05  0.000110392954052  0.000113810505483  0.000125242261861  6.17951508373e-05  0.000142955840541  9.53507017155e-05  0.000166910280405  7.66924517842e-05  0.000179705869916  8.33139611544e-05  0.000128977794833  8.48811293518e-05  0.000191690226515  7.30678617945e-05  0.000165587418876  8.19457053305e-05  0.000124357703855  6.4937464538e-05  0.000159365079599  8.45126147873e-05  0.000126153906533  9.0301729473e-05  0.000128638192069  7.11480930406e-05  0.00012954704707  8.42651379371e-05  0.000129015127692  7.36184136288e-05  0.000165998949469  7.33535649303e-05  9.65969045655e-05  7.86148664134e-05  0.000103982314486  8.96590088621e-05  0.000121054996492  7.9725873968e-05  0.000144070259585  6.70715100377e-05  0.000114274644991  5.92418715414e-05  0.000112620798272  9.94918438072e-05  0.000110236279741  9.04852370787e-05  0.000115380299328  6.19685238162e-05  0.000117589718187  7.04457204882e-05  0.000131505845584  4.63297946654e-05  9.56110004635e-05  7.30093332248e-05  0.000105708270762  7.09870399298e-05  0.000123557381024  7.66189259773e-05  8.13110228305e-05  6.19186265483e-05  8.09916230789e-05  6.48056203893e-05  9.75836547035e-05  5.92032966418e-05  0.00010618143253  6.43122530088e-05  9.9766722708e-05  8.97927398263e-05  0.00010296422626  6.29084302365e-05  9.23781733231e-05  6.00100081095e-05  7.73470551009e-05  5.71334661558e-05  0.000105644694029  7.60060474823e-05  0.000100959289724  4.40254988444e-05  6.34058713643e-05  6.05442134715e-05  0.00010374049753  6.8272725097e-05  9.84887174618e-05  6.42175125552e-05  0.000101108977373  7.16773709547e-05  8.93576232374e-05  6.37581439458e-05  9.92042549394e-05  4.67813915281e-05  8.0607302059e-05  4.69112394971e-05  8.56140059272e-05  7.42268245881e-05  0.000116021900789  5.36017889116e-05  0.000103227552528  4.52934114093e-05  0.000114562971329  5.34542700718e-05  0.000115989687439  4.83207464115e-05  9.1568587217e-05  4.38598345152e-05  7.19499157482e-05  5.19994875451e-05  9.97093749256e-05  2.99014771574e-05  6.71100586304e-05  3.8129744561e-05  6.65701520279e-05  5.46150957715e-05  8.51559807278e-05  4.52923111048e-05  8.90003123386e-05  5.964395367e-05  5.60393951074e-05  3.57327507945e-05  6.15521001144e-05  4.64816519048e-05  7.52389227663e-05  8.64755811675e-05  9.25774038597e-05  6.73747408842e-05  7.95422287212e-05  4.54187955295e-05  6.15482700393e-05  5.1403331499e-05  6.42913218335e-05  3.38182151378e-05  6.24201641685e-05  4.96078792366e-05  6.611427952e-05  5.81981124515e-05  7.49372104631e-05  5.76276491551e-05  9.78498506906e-05  5.7734566431e-05  5.036309804e-05  3.81435483796e-05  7.90476120061e-05  4.31596733855e-05  4.83443435875e-05  3.70563029538e-05  5.49690692626e-05  5.1303278075e-05  7.6293905269e-05  3.98949722108e-05  8.24397099064e-05  4.42572156117e-05  6.54704567853e-05  7.58015533064e-05  5.66332733769e-05  3.63035827483e-05  6.09686725239e-05  3.50357627792e-05  6.198504885e-05  3.36081052375e-05  7.88256289893e-05  3.81005184481e-05  7.72705000475e-05  3.63448684407e-05  5.08910186517e-05  5.57926439458e-05  4.77478693845e-05  3.69522316058e-05  4.82916115261e-05  4.02247908761e-05  6.25138259199e-05  4.94335993423e-05  7.48163711178e-05  3.05117502755e-05  4.57634184991e-05  4.21174242893e-05  4.04449441439e-05  3.24201572582e-05  3.37948536823e-05  3.92952697859e-05  4.8886721054e-05  3.22654379244e-05  5.1179039958e-05  2.78757084934e-05  3.99517909074e-05  3.436616061e-05  3.88836933682e-05  4.4461214257e-05  5.51736925681e-05  2.13044919195e-05  3.61513235655e-05  2.55846036085e-05  4.3556199081e-05  2.84676617875e-05  5.11760336509e-05  2.56228098276e-05  2.96327174635e-05  2.55754811347e-05  3.5548321935e-05  2.58125066898e-05  3.24121999782e-05  1.76847884238e-05  3.99687766455e-05  2.81170689392e-05  3.5470961451e-05  2.89667485189e-05  6.95729339616e-05  3.55687616001e-05  3.74238096055e-05  2.35790157449e-05  3.79109238367e-05  2.34074306281e-05  3.58268635479e-05  3.66522511154e-05  2.95315208188e-05  2.99640038517e-05  3.06956284211e-05  1.65228131401e-05  4.02701445157e-05  4.56407028808e-05  5.46269549604e-05  2.25289988267e-05  4.22849058222e-05  2.75643973026e-05  4.289243349e-05  2.79337971433e-05  3.69728503004e-05  2.07408809342e-05  6.16043770359e-05  2.83699394478e-05  4.58748072263e-05  2.60933446406e-05  5.3949435612e-05  2.00891668382e-05  4.00013342101e-05  3.1933584983e-05  3.99407720339e-05  3.59372652028e-05  2.40816777129e-05  5.51343604903e-05  3.18945530754e-05  1.83225902211e-05  3.22756727736e-05  1.66599340628e-05  3.93237152756e-05  1.7565313361e-05  2.66261826651e-05  1.99417631595e-05  3.36489330418e-05  2.77099824088e-05  3.29548840781e-05  2.64197152796e-05  3.46246373717e-05  3.14532210287e-05  3.43501963892e-05  2.38982153989e-05  2.22947579229e-05  2.58118065158e-05  4.7122858168e-05  2.23139000367e-05  3.29043736616e-05  3.49930264755e-05  3.01943072377e-05  3.01348735302e-05  4.28687280126e-05  1.46881548345e-05  4.33994314707e-05  8.51423333201e-06  2.51660649921e-05  2.30400759331e-05  2.23311789427e-05  3.19403775329e-05  3.40827651495e-05  1.90722806811e-05  3.56462554819e-05  1.69513821257e-05  2.1760979924e-05  1.6563477327e-05  3.74938415702e-05  2.72759115317e-05  2.48380033622e-05  1.37775699636e-05  3.26293733633e-05  1.96223416612e-05  3.21433806384e-05  1.91476463066e-05  2.85465060594e-05  2.46591196584e-05  2.82320395575e-05  1.82894559068e-05  3.3695714475e-05  2.0628093144e-05  5.19478755123e-05  2.11999539928e-05  4.08470105418e-05  1.84307497973e-05  3.53997667814e-05  2.41883303418e-05  3.31811408791e-05  2.47916247088e-05  3.3959457691e-05  2.82521268248e-05  2.8740732437e-05  1.92999532055e-05  2.2498084318e-05  1.91062712013e-05  3.75790967091e-05  1.04189834226e-05  2.97429374889e-05  1.91416500006e-05  3.58097294487e-05  5.33385968811e-05  2.68119692265e-05  1.63499227685e-05  2.39898069812e-05  1.30926977132e-05  2.69594446625e-05  1.78569836038e-05  4.37147514874e-05  1.52265839111e-05  2.99268558889e-05  2.04722331528e-05  2.4813370492e-05  1.77687389474e-05  2.87218703592e-05  1.3865996956e-05  2.46413798029e-05  1.65496449422e-05  1.79634473666e-05  1.62759685103e-05  2.06059619797e-05  5.85117636572e-06  3.10423544043e-05  1.83906132747e-05  1.76938850328e-05  1.68339503479e-05  2.79672363819e-05  1.67497631239e-05  2.13629554864e-05  2.13793008912e-05  2.8594941534e-05  1.64292176428e-05  2.63779345423e-05  1.98926143048e-05  3.37299437275e-05  1.17429352468e-05  3.41323859408e-05  1.2527692179e-05  3.29045580546e-05  1.29348499962e-05  2.57077213186e-05  8.19403315939e-06  2.33064538851e-05  1.46297966559e-05  2.35818928143e-05  1.16932982387e-05  2.64772670099e-05  1.73153369772e-05  2.04914771287e-05  1.8438924247e-05  2.29862883804e-05  1.36346277084e-05  1.95781018131e-05  1.00617246205e-05  2.30843139884e-05  2.1257755974e-05  2.73301325002e-05  1.77069454007e-05  2.370275014e-05  2.15822812775e-05  1.98946796115e-05  1.21258605728e-05  1.82388275282e-05  1.89568088356e-05  1.51346400935e-05  1.33068245489e-05  3.31771701277e-05  9.44316453259e-06  2.80532966628e-05  9.6138372317e-06  3.36564820575e-05  2.31341662254e-05  2.99258614207e-05  1.49871823175e-05  1.72027877172e-05  1.67265012648e-05  2.81312612302e-05  1.36287504259e-05  2.33780255032e-05  3.27770150065e-05  2.47200827529e-05  7.50224786623e-06  2.25565065265e-05  1.65348463148e-05  3.8409802641e-05  2.03139141559e-05  2.06529878126e-05  1.00326453579e-05  4.09799581081e-05  1.38063145293e-05  2.11773188187e-05  3.17865848561e-05  2.27403742192e-05  4.60154978897e-05  6.4163338474e-05  1.26453697832e-05  2.31466546005e-05  8.94683340764e-06  2.66740018901e-05  8.24087056665e-06  3.3533725173e-05  1.87934588701e-05  1.57730465657e-05  1.58052006369e-05  2.57790391551e-05  1.19307251833e-05  2.81090012573e-05  3.57856300328e-05  1.93856449569e-05  5.29137062944e-05  2.41958482229e-05  9.93901113498e-06  3.80814100508e-05  1.95481533798e-05  2.9241046882e-05  1.61080605262e-05  1.7721305997e-05  1.8382061331e-05  2.72385018758e-05  1.00507336651e-05  2.78260628686e-05  1.5188000631e-05  1.21245258644e-05  1.5542871813e-05  2.5036294606e-05  3.1535078989e-05  1.49242045692e-05  1.01345171032e-05  1.74812706672e-05  3.80283798277e-05  1.74500425009e-05  2.1145413576e-05  2.93505589741e-05  1.0933361767e-05  1.74208812013e-05  1.35786087166e-05  1.79146236516e-05  1.48916951958e-05  2.61489775881e-05  1.94537595327e-05  3.17232916397e-05  1.64441708855e-05  9.47197587434e-06  1.67063559526e-05  1.92361610248e-05  1.80447747177e-05  1.58560013696e-05  1.28160798746e-05  3.46144703788e-05  2.28844991961e-05  1.29751077646e-05  1.51996439313e-05  1.91622119362e-05  8.55302842441e-06  1.97294014702e-05  1.30327619836e-05  4.39166292763e-05  1.06564211802e-05  1.76433462771e-05  1.428389435e-05  1.94136285217e-05  8.78649552367e-06  1.3261689885e-05  1.08048955043e-05  9.64603392428e-06  1.15363163086e-05  1.61867141053e-05  1.51881668465e-05  2.10284939916e-05  1.57158036284e-05  2.69641529571e-05  1.43459315519e-05  1.45080259668e-05  2.0338987969e-05  2.4264786425e-05  9.9745039683e-06  1.32886059952e-05  1.81804973654e-05  1.9352117958e-05  5.63061358962e-05  1.86076918189e-05  1.05115119416e-05  1.50831470543e-05  2.30693910683e-05  8.35790074117e-06  9.88134002554e-06  1.96458839259e-05  2.36333251587e-05  1.93707263919e-05  4.08086912542e-05  1.62417450132e-05  1.22155695526e-05  1.12337774029e-05  1.92930894691e-05  1.64786801258e-05  1.38403061626e-05  1.92915145161e-05  7.00656438078e-06  1.69268020109e-05  1.70690819718e-05  1.55625068793e-05  2.36124940106e-05  1.3206497577e-05  4.61075345494e-05  2.42951112761e-05  4.24434933827e-05  1.35339163748e-05  1.14174976639e-05  5.40550596274e-05  1.67204654558e-05  1.2463957739e-05  9.94735706025e-06  1.90592362446e-05  1.5312999562e-05  3.47600052749e-05  1.07077644324e-05  1.39417121101e-05  2.98141651623e-05  1.35654590715e-05  1.29662575065e-05  8.5846340866e-06  9.53336183319e-06  1.55504746879e-05  4.49704758219e-05  3.51741016325e-05  7.10605547897e-06  3.20456904047e-05  1.78857599916e-05  2.3922128364e-05  1.06459521324e-05  1.02145664787e-05  2.14132731502e-05  1.964358667e-05  1.71867224095e-05  3.47255941005e-05  9.75957055475e-06  1.26883558219e-05  2.14551801453e-05  2.39417977103e-05  1.60070629435e-05  2.57440679236e-05  9.3384918712e-06  2.18418720393e-05  6.58712230433e-06  1.04451523178e-05  1.60872640411e-05  1.95219119219e-05  6.58538688007e-06  1.00101763003e-05  8.93503515091e-06  3.40593198162e-05  1.28896646526e-05  6.03783178852e-05  8.11459697188e-06  3.09184885844e-05  1.39026932845e-05  9.59791907773e-06  1.50585349313e-05  1.18201999768e-05  7.23883299371e-05  1.50598045893e-05  6.91887562088e-06  2.69675041001e-05  1.65945845814e-05  1.33451165301e-05  5.57516622484e-06  6.49596327609e-05  1.59620118899e-05  1.84086625789e-05  9.15252123942e-06  1.28410708589e-05  5.0752622276e-05  1.36830496227e-05  1.14065507436e-05  1.89670858076e-05  1.09905827975e-05  1.53977402266e-05  1.14233784385e-05  2.19294791587e-05  8.80024183946e-06  2.40958536946e-05  1.94161663314e-05  8.80406836664e-06  8.59992787231e-06  2.0239412661e-05  1.41349246464e-05  1.4611759094e-05  1.19936211684e-05  2.50885708388e-05  9.37201300833e-06  1.68762474812e-05  1.3630864697e-05  2.16166898653e-05  1.45547347169e-05  9.54964681292e-06  1.11873631911e-05  1.23934627348e-05  2.03760050906e-05  2.07899832033e-05  3.62992594828e-06  4.30657224044e-05  9.85961141444e-06  1.35404822593e-05  8.77340175058e-06  1.24220762736e-05  1.00100995077e-05  2.30129263218e-05  8.50077896011e-06  5.92148345446e-06  1.70317342126e-05  1.95468418854e-05  2.92505043859e-05  2.95377273637e-05  4.20243398913e-06  3.19580893066e-05  2.89026032984e-05  1.58623641577e-05  2.38254518667e-05  2.03214400767e-05  9.11766527206e-06  1.68182517356e-05  1.34825286931e-05  3.49073402618e-05  7.89822257156e-06  1.89311428586e-05  5.4628538961e-06  2.42848728631e-05  1.6992692057e-05  1.33841009117e-05  3.86395166825e-05  1.29110074179e-05  3.93091077243e-05  1.52247500564e-05  1.71102442758e-05  2.42562379682e-05  5.05176432034e-06  1.1127344077e-05  1.11951792719e-05  3.98610094304e-05  1.53049371214e-05  1.05856978601e-05  1.75865701685e-05  2.09201296095e-05  1.73177359206e-05  8.87369809836e-06  4.40317099429e-05  2.04168640779e-05  9.91855232581e-06  8.36093721077e-06  1.73993647145e-05  3.08052342279e-05  1.47635399813e-05  1.74890580807e-05  6.9329718007e-06  1.25135967231e-05  8.85478535374e-06  2.7807347128e-05  1.70794537764e-05  1.20760507914e-05  1.6776246211e-05  1.30904015143e-05  3.01567141914e-05  2.29586389079e-05  1.45550491619e-05  2.55512352559e-05  3.83738360631e-06  2.49028616193e-05  5.29286810249e-06  8.64878822989e-06  7.4516594133e-06  2.46507099732e-05  1.17524714067e-05  1.96998054678e-05  4.73249858635e-06  1.44006223223e-05  5.70232269507e-06  9.97836347733e-06  3.60930136902e-06  1.34443238603e-05  1.22682390675e-05  2.14161574895e-05  2.49381345463e-06  7.07532600909e-06  1.63359523101e-05  3.01836837905e-05  3.43826827279e-06  1.78615782791e-05  3.75880280511e-06  2.82384432267e-05  8.43180855683e-06  1.01494003501e-05  9.35444152638e-06  1.84918347112e-05  6.05970283403e-06  2.18538448142e-05  8.3746483198e-06  4.04527256452e-05  5.16890369895e-06  2.27188110292e-05  1.1391082546e-05  1.5535005488e-05  8.92748349924e-06  6.98720933128e-06  1.67260307345e-06  2.24266973912e-05  9.87873661226e-06  1.78033637855e-05  5.2244580886e-06  1.12140078253e-05  7.9567700333e-06  1.46870584566e-05  1.21692174975e-05  1.79501298655e-05  1.59823076409e-05  1.34830299758e-05  1.94756278574e-06  2.41701953908e-05  8.47365996814e-06  1.23442528291e-05  5.95394256627e-06  5.55392344824e-06  7.41200827638e-06  1.44446123567e-05  1.31588981393e-05  1.36478543393e-05  3.81603339816e-06  1.39364968918e-05  9.45299223069e-06  1.68205991884e-05  7.8332372357e-06  8.576047636e-06  2.51642908715e-05  2.08045862821e-05  9.49225562096e-06  8.99627452538e-06  4.55443117891e-06  1.17564311179e-05  1.39684250902e-05  1.73455071208e-05  7.50967911771e-06  6.2079864905e-06  9.10876997068e-06  8.50427645289e-06  9.21748708016e-06  7.31646843576e-06  8.71053114367e-06  1.80872252285e-05  4.36603696257e-06  1.18381569e-05  1.13709922305e-05  1.53071842427e-05  1.18081998842e-05  8.40401155044e-06  1.24861290293e-05  1.59930906134e-05  1.51729199113e-05  1.59342269761e-05  1.01968194204e-05  1.29090967583e-05  6.26033322588e-06  5.49466687034e-06  3.84138504907e-06  6.62759403967e-06  1.34579444857e-05  9.95844922409e-06  3.66379720851e-06  1.81467307577e-05  3.48741126651e-06  6.81037985235e-06  1.37655997394e-05  5.57724216683e-06  3.63250973703e-06  8.85249038298e-06  9.561514732e-06  9.81060834497e-06  1.44192354903e-05  1.31413398089e-05  7.28323505434e-06  1.74099173433e-05  5.48010629088e-06  1.2156933636e-05  6.3699194354e-06  8.08593870917e-06  1.13899157612e-05  3.99810822382e-06  6.90224907716e-06  1.11087351422e-05  5.04132150734e-06  2.68729125141e-05  3.64451406531e-06  9.71110778441e-06  1.48395654891e-05  2.93044121472e-06  7.59277574504e-06  1.00426448569e-05  7.55612059804e-06  6.97388879742e-06  4.62149489676e-06  1.04383838505e-05  1.0551049135e-05  6.56315916254e-06  4.53843637777e-06  1.92509314816e-05  9.75485787676e-06  9.55707420638e-06  5.60687173794e-06  1.89855305094e-05  5.51480910819e-06  1.93850554213e-05  4.22106480348e-06  1.14137111594e-05  1.49653959203e-06  1.45114265887e-05  6.095674731e-06  1.26277376623e-05  1.87134139717e-06  2.36494107905e-05  1.22368635414e-05  9.64676759187e-06  7.10370416738e-06  1.33954894159e-05  4.1903458481e-06  1.53678962349e-05  4.19375270853e-06  6.39862357003e-06  4.80675339745e-06  2.19682383116e-05  1.02355450814e-05  3.62329886192e-06  1.16309554742e-05  1.18112557163e-05  4.8557579523e-06  1.60704601617e-05  4.21529177116e-06  1.05750692453e-05  6.94507554371e-06  1.21884884269e-05  3.4948601061e-06  6.50108688781e-06  5.1241581245e-06  5.57113931794e-06  8.80317407079e-07  2.54039612552e-05  4.73630822366e-06  2.05687676223e-05  3.88076101659e-06  1.24049903537e-05  7.39276839233e-06  7.98439401483e-06  7.61773068696e-06  3.14052588647e-05  5.34675298148e-06  9.69303573351e-06  6.48019833922e-06  2.32356445141e-05  8.6630676427e-06  9.01930988999e-06  3.84401958122e-06  1.42562079485e-05  4.54327621806e-06  1.0587688026e-05  1.16172553821e-05  8.50457725312e-06  2.48645818673e-06  1.61691284844e-05  3.96060322361e-06  8.00963783819e-06  3.27226556174e-06  2.3542500086e-05  6.40411471173e-06  6.43205452499e-06  8.00511999269e-06  2.29041513068e-05  8.90328445229e-06  1.00922242081e-05  1.79131254198e-06  8.40490493338e-06  7.27529365471e-06  1.85754096971e-05  6.75218494248e-06  1.49324018551e-05  2.59453548523e-06  1.0522659975e-05  7.3039202018e-06  1.53183848982e-05  1.73785012833e-06  8.94757029416e-06  9.50742799645e-07  4.50837109606e-06  1.02603089789e-05  5.06774674296e-06  2.78347980143e-06  3.86138221292e-06  7.04253925663e-07  1.21578902957e-05  4.18590927066e-06  6.50457544829e-06  1.13029215816e-05  8.18395564546e-06  3.44034814359e-06  7.04592164056e-06  1.93669829557e-06  1.05638088849e-06  5.16306159252e-06  2.47755544516e-06  6.3539993689e-06  2.47437528065e-05  3.66057260262e-06  1.16362312067e-05  4.88874902183e-06  1.12424473402e-05  3.1555848359e-06  1.14833435563e-05  2.97367404494e-06  6.16973439821e-06  3.57995745546e-06  1.9015471686e-05  1.91891848587e-06  1.46219109472e-05  1.8193226413e-06  6.41982835337e-06  2.85650524464e-06  8.03906142899e-06  4.71663644736e-06  1.00237999865e-05  2.04020814454e-06  1.3507572244e-05  4.27845545961e-06  2.53399918714e-05  2.84468282116e-06  8.75665170102e-06  2.29237741636e-06  4.57509632914e-06  0.0  8.51969610005e-06  6.967789635e-06  5.00214054152e-06  1.66379989938e-06  1.50846422714e-05  3.3160615726e-06  6.58570171886e-06  2.27479313132e-06  9.33676549127e-06  3.85595056354e-06  4.50244195885e-06  4.85842278112e-06  3.98998259194e-06  3.76151497218e-06  4.55255703903e-06  1.54935863646e-06  3.80528860269e-06  5.49144666164e-06  2.76594438111e-06  5.32745932927e-06  2.5975752367e-06  5.34954377555e-06  3.36000139744e-06  2.36055087765e-06  1.17270903353e-05  2.53555173493e-06  1.50891925115e-05  1.74148404951e-06  9.42724637223e-06  1.91028877336e-06  6.25246267422e-06  3.43805867341e-06  4.90490731681e-06  5.12254148809e-06  1.84176210461e-06  5.28190444247e-07  1.53175228832e-06  1.57220567017e-06  9.90288410103e-06  1.9805749633e-06  8.85706918121e-06  5.37023208819e-06  5.90553868715e-06  4.03435033689e-06  5.13875221185e-06  0.0  6.21932709493e-06  1.27939463162e-06  7.4356763591e-06  6.8054708296e-06  1.14874930931e-05  2.99985085643e-06  1.49559979916e-06  6.45983098617e-06  7.33205159412e-06  3.89076992907e-06  1.44468478258e-05  5.73735927093e-06  2.89375342331e-06  1.84588660516e-06  2.9830184137e-06  2.39278655219e-06  1.08140305889e-05  5.00128965746e-06  2.21638543557e-06  1.72720826914e-06  8.09623920434e-06  1.3010405315e-06  5.87216855004e-06  2.6478118374e-06  8.9736306224e-06  5.28190444247e-07  7.28482286517e-06  1.97375513504e-06  1.07146975265e-05  1.20729244399e-06  8.65908712321e-06  7.67781994189e-06  7.92285666371e-07  6.76454428598e-07  2.93974230588e-06  1.5979674672e-06  3.93627239876e-06  1.54402924237e-06  4.42682588559e-06  7.72642498997e-06  0.0  3.49131539319e-06  8.15769908505e-06  1.62634014665e-06  4.26590206889e-06  8.45104710796e-07  5.79083268476e-06  9.50742799645e-07  4.12741823208e-06  1.817516862e-06  2.90904888612e-06  1.67847185616e-06  3.46342085731e-06  2.54172884838e-06  7.32149500489e-06  1.05638088849e-06  9.6037066265e-06  1.23244436991e-06  2.75626816271e-06  2.29559693077e-06  3.06557140881e-06  7.92285666371e-07  3.36908254288e-06  1.29913508257e-06  1.27499538407e-05  0.0  9.4634121261e-07  1.84066366935e-06  1.84051775051e-05  5.58594375871e-06  5.37668935702e-06  1.7368508e-06  3.34973251011e-06  0.0  3.75216738741e-06  8.45104710796e-07  3.8011812949e-06  5.28190444247e-07  9.44287902603e-06  1.49845332553e-06  3.68916438411e-06  9.76746021516e-07  2.10605459675e-06  1.30286976248e-06  7.8834795517e-06  0.0  1.51243177097e-05  5.79550140359e-06  3.6010217192e-06  5.28190444247e-07  4.2329381776e-06  3.2478070107e-06  7.52054017711e-06  1.32047611062e-06  1.37329515504e-06  1.24827006899e-06  2.7238922306e-06  9.00632424166e-07  4.2563139472e-06  1.2434483375e-06  5.42317312854e-06  2.71442273691e-06  2.99307918407e-06  1.88010646226e-06  7.72965323124e-06  7.39466621946e-07  2.68564525883e-06  7.92285666371e-07  1.16843966379e-05  0.0  7.72506068387e-06  1.65499672531e-06  9.02798862867e-06  8.36301536725e-07  5.89685730226e-06  1.93918614912e-06  8.73014373329e-06  2.27121891026e-06  6.77385802416e-06  1.33808245876e-06  3.1142043597e-06  9.24333277433e-07  4.26242707108e-06  0.0  2.83362767233e-06  7.04253925663e-07  7.03150232506e-06  4.94964341137e-06  1.27378448495e-05  2.66296015641e-06  1.35455711495e-06  9.50742799645e-07  9.28638866701e-06  7.79709703413e-07  3.79725485178e-06  1.92835411053e-06  1.14815688509e-05  9.05469332996e-07  1.33397030451e-05  1.05638088849e-06  5.49644899065e-06  4.11499377561e-06  9.50742799645e-07  1.13746666554e-05  1.73921858763e-05  1.57571947345e-06  5.2771065562e-06  1.81093866599e-06  7.69617968303e-06  0.0  1.22965603423e-06  2.93472671594e-06  1.71410752778e-06  2.49096265716e-06  0.0  1.43994775872e-06  1.83151751447e-06  1.07709423925e-06  3.59833827639e-06  1.75057404379e-06  3.35628966124e-06  4.39393462513e-06  7.39466621946e-07  1.68140624752e-06  2.27392757921e-06  9.5828837742e-07  9.05469332996e-07  3.92928487409e-06  2.539582338e-06  9.05469332996e-07  0.0  2.90332371606e-06  1.91816797442e-06  0.0  0.0  1.26765706619e-06  3.47450389429e-06  3.94633717631e-06  1.00671001365e-05  0.0  1.41651073685e-06  3.64274154769e-06  2.0555921898e-06  2.93503627811e-06  8.45104710796e-07  0.0  4.09614162646e-06  0.0  1.84309933016e-06  1.29406658841e-06  5.28190444247e-07  3.17668824326e-06  0.0  2.63508472763e-06  7.92285666371e-07  1.51414594018e-06  7.92285666371e-07  8.98850405123e-07  3.90033254902e-06  0.0  4.34729741996e-06  1.02997136628e-06  1.01865299962e-06  2.27388653877e-06  3.66937270629e-06  2.03819371427e-06  2.36566431771e-06  1.76943798823e-06  2.6233458731e-06  0.0  7.71970649285e-07  2.5905497344e-06  8.45104710796e-07  0.0  1.21794502438e-06  3.46258180118e-06  6.14623648983e-06  8.6540150519e-06  3.21223858827e-06  4.45683731946e-06  0.0  0.0  0.0  0.0  3.60018796563e-06  9.4827173324e-07  1.00078189436e-06  0.0  1.23244436991e-06  0.0  0.0  0.0  0.0  2.17019200783e-06  1.58457133274e-06  7.35693833059e-07  0.0  1.35728938401e-06  0.0  7.35510835544e-06  5.28190444247e-07  9.25842127621e-06  0.0  5.22012714016e-06  3.86809580303e-06  2.77695243443e-06  0.0  0.0  0.0  1.26765706619e-06  1.33559267215e-06  5.28190444247e-07  1.23244436991e-06  1.19605434944e-06  1.43937612404e-06  7.04253925663e-07  0.0  5.28190444247e-07  0.0  1.60663709224e-06  3.45802805682e-06  0.0  0.0  9.05469332996e-07  1.76863769968e-06  0.0  1.3553018542e-06  0.0  8.56842276224e-07  9.45571704387e-07  9.20560488546e-07  1.76943798823e-06  7.92285666371e-07  7.04253925663e-07  7.92285666371e-07  0.0  2.64466911696e-06  1.23244436991e-06  0.0  1.00078189436e-06  0.0  1.28526341434e-06  0.0  5.28190444247e-07  9.81158719964e-07  2.04553753863e-06  0.0  0.0  1.78851153205e-06  9.50742799645e-07  0.0  1.24174853763e-06  0.0  7.92285666371e-07  0.0  0.0  0.0  1.79584751044e-06  0.0  0.0  6.6904122938e-07  0.0  0.0  0.0  2.77733687994e-06  0.0  0.0  0.0  5.28190444247e-07  1.48698492749e-06  1.35230297072e-06  0.0  0.0  7.04253925663e-07  1.11570073839e-06  0.0  1.53741147165e-06  0.0  0.0  0.0  1.97694137704e-06  8.2564506285e-07  0.0  0.0  7.04253925663e-07  0.0  8.72771829304e-07  0.0  0.0  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  7.04253925663e-07  7.04253925663e-07  5.63403140531e-07  0.0  0.0  1.83015467682e-06  0.0  2.79186377674e-06  0.0  0.0  0.0  0.0  8.8605860756e-07  0.0  9.26247010927e-07  0.0  5.28190444247e-07  0.0  0.0  0.0  8.56842276224e-07  0.0  8.45104710796e-07  0.0  0.0  1.13764095684e-06  0.0  1.40103849151e-06  0.0  0.0  0.0  0.0  1.18842849956e-06  0.0  0.0  0.0  1.32447755338e-06  7.51204187374e-07  0.0  8.03405465197e-07  0.0  0.0  1.2397803483e-06  0.0  1.58457133274e-06  2.59677629317e-06  0.0  0.0  1.58457133274e-06  1.53716962621e-06  0.0  5.28190444247e-07  0.0  1.49375597968e-06  0.0  5.14685631293e-06  1.81279921942e-06  0.0  0.0  0.0  2.42691426344e-06  2.49485114337e-06  9.24333277433e-07  5.75140705958e-07  0.0  0.0  7.04253925663e-07  0.0  2.18535938134e-06  7.04253925663e-07  7.51204187374e-07  0.0  0.0  0.0  9.85955495929e-07  1.30847178234e-06  0.0  0.0  0.0  2.26870371767e-06  0.0  9.1342336085e-07  9.36115987343e-07  0.0  0.0  0.0  0.0  0.0  0.0  3.00151326255e-06  8.36301536725e-07  0.0  0.0  7.24375466397e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.85174330704e-07  8.45104710796e-07  0.0  9.28734864468e-07  0.0  0.0  8.80317407079e-07  0.0  3.03410411426e-06  2.39400603951e-06  1.12775797556e-06  0.0  8.11149610809e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.39687372906e-07  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.79709703413e-07  1.92025280555e-06  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.23203058698e-06  0.0  0.0  9.33136451504e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.79709703413e-07  0.0  0.0  7.92285666371e-07  0.0  7.92285666371e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.72723057325e-07  1.81712577076e-06  0.0  0.0  0.0  0.0  0.0  0.0  1.81245995183e-06  0.0  9.24333277433e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.92285666371e-07  0.0  0.0  0.0  0.0  0.0  0.0  1.20223471976e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.92285666371e-07  0.0  0.0  0.0  9.28734864468e-07  0.0  8.73915098664e-07  9.35984537233e-07  8.88320292598e-07  9.24333277433e-07  0.0  8.56842276224e-07  0.0  0.0  0.0  7.51204187374e-07  0.0  0.0  9.60346262268e-07  0.0  9.05469332996e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.46719567847e-06  0.0  0.0  0.0  0.0  8.45104710796e-07  0.0  0.0  6.71126998846e-07  0.0  0.0  0.0  0.0  0.0  8.45104710796e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.49296698449e-07  0.0  1.42086318336e-06  0.0  0.0  0.0  0.0  0.0  0.0  7.04253925663e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.01306868881e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  8.0028855189e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  7.79709703413e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.15530103362e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.05469332996e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.0626360434e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.77522748644e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.45104710796e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.68349147787e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.80317407079e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.24333277433e-07  0.0  0.0  0.0  0.0  0.0  7.92285666371e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  5.28190444247e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  8.73915098664e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.83106020672e-06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.05469332996e-07  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0'
**** sh-liblinear-ovo (multiclass one versus rest) (~11%)
***** call
      - liblinear-ovo-1.96/predict test train.model output
      - liblinear-ovo-1.96/train train
***** result
      Accuracy = 11.1979% (86/768)
**** scikit-learn liblinear (~8%)
***** default (git: f956a6) (~8%)
****** call
       test(X, y, svm.LinearSVC())
****** result (~8%)
       [ 0.07026349  0.09210526  0.10131579  0.025       0.1125    ]
       mean = 0.0802369081424
***** scikit-learn-liblinear-Crammer-Singer (~3%)
****** call
       >>> svc_liblinear = svm.LinearSVC(multi_class='crammer_singer')
       >>> cross_validation.cross_val_score(svc_liblinear, X, y, cv=5, n_jobs=-1)
****** results ~0.03
       array([ 0.03713355,  0.04690554,  0.03127036,  0.02931596,  0.03715776])
**** scikit-learn AdaBoostClassifier (git: f95a6) (~4%)
***** call
      test(X, y, ensemble.AdaBoostClassifier())
***** result
      [ 0.03638645  0.0375      0.03947368  0.03684211  0.0375    ]
      mean = 0.0375404477316
** features
*** *variable lengths* including (~78%\downarrow) / excluding (78%\uparrow)
**** braycurtis
***** call (jeweils mit reload(counter))
      test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc', func=distance.braycurtis))
***** result
****** without lengths (git: 0603b7) (78%)
       [ 0.78809524  0.79        0.7625      0.76        0.7875    ]
       mean = 0.77761904799999992
****** with lengths (reload(counter) after edit (git: 3f8f17a) (78% \downarrow)
       [ 0.78571429  0.79        0.765       0.76        0.785     ]
       mean = 0.77714285800000005
**** default (euclidean)
***** call
      test(X, y, neighbors.KNeighborsClassifier())
***** result
****** without lengths (==)
       [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
****** with lengths (==)
       [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]

*** mapping of panchenko's *Number Marker* s
**** with panchenko's dictionary mapping (1\to1, 2\to2, 3..5\to3, 6..8\to4, 9..13\to5) (-)
***** results
svc_linear
[ 0.79047619  0.74        0.77        0.8         0.78      ]
knn
[ 0.85714286  0.89        0.89        0.9         0.9       ]
panchenko-rbf-svm
[ 0.20952381  0.2         0.22        0.2         0.22      ]
liblinear
[ 0.61904762  0.6         0.7         0.76        0.63      ]
**** without panchenko's dictionary mapping (+ for liblinear)
svc_linear
[ 0.79047619  0.74        0.77        0.8         0.78      ]
knn
[ 0.85714286  0.89        0.89        0.9         0.9       ]
panchenko-rbf-svm
[ 0.20952381  0.2         0.22        0.2         0.22      ]
liblinear
[ 0.8   0.61  0.73  0.72  0.71]
*** *Number Marker* log-mapping bases (git 51fd56 +change) (each ~74% with 20c)
**** call
     test(X, y, neighbors.KNeighborsClassifier());
**** 2
     [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
     mean = 0.74445238000000002
**** 1.4
     [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
**** 2.4
     [ 0.7547619  0.7525     0.7175     0.7425     0.755    ]
**** result
     same for all, parameter is not that important
*** *alle Paketdaten* auf duckstein (2+2 GB) (Crash)
**** result
     MemoryError
     >>> Killed
*** *normalized size markers* (git abbf51c) (76 classes) (~62%)
**** call
     test(X, y, neighbors.KNeighborsClassifier())
**** result (~62%)
[ 0.63174404  0.625       0.61447368  0.61447368  0.60394737]
mean = 0.617927755399
*** *normalized size markers** (git abbf51c) (20 classes) (~79%)
**** knn braycurtis (~79%)
***** call
      test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc', func=distance.braycurtis))
***** result (~79%)
[ 0.80714286  0.795       0.77        0.78        0.805     ]
mean = 0.791428571429
**** knn cityblock (~79%)
***** call
      test(X, y, neighbors.KNeighborsClassifier(metric='pyfunc', func=distance.cityblock))
***** result
[ 0.78809524  0.8         0.775       0.7775      0.7875    ]
mean = 0.785619047619
**** knn (~76%)
***** call
      test(X, y, neighbors.KNeighborsClassifier())
***** result
[ 0.77619048  0.7475      0.7325      0.78        0.78      ]
mean = 0.763238095238
*** *duration* (git c435)
**** no change in rbf
**** +2 ca in Deci

* WAIT Discussion
  intel model: interdependences (html bigger \to more embedded) not mentioned
* WAIT Acknowledgements
  - Elena
  - Daniel Arp
  - Prof. Dr. Konrad Rieck
  - Tao Wang
  - ...
* WAIT Further work
  - source cover traffic: user gives domain as starting point
  - how to generate
    - how often, which parameters
    - just triggered by start and until end, or for each load
  - background if non-active (IPP self-similar)
    - 802.16 model
  - does a new connection to another site create a measurable response
    (with variable-length packets)?
  - provable protection
  - compressible cover traffic
** onion host for cover traffic
   As indicated f.ex. by Wang and Goldberg,
   \cite{wpes13-fingerprinting}, network load already is a bottleneck
   on Tor, with the key bottleneck being exit nodes\cite{wtfpad}. The
   exit nodes might be spared the extra traffic by using =.onion=
   traffic generators (or, alternatively, hosts). A traffic generator
   could be further optimized by using tor proposals ... (see todo) to
   reduce latency, if this does not reduce privacy.
*** TODO tor proposals as of tor.sx
*** TODO read/skim and cite "on performance..."
* lit
** firefox pipelining
   [[file:~/da/docs/master.bib::firefox-pipelining][2015: Enable HTTP]]
** HTTP/1.1
   [[file:~/da/docs/master.bib::rfc2616][R. & Berners-Lee 1999: Hypertext Transfer Protocol]]
** RFC7230
   [[file:docs/master.bib::rfc7230][R. & J. 2014: Hypertext Transfer Protocol HTTP]]
* TODO appendices [0/3]
** TODO [[file:./bin/one_site.py]]
   insert file here
** TODO [[file:../bin/extract_attribute.py][extract_attribute.py]]
   insert file here
** TODO [[file:../bin/wsgi.py]]
* unused
** from [[*transform to panchenko-features]]
  The code to examine a single trace file is in =analyze_file()=
  It
  - opens the filename in tshark
  - splits the output by tokens
  - gives the relevant values (source IP, size, timestamp) (with the
    timestamp not used by Panchenko) to a =Counts=-object, which
    aggregates it

  [...]
  For a single line, a =Counter=-object aggregates bytes (incoming,
  outgoing), packets (incoming/outgoing), distills into a size/packets
  array and (size+timestamp)/packets array.
  [...]
  This is used in =postprocess()= to determine
  - size markers, (via the =_sum_stream()=-function),
  - the html marker as the first of those
  - the total transmitted bytes incoming and outgoing
  - number marker (via the =_sum_numbers()=-function)
    - slightly extended, as the number 16 was occuring
      everything above 14 was mapped to the same as 14
    - a bit unclear, currently, 3-5 \to 3, 6-8 \to 4, 9-13 \to 5, 14-\infty \to 6
  - occurring packet sizes incoming and outgoing (binned in steps of 2)
  - percentage of outgoing packets
  - number of packets incoming and outgoing.
** start browser with -marionette parameter
   Each modern Firefox, and thus also the tor-browser-bundle, has
   marionette-support built-in. It needs to be enabled on the
   command-line via the =-marionette= switch, for example


   This starts the Tor browser with marionette enabled.
*** marionette support page link
** Sally installation
   Sally is a tool to transfer text into points in a vector space.

   It is installed on Ubuntu Vivid Vervet by following the official
   instructions, then changing =vivid= in the file
   =/etc/apt/sources.list.d/mlsec-ubuntu-sally-vivid.list= to
   =devel=.
** from getting tbb to work
  One external repository is required, which can be installed via

  =add-apt-repository ppa:ubuntu-toolchain-r/test=
  =apt-get update=
  =apt-get dist-upgrade=

  Furthermore, the binary needs some firefox libraries, which can be
  retrieved most easily via =apt-get install firefox=.

  Afterwards, the binary can be started by typing =./firefox=.
** throttling
   As especially outgoing web requests are often quite small, and this
   paper has at the moment a 1:1 rate of outgoing vs incoming for the
   requests, throttling the amount of data leaving the end user might
   well suffice for reducing the bandwidth of the side-channel enough
   to make it insignificant.
