#+TITLE: Selective Cover Traffic
#+TODO: TODO CHECK | DONE
* Configuration							    :ARCHIVE:
#+LATEX_CLASS_OPTIONS: [a4paper,10pt]
#+LATEX_HEADER: \usepackage{adjustbox}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \renewcommand*{\maketitle}{\thispagestyle{empty}
#+LATEX_HEADER:
#+LATEX_HEADER: \hspace{20cm}
#+LATEX_HEADER: \vspace{-2cm}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{figure}[H] \hspace{11cm}
#+LATEX_HEADER: \includegraphics[width=3.2 cm]{pictures/HU_Logo}
#+LATEX_HEADER: \end{figure}
#+LATEX_HEADER:
#+LATEX_HEADER: \begin{center}
#+LATEX_HEADER:   \vspace{0.5 cm}
#+LATEX_HEADER:   \huge{\bf Selective Cover Traffic} \\ % Hier fuegen Sie den Titel Ihrer Arbeit ein.
#+LATEX_HEADER:   \vspace{1.5cm}
#+LATEX_HEADER:   \LARGE  Diplomarbeit \\ % Geben Sie anstelle der Punkte an, ob es sich um eine
#+LATEX_HEADER:                 % Diplomarbeit, eine Masterarbeit oder eine Bachelorarbeit handelt.
#+LATEX_HEADER:   \vspace{1cm}
#+LATEX_HEADER:   \Large zur Erlangung des akademischen Grades \\
#+LATEX_HEADER:   Diplominformatiker \\ % Bitte tragen Sie hier anstelle der Punkte ein:
#+LATEX_HEADER:          % Diplominformatiker(in),
#+LATEX_HEADER:          % Bachelor of Arts (B. A.),
#+LATEX_HEADER:          % Bachelor of Science (B. Sc.),
#+LATEX_HEADER:          % Master of Education (M. Ed.) oder
#+LATEX_HEADER:          % Master of Science (M. Sc.).
#+LATEX_HEADER:   \vspace{2cm}
#+LATEX_HEADER:   {\large
#+LATEX_HEADER:     \bf{
#+LATEX_HEADER:       \scshape
#+LATEX_HEADER:       Humboldt-Universit\"at zu Berlin \\
#+LATEX_HEADER:       Mathematisch-Naturwissenschaftliche Fakult\"at II \\
#+LATEX_HEADER:       Institut f\"ur Informatik\\
#+LATEX_HEADER:     }
#+LATEX_HEADER:   }
#+LATEX_HEADER:   % \normalfont
#+LATEX_HEADER: \end{center}
#+LATEX_HEADER: \vspace {5 cm}% gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: %{3.2 cm} bei Verwendung von scrreprt, gegebenenfalls kleiner, falls der Titel der Arbeit sehr lang sein sollte
#+LATEX_HEADER: {\large
#+LATEX_HEADER:   \begin{tabular}{llll}
#+LATEX_HEADER:     eingereicht von:    & Michael Kreikenbaum && \\ % Bitte Vor- und Nachnamen anstelle der Punkte eintragen.
#+LATEX_HEADER:     geboren am:         & 13.09.1981 && \\
#+LATEX_HEADER:     in:                 & Northeim && \\
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     Gutachter:          & Prof. Dr. Konrad Rieck (Universität Braunschweig) && \\
#+LATEX_HEADER: 		        & Prof. Dr. Marius Kloft && \\% Bitte Namen der Gutachter(innen) anstelle der Punkte eintragen
#+LATEX_HEADER: 				 % bei zwei männlichen Gutachtern kann das (innen) weggestrichen werden
#+LATEX_HEADER:     &&&\\
#+LATEX_HEADER:     eingereicht am:     & \dots\dots \\ % Bitte lassen Sie
#+LATEX_HEADER:                                     % diese beiden Felder leer.
#+LATEX_HEADER:                                     % Loeschen Sie ggf. das letzte Feld, wenn
#+LATEX_HEADER:                                     % Sie Ihre Arbeit laut Pruefungsordnung nicht
#+LATEX_HEADER:                                     % verteidigen muessen.
#+LATEX_HEADER:   \end{tabular}
#+LATEX_HEADER: }}
* TODO organize [[*Mozilla%20Add-On%20Sdk][addon]] sections
* TODO [#A] Final Sections
** [[file:~/da/da.org::*skype%2013.5.16][proposal]]							    :ARCHIVE:
** LATER Abstract
** Introduction (=basics)
*** Tor
**** brainstorm
     - origin
     - usage
       - socks
       - web browsing
       - hs
     - internals
       - relevant
       - has crypto, but not much more
***** depend [[*Website%20Fingerprinting][Website Fingerprinting]]
     - effect on traffic/wf
       - delay
       - padding
**** Related Work
     - tor design
     - tor spec
     - onion routing papers
*** Website Fingerprinting
**** brainstorm
     - see traffic
       - http
	 - today
     - induce pattern
       - naive
       - machine learning
       - features
     - examples
       - forest
       - knn
       - panchenko1
	 - algo
       - p2
         - how to get to run
**** Related Work
     - origin: ssl-schneier-personal comm
       - berkeley
       - term
     - application to anonymity systems
       - hintz: safeweb, wf-term
       - herrmann: breakable
       - panchenko
       - it started for real
       - dyer
       - cheng
       - wang
       - dts-approach (?)
*** Defenses
**** brainstorm
     - obfuscate features
     - specific features
       - morphing
     - general obfuscation
       - deterministic
         - fixed data rate
	 - supersequence if known
       - stochastic
**** related work
     - wright
     - luo
     - panchenko decoy
     - padding (sslv2 \to 3)
     - requestpolicy
     - text-only browsing
*** Algorithms
**** brainstorm
     - SVM
     - knn
     - ?extratrees?
     - features important or also classifier
     - no classifier fits all
**** related work
     - mitchell
     - sklearn
     - td: later some papers (see sklearn)
** ... (=methods)
*** Motivation
**** brainstorm
     - privacy anecdotes
     - offline argument
     - online arguments pro/con
     - little cost, big benefit
     - if decided for extra traffic, what is a good approach
       - *website* fingerprinting: HTTP/HTML-specific features are exploited
	 - application-level problem, application-level defense
*** Design and Implementation (=Implementation)
**** brainstorm
     - aim: selective cover traffic
       - select based on web site
       - and target
       - simultaneous to real traffic
     - firefox browser extension / addon
       - addon sdk
       - maybe mention next generation
     - good code
       - tests
	 - unit tests
	 - by hand
       - good parts
       - js garden
       - style guide
       - version control
     - algorithm
     - implementation
       - classes
     - server
       - later: .onion (link to related work)
*** Bloom Filters
**** brainstorm
     - stochastic fixed-width data structure
     - works flawlessly if element is inside
       - might fail if not
     - based on this: bloomsort: combine filters
       - sort into bins
	 - based on target distribution
	 - one bloom filter per bin
       - check size: check all filters
	 - if one returns: fine
	 - if none returns: ok: clear that not inserted, default value
	 - if two return: error, fall back to default value
       - error estimation?
       - +: fixed size
       - -: error both ways, and difference bin-size to real size
**** TODO related work
**** TODO maybe move to intro (talk to daniel again)
** TODO Results and Evaluation
*** panchenko v1 vs cumul
**** wenn noch megaviel zeit: wang-knn (sota?)
**** brainstorm
    - panchenko1: effectiveness (+ added)
      - different classifiers
    - panchenko2:
      - better
      - or results
      - panchenko's dataset
*** Evaluation of Defenses
**** sota (practical): wtfpad
**** sota (theoretical): walkie-talkie
**** (maybe) vs optimal attacker
**** brainstorm
     - addon
       - different versions
       - different factors
* TODO topics [0/129]
** Teaser
   #+BEGIN_QUOTE
   "They who can give up essential liberty to obtain a little temporary
   safety, deserve neither liberty nor safety" - Benjamin
   Franklin\cite{franklin}
   #+END_QUOTE

   In Germany, the basis of all laws is the "Grundgesetz", which
   ensures free speech (Art. 5 GG) and protects private communication
   (Art. 10 GG).

   The Fourth Amendmend to the United States Constitution is
   interpreted as providing similar protections to Art. 10 GG, as of
   \cite{katz}.

   With new technologies for communication and information emerge new
   challenges to secure these rights.

   The internet has offered many new ways to communicate and,
   conversely, wiretap, of which website fingerprinting and page-marker
   detection are examples.

   Yet, there may exist user-friendly ways to hamper, or even to deter,
   this surveillance.
*** evtl rein
    Auch in der DDR gab es das Recht auf freie Meinungsäußerung, nur
    hat es niemand genutzt, da durch die Ueberwachung die Angst vor
    Repressalien zu groß war. (td: quote)
*** WAIT may to (ohne) (falls richtig)
** TODO Website Fingerprinting [0/60]
*** tmp - reorder
**** 

*** TODO What is Website Fingerprinting
    Website fingerprinting\cite{hintz02} can be used to discover which
    webpages or websites a user visits via an anonymizing proxy. It is
    a type of traffic analysis\cite{applied96}, where characteristics
    of (encrypted) traffic are used to infer conjectures about metadata
    and content.
*** TODO What happens during a website request
    When a browser such as Mozilla Firefox[fn:: \url{mozilla.org}]
    retrieves a website, it does many things under-the-hood.

    First, it retrieves the main object

    When a user visits a webpage, the browser first sends a request
    for the HTML page. The webserver answers with that HTML page. The
    browser requests the objects embedded into that page, such as
    stylesheets (\verb|<link rel="stylesheet">|), fonts
    (=@font-face=\cite{moz-fontface}), images (=<img>=),
    scripts(=<script>=), etc.  Each of these files has a specific
    size, which might be detected in the TCP-flow.

    Thus, the objects embedded within a page could allow a local
    passive observer to infer which web page from a set of pages the
    user requested.

    - browser
    - first html
    - then referenced objects
      - if not in cache
    - 1.0: one connection for each as default
    - 1.1: persistent as default: queue
**** TODO maybe picture
**** say that biggest part images
**** join with [[Hurdles][file:~/da/git/diplomarbeit.org::Hurdles]]
*** CHECK Why could website fingerprinting be a problem
    As a typical scenario, consider the government of some state. A
    whistleblower posts something very critical of the regime on a
    well-known critical website. The whistleblower uses Tor or some
    other anonymity service to protect his identity. The government
    monitors and records all Tor connections. Even though Tor
    obfuscates the user's traffic, the specific data-pattern of the
    website allows the government to limit its search to, say three,
    subjects. This gives the whistleblower away.[fn:: Such has not been
    observed.]
*** TODO visual inspection of data [0/5]
    To exemplify the task of a websitefingerprinter, consider the
    following pictures which represent complete (considered to contain
    all relevant information \cite{a-systematic} packet trace data in
    the form of (delay, packet size), which is
**** facebook.com
     #+CAPTION: facebook.com example 1
     #+ATTR_LATEX: :width 0.3\linewidth
     [[./pictures/facebook_com@1445350531.png]]
     #+CAPTION: facebook.com example 2
     #+ATTR_LATEX: :width 0.3\linewidth
     [[file:pictures/facebook_com@1445422155.png]]
     #+CAPTION: facebook.com example 3
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[file:pictures/facebook_com@1445425799.png]]
     #+CAPTION: facebook.com example 4
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[file:pictures/facebook_com@1445429729.png]]
**** craigslist.org
     #+CAPTION: craigslist.org example 1
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[./pictures/craigslist_org@1445352269.png]]
     #+CAPTION: craigslist.org example 2
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[./pictures/craigslist_org@1445428146.png]]
     #+CAPTION: craigslist.org example 3
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[./pictures/craigslist_org@1445435476.png]]
     #+CAPTION: craigslist.org example 4
     #+ATTR_LATEX: :width 0.3  \linewidth
     [[./pictures/craigslist_org@1445442917.png]]
**** creation of pictures
     The pictures were created by the commands

     #+BEGIN_EXAMPLE
      for fb in $(ls | grep facebook); do
        python ~/da/bin/counter.py ./$fb  | tail -1 | sed 's/),/\n/g' | \
            tr -d "'()][" > /tmp/times;
        gnuplot -e "set terminal png size 1024,680; \
                set output \"/tmp/${fb}.png\"; plot '/tmp/times' with boxes;";
      done
     #+END_EXAMPLE

     and respectively for craigslist, in the directories containing the
     pcap files.

     These commands first extract the timing attributes (at git commit
     791af76 the last line of the output of counter.py), format it for
     gnuplot (inserting appropriate newlines via =sed= and removing
     extra characters via =tr=), and =gnuplot= s it to a png file with
     the name of the trace file as prefix.
**** TODO when done, format in LaTeX (if not here) to two-column layout
**** TODO visual representation of CUMUL
     - Panchenko et al.'s recent approach allows for the visual
       comparison of website traces.
     - see images etc
     - see how it's done
     - example
       #+CAPTION: CUMUL example from {\url https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf}
       #+ATTR_LATEX: :width \linewidth
       #+BEGIN_EXAMPLE
       [[./pictures/cumul_resized_aus_paper.jpg]]
       #+END_EXAMPLE
***** TODO or just link here to CUMUL
***** TODO think about order of this (at cumul, at visual, mention other...)
      then formulate correctly
***** TODO get picture to work
*** tools
**** TODO capture alternatives [0/1]
     Several applications can capture network traffic to files. The most
     well-known and oldest of these is tcpdump
     [fn:: \url{http://tcpdump.org}] It is a command-line utility, which is
     available on many UNIX-like systems and Windows.

     A modern contender with a GUI is wireshark. It also sports a
     command-line version, tshark. As it offers TLS packet reassembly,
     tshark was used in this thesis.

     Both programs rely on the libpcap library for access to network
     packets.
***** TODO subsect to [[*by-hand initialization to retrieve websites][by-hand initialization to retrieve websites]]
**** shell script
     Simply calling =firefox website= loads the website in Firefox. This
     is the approach Wang recommended(\cite{wang-scripting}.
***** TODO how to check that page has loaded
**** Selenium
     Selenium is the de-facto standard for testing web applications. It
     has drivers for several browsers, allowing it to control them, and
     evaluate the retrieved page. Its documentation is currently
     transferring from Version 1 to Version 2.
**** Chickenfoot
     Chickenfoot was a Firefox addon which allowed browser scripting. It
     was developed at MIT\cite{chickenfoot}. The most recent GitHub
     release[fn:: \url{https://github.com/bolinfest/chickenfoot}] is for
     Firefox 4.
**** CHECK Marionette
     <<Marionette>> Marionette is the next generation mozilla testing
     framework. It is works just like Selenium and was designed to be
     integrated into it. It was chosen for this thesis, as it made the
     Tor Browser Bundle easily accessible.

     After installation of the library (see below), controlling the browser
     takes two easy steps:

     1. start the Tor Browser Bundle with the `-marionette` switch

        #+BEGIN_SRC sh
          cd tor-browser_en-US/Browser
          ./firefox -marionette
        #+END_SRC

     2. attach to a running browser in Python

        #+BEGIN_SRC python
          from marionette import Marionette
          client = Marionette('localhost', port=2828);
          client.start_session()
          client.navigate('http://cnn.com'); # navigate loads a website
        #+END_SRC

     Marionette has the benefit that the =client.navigate()= call
     returns only after the page has loaded, (and throws an error if
     the page could not be loaded). This obsoletes f.ex. Panchenko et al.'s
     \cite{panchenko} need to test whether a page loaded completely.
**** CHECK Marionette installation
     Marionette exists as a Python Package. It is thus easily installed
     via

     pip install marionette_client

     After installation pip via =sudo apt-get install python-pip=). Using
     a virtualenv is highly recommended in the documentation. If using
     only Marionette, it proved to be unnecessary. The combined
     installation of Marionette with Mozmill broke Marionette.
***** TODO merge with above and split out pip install (also needed for wsgi)
**** criteria for tool to retrieve websites
     - script tor browser: load new page
     - easy set-up
     - should
       - register page load or error
     - might
       - set tor's paranoia slider
       - install extra addon
**** TODO who used which retrieval method
     - who did sth
       - p: 
         1. chickenfoot only
         2. Chickenfoot, iMacros, and Scriptish
       - h
       - ll
       - w
       - c
       - d
       - j
     - what did they use
       - list
       - chickenfoot
       - modified browser
       - selenium: daniel
       - plain tor bundle
*** setup
**** TODO by-hand initialization to retrieve websites
     After installation, the tor browser bundle performs some
     initialization steps. To complete these easily, start the tor
     browser bundle-firefox by hand once, set the connection type and
     have it load any website via Tor. All this also downloads Tor
     metadata, which allows to connect more quickly later on.

     Once the Browser Bundle is working, as it runs when starting
     manually, webpages can be retrieved automatically. This is done
     via the [[one-site.py][one-site.py]] script.

     The script

     1. starts the Tor Browser Bundle's =firefox= binary, enabling
        remote-control via the [[Marionette][=-marionette=]] command-line argument,
        waiting up to 60 seconds for its initialization
     2. starts the =tshark= capture
     3. loads the page (given as first parameter) via Marionette
     4. waits up to 600 seconds for the page load to finish
     5. waits 3 more seconds (for the last cover traffic to finish)
     6. ends the capture
     7. ends Firefox

     This setup (restart after each trace) avoids caching issues with
     website fingerprinting, as the Tor Browser Bundle cleans the
     cache between restarts (as mentioned f.ex. in \cite{critique}). If a
     browsing-session scenario is desired, the script could be
     modified to omit terminating the browser instance.
***** TODO this is not [only] by-hand! split or change title
**** tshark installation
     You also need to install =tshark= [fn:: via f.ex. =sudo apt-get
     install tshark= on Debian-based systems] and enable the user to
     capture packets [fn:: via (Debian-based) =sudo dpkg-reconfigure
     wireshark-common= and adding the user to the =wireshark= group
     (in =/etc/groups=)].
**** TODO how to get tor browser bundle to work
     In order to start the tor browser bundle via the =./firefox=
     command, you need libraries, which are bundled with the binary.
     They can be found inside the =/TorBrowser/Tor= directory.

     The library path environment variable can be set on the command-line via
     #+BEGIN_SRC sh
     export LD_LIBRARY_PATH=/lib:/usr/lib:/path/to/bundle/Browser/TorBrowser/Tor
     #+END_SRC
     The script [[one-site.py][one-site.py]] uses this internally.
***** TODO where exactly is the torrc: directory
***** TODO is old, still use, or remove?
**** Avoiding safe mode on restart
     If Firefox was killed via a signal (as opposed to closing the
     window), it prompts to start in Safe Mode afterwards.

     This behavior can be avoided in three ways:

     You can set the firefox preference
     =toolkit.startup.max_resumed_crashes= to -1, you can set the
     environment variable =MOZ_DISABLE_AUTO_SAFE_MODE= (did not work
     in Tor Browser Bundle version ...), or --- as a last resort ---
     you can remove the =toolkit.startup.recent_crashes= line in the
     =prefs.js= config file which saves the number of consecutive
     kills via =sed -i '/toolkit\.startup\.recent_crashes/d'
     /path/to/prefs.js=.
***** TODO TBB current version
**** headless configuration
     If you want to capture on a headless server, you can use the
     =xvfb=-package. which is installed via =sudo apt-get install xvfb=.

     Then, you can run the X Virtual Framebuffer via

     =Xvfb :1=

     tell the browser to use it via

     =export DISPLAY:1=

     and start the retrieval as mentioned above.
**** TODO thoughts on size of data set
     - computable (n^2 for svm with good results)
     - number of instances negligible for computation
       - check this
     - stable results
     - recent papers
       - Panchenko: 775 a 20
       - Wang:
         - 100 a 90 of sensitive pages
         - 5000 a 1 of non-monitored pages
       - Cai: 400 samples of bbc.co.uk
         - 100 \to 800 once a 20 \to 40 twice
     - (currently closed world)
***** TODO more complete list?
**** TODO filtering tshark files [0/4]
     Although this requirement might later be removed (see [[*Further%20work][further
     work]]), the addon currently needs a generator of cover traffic to
     work. While it can be set in the add-on's preferences, this
     generator ran on the same host as the tor client. Thus, the
     capture files also contained traffic of the cover traffic
     server. As they do not belong to the Tor traffic, are not what
     the adversary sees, and might distort the result, they were
     filtered. (Even though the accuracy results were not greatly
     changed by this).

     Fortunately, =tshark= offers a way to filter these files as
     mentioned in \cite{splitcap}. The (read) filter commands are
     described in the manual \cite{wireshark-filter}, with the tcp
     protocol specific fields as given in \cite{tcp-filter-fields}.

     The script to solve this is in the appendix [[7777]]. As the server
     ran on port 7777, which was allowed only as an incoming port by
     the firewall, it suffices to filter by port name. (Otherwise, the
     read filter would need to be modified).
***** TODO implementation
     - summary approach: file 7777.sh takes each (pcap) file in
       current directory, filters the port 7777 out
     - apply this to each subdirectory
     - then move all files to a common directory
****** TODO include script from duckstein
***** TODO link to man tshark
*** TODO what happens when retrieving a website
    The complete data of google.com can be retrieved via

    =mkdir site; cd site; wget -p -H google.com=

    which yields (in germany) the files (=find . -type f -ls=, formatted)

    |  size | url                                                               |
    |-------+-------------------------------------------------------------------|
    |       | <65>                                                              |
    | 18979 | google.com/index.html                                             |
    | 17284 | www.google.de/images/nav_logo229.png                              |
    |  1834 | www.google.de/images/icons/product/chrome-48.png                  |
    |  5482 | =www.google.de/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png= |
    |  5430 | =www.google.de/images/branding/product/ico/googleg_lodp.ico=      |
    |  8080 | www.google.de/robots.txt                                          |

    thus, there should be 5-6 (depending on robots.txt) requests
**** TODO tshark for normal (non-tor) retrieval
*** practical wf: analyzing traces
**** TODO how to process the data
     The aim of processing is to extract features relevant for machine
     learning from the original trace files, which are in =pcap= format.

     Of the several tools available for reading =pcap=, =tshark= was
     chosen. It is the command-line version of the Wireshark protocol
     analyzer[fn:: \url{http://www.wireshark.org}].

     - tshark internally
     - python triggers
     - collects,
     - sums in the end
     - displays
     - =Counter=-class
***** TODO why filtering allowed
***** TODO see if merge/unify with [[transform to panchenko-features]]
***** TODO and if include stuff from [[*from%20%5B%5B*transform%20to%20panchenko-features%5D%5D][from {{*transform to panchenko-features}}]]
**** transform to panchenko-features
     In encoding packet sizes, this thesis follows Panchenko et al.'s
     approach, who recorded "incoming packets as positive, outgoing ones
     as negative numbers."\cite{panchenko}
**** usage of counter.py to extract features from pcap
     Once the website traces are stored in pcap-files, feature vectors
     need to be extracted. A feature vector is represented by a Python
     class `Counter`, which can be created from a pcap file, or persisted
     to a json file containing timing and packet size information (to
     save time and space).

     To create a counter, you can use `counter.Counter.from(filename1,
     filename2, ...)`. This is also called indirectly when using
     `counter.py` from the command line, as in

     python -i /path/to/counter.py

     This extracts data from all pcap files in the current directory and
     subdirectories (excluding Address Resolution Protocol messages and
     ACKs). The filename of the pcap files needs to be `domain@tstamp`,
     for example `craigslist.org@1445352269`. The part up to the
     separator `@` is treated as the URL. If JSON-files of the name
     `domain.json` (for example `craigslist.com.json`) exist, those are
     preferred instead of the pcap files.

     In the interactive shell, there is a dictionary called `COUNTERS`,
     with the domain names as keys and an array of `Counter`s as
     values. To persist these to JSON, you can use `save` in the
     python interactive shell, for example

     >>> Counter.save(COUNTERS)

     To distill the features from a single `Counter`, call its
     `panchenko()`, to inspect single features, call
     `get('feature_name')` (for example
     =COUNTERS['cnn.com'][0].get('duration')')=.

     `panchenko()` yields a feature vector with default padding of
     Panchenko's variable-length features. Since Panchenko et
     al\cite{panchenko} gave explicit size conversions, the sizes have
     not been normalized further. The default padding (300 per
     feature) might not be large enough for some traces.
***** TODO maybe rename counter.py to trace.py
**** LATER and MAYBE how to get wang/goldberg to work
     As the =notes= file says:

     "svm-train and svm-predict come from the libSVM package."
***** maybe to unused
**** TODO libsvm (short)
     LibSVM is a library for support vector machine classification and
     regression. It is used under-the-hood for scikit-learn, yet one part
     of functionality required a specific module which was not
     integrated.

     Its input format is very simple: First a number determining the
     class of the data, then a colon, finally all the data for an
     instance, separated by whitespace.
***** TODO link to code to generate
**** TODO transform features to vector
     Once the =Counter=s data is obtained, it needs to be transformed
     to input for scikit-learn's\cite{scikit-learn} classifiers.

     The code to convert these features to classification input can be
     found in `analyse.py` (see [[analyse.py][appendix]]). This determines the maximum
     length of all variable-length features, 0-pads Panchenko's features
     with zeroes to the same length, and converts them to an array fit
     for input into scikit-learn's classifiers. When called from the
     command line, as

     python -i /path/to/analyse.py

     , it will extract the feature vectors from JSON or pcap files in
     the current directory, and run 5-fold cross-validated classifiers
     against the data.

     =Counter= input features are transformed into scikit-learn input
     in the =to_features()= function, which normalizes all vectors to
     have the same size (padding with 0s), and creates the feature
     matrix =X= with numeric class labels =y= (and class names in
     =y_domain=).

     If you wish to run LibSVM on the command-line, there is also
     =to_libsvm(X, y, fname='libsvm_in')=, which can be called with the
     output of =to_features=. It writes lines in X with labels in y to the
     file 'libsvm_in' (by default).
***** TODO ref stackoverflow why 0 padding
****** TODO or better, some statistics text
***** TODO see also =to_features_cumul=
**** LATER effect of panchenko's weighting schema
     Currently, fixed attributes are weighted heavily in favor of total
     incoming/outgoing bytes.
***** maybe
**** TODO scikit-learn
     The python module scikit-learn\cite{scikit-learn} is described as a
     collection of "tools for data mining and data analysis".

     It combines python's ease-of-use with the efficiency of libraries
     written in C, such as LibSVM. It offers many different classifiers
     and regressors, such as K-NN, SVM, decision trees, linear
     approximation, random trees, etc.
***** TODO regressor? wording
*** TODO History of Website Fingerprinting
    The idea of using traffic analysis to gather information about
    encrypted traffic was mentioned in \cite[10.3]{applied96} and
    applied in the analysis of SSL 3.0 by Wagner and
    Schneier\cite{SSL}, where it was attributed to Bennet Yee.

    - quantifying etc

    The term /website fingerprinting/ was coined by
    Hintz in 2002. A successful attack against single-hop proxies was
    carried out by Herrmann et al. in 2009.

    The website fingerprinting attack scenario is already described in
    the original Tor design paper\cite{tor-design}. Previous to
    Panchenko et al.\cite{panchenko}, it was considered "less
    effective"\cite{tor-design} against Tor, due to stream/circuit
    multiplexing and fixed cell sizes.
**** index? traffic analysis
*** TODO defenses
    - walkie-talkie
    - wtfpad
    - supersequence
    - tamaraw
    - buflo

    There are other methods of defense, which might help mitigate
    website fingerprinting. A certain browser extension and text-only
    browsing might reduce the fingerprint.
**** CHECK Additional Plugin: requestpolicy
     In addition to the security-centric addons deployed with the
     Tor-Browser-Bundle, there is an additional addon with orthogonal
     protection:
     RequestPolicy[fn::\url{https://requestpolicycontinued.github.io/}]
     controls which third-party content to load on a given page. Every
     query to the original domain is allowed, while requests to other
     domains must be temporarily or permanently approved. It comes
     with a restrictive set of pre-defined rules (for example google
     pages are allowed to access gstatic). Both a blacklist and a
     whitelist mode exist.

     This could easily (and individually) alter the request/response
     characteristic of a website. More study might shed some light.

     RequestPolicy hindered early versions of the Addon, as it blocked
     [[page-worker]]s. If both are deployed alongside, it should be
     carefully checked.
***** TODO move below tbb
***** MAYBE also cite requestpolicy (orthogonal)
**** CHECK write new plugins
     Instead of inserting dummy traffic into the connection, one could
     throttle the "data rate" of request and responses (or only
     requests or the ratio) --- optionally padding with dummies up to
     the maximum rate.

     This approach has been used by f.ex. \cite{effective}, and has
     been proven to work, albeit requiring higher latency, it has not
     been explored further, as
     - it might be hard to implement in a plug-in, and
     - randomized defenses seem offer adequate defense at reduced
       latency and bandwith
***** TODO move to description of other defenses
**** CHECK tor browser bundle defense
     After the attack by Panchenko et al. \cite{panchenko}, the Tor
     Project deployed an experimental defense \cite{experimental} in
     the Tor Browser Bundle.

     This defense enables HTTP pipelining and randomizes both the
     number of concurrent requests and their order.  It was shown to
     be ineffective by \cite{ccs2012-fingerprinting}, and confirmed by
     \cite{wpes13-fingerprinting} and \cite{effective}.
***** TODO HTTP pipelining refer to/elaborate, make own show subsubsection
**** TODO running an OR
     - hinted by ...
     - extra traffic
     - depends on data rate: if all is easily decorrelatable, maybe no
       extra protection
**** CHECK text-only
     As the sizes and interconnection of HTML and embedded content is
     what makes a webpage easily identifyable, using a text-only
     non-javascript browser such as Lynx might be a mitigation for those
     who consider this trade-off acceptable.
***** TODO lynx link
*** distribution of (main) features
    These distribution histograms show how Panchenko's main features
    are distributed. They are stacked histograms with classes
    separated by colors. They are compared (visually) to the HTTP
    Traffic Model\cite{newtrafficmodel}.

    [[file:pictures/all_count_in.png]]
    shows the number of downstream/incoming packets.

    The general form of a gamma distribution may be
    fitting. Conceptually, this should be approximately

    num_embedded (gamma) * size_embedded (lognormal) / packet_size

    [[file:pictures/all_count_out.png]]
    shows the number of upstream/outgoing packets.

    Conceptually, the

    [[file:pictures/all_length_0.png]]
    the length of the Size Marker feature vector.

    [[file:pictures/all_num_sizes_in.png]]
    number of different packet sizes downstream/incoming.

    [[file:pictures/all_num_sizes_out.png]]
    number of different packet sizes upstream/outgoing.

    [[file:pictures/all_percentage_in.png]]
    percentage of incoming bytes (of total).

    [[file:pictures/all_total_in.png]]
    total bytes downstream/incoming.

    [[file:pictures/all_total_out.png]]
    total bytes upstream/outgoing.
**** TODO compare to HTTP model
*** Hurdles to website fingerprinting
    <<Hurdles>>
    The progress of web protocols made website fingerprinting
    harder. In the original HTTP/1.0\cite{rfc1945} protocol, each
    request used a separate TCP-connection. This facilitated the
    original attacks against HTTPS browsing\cite{quantifying} and the
    anonymizing web proxy SafeWeb\cite{hintz02}, which both extracted
    the exact file size of each embedded object.

    Building a new connection for each transferred object proved to be
    inefficient. Some HTTP implementations \cite{rfc2068} used
    persistent connections. These were included HTTP/1.1
    \cite{rfc2616}. Due to this, it was no longer trivial to extract
    the files' sizes. You had to determine the start and end of each
    request. (which was still possible by seeing when the client sent
    a new request).

    [[./pictures/HTTP_persistent_connection.png]]

    In addition to persistent connections, HTTP/1.1 allowed pipelining
    several HTTP requests in a single connection without waiting for
    the files to arrive in between.

    [[./pictures/HTTP_pipelining2.png]]

    As this created problems with some servers, pipelining was disabled
    by default in Firefox \cite{firefox-pipelining} and Chrome
    \cite{chromium-pipelining}, and not implemented in Internet
    Explorer \cite{ie-pipelining}.

    After the Panchenko paper\cite{panchenko} an additional no-cost
    defense prototype was implemented in the Tor-Browser-Bundle
    \cite{experimental}: Firefox's built-in request pipelining was
    enabled with added request order randomization.  Cai et al. found
    fingerprinting to be easier with this defense enabled than
    without. \cite{ccs2012-fingerprinting}

    Originally, a browser should open at most two connections per host
    \cite{rfc2616} to retrieve the files one-by-one. An update
    \cite{rfc7230} removed this fixed limit, but encouraged clients
    "to be conservative when opening multiple connections".
*** Who could attack via WF
    As website fingerprinting requires very litte resources, a specific
    attacker could be a WLAN sniffer, an ISP, up to maybe even a nation
    state.
*** CHECK Panchenko et al.'s Attack via Website Fingerprinting
    The first website fingerprinting
    attack\cite{ccsw09-fingerprinting} to also target Tor had yielded
    little accuracy. This was due\cite{experimental} to Tor's
    multiplexing and fixed cell-size.

    Panchenko et al\cite{panchenko} were the first to publish a
    successful website fingerprinting attack on Tor. They extracted
    HTTP-specific features from the packet trace and used those in a
    hand-tuned support vector machine with a radial basis function
    kernel.
**** practical wf: Capturing traces
**** TODO move to subsection related work
** TODO Tor [0/15]
*** How to use Tor
    Tor offers a SOCKS5\cite{rfc1928}-(TCP\cite{rfc793}-)proxy for
    users. (If an application has no proxy settings, the
    =tsocks=-program provides transparent proxying). After setting this,
    all traffic is routed through the Tor network, anonymizing the IP
    address, with a fixed message length to hinder traffic analysis.

    (Tor also offers the possibility to "hide" a (TCP-)internet service
    via "hidden services" so that noone can see its location).
*** CHECK how Tor works internally
    A TCP stream is triply-encrypted and sent along a path of three
    intermediaries, none of whom can link its origin to its destination.

    After the third hop (the /exit node/), the message most often leaves
    the Tor network to any server on the Internet. It could also be
    forwarded to a location-hidden server via Tor's /hidden services/.

    To build a TCP channel, called /circuit/ in Tor, the Tor client
    software (/Onion Proxy/) uses a telescoping approach:

    First, Tor builds a cryptographically secure connection to its first
    hop (each hop is called /Onion Router (OR)/). This connection is on
    top of TLS, using additional, same-length, Tor encryption.

    Through this connection, the onion proxy establishes a connection to
    a second hop, and through the second, to a third hop. (The actual
    TCP connection is from the first hop to the second, and from the
    second to the third). Each message to be sent is encrypted three
    times, sent to the first hop, which decrypts it once, and sends it
    on. The second and third hop do likewise, with the third hop sending
    the plain-text-message in to the desired recipient.

    Tor sends data traffic in 512-byte cells.
**** TODO pictures with attribution
*** TODO Tor's Cryptography
    While Tor's anonymity depends on cryptography, website
    fingerprinting gains information despite the cryptography. Thus,
    this section will provide only a high-level overview.

    Cryptography in Tor provides confidentiality and integrity
    \cite{applied96} of the passed messages and authenticates
    \cite{applied96} the intermediaries.

    - identity keys
    - onion key / signing
    - short-term connection

    - previous versions \cite{tor-design} used only RSA, now also
      ed25519 \cite{tor-spec} for long- and medium term keys


    In Tor, each Onion Router has  types of keys. One is an
    /identity key/ of the onion router, a long-term key used for
    signing (=authentication) only. This signs a
    (medium\cite{tor-spec} to short\cite{tor-design})-term /onion
    key/, which is kept at least one week after advertising.

    - fixed minimum messages size of 512 Byte
      - extensions allow other sizes
      - all data in same-size cells
    - directory of all onion routers in directory servers
    - encrypted from client to "guard node"
    - passed to two other hosts, chosen by client "at random"
    - exits at "exit node" as normal tcp connection (except for hidden services)
    - negotiation wf-detectable? (packet sizes)
**** TODO check if true
     for the purpose of website fingerprinting, a high-level overview
     of Tor's cryptography should suffice
**** TODO ref for data in same-size cells
*** TODO Who uses Tor
    From the beginning, the Tor Project envisoned a broad user base.

    Tor is a development of the Onion Routing Project
    \cite{anonymous-connections}. It was originally developed by the
    Naval Research Laboratory of the US Navy with the primary purpose
    to protect government communication.\cite{who-uses-tor}

    Today, it is an anonymization network with a diverse user
    base. It has 6'000 nodes and is accessed by more than 3 million
    daily clients.

    The main user groups as listed by the Tor Project
    \cite{who-uses-tor}, in order, are:

    - normal people,
    - military,
    - journalists and their audience,
    - law enforcement officers,
    - activists & whistleblwers,
    - high & low profile people,
    - business executives,
    - bloggers, and
    - IT professionals.

    They use it for diverse purposes, such as

    - privacy,
    - censorship avoidance,
    - covert ops,
    - publishing,
    - safety,
    - online surveillance,
    - anonymous tip lines,
    - whistleblowing,
    - blogging private opinions,
    - evaluating competition, and
    - troubleshooting IT systems.
**** TODO client numbers with link (footnote?)
*** TODO Tor Threat Model [0/3]
    Tor does not attempt to protect against a global passive adversary,
    who can observe all connections. A Tor adversary can\cite{tor-design}

    - observe a fraction of all traffic,
    - generate, modify, and delete traffic,
    - operate its own Onion Routers, and
    - compromise a fraction of the other ORs

    This adversary is not purely passive, but lacks global information.

    Tor is not intended to protect you
    - if someone can monitor a big amount of internet traffic (td: quote)
    - if someone can exploit your browser
    - if someone can exploit your computer
    - if you enter identifying information while using tor
**** TODO maybe schneier re adversaries
***** TODO book from library                                        :library:
**** TODO quote big amount internet traffic, read paper
*** Tor Browser
    There are many technologies based on the Tor protocol. Top of
    Tor's Software & Services list\cite{tor-ecosystem} is the Tor
    Browser Bundle. This is a modified version of Firefox which uses
    Tor and comes with built-in privacy and security enhancements and
    Add-ons.

    Among the additional privacy features the Tor-Browser-Bundle
    provides are added request randomization and enabled pipelining.
*** TODO tbb anti-wf modification
    - pipelining enabled
    - request order randomization
*** TODO sort subtopics
*** TODO Tor overview
** TODO Addon Design and Implementation [0/50]
*** [[*description of add-on][description of add-on]]
*** Defenses
*** TODO Mozilla Add-On Sdk [0/12]
**** CHECK Introduction to the Mozilla Add-On Sdk
     #INDEX: XUL
     #INDEX: XML User-interface Langage
     The Add-on SDK by Mozilla facilitates the development of
     Firefox-Addons.

     It allows users to create addons using HTML and Javascript only, as
     opposed to the previous use of
     XUL[fn:: \url{https://developer.mozilla.org/en-US/docs/Mozilla/Tech/XUL}],
     the XML User-interface Language.

     The addon execution entry point (like =main= in C and Java) can be
     configured via the =preferences.json= file. By default, the main
     addon-script is called =index.js=.

     The SDK contains many tools to interact with the browser. URLs can
     be loaded in the background via the =page-worker= module; the
     =page-mod= module injects JavaScript code into the page the user is
     browsing to. User-created code can be tested via unit tests.

     If none of the easily accessible high-, or low-level modules
     suffice, much of the browser's functionality is accessible via
     the Components
     object[fn:: \url{https://developer.mozilla.org/en/Components_object}],
     which can be accessed as =require("chrome")=.
**** TODO Debugger
**** TODO Available Data
     Firefox offers several ways for an add-on to listen for web activity.

     - contents of main page
       \to links to each domain
     - page-mod
       - problems: only when page is loaded, problem for cover traffic
       - but +: ends of all the loading (and processing)
     [[file:docs/lit.org::*%5B%5B./Intercepting%20Page%20Loads%20-%20Mozilla%20|%20MDN.html%5D%5BIntercepting%20Page%20Loads%5D%5D][Intercepting Page Loads*]] lists several
     - load events
     - http observer
     - webprogersslistener
     - xpcom
       - policymanager
       - documentloader
***** each load of page
***** end of page load
***** TODO as references or as footnote?
      ref, as completely read?
**** separation of scripts
     As a security measure, there is a separation between

     1) /add-on scripts/, which are run in the browser context, but
	cannot access the web page, and
     2) /content scripts/, which are run in the page context. They can
	access the DOM, but not add-on scripts. nor
     3) /page scripts/, which are those included in the website via
	f.ex. =<script>= tags

     Bridging this separation, f.ex. accessing page scripts (and vice
     versa) is possible, but needs some extra work.
***** LATER index: page scripts, content scripts, add-on scripts
**** CHECK message-passing
     There is a mechanism to pass content from the add-on to the
     content scripts, as shown in the example.

     A single string can be passed. As this string can be any serialized
     JSON\cite{rfc7159} object, this is not much of a limitation. (It
     effectively disallows the passing of functions and circular
     objects).

     In a content-script, a message can be sent via
     =self.port.emit('message_type', param)= and received via
     =self.port.on('message_type', function(param))=.

     In the Addon-Context, a =worker= object is used and the
     content-script's =self= is replaced by a =worker=. The worker is
     initialized via the =onAttach= parameter of f.ex. the page-mod.
**** TODO collect/list all addon sections
**** CHECK page-worker
     <<page-worker>>
     A =page-worker= creates "a permanent, invisible page and access[es]
     its
     DOM."[fn:: \url{developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-worker.html}]

     New pages can be loaded in the background, which would allow for the
     retrieval of camouflage traffic, as described by \cite{panchenko}.

     A minimal new page-worker is created via

     #+BEGIN_SRC js
       var pageWorker = require("sdk/page-worker").Page({});
     #+END_SRC

     The page-worker's page can be set dynamically via

     pageWorker.contentURL = "http://en.wikipedia.org/wiki/Cheese"

     This fetches only the file pointed to. The retrieval of included
     images, stylesheets, etc, is not automatic.

     A page-worker was used in the initial prototype. The RequestPolicy
     addon blocked this method of retrieval.

**** TODO page-mod
     <<page-mod>>
     The
     page-mod[fn:: \url{http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html}]
     module injects "scripts in the context of web pages whose URL
     matches a given pattern."

     The pattern can be given as ="*"= or =/.*/= to run on every
     user-visited page.

     It thus offers the possibility to check for the end of a web page
     load by the user.

     A page-mod example is

     #+BEGIN_SRC js
       const pageMod = require("sdk/page-mod");
       pageMod.PageMod({
           include: /.*/,
           contentScriptFile: "./getLinks.js",
           onAttach: function(worker) {
               worker.port.on("links", function(JSONlinks) {
                   addToCandidates(JSON.parse(JSONlinks));
               });
           }
       });
     #+END_SRC

     , which is run on every page, applies the =getLinks.js= script and
     listens for its feedback, which is then used via
     =addToCandidates()=.

     The page-mod has a =contentScriptWhen= parameter, which specifies
     when to attach the script to the page. Valid values are =start=,
     =ready=, and =end=, the last of which triggers at the
     =window.onload= event, when the complete page, including
     JavaScript, CSS, and images has loaded.

     A page-mod offers many other options such as f.ex. stylesheets,
     script parameters, etc.
***** link page-mod
      http://developer.mozilla.org/en-US/Add-ons/SDK/High-Level_APIs/page-mod.html
**** CHECK Installation and Use of Jpm (the build tool)
     (SDK-)addons can be built via the =jpm=-tool. It is available as a
     NodeJS-Module via the built-in NodeJS Package Manager =npm=.

     Installing =jpm= is a two-step process. Firstly, install NodeJS
     either via built-in tools[fn:: for example =apt-get install
     nodejs-legacy= in Debian and Ubuntu] or via
     download[fn:: \url{https://nodejs.org}] then, do a

     npm install jpm

     to install jpm[fn:: for the current user, global installation is done
     via =npm install -g jpm=].

     Once =jpm= is installed, new addons can be created via =jpm init=,
     unit-tested via =jpm test=, live-tested via =jpm run=, the addon
     package built via =jpm xpi=.

     Another command that may be of use is =jpm sign=: as of Firefox
     version 47, Mozilla enforces that all addons be
     signed\cite{addon-signing}. If they are distributed via Mozilla's
     Addon Marketplace[fn:: \url{https://addons.mozilla.org}], they are
     checked and signed automatically. Otherwise, you can request an
     API key for signing and sign via the command
     [fn:: \url{https://developer.mozilla.org/en-US/Add-ons/SDK/Tools/jpm\#jpm_sign}]
     =jpm sign --api-key $SIGNING_KEY --api-secret $SIGNING_SECRET=.
**** TODO interacting with page-scripts
     By default, content-scripts are isolated from the modifications
     done by page-scripts.[[Interacting with page scripts]]

     To access object inside the page-scripts context, you can use
     =unsafeWindow=.

     The reverse is only true for primitive values. If page-scripts
     need to see altered behavior, it is possible to override
     functionality of the page by using =exportFunction=, as in

     exportFunction(open,
		    unsafeWindow.XMLHttpRequest.prototype,
		    {defineAs: "open"});

     This exports the (previously-defined) function =open()= to the
     XMLHttpRequest.prototype, where it replaces the built-in
     functionality.
***** Interacting with page scripts
=developer.mozilla.org/en-US/Add-ons/SDK/Guides/Content_Scripts/Interacting_with_page_scripts.html=
**** TODO [#C] <<<DOM>>>
     domain-object-model
**** TODO unit tests jpm
     JPM also offers the ability to write unit-tests.
*** LATER [#B] Design
    #+BEGIN_LATEX
    \begin{adjustbox}{max width=\textwidth}
    \input{pictures/model.tex}
    \end{adjustbox}
    #+END_LATEX
**** needs to do
     - make wf harder such that it is impossible
**** by
     - generating cover traffic
**** procedural
***** check which urls user loads
****** aggregate by domain
***** for each loaded url, maybe load something else
      - this generates the cover traffic over the loading of the website
      - yet only augments bursts, does not equalize them
**** TODO modules [0/14]
***** TODO replace with pic [[shell:dia pictures/model.dia &]]
***** TODO how to show (singleton)-module in jUML
***** TODO Watcher
      - notifies when user loads sth, and when finished
      - implements nsIObserverService
****** TODO link nsIObserverService
****** methods
       - loads
       - endsLoad
***** CHECK CoverTraffic(Loader=default)
      <<CoverTraffic>> The =CoverTraffic= module provides requests for a
      single host contacted. This is the only module used with a
      constructor, as it requires several instances, one for each host.

      The cover traffic provided tries to mimic the [[HTML traffic
      model]]s parameters.

      There are two strategies implemented which have to be set by
      modifying the source code.

      One strategy deals with size estimation: for each page, the size
      of its HTML request and the number of embedded elements are
      stored in a statistic data structure depending on bloom filters,
      called [[SizeCache]]. If the size is known, it can be used or
      randomly guessed from the HTML traffic model.

      Another strategy deals with target sizes. The size-cache stores
      approximate sizes, due to binning of values. One strategy is to
      pad both the HTML request size and the number of embedded
      elements up to the bin border. The other strategy determines a
      target distribution for each parameter, multiplies by the
      overhead parameter, and tries to attain that.

      Thus, on creation the site's and a target HTML size and number
      of embedded elements are determined. As creation is synchronous
      with the first HTML request, another request to pad up to the
      target HTML size is sent. As the target number of requests for
      embedded elements is a certain multiple of the actual number of
      requests, on each such request[fn:: signaled by the =loadNext=
      call], a probability is sampled, potentially resulting in a
      request for a cover embedded element. The cover element sizes
      are once again drawn from the HTML traffic model.

      After the page has finished loading, the =CoverTraffic='s
      =finish()= method is called. If the number of embedded elements
      requests has been to low, the remainder are then dispatched.
****** LATER check if still two strategies
****** TODO link to number of embedded elements and HTML request
****** TODO link to sizecache
***** TODO Loader(Source=default2)
      loads new cover page (mockable)
***** TODO Stats - Static functions
      statistical distributions (html, embedded, etc)
***** TODO CoverUrl
      source for cover traffic
      fixed domain, size as parameter
***** TODO BloomSort
      <<bloomsort>>
      sorts elements by size using Bloom Filters
      +add(id, value)
      +query(id): value
      +save
      +restore
***** TODO Random
      provides randomization methods
      +string(length:number) pseudo-random string
      +uniform01() secure random float in the range [0,1)
***** CHECK SizeCache
      <<SizeCache>> The SizeCache element stores approximations for
      both the HTMLsizes (=htmlSize()=) and number of embedded objects
      (=numberEmbeddedObject()=) per URL, using a [[bloomsort][BloomSort]] data
      structure for each.

      Exceptions from the BloomSort are passed on. This module is a
      facade \cite{gof} that initializes the bloom filters and
      simplifies access.
***** CHECK User
      The [[user.js][User]] module handles user action. It is the main controller.

      On each loading of a object via HTTP(S), it receives a message
      from the =Watcher= module via =loads()=, with the loaded URL as
      parameter.

      If it is a new request to the host, loading of an HTML page is
      assumed and a new =CoverTraffic=-Object is generated.

      If the host is known (as defined below), an embedded page is
      assumed and the (existant) related =CoverTraffic=-Object is told
      that an embedded element was loaded.

      After the first request, the host is known. At completion of the
      page load, indicated either by a [[page-mod]]'s integration into the
      page (at =end=), or the end of a timeout of =User.TIMEOUT=
      seconds, the CoverTraffic-object is notified of the ending and
      removed from the internal host-to-CoverTraffic mapping.
**** TODO cover traffic distribution generation
     - each retrieval maybe triggers additional retrieval(s)
       - based on statistical model
     - for each page being retrieved
       - either size can be estimated or it must be guessed
       - either number of embedded elements can be estimated or must
         be guessed
       - determine target size and number of embedded elements
       - fill up HTML traffic with another request with content size
         page.size - target.size
       - for each embedded element
         - generate request for additional embedded element(s) with
           probability (target.number_embedded - page.number_embedded)
           /page.number_embedded
           - if probability > 1, generate those certainly and iterate
             with probability -1, until probability < 0
**** TODO HTML traffic model
     <<HTML traffic model>>
     - intel
       - html object lognormal with params \mu = 7.90272, \sigma = 1.7643
       - embedded objects
     - test
       - download html top 10000
       - analysis
     - link to
**** TODO browser caching
     - browsers cache
     - only helps in cover traffic, (unless warm/cold site model is used)
***** LATER where to put this?
**** TODO Parameter: Sizes of HTML-Documents
     :PROPERTIES:
     :CUSTOM_ID: find sizes of HTML-documents
     :END:
     The statistical size generation works with application-level
     sizes on the network, as the authors of the HTML traffic
     model\cite{newtrafficmodel} analysed logfiles of the Squid
     proxy[fn:: \url{http://www.squid-cache.org}].

     The HTML-sizes could not be trivially obtained from the
     =Content-Length= in the browser, as there are additional headers
     and size-reduction via compression. The sizes were determined by
     retrieving the files with =wget= via squid. This is implemented
     via the [[./bin/html_top_100.sh]] script (see appendix).

     It empties the =access.log= file and the squid cache by
     restarting. Afterwards, the top-100 files are retrieved with
     =wget= via squid.

     From the log file =access.log=, the sizes are extracted via the
     command sequence

     #+BEGIN_SRC sh
       sudo cat /var/log/squid3/access.log | tr -s ' ' | cut -d ' ' -f 5,7 > /mnt/data/HTML-sizes
     #+END_SRC

     These sizes are then converted to a JSON-array via the
     [[./htmlSizeToJSON.py]]-file. It also does a check for duplicate
     values, choosing the lower one. This increases traffic, but the
     opposite might be too little traffic, thus easier website
     fingerprinting, which should be avoided.
**** TODO Estimate Parameter: Number of Embedded Objects
     <<number_embedded>>
     The second parameter for generating cover traffic is the number
     of embedded objects per HTML-page.

     These are extracted via the python script [[htmlToNumEmbedded.py][htmlToNumEmbedded.py]]
     which is called for each of the top-100's main web pages by
     [[retrieve-100-embedded.sh][retrieve-100-embedded.sh]].

     To extract, python's lxml module to parse the HTML's
     DOM extracts the URLs of embedded files from the attributes of
     several tags, f.ex. the =src= element of =img= tags.

     This implementation currently omits some possibly embedded
     elements, f.ex. those embedded in css files and =style= tags via
     the =@url= css-directive. It seems better for cover traffic to
     slightly underestimate the number of embedded elements. This
     might generate more traffic than strictly necessary, but here,
     safe seems better than sorry. Extracting just the right URLs is a
     matter of [[*Further%20work][further research]].
***** TODO read dom reference
***** TODO link to lxml website
**** TODO bloom-sort usage
     It is impractical to store the sizes of all URLs. Another
     possibility is to use Bloom Filters to aggregate groups of URLs
     with similar values, as described in [[*bloom-sort][bloom-sort]].

     Each groups gets borders (/splits/) and a size which represents each
     contained element.

     Determining the optimal number of groups, splits and sizes is a
     topic of [[*Further%20work][Further work]]. Here, initially the quantiles of the
     HTML-model (see [[*HTML%20traffic%20model][HTML traffic model]]) were used. When the data were
     to be inserted, it turned out that especially the numbers of
     embedded elements did not match the theoretically proposed groups:

     For three groups, the splits would be given by the 33 1/3 and 66
     2/3 quantiles, as 0.0107 and 1.481. As the number of embedded
     elements is a whole number, two thirds of the information would
     be if an element is 0, the next group would contain all other
     elements: The (representative) sizes of the groups were given as
     7.915E-05, 0.188, and 8.260 (quantiles 16 1/6, 50, and 83 5/6).

     The data to be inserted (see previous section) had the splits
     (quantiles) at 10 2/3 and 36 2/3 and the sizes at 6, 20, and 59
     2/3.

     In addition to using the observed sizes for the bloom filter, the
     number of groups was increased to 5.
***** TODO error rate computing
      - sources of error
        - filter tells that is has element when it has not
      - how does error appear
        - collision: one of several, the other might be true
        - replacement: simulates being another url
      - rates of error
        - "add" the error rates of the filters? (times population density?)
***** maybe graphics?
***** LATER check "see previous section"
*** TODO Implementation [0/5]
**** TODO js coding best practices
     JavaScript\cite{ecma} is arguably a language with some great
     parts, but also several bad ones\cite{javascript}. Approaches to
     mitigate these include

     - "use strict";
     - unit tests
     - mention "good parts"?
       - for what exactly?
       - and javascript garden
     - jshint
***** mention bad parts?
**** Unit Testing
**** LATER Cover add-on
     Defends against website fingerprinting by injecting artificial
     cover traffic into the communication.
***** when stable
      also cover against website fingerprinting by injecting really
      artificial cover traffic

      for every request, do one as well,
***** why as an add-on
      This is one of the few low-latency communication methods, Instead
      of burdening all of Tor with extra bells and whistles, this solves
      this deanonymization problem at the application layer, where its
      origins are. (Separation of Concerns)
**** TODO http server for testing
**** TODO description of add-on
     The add-on tries to defend against website fingerprinting by
     adding HTTP-distributed extra traffic.

     To do so, it detects the start of each web request. If it is a
     request for a HTML page, an additional HTML page is requested.

     If the request is determined to be for an embedded
     object[fn:: currently, the first page from a domain is the HTML,
     all others within a certain time window are considered embedded],
     an additional embedded object-size page is requested with a
     certain probability.

     Both the sizes of the extra HTML and the extra number of embedded
     objects are determined based on the [[HTML traffic model][HTML traffic model]].

     - detect start of transmission
       - request extra HTML doc to obfuscate that
       - maybe do something to IPP-model (trigger off/on-state on some)
     - always send dummy traffic
       - on each request
       - better: leave some out
     - better: delay some requests (f.ex. images)
     - detect end of page load
       - maybe do something to IPP-model (trigger on/off-state on some)
     - request size uniform [0, 300)
       - except if request.len > 300
     - source: cover traffic server
     - size distributions
       - html
       - numembedded
     - td describe best algo only?
**** TODO Apache mod_wsgi
     =mod_wsgi= is a module for the Apache web
     server[fn:: \url{https://httpd.apache.org/}]. It executes python
     scripts which implement the WSGI standard\cite{pep3333}. An
     apache httpd serving only WSGI is easily set up via the
     =mod_wsgi-express= command, which is included in the =mod_wsgi=
     python package[fn:: \url{https://pypi.python.org/pypi/mod_wsgi}].

     Installation (Ubuntu Server Edition and Linux Mint 17.1 Rebecca)

     - apt-get install apache2-bin apache2-dev python-dev
     - pip install mod_wsgi

     start via

     - ~/.local/bin/mod_wsgi-express start-server wsgi.py

     (here, also --port 7777), as for the script wsgi.py see appendix [[wsgi.py][appendix]].
**** CHECK python web server nichol.as
     The naïve implementation based on Python's BaseHTTPServer did not
     perform flawlessly (see [[*Non-parallelized-based web server for cover traffic][Non-parallelized-based web server for
     cover traffic {0/1}]]), even for the queries of a single
     addon. This prompted the search for a python-based,
     adequately-performing technology stack.

     Luckily, an evaluation of Python web server performance had been
     performed by Nicholas Piël \cite{nicholas}. It shows the apache
     server with the mod_wsgi module as well-performing. As it was noted
     to be very easy to set up, it was chosen for this evaluation.
*** Evaluation
**** add-on
***** TODO differences to adaptive padding/wtfpad
- delay of some possible (f.ex. images)
- knowledge of packets
- end of transmission detectable
- different target distributions
- multiple distributions
- optionally no cooperator necessary
    dummy packets chosen as response to real request (as in web traffic)
- add evaluation values
- similarities: no delay
  - also has app_hint
- currently uses exit nodes
- this has no gap traffic, aims less at global adversary, more at ISP
****** TODO understand adaptive padding histogram
***** TODO differences to walkie-talkie
***** TODO differences to panchenkos
      - feature extraction via python class directly from pcap
        - packet data saveable to JSON
***** TODO why several covers
      - competition
      - when this started, walkie-talkie and juarez had not yet published
      - harder to break
        - more effort: one classifier for each cover scheme
** TODO Bloom Filters
*** TODO what is a bloom filter
    A Bloom Filter is a data structure to test membership in a set. It
    has a fixed size and a certain one-way error rate. If an item is in
    the set, the Bloom Filter is guaranteed to report this. If an item
    is not in the set, there is a certain probability, the /error rate/,
    of reporting that it belongs.

    This error rate is dependent on the size of the bloom filter and the
    number of inserted elements.
*** TODO bloom usage and implementation
    - bloom sort
      - error rate computation
    - size taken from example...
      - maybe change when altered
*** CHECK bloom-sort
    By ordering data into bins, it becomes possible to use bloom filters
    for the estimation of sizes, using one bloom filter for each bin.

    To achieve this, sensible separation criteria (called /splits/) for
    the bins need to be found. Afterwards, each bin needs to be assigned
    a value (called /size/) for all contained elements. See section
    [[*bloom-sort%20usage][bloom-sort usage]] on determining the sizes and splits.

    This data-structure, called /bloom-sort/ is initialized with an
    array of splits, and an array of sizes. The sizes-array needs to
    have one more element than the splits-array, as the bins are bounded
    on the left by 0, and on the right by infinity.

    #+BEGIN_SRC js
      /**
       ,* @param {sizes Array} array of values for each bin, must be sorted
       ,* @param {splits Array} array of bin borders, must be sorted
      ,*/
      function BloomSort(sizes, splits) {
          this.sizes = sizes;
          this.splits = splits;
          this.filters = [];
          for ( let i = 0; i < sizes.length; i++ ) {
              this.filters[i] = new Bloom.BloomFilter(NUM_BITS, NUM_HASH);
          }
      }
    #+END_SRC

    Thus, you get

    -\infty \le size0 \le split0 \le size1 \le split1 \le ... \le split(n-1) \le sizen < \infty

    Given the splits, it becomes possible to add the elements to their
    bins:

    #+BEGIN_SRC js
      BloomSort.prototype.add = function(id, size) {
          this.filters[_.sortedIndex(this.splits, size)].add(id);
      };
    #+END_SRC

    where =_.sortedIndex()= gives the index at which =size= would be
    inserted into the sorted =this.splits= array.

    The retrieval of element sizes looks into each bloom filter,
    checking whether it might contain the element =id=. If one bloom
    filter reports containment, its corresponding element- =size= is
    returned. If several or no bloom filters report containment, an
    exception is thrown. The exception is used to allow all possible
    return values, not blocking one of them, say =-1=, for the error
    condition.
    #+BEGIN_SRC js
      /** determines size of element, raises exception if unclear */
      BloomSort.prototype.query = function(id) {
          let pos = -1;
          for ( let i = 0; i < this.filters.length; i++ ) {
              if ( this.filters[i].test(id) ) {
                  if ( pos === -1 ) {
                      pos = i;
                  } else {
                      throw {
                          name: 'BloomError',
                          message: 'Contains multiple entries'
                      };
                  }
              }
          }
          if ( pos === -1 ) {
              throw {
                  name: 'BloomError',
                  message: 'Contains no entries'
              };
          }
          return this.sizes[pos];
      };
    #+END_SRC

    It can be used by initializing with
    #+BEGIN_SRC js
    let htmlSizes = new BloomSort.BloomSort([400, 1000, 20000], [700, 10000]);
    #+END_SRC

    then adding elements via =htmlSizes.add("http://google.com/", 613)=
    and querying via =htmlSizes.query("http://google.com/")=, which
    would yield =400=. (see usage in [[file:cover/js/size-cache.js::let%20numEmbeddeds%20%3D%20new%20BloomSort.BloomSort(NUM_EMBEDDED_SIZES,][size-cache]])
* MAYBE-then-LATER torben
  Torben is a deanonymization attack based on injected website content
  in combination with Pattern Recognition. The authors show that when
  the user's browser sends requests of certain sizes for responses of
  certain sizes, this can be recognized in the encrypted TLS-Traffic
  from the Guard Node to the Onion Proxy.

  Each request/response pair corresponds to a certain amount of
  information (the authors show their approach with four request and
  response sizes, yielding a four-bit side-channel per request). This
  channel is used to encode a hash of the currently visited page.

  The requests are performed via XMLHttpRequest, but they authors also
  mention using HTTP redirects for the same effect.



  inject additional traffic into communication via JS XMLHttpRequest
  fixed request/response sizes of 2k, 4k, 6k, 8k bytes
  \to quad bits, concatenate, data transfer rate rate
  after 30 or 120 ms (tor latency bigger)
  detect via svm (how)
  setzt auf tcp an statt auf ip, (weil tor ja tcp ! yeah!)
** LATER talk to daniel whether mention or not
* MAYBE why privacy
  - fundamental human need
  - concentration camp:
    "solitude in a Camp is more precious and rare than bread." -- primo levi
* TODO extract dom tags python
  - diveintopython
  - see code
* CHECK modified top-100
  The files for retrieval were from the alexa-top-1m[fn:: Available
  at \url{ http://s3.amazonaws.com/alexa-static/top-1m.csv.zip}],
  from September 30, 2015. Similar to \cite{wpes13-fingerprinting},
  similar sites were removed. Also removed were those sites which
  failed to respond to python's =urllib=. The list of sites with their
  Alexa index can be found in appendix [[top-100]].
** TODO complete list in appendix
* CHECK wsgi.py cover traffic server and generator
  With the technology stack to implement the cover traffic generator
  being settled, implementation becomes a single-page file, see
  [[wsgi.py]].

  One detail is that the length of the content gets inflated by the
  content-headers. To decrease this again, the length (which in turn
  depends on the required length) needs to be calculated and
  subtracted from the body-length. Some uncertainty arises because the
  =Proxy-Connection: keep-alive= header is headed in some
  circumstances. The implementation errs on the side of returning too
  much data.

  Once the size is computed, a pseudo-random choice from the list of
  all printable characters is returned to the HTML query.

  To test this algorithm, the first 1000 sizes are retrieved via
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          curl -D /tmp/curlheaders/$i.head 127.0.0.1:8000/?size=$i > /tmp/curlheaders/$i.body; 
      done
    #+END_SRC
  which outputs the header and body of each query to the files,
  f.ex. =134.head= and =134.body=.

  This data is then evaluated by hand to check the sizes:
    #+BEGIN_SRC sh
      for i in $(seq 1000); do
          echo "$i: $(cat ${i}.* | wc -c)";
      done
    #+END_SRC
* TODO differences theoretical HTML-num embedded and observed
  - redirects
    - html had 176 elements, embedded only 100
    - the others were redirects (f.ex. from google.com to
      www.google.de)
    - these could be counted as having 0 embedded elements,
      - yet still a difference remains
  - it fits better if you enlarge the sizes by 0 for each redirected
    element (there are 176 elements in the html filter, including
    redirects, and only 99 in the embedded filter, if you pad the
    embedded filter by 0 for each of those, it is not a perfect fit,
    but better)
  - growth of websites
  - [[*Further%20work][Further work]]
* TODO panchenko CUMUL
* TODO truncated distributions
  - html: truncated lognormal instead of lognormal
    truncated at 0.999918739 quantile
  - embeddedSize: ebd
  - numEmbeddedObjects: truncated gamma instead of gamma
* CHECK Variations of Cover Traffic
  There are two variations how to generate Cover Traffic.

  1. Does knowing web page characteristics, such as [[#find sizes of HTML-documents][the size of the
     HTML-webpage]] and [[number_embedded][number of embedded objects]] help in generating
     cover traffic? While it seems so at first, it can be evaluated if
     [[HTML traffic
      model][educated guessing]] might work better.

     In a closed world, it is possible to always know these sizes
     beforehand. If unknown, the random variates from the [[HTML traffic model][HTML traffic
     model]] are used. [fn:: The size of each embedded element is always
     drawn from the HTML traffic model.].

  2. Given a webpage and its size, how much traffic should be generated?

     While just adding random traffic to each page might enhance
     anonymity, always adding from the same distribution would
     probably be wasteful, as site-specific values might prove to
     obfuscate better.  A target size and number of objects has to be
     determined. How to choose this is the second parameter.

     One approach is to group the webpages by their size into bins and
     to set the bin border as the target size, as all webpages in the
     bin must have a size less than or equal the border. This approach
     mimics that taken by Wang et al. in \cite{effective} with the
     bins being equivalent to the anonymity sets / partitions. For the
     biggest bin, its median size is currently chosen.[fn:: The optimal
     size for the biggest bin is a parameter that should be evaluated
     as well.]

     The other approach is to have a single target distribution from
     which values are sampled each time, once again from the [[HTML traffic model][HTML
     traffic model]].
** TODO move this f.ex. to coverTraffic, maybe move this section below
  From the target values, the webpage's html-size and number of
  elements (variation A/B) is subtracted.

  At the same time of the HTML-query, another query for the remaining
  HTML-size (or a token amount if too small) is sent. Concerning the
  embedded elements, the ratio of (target-site)/site is computed. For
  each element, this ratio determines the number of requests for
  embedded elements (these are always of random sizes, once again from
  the [[HTML traffic model][HTML traffic model]]. See [[*cover%20traffic%20distribution%20generation][cover traffic distribution generation]]
  for the algorithms.
** TODO end move section
  This leads to the following variations:

  1) bloom binning (I) with known sizes (A)
  2) bloom binning (I) with random sizes (B)
  3) one target distribution (II) with original size from bloom (A),
  4) one target distribution (II) with random sizes (B)

  | SIZES \ TARGETS | I: bloom binning | II: one distribution |
  |-----------------+------------------+----------------------|
  | A: known sizes  |                  |                      |
  | B: random sizes |                  |                      |
* CHECK which features work well
  As stressed by Perry in \cite{critique}, analysis of which feature
  contribute the most towards classification is important. Panchenko
  et al. \cite{panchenko} provided a qualitative analysis. Hayes and
  Danezis \cite{kfingerprint} used forests of randomized trees, which
  provide feature importance estimation.[fn:: f.ex. in scikit-learn
  \cite{scikit-learn} via the =_feature_importances= attribute]

  As Dyer et al \cite{oakland2012-peekaboo} noted and experiments with
  Panchenko et al's \cite{panchenko} estimator support (see
  [[different-classifiers]]), you can get good accuracy with several
  classifiers, given the right features.

  As determined by Hayes and Danezis \cite{kfingerprint}, the top-five
  features are the number (both absolute and percentage of total) of
  both incoming and outgoing packets. The standard deviation of the
  packet ordering list [fn:: Panchenko et al \cite{panchenko} call
  these features /Number Markers/] completes the top five. Each added
  feature increases accuracy, yet with nearly the same accuracy for 30 as
  for the total of 150 features.
** MOVE to wf
* TODO does this hide bursts?
  - meta-bursts as described in walkie-talkie
  - are those hidden, too, or can the number of bursts be found out
  - easy to implement, maybe do this
** maybe see cumul-graphics
* TODO mention tor browser bundle version etc
* TODO why defense better
* TODO which sites well-protected, which less
* TODO bursts on addon site load finish
  One characteristic which identified sites well as per Dyer et
  al.\cite{oakland2012-peekaboo} and Wang and Goldberg
  \cite{wang2015walkie} is the number of bursts.

  As the addon would conceptually only increase burst sizes, and not
  alter their number, this should be covered as well. To address this,
  the per-site traffic module [[CoverTraffic]] remembers the number of
  unsent requests for embedded elements. When the page loading is
  finished, this number (which should be 0 or less in more than half
  the cases) of embedded objects is requested. As the cover traffic
  currently comes from a single server, the multiple connection limit
  (compare [[Hurdles]]) should automatically lead to multiple bursts if
  the number of embedded objects is high enough.

  This should emulate normal browser traffic better than the proposed
  probabilistic schemes by Wang and Goldberg (normal and uniform
  distribution). It might be that Wang and Goldberg's deterministic
  padding to common values performs better, but that seems require a
  priori knowledge of website burst sizes.
** TODO maybe move to [[CoverTraffic]]
* TODO machine learning
** TODO knn
** TODO svm
** TODO features
** TODO extremely randomized trees
*** brainstorm
    - decision trees
    - ensemble methods
* TODO addon weaknesses/uncertainties
  - all HTTP gets treated the same
    - redirects
    - iframes
    - normal pages
  - request sizes not altered
    - can clearly see each cover request (as each should have size < 500)
  - sizes have grown since 2007
* TODO strong assumptions on feasibility
  - as critiqued in \cite{critique}
  - if protects against this, should also protect against worse
  - additional (?defense?) as proposed in critique
** TODO follow critique at all?
* TODO npm short installation/description
* TODO panchenko v1 different classifiers
  <<different-classifiers>>
  - experiment
  - different classifiers, different results
  - much easier to just use knn
  - more work for svm parameter estimation
* TODO outlier removal
  As described by \cite{panchenko2}, the CUMUL approach is greatly
  enhanced by outlier removal. In his software, he uses both a
  median-based as well as a 25%/75%-quantile-based approach.

  - implemented in [[file:bin/extract_attribute.py::def%20remove_quantiles_panchenko_2(counter_list):][ex-att]]
  - quantiles:
    - numpy instead of his original code for code clarity
    - just take quantiles, use his limits
** TODO run test, include results
** TODO link to panchenko's software
* TODO how to set up wfpad
  - tor server: listen on ORPort X
  - wfpad server script: send to X, listen on Y
  - wfpad client script: send to Y, listen on Z
  - in tbb/on 2nd tor (a.k.a. client): send traffic to bridge
    =Bridge 127.0.0.1 Z=
  - modify capture
    - localhost (=-i lo=)
    - =port Y=
  - start capture
  - bug on multiple uses:
    #+BEGIN_EXAMPLE
    exceptions.IOError: [Errno 24] Too many open files: '/proc/23634/stat'
    #+END_EXAMPLE
    - try temporary fix: increase number of file descriptors, set
      #+BEGIN_SRC sh
        username        hard    nofile  10000
      #+END_SRC
      in =/etc/security/limits.conf=
    - bug report in appendix, needs some code to mitigate
** TODO scripts
* misc: tex bibliography
\bibliography{docs/master}
\bibliographystyle{plain}
* UPTO HERE +BIB, TEXING WORKS -------------------------------------------     
* WAIT Discussion
  intel model: interdependences (html bigger \to more embedded) not mentioned
* WAIT Acknowledgements
  - Elena
  - Daniel Arp
  - Prof. Dr. Konrad Rieck
  - Tao Wang
  - ...
* TODO Further work
  - bigger world sizes
  - open world
  - source cover traffic: user gives domain as starting point
  - how to generate
    - how often, which parameters
    - just triggered by start and until end, or for each load
  - background if non-active (IPP self-similar)
    - 802.16 model
  - does a new connection to another site create a measurable tor-response
    (with variable-length packets)?
  - provable protection
  - size of bloom filter
  - number of bloom filters,
  - which and how many items to prepopulate
    - country-specific f.ex. google.com
    - leave out redirect from prepopulation
  - automatic update of bloom-filter
    - with currently visited sites
  - loading further items
  - The choice of cover traffic domains was explicitly taken out of
    the research focus. Currently, all cover traffic is dynamically
    generated by a web server written in Python.

    There exists basic code to use a list of webpages, given their
    sizes. It could be augmented by following links.
    - update from visited URLs
  - no morphing (delay, segmentation)
    - justify why good idea
  - bloomsort save/restore
  - number of embedded elements lacks <style> tags and some in <link>
    - does not honor reloads/cacheing
      - or does it? (maybe only called on cacheing)
    - but better than too many?
      - some approaches yes, binning no
  - elaborate on [[number_embedded]]
  - how to set splits and sizes
  - [[differences theoretical HTML-num embedded and observed]]
  - improve code to include css, (iframes?), js in number of embedded elements
  - web pages got bigger. See if \cite{newtrafficmodel}'s values are
    still accurate.
    - or only rely on quantiles of observed data
      - but these are hard to gather
	- use networkmanager code to do that

    - cite web-doom
  - more elaborate tests with different world sizes / open world / etc
  - User class: should aggregate smarter, not by-host, but by-page
    with every page-embedded element as just that.
    - indexed by host as workaround, can do better later
      - hard to find out which is HTML, which is non-HTML-traffic
      - so all is lumped together per domain
	- first request seen as HTML
	- other requests as non-HTML
    - == determine if HTML page by suffix (not clear as of ... and
      ... (link to SO))
  - bursts maybe less hidden (number of)
  - time not hidden (no delays of single files)
** TODO also helps against global observer if .onion generator is used
   - murdoch/danezis: correlation
   - this creates additional traffic which might hinder correlation attacks
   - further work
   - if cover traffic server is used by enough clients at once
   - or is unobservable (hidden service)
   - information-theoretical / stochastical analysis
   - quote perry critique
*** TODO first read murdoch/danezis paper
** onion host for cover traffic
   As indicated f.ex. by Wang and Goldberg,
   \cite{wpes13-fingerprinting}, network load already is a bottleneck
   on Tor, with the key bottleneck being exit nodes\cite{wtfpad}. The
   exit nodes might be spared the extra traffic by using =.onion=
   traffic generators (or, alternatively, hosts). A traffic generator
   could be further optimized by using tor proposals ... (see todo) to
   reduce latency, if this does not reduce privacy.
*** TODO tor proposals as of tor.sx
*** TODO read/skim and cite "on performance..."
** more thorough evaluation
   - only two panchenko approaches
   - assumption: can split traces
** TODO always also link in text
*** TODO check with darp
** TODO links to original, back to further work
** Exactly distinguishing HTML and embedded requests
   The current version of the [[user.js][User module]] separates
   CoverTraffic by DNS-domainname. As it often happens that one HTML
   page has embedded elements from different domains, this does not
   perfectly represent reality. It would be more exact to analyse the
   HTML page and at least return the domains of all embedded elements.
* TODO appendices [0/3]
\appendix
** 1Script ~one-site.py~: capture pcap traces
   <<one-site.py>>
   #+INCLUDE: "./bin/one_site.py" src python
** 2Script ~analyse.py~: classify the data
   <<analyse.py>>
   #+INCLUDE: "./bin/analyse.py" src python
** 3Script ~counter.py~: parse pcap files
   #+INCLUDE: "./bin/counter.py" src python
** Cover Traffic Server: ~wsgi.py~
   <<wsgi.py>>
   #+INCLUDE: "./bin/wsgi.py" src python
** 4Script ~htmlToNumEmbedded.py~: extract embedded objects
   <<htmlToNumEmbedded.py>>
   #+INCLUDE: "./bin/htmlToNumEmbedded.py" src python
** 5Script ~html-top-100.sh~ to retrieve html pages via squid
   #+INCLUDE: "./bin/html_top_100.sh" src sh
** 6Script retrieve-100-embedded.sh run htmlToNumEmbedded
   <<retrieve-100-embedded.sh>>
   #+INCLUDE: "./bin/retrieve_100_embedded.sh" src python
** modified top-100
   <<top-100>>
   #+INCLUDE: "./data/top-100-modified.csv" example
** TODO Remove same-host cover traffic server from traces: =7777.sh=
   <<7777>>
   #+INCLUDE: "./bin/7777.sh" src sh
** Addon
*** Control module User
    <<user.js>>
    #+INCLUDE: "./cover/js/user.js" src js
* unused
** from [[*transform to panchenko-features]]
  The code to examine a single trace file is in =analyze_file()=
  It
  - opens the filename in tshark
  - splits the output by tokens
  - gives the relevant values (source IP, size, timestamp) (with the
    timestamp not used by Panchenko) to a =Counts=-object, which
    aggregates it

  [...]
  For a single line, a =Counter=-object aggregates bytes (incoming,
  outgoing), packets (incoming/outgoing), distills into a size/packets
  array and (size+timestamp)/packets array.
  [...]
  This is used in =postprocess()= to determine
  - size markers, (via the =_sum_stream()=-function),
  - the html marker as the first of those
  - the total transmitted bytes incoming and outgoing
  - number marker (via the =_sum_numbers()=-function)
    - slightly extended, as the number 16 was occuring
      everything above 14 was mapped to the same as 14
    - a bit unclear, currently, 3-5 \to 3, 6-8 \to 4, 9-13 \to 5, 14-\infty \to 6
  - occurring packet sizes incoming and outgoing (binned in steps of 2)
  - percentage of outgoing packets
  - number of packets incoming and outgoing.
** start browser with -marionette parameter
   Each modern Firefox, and thus also the tor-browser-bundle, has
   marionette-support built-in. It needs to be enabled on the
   command-line via the =-marionette= switch, for example


   This starts the Tor browser with marionette enabled.
*** marionette support page link
** Sally installation
   Sally is a tool to transfer text into points in a vector space.

   It is installed on Ubuntu Vivid Vervet by following the official
   instructions, then changing =vivid= in the file
   =/etc/apt/sources.list.d/mlsec-ubuntu-sally-vivid.list= to
   =devel=.
** from getting tbb to work
  One external repository is required, which can be installed via

  =add-apt-repository ppa:ubuntu-toolchain-r/test=
  =apt-get update=
  =apt-get dist-upgrade=

  Furthermore, the binary needs some firefox libraries, which can be
  retrieved most easily via =apt-get install firefox=.

  Afterwards, the binary can be started by typing =./firefox=.
** throttling
   As especially outgoing web requests are often quite small, and this
   paper has at the moment a 1:1 rate of outgoing vs incoming for the
   requests, throttling the amount of data leaving the end user might
   well suffice for reducing the bandwidth of the side-channel enough
   to make it insignificant.
** in-browser vs tcp-level ( ???) (generation?)
** how sally works
   - configuration file
     - input
     - features
     - output

** problematic websites
   The above setup worked on most websites.
   The websites sina.com.cn and xinhuanet.com both did not terminate loading.
   This might need further looking into.

   - do they load completely when not Tor, repeat necessary
   - is this by design?
*** exclude
    "scheint sonst zu klappen"
** Plugins: noscript and requestpolicy
   There exist two plugins, which should both allow mitigation of this
   attack. Used in parallel, they may hinder normal browsing somewhat
   (which is why they are not enabled/installed by default in the Tor
   Browser Bundle).

   The first is NoScript, which selects which Javascript sources to
   run and which to block. This is installed by default in the Tor
   Browser Bundle for the additional security benefits it brings (XSS
   defense etc), but not fully enabled. It is recommended by Edward
   Snowden and many others\cite{noscript}.

** what sets Tor apart / other anonymity networks
   There are other anonymity networks, such as JonDonym, I2P, MixNet
   and freedom.

   Tor is an anonymity service.
   - decentralized
   - biggest
   - high throughput
   - rather low latency, usable for web browsing
   - also hidden services

   Using a client called /Onion Proxy/ on the local computer, almost all
*** TODO ref onion routing
*** TODO onion routing
** TODO Non-parallelized-based web server for cover traffic [0/1]
   This approach did not scale to several parallel connections, so it
   was not used. It is included as a reference of what seems to work,
   but did not.

   The python module =TrafficHTTPServer= can be started on the
   command-line via

   python TrafficHTTPServer.py portname

   with portname set to 8000 by default. It generates cover traffic of the
   size given by the =size= parameter, for example the command

   wget 'http://localhost:8000/?size=10'

   retrieves a document with 10 bytes content from a TrafficHTTPServer
   running at localhost port 8000.
*** Problems
    - did not scale: did not respond immediately for parallel
      connections
      - obviously delays problematic as they in effect create less
        cover traffic
    - maybe further work: test that really works worse
*** Script
    #+INCLUDE: "./bin/unused/TrafficHTTPServer.py" src python
