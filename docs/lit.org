** [[./2015-asiaccs.pdf][Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]]
*** quotes
    - (a) web pages can be easily manipulated to load content from
      untrusted origins and (b) despite encryption, low-latency
      anonymization networks cannot effectively hide the size of
      request-response pairs.
    - A large body of work has studied passive attacks based on
      traffic analysis, most notably website fingerprinting and
      traffic confirmation attacks.
    - First, web pages can often be manipulated to load content from
      untrusted origins, for example, using advertisements or
      user-provided content.
    - deanonymize Tor users in a short period of time.
*** summary
    request packets per JS. they have sizes of 2k, 4k, 6k, 8k = 2bit
    information. Wait after page load, trigger js xmlhttprequest.

    0) [@0] 
       deanonymization via traffic injection and identification of
       request-response pairs
    1) user's browser does side-channel communication via
       user-submitted content or advertising, this is detected between
       entry node and onion proxy.
    2) background
       1) low-latency anonymization
       2) active attacker: remote markers via embed (ads) or local (at server)
       3) side-channel (see 3)
    3) 
       1) preprocessing:
          1) not ip, but TLS (removes *1 layer* of retransmissions)
	  2) empty out, merge adjacent, filter control (discard 512b-cells), merge again, sizes as multiples fo 2k
       2) side channel: messages: 2k, 4k, 6k, 8k bytes, 4 bits,
       3) transmission:
	  1) http via xmlhttprequest: with random parameter to avoid caching
	  2) use for web page markers, single easy, else 20byte sha1 (hammingdist)
       4) detection: sliding window (faster), svm, see [1]
    4) Evaluation: fixed set vs 60000 vs live
       1) Data Collection via Selenium with Firefox, if not loaded, discard
       2) extracting via Sally, learning via LibSVM, generate, record 50 transmissions, train
       3) closed-world: 
          reverse proxy, inject js, delay 30secs, 120secs on slow load
	  transmission takes 12-20 secs, compare to fingerprinting, 95%
       4) open-world: 91%, no false positives
       5) users: 31 of 34, no false positives
    5) limitations and defenses
       detect web page markers
       chaff
    6) related
       liberatore: look at traffic
       both ends: better
    7) conclusion
       owie, detect as first countermeasure
** [[./2014-torben.pdf][Torben: Deanonymizing Tor Communication using Web Page Markers]]
*** quotes
    - As a result, the few known cases of deanonymization of Tor have
      been reported to instead make use of advertisement networks or
      rely on vulnerabilities in browser implementations [29, 30] and
      are thus unrelated to insecurities of Tor in general.
    - As a consequence, there is an urgent need for defenses in
      anonymization services protecting users from active attacks at
      the application layer.
    - A web page[sic: r] marker is implanted using embedded or user-provided
      content, such as an advertisement or an image link. The marker
      induces a traffic pattern visible at the entry node, for
      example, using a chain of HTTP redirects or JavaScript code
      generating HTTP requests.
    - each circuit is only used for 10 minutes until a new circuit is
      created
    - For example, the Walsh-Hadamard code can be used to encode
      messages of length k ≤ 7 as code words of length 2 k with
      maximized minimum Hamming distances.
    - eight different sizes corresponding to the alphabet

      A = {− f(1, 1 ) , . . . , − f(0, 0 ) , f(0, 0 ) , . . . , f(1, 1 )} .
    - # ( x, p ) returns the number of occurrences of the positional
      n-gram p = ( s, i ) between the positions i and i + τ in the
      sequence x.
    - The vector φ ( x ) encodes information about the symbols, their
      order and their position in x—thus reflecting the basic
      properties of a web page marker.
    - This reliability rests on the design of the side channel that
      makes use of atypical request-response pairs for transmitting
      information (Section 3.2).
    - Whether such chaff traffic can be selectively injected to only
      destroy indicative traffic patterns is an interesting question
      for further research.
    - the attacker can expose the web pages a user visits within a
      couple of seconds.
*** summary
    0) [@0] Abstract: torben presented
    1) Introduction

       much research into passive attacks: high false positive,
       changing web content leads to problems, active attacks such as
       patch selection and watermarking

       instead: mostly browser vulnerabilities

       torben introduced (see also [[Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]])
    2) Background
       1) The Tor Network: 

          bunch of routers, symmetric keys per hop in the circuit,
          onion encryption, each router only sees neighbors, each
          circuit only used for 10 minutes

       2) Attack Scenario:

	  Attacker can insert markers into web page of interest and
          analyze traffic between OP and guard.

    3) A Side-Channel Attack on Tor
       0) [@0]

	  - Tor ist low-latency
	  - request-response paare sichtbar im TLS Traffic (mit filtern)
	  - sollte der gegner die Website beeinflussen koennen
	    - via js oder http redirect (andere moeglichkeiten, css?)
	    - als direkte beeinflussung oder user content
	  - kann er versuchen, ueber diesen side-channel zu kodieren,
            welche websites besucht werden.
	  - Schritte
	    1) Preprocessing of network traces. (3.1)
	    2) Side channel design (3.2)
	    3) Transmission of web page markers (3.3).
	    4) Detection of web page markers. (3.4?)
       1) Vorverarbeitung:
	  merkmal: groesse der kontinuierlich in eine richtung uebermittelten daten
	  1) TCP statt IP analyse via tshark
	  2) Filtering and Merging TLS Records
	     a) filter non-tor-records (\le 100 bytes)
	     b) merge continuous to obtain amount of flow (packes sizes random)
	     c) filter control cells (512 bytes) and merge again (HTTP
                does not fit into 512 bytes)
	     d) normalize sizes, multitudes of 2000 bytes
       2) Side Channel Design
	  map two bits q = q_i, q_j to
          q_i, q_j \to ( q_i + 2q_j ) · s + c     (with s, c = 2000)
	  = q \cdot s + c
	  map four bits to request and response sizes, two bits each.
       3) Transmission

	  request: get with "random" parameter of fitting lenth
	  response: any page of acceptable size, *any host*

	  hash URLs to SHA-1 (optimal when fixed: walsh-hadamard code)
       4) Detection
	  1) 
	     - gelesene Sequenz gegeben (experiment: 100 symbols)
	     - A = {2,4,6,8}^2 (Torben-Alphabet, mit minus fuer request)
	       |A| = 16

	     - S=A^n alle n-gramme von A
	       |S| = 16^n

	       \to (eigenes) n = 40
	       == 2^160

	     - positional n-grams:

               P = S \times N, 
               mit Element p=(s,i) mit s \in S, i Position von s in Sequenz

	     - \varphi bildet von allen n-grammen A^* nach R^{|P|}.  
	       \varphi(x) \to (#(x,p))_{p \in P}

               Jedes n-gramm (s,i) hat seine Haeufigkeit zwischen i und
               i+\tau als wert

	     - \tau is toleranz-parameter

	  2) Probabilistic Classification

	     - SVM trainiert mit Sequenzen der Marker

	     - Riesiger Vektorraum, aber sparse

	     - P "nur" multiplikativ mit Fenstergroesse, nicht anders, puh

    4) Evaluation

       mehrere Experimente: cw, ow, users

       cw: unrealistisch, aber haeufig verwendet in website-fingerprinting, vergleich
       ow: 60000 webseiten

       1) Data

	  - Selenium WebDriver mit Tor bb

	  - wenn nicht load in 3 min, diese seite verwerfen

	  - remove similar, vergleich mit fingerprint (die failen)

       2) Detection
	  
	  - Sally verwandelt von Netzwerktraces in positional n-grams

	  - learning libsvm

	  - auf selbem rechner ausser Cai: cluster

	  - 100 marker, 50 uebertragungen jeweils gemessen

	  - n = 3,

	  - the tolerance to τ = 9 and

	  - the SVM regularization to C = 0.1

       3) Closed-World Evaluation
	  
	  - top 100 seiten je 50 mal

	  - jeweils im februar und april 2014

	  - js via reverse proxy

	  - marker nach 30 \to 120 sec delay

	  - transmission time 12-20 secs

	  - complete marker: 300 packets, \sim 390000 bytes

	  - vergleich mit Hermann.., Panchenko.. und Cai.. (mit
            Fingerprints vom Februar)

	  - torben imm 95%, die anderen schlechter

	  - false classification favors particular markers

       4) Open-World Evaluation

	  - 60000 von Alexa (top million \ top 100)

	  - few (as before, top 100) with markers
	    \to evaluate false positives

	  - detect 91% with no false positives

	  - reliable,
            due to atypical request-response-pairs

       5) Live Evaluation

	  - 4 users, 2 hours each

	  - if probability score below threshold of t=0.1, do not select

    5) Limitation and Defenses

       - torben works reliably

       - limitations?

       - detect web page markers: arms race: attackers change params,

       - chaff traffic: "might lower Tor’s overall performance."

    6) Related Work: first early, then active and passive vs low-latency

       1) Attacks on Encrypted Communication

	  http pattern of access detectible via tls

	  countermeasures fail to address size of data traffic

       2) Passive Attacks against Tor

	  - hermann: ip lengths

	  - panchenko: data sent before direction change,

	  - cai: ordering w/ displacements

	  - wang: tls

	  - high false-positives

	  - counter: morphing,

       3) Active Attacks against Tor

	  - passive: longer period

	  - solve: active attack

	  - 1: reveal communication path

	  - murdoch: similar, but path \to infeasible

	  - watermarking: inject specific patterns, inter-packet delays

	    - needs to control exit node, tcp level (not app)

    7) Conclusion
** [[./acmccs-wpes11-fingerprinting.pdf][Panchenko - Website Fingerprinting in Onion Routing Based Anonymization Networks]]
*** words
    - local eavesdropper

    - closed-world assumption:
      the victim retrieves only web pages from the predefined set and
      the attacker knows the set of possible web pages.

    - cross-validation:
      the data is split into n evenly large parts, the /folds/. Then,
      the entire process of training and testing is repeated n times,
      using one of the n folds as test data and the remaining n − 1
      folds as training data in turn. The results are averaged and
      therefore more solid and meaningful.

    - detection-rate: correct / all (in %)

    - true-positive: correct censored / all censored (in %)

    - false positive: uncensored as censored / all uncensored (in %)
*** summary

    0. [@0] Abstract: local website fingerprinting based on volume,
       time & direction

    1. INTRODUCTION:

       most attacks need some additionaly knowledge, f.ex. seeing both
       ends,

       - between OP and guard node easily observable

       - and gives 80% (73% open) against JAP and 55% against Tor

       - f.ex. ensure that censored /pages/ are not viewed

       - studies influence of supposed features,

       - propose camouflage

    2. RELATED WORKS

       - Hintz coined "website fingerprinting" in 2002 (paper)

       - 1996 Wagner/Schneier

       - 1998 Berkeley project about SSL traffic analysis

       - Bissias: IP packet sizes and inter-arrival times

       - Liberatore and Levine:
         - OpenSSH tunnels,
	 - Jaccard + naive Bayes classifier,
	 - consider only packet size of transmitted data,
	 - neglect timing and order information

       - Wright: morphing as countermeasure

       - Herrmann:
         - multinomial naive Bayes classifier
	 - OpenSSH, OpenVPN, Stunnel, Cisco IPsec-VPN, JAP, Tor
	 - 90%, 90%, 90%, 90%, 20%, 2.95%

    3. DATA SETS

       0) [@0]

	  - Firefox modification: No JS, Flash, Java, Cache

	  - Scripting Chickenfoot

	  - Data from Herrmann et al and Open-World-Dataset

       1) Closed-World Dataset

          - incoming size as positive, outgoing as negative

	  - only fully-loaded pages: users reload else \to load time to 600s

       2) Open-World Dataset

	  - Alexa top 1000000,

	  - three censored: sexually explicit, top, random from alexa

       3) Countermeasure Dataset

	  - applied to closed-world (more difficult to camouflage ==
            easier to detect)

	  - if hampered, then also in open-world

	  - at the same time load a random website

       4)

	  - open: attacker has not seen the user's normal pattern

	  - separate dataset for tuning features and optimizing SVM

    4. A NEW APPROACH

       0) [@0] features, machine learning, compare to bayes, improve via SVM

       1) Features

	  facilitate subsequent classification, describes most relevant

	  Without Packets Sized 52: no ACKs

	  - *Size Markers* of uninterrupted flow (except ACK), grouped
            by 600 bytes

	  - *HTML Marker*: size of html document (first uninterrupted flow)

	  - *Total Transmitted Bytes*: grouped by 10000 bytes

	  - *Number Markers*: number of uninterruped packet flow in
            direction, grouped by 1,2,3-5,6-8,9-13,14

	  - *Occurring Packet Sizes*: grouped by 2, in/out

	  - *Percentage Incoming Packets* grouped by 5%

	  - *Number of Packets*, in/out grouped by 15

	  not working

	  - incoming/outgoing packets,

	  - leaving out frequent/rare sizes,

	  - including TLS/SSL record sizes,

	  - leaving empty TLS records,

	  - preserving packet order,

	  - rounding packet sizes,

	  - rounding packet frequencies

       2) Support Vector Classification

	  Uses SVM instead of Bayes classifier

	  SVMs try to separate the points via a hyperplane, maximizing
          the distance between the closes instances (= support
          vectors) and the plane

	  He uses a radial basis function kernel with parameters C=2^{17}
          and \gamma = 2^{-19}.

	  Finding these was the longest computation time.

    5. EXPERIMENTS AND RESULTS

       1. Experiments

	  closed-world: ten-fold stratified cross-validation

	  open-world: sufficient amount of data, not necessary

	  how:

	  - five censored, Sex Exp, Alexa Top, Alexa Random:
	    35 instances as training, 25 as test

	  - uncensored:
            4000 at random from top 1000000 for training,
            1000 for test, disjoint from training

	  - 20 times measured, each with new uncensored

       2. Results
	  0) [@0]
	     - Closed-World: recognition rates of 54.61% Tor, 80 % JAP

	     - Open-World: true positive rate of up to 73%

	  1) Results on Closed-World Dataset

	     - Final Result via checking if really loaded and removal
               of redirects

	     - JAP premium cascades worse in WF than free cascades

	  2) Results on Open-World Dataset
	     1) Experiment 1
		- 5 censored pages, 35 instances each, 4000 uncensored
		  pages, 1 each

		- test: 1000 which differ from the 4000

		- top ranked most easy to distinguish

	     2) Experiment 2

		- 5 censored, 20 training and 2 testing

		- uncensored variable: tp and fp both fall with more
                  examples of uncensored sites (measured up to 4000)

	     3) Experiment 3

		- censored from whole of alexa, varying number, 35
                  instances each

		- unsensored, 4000, 1 instance each

		- the more censored pages, the less clear the
                  classifier: fp rises, less impact on tp

	     4) Experiment 4

		- 5 censored, varying instance numbers

		- 4000 uncensored, 1 instance each

		- the more instances, the clearer, converges at about 35

	     5) Summary:

		- Your ISP could find out what you do online

    6. COUNTERMEASURES

       - padding works rather bad

       - camouflage: load randomly chosen web page simultaneously

       - used in both training and testing

       - to 3% where close to random would be optimal

    7. CONCLUSION AND FUTURE WORK

       Website Fingerprinting is possible in Tor and JAP, camouflage hampers.

       Next:

       detect: additional feature selection, active content, embedded
       links, analyse specific webpages,

       deter: browser plug-in, user feedback per page, parallel camouflage

*** quotes
    - We first define features for website fingerprinting solely based
      on volume, time, and direction of the traffic.

    - Finally, we show preliminary results of a proof-of-concept
      implementation that applies camouflage as a countermeasure to
      hamper the fingerprinting attack. For JAP, the detection rate
      decreases from 80% to 4% and for Tor it drops from 55% to about
      3%.

    - Several attacks against anonymization networks have been
      discovered, e.g., [6, 17, 19, 18], most notable the traffic
      confirmation attack.

    - Totalitarian regimes such as China or Iran usually do not have
      control over the communication party located in western
      countries precluding a traffic confirmation attack.

    - this attack is very realistic and anonymization networks must by
      all means be secure with respect to local attacks.

    - Instead of transforming websites, we obfuscate the page by
      loading another page in parallel.

    - In practice an attacker first retrieves a certain amount of
      relevant web pages by himself as training data for
      fingerprinting, using the anonymization network that he assumes
      his victim uses as well.

    - For each fetch of a URL, we store the sizes of packets in the
      observed order while recording incoming packets as positive,
      outgoing ones as negative numbers.

    - We once more achieve alarming detection rates motivating the
      need for additional countermeasures for anonymization networks

    - Successful countermeasures should decline the detection rates of
      all web pages to a level that is almost similar to random guess
      and at the same time cause only little performance losses.

    - We expect even better obfuscation for additional background
      pages as it will be more challenging for the attacker to extract
      the original statistics from the merged packets. Still, it has
      to be explored whether more sophisticated statistical measures
      can achieve this extraction.

*** ref
   #+BEGIN_SRC bibtex
     @inproceedings{panchenko,
       Author={Panchenko, Andriy and Niessen, Lukas and Zinnen, Andreas and Engel, Thomas},
       Booktitle={Proceedings of the 10th ACM Workshop on Privacy in the Electronic
     Society},
       Title={Website fingerprinting in onion routing based anonymization networks},
       Pages={103--114},
       Year={2011}
     }
   #+END_SRC
** [[https://blog.torproject.org/blog/experimental-defense-website-traffic-fingerprinting][Experimental Defense for Website Traffic Fingerprinting]]
   - [[./experimental.html][local copy]]
*** summary
    - prior: belief that all was well (failed attempts)
    - panchenko: showed that not
    - disagree with background fetch: additional traffic
    - first attempt at mitigation: enable http pipelining and
      randomize pipeline size, request further research
    - other: http to spdy && ofbsp
*** quotes
    - Despite these early results, whenever researchers tried naively
      applying these techniques to Tor-like systems, they failed to
      come up with publishable results (meaning the attack did not
      work against Tor), due largely to the fixed 512 byte cell size,
      as well as the multiplexing of Tor client traffic over a single
      TLS connection.
    - We create it as a prototype, and request that future research
      papers do not treat the defense as if it were the final solution
      against website fingerprinting of Tor traffic.
    - However, the defense could also be improved. We did not attempt
      to determine the optimal pipeline size or distribution, and are
      relying on the research community to tweak these parameters as
      they evaluate the defense.
    - as these translations are potentially fragile as well as
      labor-intensive to implement and deploy, we are unlikely to take
      these measures without further feedback from and study by the
      research community.
** [[./article-2456.pdf][A Critical Evaluation of Website Fingerprinting Attacks]]
*** summary
    0) [@0] ABSTRACT

       many WP papers do not use practical scenarios: browsing habits,
       location, version tbb,

    1) INTRODUCTION

       old studies did less about localization, tbb version and
       browsing habits, this addresses

       - evaluates these assumptions

       - what defeats the accuracy

       - how to reduce false positive rates

       - adversary's cost

    2) WEBSITE FINGERPRINTING

       find out which site or page is visited from network traffic only

       - first within single website

       - then within set of websites

       - then hintz's safeweb anonymizing web proxy ++

       - then Hermann: 3% success

       - Shi 50% for 20 pages, Panchenko 54% for Herrmann's dataset

       - cai et al, wang and goldberg: over 90%, *100 pages*

    3) MODEL

       passive local attack, targeted vs non-targeted

       1) Assumptions

	  listed by papers that explicitly mention assumptions

	  client-side, adversary, web assumptions

	  - client:

	    closed world: user may only visit certain pages, or only
            certain pages from a set are searched for

	    browsing behavior: users only have one tab open at a time,
            sequential browsing

	  - websites:

	    (?) all websites are built using templates

	    localized versions: but language of webpage is determined
            by exit node (really?)

	  - adversary:

	    page load parsing: page load start/stop are detectable

	    no background traffic: tor separable from other traffic

	    replicability: adversary can replicate user's setup (tbb
            version, OS, network connection)

    4) EVALUATION

       some assumptions distort the model

       1) Datasets

	  Alexa top sites and ALAD

       2) Data collection

	  - tbb with selenium

	  - dumpcap

	  - tor configured via stem

	  - circuit renewal to 600000 (? cf. wang/goldberg)

	  - disable UseEntryGuards

	  - batches: page 4 times, 5-10 batches of data per time

	  - 5 seconds before each crawl, 5 second pauses between each visit

	  - round-robin, hours apart

	  - two physical, three cloud-based virtual machines

	  - Linux Container based virtualization

	  - disabled OS updates (how about time, claws updates?)

	  - one crawler per machine at a time

	  - average CPU load low

       3) Methodology

	  - control crawl : default value

	  - test crawl: value of interest

	  - less controllable: time and tor-path-selection

	    - k-fold cross-validation and

	    - minimizing time gap control-to-test

	  - compared other papers

	  - chose the faster of the two by W[32]

	  - also own decision tree with panchenko "merkmale"

       4) Time

	  website fingerprints decay as time goes on: 50% after 9 days

       5) Multitab browsing

	  decays a lot, halved when only one of them counts as success

	  delays (0.5, 3, 5 sec) matter very little

       6) Tor Browser Bundle Versions

	  2.4.7 dissimilar to others

	  3.5 similar to 3.5.2.1

	  accuracy greatest for NumEntryGuards = 1, UseEntryGuards = 1

	  lowest for UseEntryGuards = 0, +2% for NumEntryGuards =3 (default)

       7) Network

	  differences in where the puter is matter greatly: backbone
          gives different pattern

       8) The importance of false positives

	  - Open-world: 

	    4 top sites vs 32710 other sites.

	  - The base rate fallacy

	    If there is a low chance that the user visits the
            fingerprinted websites, then the occurrence of false
            positives relative to true positives rises.

	  - User’s browsing habits

	    three random users from ALAD, 100 URLs each

	    tried to match everything, failed

    5) CLASSIFY-VERIFY

       probabilistic SVM

       with rejection if P_1 or P_1 - P_2 lower than threshold

       1) Evaluation and result

	  this combination greatly decreases the number of false
          positives

    6) MODELING THE ADVERSARY’S COST

       1) Data collection cost:

	  data D = n (training pages) \cdot m (versions) \cdot i (instances)

	  collection cost col(D) 

       2) Training cost:

	  train(D, F(=features), C(=classifier)) = D \cdot c

       3) Testing cost:

	  Test data T = v (=victims) \cdot p (=pages /victim /day)

	  test = col(T) + test(T, F, C)

       4) Updating cost:

	  update(D, F, C) / d(=days until change)

       5) Total cost:

	  init(D,F,C,T) = col(D) + train(D,F,C) + col(T) + test(T,F,C)

	  cost(D,F,C,T,d) = init(D,F,C,T) + update(D,F,C)/d

    7) CONCLUSION AND FUTURE WORK

       practical scenarios
*** quotes
    - The main objective of an adversary in a typical WF scenario is
      to identify which page the user is visiting.
    - Wang and Goldberg concluded that sites that change in size are
      hard to classify correctly
    - Over 50% sites were pages other than the front page
    - Classifiers designed for WF attacks are based on features
      extracted from the length, direction and inter-arrival times of
      network packets, such as unique number of packet lengths or the
      total bandwidth consumed.
    - In most cases, classifier W performed better than the others.
    - the accuracy drops extremely fast over time.
    - We observe a dramatic drop in the accuracy for all the
      classifiers with respect to the accuracy obtained with the
      control crawl
    - This might imply that the specific learning model is not as
      important for a successful attack as the feature selection.
    - The average page load for the test crawl for the 5 second gap
      experiment is 15 seconds, leaving on average 30% of the original
      trace uncovered by the background traffic. Even in this case,
      the accuracy with respect to the control crawl drops by over
      68%.
    - In practice, many TBB versions coexist, largely because of the
      lack of an auto-update functionality. (*new versions include updater*)
    - Even though we fix the entry guard for all circuits in a batch,
      since we remove the Tor data directory after each batch, we
      force the entry guard to change. On the other hand, allowing Tor
      to pick a different entry guard for each circuit results in a
      more balanced distribution because it is more likely that the
      same entry guards are being used in each single batch, thus
      there is lower variance across batches. We must clarify that
      these results are not concluding and there may be a different
      explanation for such difference in standard deviation.
    - the accuracy drop between the crawls training on Leuven and
      testing in one of the other two locations is relatively greater
      than the accuracy drop observed in the experiments between
      Singapore and New York. Since the VM in Leuven is located within
      a university network and the other two VMs in data centers
      belonging to the same company
    - One possible reason for low TPR is due to the effect of inner
      pages.
    - Bayesian detection rate [...] is defined as the probability that
      a traffic trace actually corresponds to a monitored webpage
      given that the classifier recognized it as monitored.[...]

      P (M | C) 
      = [P (C | M) P (M)] / [P (M) P (C | M) + P (¬M) P (C | ¬M)]
    - The results show that the BDR doubles when we use the
      Classify-Verify approach.
    - 10-fold cross-validation, where a threshold is determined by
      using 90% of the data and then tested on the remaining 10%.
    - train with different localized versions
    - When each of these assumptions are violated, the accuracy of the
      system drops significantly, and we have not examined in depth
      how the accuracy is impacted when multiple assumptions are
      violated.
    - it seems that the non-targeted attack is not feasible given the
      sophistication level of current attacks.
    - We believe that further research on evaluating the common
      assumptions of the WF literature is important for assessing the
      practicality and the efficacy of the WF attacks.

** [[./cacr2014-05.pdf][Effective Attacks and Provable Defenses for Website Fingerprinting]]
*** summary
    0) [@0] Abstract

       effective for seldomly visited pages

       85% tpr vs 0.6% fpr

    1) Introduction

       tor, ssh tunnels, vpn, ipsec are vulnerable to website
       fingerprinting

       contributions:

       - better attack

       - large open-world setting

       - best defense: supersequences over anonymity sets

    2) Basics

       1) Website Fingerprinting on Tor

	  two assumptions:

	  - clear start and end of trace

	  - no other activity

       2) Classification

	  kNN is multi-modal: different settings yield different
          traces for the same page

    3) Related Work

       HTTP/1.0 (resource lengths)
       
       \to HTTP/1.1, VPN, SSH-Tunnel (packet lengths)

       \to TOR (padded packet lengths)

       1) Resource length attacks

	  HTTP/1.0: each resource a separate tcp connection

       2) Unique packet length attacks

	  HTTP/1.1: combined in tcp connection, yet packets length
          distinguishable

       3) Hidden packet length attacks

	  extract features:

          - burst patterns

	  - main document size

	  - ratio incoming/outgoing

	  - total packet counts

	  use SVN

	  Dyer: less features, n-grams

	  Cai: edit distance of packet sequences, modified kernel of SVM

	  W&G: modified edit distance algo

       4) Defenses

	  simulatable vs non-simulatable

	  - simulatable: transform packet sequence, does not look at
            contents, cheaper

	  - non-simulatable: in-browser, access to client data

	  deterministic vs random

	  - deterministic: always outputs the same sequence on the
            same input

	  - random: can differ
            |-----------------+-------------------------+-----------------|
            |                 | random                  | deterministic   |
            |-----------------+-------------------------+-----------------|
            | simulatable     | morphing & panchenko    | padding & BuFLO |
            | non-simulatable | Tor's packet reordering | parts of HTTPOS |

    4) Attack

       k-NN, large feature set with weight adjustment

       1) k-NN classifier

	  points with classes, "lowest distance chosen"

          lots of features, weighted & learned distance

	  similar to SVM

       2) Feature set

	  diverse:

	  - general features: total size, total time, number of
            incoming and outgoing packets

	  - unique packet lengths: 1 if a size occurs, 0 if not, for
            each size incoming and outgoing (useless on Tor)

	  - packet ordering: number of packets before this, number of
            incoming between outgoing packets (burst, see Cai)

	  - Concentration of outgoing packets: number of outgoing in
            last 30 packets (non-overlapping span)

	  - bursts: sequence with no two adjacent incoming packets,
            maximum length, mean length, number of bursts

	  - initial lengths: length of first 20 packets

	  pads with special character X for empty values, such that
          d(X, y) := d(y, X) := 0

       3) Weight initialization and adjustment

	  R rounds of learning

	  focus on one point P_{train} \in S_{train}, do two steps

	  - weight recommendation

	    - compute distances to all other P' \in S_{train}

	    - take closest k_{reco} points in S_{train} as S_{good}
	      and closest k_{reco} points in all other classes S' as S_{bad}

	    - with d(P, S) := { d(P, s) | s \in S } define:

	      d_{maxgood_i} := max({d_{f_i}(P_{train}, P) | P \in S_{good} }) and compute:

	      n_{bad_i} := |{P' \in S_{bad} | d_{f_i}(P_{train}, P') \le d_{maxgood_i} }|

	      number of classes "closer" by feature than worst good candidate

	      the closer to k_{reco}, the worse

	  - weight adjustment

	    - for features worse than the best,
              reduce by \Delta w_{i} = w_{i} \cdot 0.01

	    - other features, afterwards, increase equally such that
              d(P_{train}, S_{bad}) remains the same

	    - both:

	      - \Delta w_i \cdot n_{bad_i}/k_{reco}

	      - multiply by overall badness 0.2 + N_bad/k_{reco} with
                N_{bad} = |{P' \in S_{bad} | d(P_{train}, P') \le d_{maxgood} }|

	  - best results for k_{reco} = 5

	  - random vector between 0.5 and 1.5

    5) Attack evaluation

       better than all others

       1) Attack on Tor

	  - 90 instances each of 100 sensitive pages

	  - 1 instance each of 5000 non-monitored pages

	  - regular circuit resetting, no caches and time gaps between
            multiple loads of the same page

	  - weight adjustment: 6000 rounds, 100 pages, 60 instances
            (each instance once)

	  - only classified if all k neighbors agree, varying 1 \le k \le 15

	  - W&G has 10x higher FPR

	  - accuracy levels off after 800 rounds of weight adjustment

	  - 0.1 CPU seconds to test one instance,

       2) Training confidence

	  - FPR good for k=6, TPR good for k=2 (|C_{0}| = 500)

       3) Attack on Other Defenses

	  - evaluated defenses:

	    - traffic morphing,

	    - HTTPOS split,

	    - Panchenko decoy,

	    - BuFLO

	  - implemented as simulations.

    6) Defense

       Tamaraw++

       supersequences (provably best in sumulatable, deterministic
       class)

       approximation of optimal strategy

       1) Attacker’s upper bound

	  - Attacker: given observation (packet sequence p), find class C(p)

	  - trains on the same data

	  - optimal strategy: find class that occurs the most often

	  - with possibility set Q(p) := {C_{1}, C_{2}, ...} classes with
            the same observation p define

            Accuracy Acc(p):= |{C ∈ Q(p)|C = C_{max }}| / |Q(p)|

	  - non-uniform accuracy:= mean of accuracies Acc(p) (p \in S_{test})

	  - uniform accuracy:= maximum of accuracies Acc(p) (p \in S_{test})

	  - defense with optimal uniform accuracy leads to optimal
            non-uniform accuracy

       2) Optimal defense

	  - bandwidth-optimal, simulatable defense

	  - packet sequence as sequence of +1/-1

	  - anonymity set: set of packet sequences p_{i} with D(p_{i}) the
            same

       3) Anonymity set selection

	  - client cannot always choose freely:

	    - before page load

	  - levels of information

	    1. no information: all have to map to single set

	       solution: single supersequence

	    2. sequence end information: when is the query ended

	       solution: single supersequence with stopping points

	    3. class-specific: class is clear, but f.ex. multi-modal
               mode is not

	    4. full: prescience

	  - clustering to find anonymity sets

	    - except in level 1 or 2: one set

	    - level: cluster by distance for prefixes p',q' of length
              min(|p|,|q|):
              2 |f_{SCS}(p', q')| - |p'| - |q'|

	      step 2: stopping points by prefix

	    - level 4: only by distance with whole p,q

       4) SCS approximation

	  - NP-hard problem

	  - approximation algo:

	    - counters c_{i} for each packet sequence p_{i} of n, init at 1

	    - if p_{i}[c_{i}] outgoing for more than n/4-ths, add outgoing,
	      increment c_{i} where outgoing

	    - else, add incoming, increment c_{i} where incoming

	  - cannot have bounded error

    7) Defense evaluation

       best: only two supersequences, supersequence is way better,
       also than tamaraw, as it has uniform accuracy

    8) Discussion

       1) Realistically applying an attack

	  attacker's assumption: start/end is clearly defined and
          trace is from a single page load

       2) Realistic consequences of an attack

	  - many sensitive pages are among the top-100

	  - if local and temporal area is known, identifying get way
            easier

       3) Reproducibility of our results

	  attack, defense, other attacks & defenses and data available

    9) Conclusion

       - pages multi-modal

       - adjusting distance weights

       - knn very fast

       - performs well

       - powerful against all known defenses

       - provable defense

       - better overhead, same security level
*** quote
    - We found that our new attack is much more accurate than previous
      attempts, especially for an attacker monitoring a set of sites
      with low base incidence rate.
    - Privacy technologies are becoming more popular: Tor, a
      low-latency anonymity network, currently has 500,000 daily users
      and the number has been growing [21].
    - Only with a provably effective defense can we be certain that
      clients are protected against website
      fingerprinting. (*certainty* necessary?)
    - a training and testing time that is orders of magnitude lower
      than the previous best.
    - Tor developers remain unconvinced that website fingerprinting
      poses a real threat.
    - An attacker can deal with multi-modal data sets by gathering
      enough data to have representative elements from each mode.
    - Random defenses (noise) have the disadvantage that choosing a
      good covering is not guaranteed,
    - Implementation of random defenses must be careful so that noise
      cannot be easily distinguished from real packets.
    - We then train the attack to focus on features which the defense
      fails to cover and which therefore remain useful for
      classification.
    - The k-NN classifier needs a distance function d for pairs of
      packet sequences. The distance is non-trivial for packet
      sequences.
    - The total number of features is close to 4,000 (3,000 of which
      are just for the unique packet lengths).
    - Note that we are not claiming these particular choices of
      parameters and constants yield an optimal attack
    - Our list of 100 monitored pages was compiled from a list of
      blocked web pages from China, the UK, and Saudi Arabia.
    - After |C_{0} | > 2500 [non-monitored pages], we could not see a
      significant benefit in adding more elements.
    - if the base incidence rate of the whole sensitive set is 0.01
      (99% of the time the client is visiting none of these pages),
      and our new classifier claims to have found a sensitive site,
      the decision is correct at least 80% of the time, the rest being
      false positives.
    - The testing time amounts to around 0.1 CPU seconds to classify
      one instance for our classifier and around 800 CPU seconds for
      Wang and Goldberg’s classifier, and 450 CPU seconds for that of
      Cai et al.
    - almost all of the graph in Figure 1 can be drawn only by varying
      k with |C_{0}| = 5000, suggesting that it is advantageous for the
      attacker to have a large number of non-monitored training pages.
    - Then, we must determine the SCS of all the packet sequences in
      the anonymity set. This is in general NP-hard. [13]
    - In fact, it is known that any polynomial-time approximation
      algorithm of shortest common supersequences cannot have bounded
      error [13].
    - It is possible that a clever clustering strategy for class-level
      information could achieve lower bandwidth overheads.
    - the start of a packet sequence generally contains around 3 times
      more outgoing packets than the rest of the sequence. If the user
      is accessing a page for which she does not have a current
      connection (i.e. most likely the user is visiting a page from
      another domain), then the user will always send one or two
      outgoing connections (depending on the browser setting) to the
      server, followed by acceptance from the server, followed by a
      GET request from the main page, and then by data from the
      server. This particular sequence is easily identifiable.
    - On Tor, users are discouraged from loading videos, using
      torrents, and downloading large files over Tor, which are types
      of noise that would interfere with website fingerprinting.
*** questions
    - features perfectly covered by a defence (such as unique packet
      lengths in Tor) will always have n_{bad_i} = k_{reco} , its maximum
      possible value.

      why *always*?

    - Then, accuracy is computed over the remaining 30 instances each,
      on which we perform all-but-one cross validation.

    - As we work with Tor cells, in the following a packet sequence
      can be considered a sequence of -1’s and 1’s (downstream and
      upstream packets respectively),

      so timing information is omitted?
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2014-critical,
        title = {A Critical Evaluation of Website Fingerprinting Attacks},
        author = {Marc Juarez and Sadia Afroz and Gunes Acar and Claudia Diaz and Rachel
              Greenstadt},
        booktitle = {Proceedings of the 21th ACM conference on Computer and Communications
              Security (CCS 2014)},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.cosic.esat.kuleuven.be/publications/article-2456.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** [[./ccs14.pdf][Cai - A Systematic Approach to Developing and Evaluating Website Fingerprinting Defenses]]
*** summary
    0) [@0] ABSTRACT

       - systematic analysis of features

       - proven lower bounds of bandwidth cost

       - mathematical framework for open-world given close-world

       - tamaraw, better than BuFLO

    1) INTRODUCTION

       fingerprinting attacks

       - dyer: 80%, which of 128 pages (5)

       - cai: 75% against countermeasures (3)

       - Cai: bundle defenses inffective (13)

       - Luo: HTTPOS (11)

	 - Cai: little benefit

       - Wright: traffic morphing (19)

         - Dyer, Cai: little protection

       - Dyer: BuFLO

       - real world vs close-world (14)

       - danger in real world

       - state-of-the-art: only lower bound

       - ideal attacker: websites distinguishable unless exact same
         pattern

       - abstract model:

	 - how far from optimal,

       - which traffic features leak most information

       - provably secure: tamaraw

       - evaluate tamaraw with above techniques

    2) WEBSITE FINGERPRINTING ATTACKS

       - cai and chen aim at identifying web sites instead of web
         pages

       - wf explained

         - only encrypted proxy

	 - page has characteristic dl/ul traffic pattern

       - two assumptions retained

	 - page start noticeable

	 - no background traffic (file downloads, music streaming, etc)

    3) FEATURES AND METHODOLOGY

       wf tries to classify by features, defense tries to hide them

       1) Packet Sequences and their Features

	  - time and length (positive for outgoing, negative for incoming)

	  - unique packet lengths (problem with tor)

	    (∃L ∈ P_{\ell} | L \not∈  P'_{\ell}) ∨ (∃L ∈ P_{\ell}' | L \not∈ P_{ell}' )

	    exists a length L
            which is in P, but not P'
            or in P', but not P

	  - packet length frequency (how often packet length occurs)

	    \exists L | n_{L}(P_{l}) \neq n_{L}(P_{l}') \wedge n_{L}(P_{l}) > 0 \wedge n_{L}(P_{l}') > 0

	    exists a length L
	    which occurs n_L times in P and not n_L times in P'
	    and with both occurances greater than 0

	  - packet ordering:

	    for the multiset of packet lengths M(P)
	    M(P) = M(P')
	    and P \ne P'

	  - interpacket timing:

	    two packets cannot be dependent, if their interpacket
            times is less than one RTT

	    exists 1 \le i \le min(|P|, |P'|)
	    such that the timing t(P_i) \ne t(P'_i)

	  - this is a complete feature set (fact 1) (?td: think?)

	  - features are rather independent (fact 2) (?)

       2) Comparative Methodology

	  - "To determine if a defense is able to hide a feature, we
            apply the defense to two classes, C and C 0 , which differ
            only by that feature. Then, we say that a defense is
            successful in hiding the feature if after applying the
            defense, there is no discernible difference between C and
            C 0."

	  - several generators

	    1. small changes G_{1}: length + v, upto MTU
	    2. large changes G_{2}: length + 1000, upto MTU
	    3. length diffusion G_{3}: increased by position i/5, upto MTU
	    4. append incoming packets G_{4}: length MTU
	    5. append outgoing packets G_{5}: length first outgoing
	    6. insert incoming packets G_{6}: length MTU, one per 5 packets
	    7. Adjacent Transpositions: "v packets are transposed with
               the previous packet"
	    8. Short-Distance Transpositions: v packets are transposed
               with the packet 4 elements ago.
	    9. Long-Distance Transpositions: v packets are transposed
               with the packet 19 elements ago.
	    10. Delays: Each packet is delayed by a linearly
                increasing amount of time, multiplied by v.

       3) Classification and Experimental Setup

	  C = 400 samples of bbc.co.uk
	  C' = generator(C)

	  200 training, 200 testing

	  4 feature classifiers

	  - Unique Packet-Lengths: (like jaccard of Liberatore)

	  - Packet-Length Frequencies: mean, std of (bytes and
            packets) (incoming and outgoing)

	    scored separately, multiplied (like naive bayes of Liberatore)

	  - Packet Ordering: each position: length compared to mean of
            all training packet length  (like bissias/liberatore)

	  - Interpacket Timing: total elapsed time

	  defense applied to each element c and c'

	  measured by the differences between C and c' before
          classifier can distinguish

	  setup: 100mbps ethernet, mtu 1500, imacros 9.00 firefox
          23.0, tcpdump

       4) COMPARISON OF DEFENSES

	  state-of-the-art defenses, simulated

	  1) Simulated Defenses

	     - Maximum Packet Padding (PadM): pads all to mtu

	     - Exponential Packet Padding (PadE): pad to closest power of 2

	     - Traffic Morphing (Wr-Morph): mimic target page

	     - HTTP Obfuscation (HTTPOS): client-side only, tcp
               advertised windows, http ranges, control sizes of
               outgoing and incoming

	       (here: just split packet without extra packets)

	     - Background Noise (Pa-Decoy): load decoy in background

	       (here: alexa top 800)

	     - Buffered Fixed Length Obfuscator (BuFLO): packets at
               fixed intervals with fixed lengths

	  2) Comparative Results

	     - "The full results are given in Table 3"

	     - v from 1 to 180,

	       - best feature classifier

	       - minimum value v for 55 % accuracy

	       - minimum value v for 75 % accuracy

	       - * means unable to

	     - PadM covers: unique packet lengths and orderings,
               better than PadE

	       - both beaten by frequency analysis

	     - HTTPOS broken (f.ex. packet ordering)

	     - PaDecoy, BuFLO work against Panchenko and frequency attacks

	     - Pa-decoy does not completely cover total time (fails
               half the time)

	     - BuFLO similar over 10seconds

	     - HTTPOS client-only

       5) THEORETICAL FOUNDATIONS

	  Model of WF attacks, lower bounds for bandwidth overhead.

	  1) Security vs. Overhead Trade-Off

	     dissimilarity of websites increases overhead

	     offline version

	     1) Definitions

		- w: website

		- t: packet trace

		- W: random variable for w (attacker knows distribution)

		- T_{w}^{D}: random variable for t with defense (attacker knows d.)

		- T_{w}: random variable for t without defense

		- A(t) = argmax_{w} Pr[W = w] Pr[T_{w}^{D} = t]

		  attacker output (determine website w)

		- D *non-uniformly \epsilon-secure* for W iff Pr A(T_{W}^{D}) = W ≤ \epsilon.

		- D *uniformly \epsilon-secure* if max_{w} Pr A(T_{W}^{D}) = w ≤ \epsilon.

		- B(t): total number of bytes transmitted in trace t.

		- BWRatio_{D}(W): E[B(T_{W}^{D})] / E[B(T_{W}^{})]

                  bandwidth ratio of defense D

	     2) Bandwidth Lower Bounds

		- THEOREM 1. Suppose n is an integer. Let W be a
                  random variable uniformly distributed over w_{1}, ... ,
                  w_{n}, i.e. W represents a closed-world
                  experiment. Suppose D is a defense that is
                  \epsilon-non-uniformly-secure against A_{S} on
                  distribution W. Then there exists a monotonically
                  increasing function f from S = {s_{1} , ... , s_{n}} to
                  itself such that

		  - |f(S)| ≤ \epsilon n
		  - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).

		- A_{S}(t) = argmax_{w} Pr[B(T_{w}^{D}) = B(t)]

		  optimal, looks only at total size

		- "Such an f is equivalent to a partition S_{1}, ... , S_{k}
                  of S satisfying k ≤ \epsilon n and minimizing
                  \sum_{i=1}^{k} |S_{i}| max_{s \in S_{i}} s.

		- THEOREM 2. Let W be uniformly distributed over w_{1},
                  ... , w_{n}, i.e. W represents a closed-world
                  experiment. Suppose D is a deterministic defense
                  that is uniformly-\epsilon-secure against A_{S} on
                  distribution W. Then there exists a monotonically
                  increasing function f from S = {s_{1} , ... , s_{n}} to
                  itself such that

		  - min_{i}|f^{-1}(s_{i})| \ge  1/ \epsilon
		  - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).

	  2) From Closed to Open World

	     - "researchers need only perform closed-world experiments
               to predict open-world performance."

	     - single w^{*}, find out if visited or not

	     - construct open-world from closed-world by selecting
               websites w_{2}, ..., w_{n} and determining if A(t) = w^{*

	     - compute false-positive rate by (p_{i} probability of w_{i})

	     - R_{n} = 1/n \cdot Pr[A(T_{w*}^{D}) = w^{* }] + \sum_{i=2}^{n} Pr[A(T_{wi}^{D}) = w_{i}^{}]
	       "the average success rate of A in the closed world"

	     ... compute FPR, TPR, TDR (true-discovery rate)

	     - algorithm

       6) TAMARAW: A NEW DEFENSE

	  theoretically provable BuFLO

	  1) Design

	     1) Strong Theoretical Foundations:

		optimal partitioning and feature hiding against A_{S}
                attackers

	     2) Feature coverage:

		not only total size, but all features (except for total
                downstream transmission size)

	     3) Reducing Overhead Costs:

		reduces BuFLO's overhead (bandwidth and time)

	     differences to BuFLO:

	     - 750 bytes, not MTU (most packets)

	     - distinguish incoming/outgoing

	     - time to next supersequence, not fixed

	     Tamaraw as follows:

	     - "We denote the packet intervals as ρ_{out} and ρ_{in}
               (measured in s/packet)."

	     - "In Tamaraw, however, the number of packets sent in
               both directions are always padded to multiples of a
               padding parameter, L"

	  2) Experimental Results

	     0) [@0]

		- "our objective in the choice of ρ_{in} and ρ_{out} is to
		  minimize overhead."

		- "as ρ in and ρ out increased, size overhead decreased
		  while time overhead increased"

		- padm better in some accounts

	     1) An Ideal Attacker

		- "evaluate the partitions produced by Tamaraw"

		- "For a partition of size |S|, the attacker can at
                  best achieve an accuracy of 1/|S| on each site in
                  the partition."

	     2) Closed-world Performance

		much better overhead ratio than BuFLO (configurable)

	     3) Open-world Performance

		Much better than agains Tor, BuFLO

       7) CODE AND DATA RELEASE

	  all available (notes: ask)

       8) CONCLUSIONS

	  classify and qualify WF defenses

	  tamaraw

       9) ACKNOWLEDGMENTS

	  Panchenko talked
*** quote
    - the Tor project now includes both network- and browser-level
      defenses against these attacks
    - an attacker could infer, with a success rate over 80%, which of
      128 pages a victim was visiting, even if the victim used
      network-level countermeasures.
    - In our ideal attack, two websites are distinguishable unless
      they generate the exact same sequence of network traffic
      observations.
    - The structure of a page induces a logical order in its packet
      sequence.
    - BuFLO unnecessarily wastes bandwidth hiding the number of
      upstream packets and does not adequately hide the total number
      of downstream packets.
    - This means that the attacker is weak, but is also resource-light
      and essentially undetectable
    - We indicate the packet length as a positive value if the packet
      is outgoing and as a negative value if it is incoming.
    - Packets are sent at fixed intervals with fixed length, and if no
      data needs to be sent, dummy packets are sent instead.
    - Pa-Decoy fails to completely cover interpacket timing because it
      only covers the total transmission time roughly half the time
      (i.e., when the decoy page takes longer to load than the desired
      page)
    - a set of similar websites can be protected with little overhead,
      a set of dissimilar websites requires more overhead.
    - show how to derive open-world performance from closed-world
      experimental results
    - DEFINITION 1. A fingerprinting defense D is *non-uniformly
      \epsilon-secure* for W iff Pr A(T_W^D) = W ≤ \epsilon. Defense D is *uniformly
      \epsilon-secure* for W if max_w Pr A(T_w^D ) = w ≤ \epsilon.

      These are information-theoretic security definitions – A is the
      optimal attacker described above. The first definition says that
      A’s average success rate is less than, but it does not require
      that every website be difficult to recognize. The second
      definition requires all websites to be at least \epsilon difficult to
      recognize. All previous papers on website fingerprinting attacks
      and defenses have reported average attack success rates in the
      closed-world model, i.e. they have reported non-uniform security
      measurements.
    - if the fingerprinting attacker is a government monitoring
      citizens Tor usage, then W would be distributed according to the
      popularity of websites among that nation’s Tor users.
    - Cai, et al., showed that the Alexa top 100 websites were about
      as similar as 100 randomly chosen websites [3], i.e. that the
      most popular websites are not particularly similar to eachother.
    - true-discovery rates for the open-world attack and defense
      evaluations in this paper. Given an open-world classifier, C,
      its true-discovery rate is defined as TDR(C) = Pr[W = w^∗ |
      C(T_W^D) = 1]. Intuitively, the true-discovery rate is the
      fraction of alarms that are true alarms.
    - In our implementations of BuFLO and Tamaraw, we pessimistically
      required that the original logical ordering of the real packets
      must be maintained.
    - A practical implementation could achieve a lower size and time
      overhead as re-ordering is possible for both defenses when
      subsequence is not consequence;
    - we eliminate the network variability and make the defense system
      deterministic, which, as shown in the Appendix, does not reduce
      the security of the defense.
    - at a size overhead of 130%, there are 553 partitions
      (non-uniform security of 69%) in BuFLO (τ = 9) and 18 partitions
      (non-uniform security of 2.25%) in Tamaraw.
    - By showing that the TDR becomes extremely low when attacking
      Tamaraw, even for the first 100 websites, we show that it’s
      extremely low for all websites.
    - The lower bounds of bandwidth costs are surprisingly low,
      suggesting that it may be possible to build very efficient
      defenses.
*** code
**** [[../sw/attacks/svm.py][svm.py]]
     #+BEGIN_SRC python
       #data is in this format:
       #each data[i] is a class
       #each data[i][j] is a standard-format sequence
       #standard format is: each element is a pair (time, direction)
     #+END_SRC
     - str_to_sinste: helper function, splits string
     - load_one: appends lines to data, returns
     - load_all: appends load_one to data, returns
     - extract: extracts features from data
       - sizemarkers: pad to 300 with 0
       - html size: my naive approach
       - total transmitted: sums up
       - number markers: pads to 300
       - unique packet: unique lengths (-/+)
       - percentage incoming
       - number of packets
     - "main"
       - splits data in test and training
       - saves test and training files
***** problemsmaybe:
      - unique packet no fixed length
**** [[file:~/da/git/sw/attacks/svm-run.py]]
     runs
     - python svm.py i
     - svm-train -c c -g g svm.train svm.model
     - svm-predict svm.test svm.model svm.resultst >> temp-acc
     for i folds from 1 to 10
**** [[file:~/da/git/sw/attacks/svmdotest.rb]]
     cleans up, runs
     - clgen_stratify cltor_matrix 36 40
     - svm-train -t 4 -c 1024
     - svm-predict
*** problemsmaybe
    - append small packets generator lacking
    - "Essentially, these two assumptions are equivalent to assuming
      that w^{∗} is not particularly difficult or easy for A to
      recognize."
    - We also show that, in some contexts, randomized defenses offer
      no security or overhead advantage compared to deterministic
      defenses.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2014-fingerprinting,
        title = {A Systematic Approach to Developing and Evaluating Website Fingerprinting
              Defenses},
        author = {Xiang Cai and Rishab Nithyanand and Tao Wang and Rob Johnson and Ian Goldberg},
        booktitle = {Proceedings of the 21th ACM conference on Computer and Communications
              Security (CCS 2014)},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.cs.stonybrook.edu/~rob/papers/ccs14.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** [[./guide_libsvm.pdf][A Practical Guide to Support Vector Classification]]
*** summary
    0) [@0] Abstract

       SVM cookbook

    1) Introduction

       0) [@0]

	  - separate into training and testing sets

	  - training set instance:

	    - "target value" = class label

	    - "attributes" = features or observed variables

	  - goal: produce model which predicts target values of test data
            given only its attributes

	  - four basic kernels (other developed)

	    - linear

	    - polynomial

	    - radial basis: exp(-γ || x_{i}- x_{j} ||^{2} )

	    - sigmoid

       1) Real-World Examples

	  data by users

       2) Proposed Procedure

	  - transform data for input

	  - scale

	  - with rbf:

	    - find C,\gamma by cross-validation

	    - train whole training set

	  - test

    2) Data Preprocessing

       1) Categorical Feature

	  - use m numbers to represent a m-category attribute
	    one is one, others are zero

	    +: more stable

       2) Scaling

	  +: avoid attributes in greater numeric ranges dominating
          those in smaller numeric ranges

	  +: avoid numerical difficulties

	  how: linearly scale to [-1, +1] or [0,1]

	  care: same scale for training and testing (which might then
          be [-1.1, +0.8])

    3) Model Selection

       1) RBF Kernel

	  - includes linear kernel

	  - sigmoid similar for certain parameters, yet sometimes invalid

	  - polynomial has more hyperparameters

	  - fewer numerical difficulties: goes to 0

	  - large features: linear kernel

       2) Cross-validation and Grid-search

	  - high training accuracy not useful \to cross-validation

	  - avoids overfitting better

	  - grid-search: all pairs of e.g.

            - \gamma \in {2^{-15}, 2^{-13}, ..., 2^{3}}
            - C \in {2^{3}, ..., 2^{-13}, 2^{-15}}

	  - advantages: parallelizable, better feeling

	  - first coarse grid, then finer grid

    4) Discussion

       - many features \to select which ones to use

    5) Appendix

       A) Examples of the Proposed Procedure

	  there are automated scripts easy.py and grid.py

	  first scale, then grid, then test \to better, automatic with scripts

       B) Common Mistakes in Scaling Training and Testing Data

	  - use the same scaling factors

	    $ ../svm-scale -l 0 -s range4 svmguide4 > svmguide4.scale
	    $ ../svm-scale -r range4 svmguide4.t > svmguide4.t.scale

       C) When to Use Linear but not RBF Kernel

	  RBF \ge linear only after searching (C, \gamma) space

	  1) Number of instances << number of features

	     linear kernel 98.6111 vs rbf kernel 97.2222

	  2) Both numbers of instances and features are large

	     liblinear faster and more accurate

	  3) Number of instances >> number of features

	     use liblinear -s 2, way faster than default -s 1
** [[./skl/tutorial.html][An introduction to machine learning with scikit-learn]]
*** summary
    1) Machine learning: the problem setting

       - supervised learning

	 - classification: classes

	 - regression: continuous variables

       - unsupervised learning:

         - clustering: similar examples within the data

	 - distribution of data: density estimation

       - training set and testing set:

	 - training: learn properties

	 - testing: test properties

    2) Loading an example dataset

       - from sklearn import datasets

       - digits = datasets.load_digits()

       - digits.data: features

       - digits.target: class

       - digits.images[0] (here)

    3) Learning and predicting

       - =estimator= offers =fit(X, y)= and =predict(T)=

       - from sklearn import svm

       - clf = svm.SVC(gamma=0.001, C=100.)

       - clf.fit(digits.data[:-1], digits.target[:-1])

       - clf.predict(digits.data[-1])

    4) Model persistence

       - from sklearn.externals import joblib

       - joblib.dump(clf, 'filename.pkl')

       - clf2 = joblib.load('filename.pkl')
	 
	 followed by clf2.predict(...)
** [[./paper-ssl-revised.pdf][Analysis of the SSL 3.0 protocol]]
*** summary
    0) [@0] Abstract
       
       some minor flaws, yet easily corrected, good stuff

    1) Introduction

       cryptographic security of SSL 3.0

       background, possible attacks, cryptographic protection, high-level view

    2) Background

       SSL consists of record layer and (connection) layer

       SSLv2 had key weaknesses, end deletion, and protocol
       degradation

    3) The record layer

       standard crypto problems, ok

       1) Confidentiality: eavesdropping

	  lots of known plaintext, but should be ok

       2) Confidentiality: traffic analysis

	  possible to determine request length, response lengths,
          determine which URL was visited

	  supports random padding for block ciphers, but not stream
          ciphers (more common)

       3) Confidentiality: active attacks

	  cut-and-pasted exchanges blocks of ciphertext, trying to
          leak the plaintext

	  short-block-attacks determine the last plaintext block: see
          when the ack is returned

       4) Message authentication

	  uses (old) HMAC, but still HMAC

       5) Replay attacks

	  includes sequence number in MACed data

       6) The Horton principle

	  is all the meaning validated?

	  (SSLCiphertext.ProtocolVersion is not), but in general, yes

       7) Summary

	  ok, minor concerns

    4) The key-exchange protocol

       better, but some scars

       1) Overview of the handshake flow

	  exchange data, compute secret, authenticate sent messages

       2) Ciphersuite rollback attacks

	  negotiation, change cipher spec, finished

       3) Dropping the change cipher spec message

	  in authentication-only mode, the change cipher spec-message
          can be untransmitted by the adversary, which allows her to
          always strip the authentication part

	  if weak encryption is used, this might allow for a online
          key search, with 4-12 (stream - block) bytes of known
          plaintext

       4) Key-exchange algorithm rollback

	  middleman tells each different ciphersuites, as this is not
          protected by hash

	  (horton principle violated)

       5) Anonymous key-exchange

	  specification unclear in what should be signed in anonymous
          mode

       6) Version rollback attacks

	  Mallory might exchange version 3 for version 2 session
          initiation to exploit the weaknesses of SSLv2. There is a
          proposed defense, which sets some padding bytes to fixed
          values.

	  There might be the danger of session resumption leading to
          use of v2. (room for further examination)

       7) Safeguarding the master secret

	  A nonce is hashed with the master secret on every session
          resume. Mallory can get a bit number of data thus hashed.

	  Replay attacks might work for that, too.

       8) Diffie-Hellman key-exchange

	  good idea, watch out to avoid server trapdooring

       9) The alert protocol

	  signify problems, mostly tear down the connection

       10) MAC usage

	   should consistently use HMACs

       11) Summary

	   some weaknesses in implementations possible

    5) Conclusion

       passive only recommendation: padding to avoid get request
       length analysis

       active: change cipher spec dropping and
       KeyExchangeAlgorithm-spoofing

       good step, minor patches recommended
*** quotes
    - We conclude that, while there are still a few technical wrinkles
      to iron out, on the whole SSL 3.0 is a valuable contribution
      towards practical communications security.
    - The SSL record layer provides confidentiality, authenticity, and
      replay protection over a connection-oriented reliable transport
      protocol such as TCP.
    - The only change to SSL’s protection against passive attacks
      worth recommending is support for padding to stop traffic
      analysis of GET (v5)
    - Diffie-Hellman is the only public key algorithm known which can
      efficiently provide perfect forward secrecy
    - To avoid server-generated trapdoors, theclient should be careful
      to check that the modulus and generator are from a fixed public
      list of safe values.
** [[./topranked.html][Does Alexa have a list of its top-ranked websites?]]
*** url https://support.alexa.com/hc/en-us/articles/200449834-Does-Alexa-have-a-list-of-its-top-ranked-websites-
*** summary
    top 1m sites at http://s3.amazonaws.com/alexa-static/top-1m.csv.zip

    updated daily
** [[./tf-idf.html][term frequency–inverse document frequency]]
*** quotes
    - tf–idf is the product of two statistics, term frequency and
      inverse document frequency. Various ways for determining the
      exact values of both statistics exist.
** [[/home/chive/import/pakdoc/rfc1928.socks5.txt][SOCKS Protocol Version 5]]
*** summary
    1. Introduction

       - firewall traversal with authentication

       - does not forward ICMP

       - both TCP and UDP

    2. Existing practice

       - SOCKS Version 4: unsecured, TCP-based

       - extends to include UDP, strong authentication, domain-name, IPv6

    3. Procedure for TCP-based clients

       1. client opens connection to SOCKS port,

       2. authentication negotiation

    4. Requests

       - connect OR bind OR udp associate

       - address type

       - address

       - port

    5. Addressing

       domain name has as first octet the name octets

    6. Replies

       ...
*** quotes
    - Compliant implementations MUST support GSSAPI and SHOULD support
      USERNAME/PASSWORD authentication methods.
** [[./notes]]
*** quotes
    - While we did some editing and customization to Firefox to enable
      data collection, in the newest version of Tor Firefox this is no
      longer necessary. It is possible to run it using just
      <torbrowserfolder>/firefox <sitename>, and we recommend this.
** TODO [#C] [[./fp.pdf][Touching from a Distance: Website Fingerprinting Attacks and Defenses]]
*** summary
    0) [@0] ABSTRACT

       web-page (!) fingerprinting, 50% regardless of defense scheme

       \to web-site fingerprinting, 90% accuracy

    1) INTRODUCTION

       - "effective attacks against HTTPOS, randomized pipelining, and
         several other defenses."

       - "Even with a 1-to-1 ratio between cover traffic and real
         traffic, our attack could identify the victim’s web page over
         50% of the time."

       - "the first demonstration that application-level defenses,
         such as HTTPOS and randomized pipelining, are not secure."

       - levenshtein-based wf, extended to web sites via hmm

       - others are broken

       - we do better

    2) RELATED WORK

       0) [@0] attack classes

	  - identify user

	  - identify server

	  - identify path

	  - user most applicable

       1) Fingerprinting attacks on encrypting tunnels

	  beginning: packets sizes

	  later: HMMs

       2) Fingerprinting attacks on Tor

	  - hermann et al: multinomial naive bayes,

	  - shi et al: cosine similarity

	  - panchenko: http-specific with svm

	  - reimplementation: 65% success rate, 100 web pages

       3) Proposed traffic analysis defenses.

	  - "padding packets, splitting packets into multiple packets,
            and inserting dummy packets"

	  - Fu et al: theoretical: constant-rate, fixed-rate

	    - random intervals better

	  - wright et al: morphing

	  - lu et al: morphing extension to distribution of size-ngrams

	  - luo et al: HTTPOS:

	    - TCP: size and ordering of packets

	    - HTTP: multiple possibly overlapping requests, pipelinig,
              extra unnecessary requests, get extra data

	    - defeatable by OP

	  - Tor: randomized pipelining

	    - worse not better

       4) Other related work.

	  - Wright et al: HMM protocol classification encrypted TCP

	  - White et al HMM partial plaintext of encrypted VoIP

    3) RECOGNIZING WEB PAGES

       - Damerau-Levenshtein edit distance

	 - best costs when "transpositions were 20 times cheaper than
           insertions, deletions, and substitutions"

	 - size rounding (up)

	 - normalization to d(t, t') / min(|t|, |t'|)

	 - several worse approaches

	   - cells instead of packets

	   - knn

	   - fixed-length via l_{2}-norm

    4) RECOGNIZING WEB SITES

       - HMM

	 - "each web page corresponds to an HMM state, and state
           transition probabilities represent the probability that a
           user would navigate from one page to another."

	 - uses classifier for probability

	 - web site template for huge pages (like amazon)

	 - AJAX: transition between different states

	 - *cold* pages: on first visit, vs

	 - *warm* pages: with some stuff cached

	 - back button as link to warm page

	 - one set of usage patterns (or a few distinct, or uniform)

    5) Congestion-Sensitive BUFLO

       - BuFLO with output queue

       - only outgoing, other ends needs CS-BuFLO as well

       - reveals

	 - maximum transmission rate T

	 - number of transmitted cells B

	 - (upstream too)

    6) EVALUATION

       1) Web page classifier

	  0) [@0] questions

	     - defenses: https, randomized pipelining, padding, morphing

	     - other classifiers:herrmann, panchenko

	     - if number of web pages goes up?

	     - if size of training set goes up?

	     - choice of web pages?

	     - state of the browser?

	  1) Experimental Setup

	     - default firefox with Tor

	     - "either 20 or 40 traces from each URL"

	  2) Attacks and Defenses

	     1) data sets

		- none: ssh

		- ssh + httpos

		- tor

		- tor + randomized pipelining

	     2) generate defenses

		- ssh + sample-based traffic morphing to flickr.com

		- ssh packet count remove packet size and direction information

		- tor + randomized pipelining + randomized cover traffic

		  only insert 1500 or -1500 at l random positions

		  *weaker than panchenko*

		- tor packet count: as ssh p-c above

	     3) Results

		- better in many cases than panchenko

       2) Web site classifier

	  1) Experimental Setup

	     - facebook:

	       - login page, user's home page, "friend profile page"

	       - warm and cold of home and profile pages

	     - imdb:

	       - home page, search results, movie, celebrity

	       - warm and cold for each page

	     - artificial transition probabilities

	     - facebook via fixed path

	  2) Results

	     - perfect for facebook,

	     - still very good for imdb

    7) DISCUSSION
*** quotes
    - Our attack converts traces into strings and uses the
      Damerau-Levenshtein distance to compare them.
    - (ends 1)
    - they are a good match for the attacker scenario faced by many
      Tor users today: they use Tor toevade censorship and persecution
      by a government or ISP that wants to know their browsing habits
      and has the ability to monitor their internet connection, but
      cannot easily infiltrate Tor nodes and web servers outside the
      country.
    - (ends 2.0)
    - these edits correspond to packet and request re-ordering,
      request omissions (e.g. due to caching), and slight variations
      in the sizes of requests and responses.
    - a better approach would be to learn optimal costs from the
      training data using the recently-proposed method of Bellet, et
      al.
    - also rounds all packet sizes *up* to a multiple of 600
    - Other normalization factors, such as |t| + |t_{0}| and
      max(|t|, |t_{0}|), yielded worse results.
    - The γ parameter is used to normalize L so that it’s outputs fall
      into a useful range. In our experiments, we found γ = 1 works
      well.
    - We tried representing traces as a sequence of Tor cells instead
      of as a sequence of packets. Classifier performance degraded
      slightly, suggesting that the Tor cells are often grouped into
      packets in the same way each time a page is loaded.
    - neighbor algorithm: to classify trace t, the attacker computed
      t^{∗} = argmin_{t'} L(t, t') over every trace in his database, and
      guessed that t was from the same web page as t^{∗}
    - Finally, we tried using a metric embedding to convert our
      variable-length trace vectors into fixed-length vectors in a
      space using the \ell_{2} - norm, and then used an SVM to classify
      these vectors. This performed substantially worse than the SVM
      classifier with distance-based kernel described above.
    - (ends 3)
    - for each *observation* o ∈ O and *HMM state* s, the probability,
      Pr[o|s], that the HMM generates observation o upon transitioning
      to state s.
    - pages p_{1} and p_{2} can be represented by a single state s only if
      Pr[o|p_{1}] ≈ Pr[o|p_{2}] for all observations o.
    - assumes that users all tend to navigate through a website in the
      same way.
    - ends (4)
    - A (d, ρ, τ ) BUFLO implementation transmits d-byte pack ets
      every ρ milliseconds, and continues this process for at least τ
      milliseconds.
    - (ends 5)
    - if a window had, say, 3 IMDB pages and 3 non-IMDB pages, we
      discarded it from the histogram.
    - (ends 6.2.2)
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2012-fingerprinting,
        title = {Touching from a Distance: Website Fingerprinting Attacks and Defenses},
        author = {Xiang Cai and Xincheng Zhang and Brijesh Joshi and Rob Johnson},
        booktitle = {Proceedings of the 19th ACM conference on Computer and Communications
              Security (CCS 2012)},
        year = {2012},
        month = {October},
        www_tags = {selected},
        www_pdf_url = {http://www.cs.sunysb.edu/~xcai/fp.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#A] [[./authorsversion-ccsw09.pdf][Herrmann - Website Fingerprinting: Attacking Popular Privacy Enhancing Technologies with the Multinomial Naïve-Bayes Classifier]] [0/2]
*** TODO summary
    0. [@0] ABSTRACT

       - attack privacy-enhancing technologies via text-mining
         techniques

       - closed-world: 97% success

    1. INTRODUCTION

       - PET website fingerprint attack

       - by local ISP, local admin, secret services

       - multinomial naive bayes

    2. SCENARIO

       - between user and PET, records traffic, can link IP to victim

       - passive, local, external attacker

       - training phase: fingerprints for all (or set of observed) websites

       - testing phase: measure user traffic, compare to fingerprints

    3. RELATED WORK

*** TODO quotes
    - influence of the browser cache on accuracy.
    - The attack consists of two phases: in the training phase the
      attacker creates traffic fingerprints for a large number of
      sites (or for a small set of interesting sites) and stores them
      together with the site URLs in a database. In the testing phase
      the attacker records the encrypted traffic of the user, creates
      fingerprints of small traffic chunks and tries to match them
      with records in the database.
*** vocabulary
    - website fingerprinting: learn the identity, i. e. the URLs, of
      websites that are downloaded over an encrypted tunnel by
      comparing the observed traffic to a library of previously
      recorded fingerprints.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccsw09-fingerprinting,
        title = {Website fingerprinting: attacking popular privacy enhancing technologies with
              the multinomial na\"{\i}ve-bayes classifier},
        author = {Dominik Herrmann and Rolf Wendolsky and Hannes Federrath},
        booktitle = {Proceedings of the 2009 ACM workshop on Cloud computing security (CCSW
              '09)},
        year = {2009},
        month = {October},
        address = {New York, NY, USA},
        location = {Chicago, Illinois, USA},
        pages = {31--42},
        publisher = {ACM},
        doi = {http://doi.acm.org/10.1145/1655008.1655013},
        isbn = {978-1-60558-784-4},
        www_section = {Traffic analysis},
        www_tags = {selected},
        www_pdf_url = {http://epub.uni-regensburg.de/11919/1/authorsversion-ccsw09.pdf},
      }
    #+END_SRC
** TODO [[./csbuflo.pdf][Cai - CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense]]
*** quotes
    - Our experiments find that Congestion-Sensitive BuFLO has high
      overhead (around 2.3-2.8x)
    - it is not currently known whether there exists any efficient and
      secure defense against website fingerprinting attacks.
    - all previously-proposed defenses provide little security.
*** summary
    0) [@0] Abstract:

       - fingerprint infers, even if tor,

       - previous defenses are ineffective,

       - spec of cs-buflo,

       - implementation

    1) Introduction

       - several website fingerprinting attacks, several defenses

       - 80%, which of 128 pages

       - BuFLO: over 400% bandwidth overhead

       - DLSVM fingerprinting attack greater than 75% success rate
         against numerous defenses, including application-level
         defenses, such as HTTPOS and randomized pipelining

       - CS-BuFLO, congestion avoidance, TCP-friendly,

       - here: adapt its transmission rate dynamically, and improve
         its stream padding: less bandwidth, hiding more

       - adapting too quickly can reveal info, solve: limit adaptation

       - alexa 200: 91% of web use

       - CS-BuFLO: 2.8 times as much bandwidth as SSH, only a 20% success rate

       - CS-BuFLO ratio 2.8. BuFLO ratio of 2.2.

    2) RELATED WORK

       - dyer: lists stuff like padM, padE, ...

       - wright: morphing

       - dyer defeats

       - Lu extends morphing

       - Dyer BuFLO

       - Fu: CPU load changes, recommend randomized intervals

       - HTTPOS

       - Tor randomized

       - Cai defeated

       - many attacks agains https, ipsec, vpn, etc

       - herrmann: tunnels attack, fails on tor

       - panchenko

       - dyer vng++

       - cai string edit distance

       - wang improved

       - danezis, yu, cai hmm to extend to web site fp
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{wpes14-csbuflo,
        title = {{CS-BuFLO}: A Congestion Sensitive Website Fingerprinting Defense},
        author = {Xiang Cai and Rishab Nithyanand and Rob Johnson},
        booktitle = {{Proceedings of the 12th Workshop on Privacy in the Electronic Society
              (WPES)}},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://pub.cs.sunysb.edu/~rob/papers/csbuflo.pdf},
        www_section = {Anonymous communication},
      }
    #+END_SRC
** TODO [[./Liberatore_2006.pdf][Inferring the Source of Encrypted HTTP Connections]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{Liberatore:2006,
        title = {{Inferring the Source of Encrypted HTTP Connections}},
        author = {Marc Liberatore and Brian Neil Levine},
        booktitle = {Proceedings of the 13th ACM conference on Computer and Communications
              Security (CCS 2006)},
        year = {2006},
        month = {November},
        pages = {255--263},
        www_tags = {selected},
        www_section = {Traffic analysis},
        www_pdf_url = {http://prisms.cs.umass.edu/brian/pubs/liberatore.ccs2006.pdf},
      }
    #+END_SRC
** TODO [[./ipccc12-tor-performance.pdf][Improving Performance and Anonymity in the Tor Network]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ipccc12-performance,
        title = {Improving Performance and Anonymity in the Tor Network},
        author = {Andriy Panchenko and Fabian Lanze and Thomas Engel},
        booktitle = {Proceedings of the 31st IEEE International Performance Computing and
              Communications Conference (IPCCC 2012)},
        year = {2012},
        month = {December},
        www_tags = {selected},
        www_pdf_url = {http://lorre.uni.lu/~andriy/papers/ipccc12-tor-performance.pdf},
        www_section = {Anonymous communication,Tor Performance},
      }
    #+END_SRC
** TODO [#C] [[./LZCLCP_NDSS11.pdf][HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows]]
** TODO [#C] [[./morphing09.pdf][Traffic Morphing: An Efficient Defense Against Statistical Traffic Analysis]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{morphing09,
        title = {Traffic Morphing: An efficient defense against statistical traffic analysis},
        author = {Charles Wright and Scott Coull and Fabian Monrose},
        booktitle = {Proceedings of the Network and Distributed Security Symposium - {NDSS} '09},
        year = {2009},
        month = {February},
        publisher = {IEEE},
        www_tags = {selected},
        www_pdf_url = {http://freehaven.net/anonbib/papers/morphing09.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[./Oya.pdf][Do dummies pay off ? Limits of dummy traffic protection in anonymous communications]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{pets14-dummy-traffic,
        title = {Do dummies pay off? Limits of dummy traffic protection in anonymous
              communications},
        author = {Simon Oya, Carmela Troncoso and Fernando P{\'e}rez-Gonz\'alez},
        booktitle = {Proceedings of the 14th Privacy Enhancing Technologies Symposium (PETS
              2014)},
        year = {2014},
        month = {July},
        www_pdf_url = {https://www.petsymposium.org/2014/papers/Oya.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis,Anonymous communication},
      }
    #+END_SRC
** TODO [#C] [[./oakland2012-peekaboo.pdf][Peek-a-Boo, I Still See You: Why Efficient Traffic Analysis Countermeasures Fail]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{oakland2012-peekaboo,
        title = {Peek-a-Boo, {I} Still See You: Why Efficient Traffic Analysis Countermeasures
              Fail},
        author = {Kevin P. Dyer and Scott E. Coull and Thomas Ristenpart and Thomas Shrimpton},
        booktitle = {Proceedings of the 2012 IEEE Symposium on Security and Privacy},
        year = {2012},
        month = {May},
        www_pdf_url = {http://kpdyer.com/publications/oakland2012.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[./pam2014-tor-nfattack.pdf][On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow Records]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{nfattackpam14,
        title = {On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow
              Records},
        author = {S. Chakravarty and M. V. Barbera and G. Portokalidis and M. Polychronakis and
              A. D. Keromytis},
        booktitle = {Proceedings of the 15th Passive and Active Measurements Conference (PAM
              '14)},
        year = {2014},
        month = {March},
        www_pdf_url = {http://www.cs.columbia.edu/~sc2516/papers/pam2014-tor-nfattack.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#D] [[./usersrouted-ccs13.pdf][Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2013-usersrouted,
        title = {Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries},
        author = {Aaron Johnson and Chris Wacek and Rob Jansen and Micah Sherr and Paul
              Syverson},
        booktitle = {Proceedings of the 20th ACM conference on Computer and Communications
              Security (CCS 2013)},
        year = {2013},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.ohmygodel.com/publications/usersrouted-ccs13.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#C] [[./webfingerprint-wpes.pdf][Improved Website Fingerprinting on Tor]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{wpes13-fingerprinting,
        title = {Improved Website Fingerprinting on Tor},
        author = {Tao Wang and Ian Goldberg},
        booktitle = {Proceedings of the Workshop on Privacy in the Electronic Society (WPES
              2013)},
        year = {2013},
        month = {November},
        location = {Berlin, Germany},
        publisher = {ACM},
        www_tags = {selected},
        www_pdf_url = {http://www.cypherpunks.ca/~iang/pubs/webfingerprint-wpes.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[./10.1.1.10.5823.pdf][Quantifying Traffic Analysis of Encrypted Web-Browsing]]
** TODO [[./hintz02.pdf][Fingerprinting Websites Using Traffic Analysis]]
*** summary
    0) [@0] Abstract

       Attack to find out whether user is visiting certain websites,
       even though he uses an encrypted proxy. 

       Plus discussion: better attack and defenses

    1) Introduction

       With normal encryption, metadata is visible. 

       With one-hop proxies, metadata is discoverable.

       Tere are several defenses.

    2) Definition of Traffic Analysis

       sender, receiver, amount of data transferred (ssl does not try
       to obfuscate)
*** quotes
    - The process of monitoring the nature and behavior of traffic,
      rather than its content, is known as traffic analysis.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{hintz02,
        title = {Fingerprinting Websites Using Traffic Analysis},
        author = {Andrew Hintz},
        booktitle = {Proceedings of Privacy Enhancing Technologies workshop (PET 2002)},
        year = {2002},
        month = {April},
        editor = {Roger Dingledine and Paul Syverson},
        publisher = {Springer-Verlag, LNCS 2482},
        www_tags = {selected},
        www_html_url = {http://guh.nu/projects/ta/safeweb/safeweb.html},
        www_pdf_url = {http://guh.nu/projects/ta/safeweb/safeweb.pdf},
        www_ps_url = {http://guh.nu/projects/ta/safeweb/safeweb.ps},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[https://www.torproject.org/projects/torbrowser/design/#proxy-obedience][The Design and Implementation of the Tor Browser]]
   - [[./browser.html][local copy]]
*** summary
    1) Introduction

       adversary mode, design requirements and implementation of 4.5

       1) Browser Component Overview

	  based on firefox esr, patches, torbutton, pref changes

	  tor laucher addon: splash screen & progrss bar

	  https-everywhere, noscript, extension prefs changed:  https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/Bundle-Data/linux/Data/Browser/profile.default/preferences/extension-overrides.js

	  pluggable transports

    2) Design Requirements and Philosophy
*** quotes
    - Unfortunately, the bias in favor of compelling attack papers has
      caused academia to ignore this request thus far, instead
      publishing only cursory (yet "devastating") evaluations that
      fail to provide even simple statistics such as the rates of
      actual pipeline utilization during their evaluations, in
      addition to the other shortcomings and shortcuts mentioned earlier.
    - These shortcomings and fallback behaviors are the primary reason
      that Google developed SPDY as opposed to simply extending HTTP
      to improve pipelining.
** TODO [[./ronathan-heyning.pdf][Traffic Analysis of SSL Encrypted Web Browsing]]
** TODO [[https://trac.torproject.org/projects/tor/ticket/8470#comment:7][#8470 Request randomization a lot less random in FF17]]
   local: [[file:///home/w00k/da/docs/lessrand.html]]
*** quotes
    - [...] the browser is often too slow to be able to keep the
      pipeline full of requests, and requests aren't packed together
      in Tor cells. This might be important, because it may also be
      the case that a browser that is driven around by selenium in a
      VM might be similarly too slow for pipelining's request
      combining to happen.
** TODO [[./tor-design.pdf][Tor: The Second-Generation Onion Router]]
*** quotes
  - Clients choose a path through the network and build a circuit, in
    which each node (or “onion router” or “OR”) in the path knows its
    predecessor and successor, but no other nodes in the circuit.
  - Rather than using a single multiply encrypted data structure (an
    onion) to lay each circuit, Tor now uses an incremental or
    telescoping path-building design, where the initiator negotiates
    session keys with each successive hop in the circuit.
  - Tor multiplexes multiple TCP streams along each circuit to improve
    efficiency and anonymity.
  - Tor’s decentralized congestion control uses end-to-end acks to
    maintain anonymity while allowing nodes at the edges of the
    network to detect congestion or flooding and send less data until
    the congestion subsides.
  - Certain more trusted nodes act as directory servers: they provide
    signed directories describing known routers and their current
    state.
  - Variable exit policies: Tor provides a consistent mechanism for
    each node to advertise a policy describing the hosts and ports to
    which it will connect. These exit policies are critical in a
    volunteer-based distributed infrastructure, because each operator
    is comfortable with allowing different types of traffic to exit
    from his node.
  - End-to-end integrity checking
  - Modern anonymity systems date to Chaum’s Mix-Net design [10].
  - Tor as described herein, Tarzan, MorphMix, Cebolla [9], and
    Rennhard’s Anonymity Network [44] build circuits in stages,
    extending them one hop at a time. Section 4.2 describes how this
    approach enables <<perfect forward secrecy>>.
  - by treating application connections as data streams rather than
    raw TCP packets, they avoid the inefficiencies of tunneling TCP
    over TCP.
  - Distributed-trust anonymizing systems need to prevent attackers
    from adding too many servers and thus compromising user paths. Tor
    relies on a small set of well-known directory servers, run by
    independent parties, to decide which nodes can join.
  - Each onion router maintains a TLS [17] connection to every other
    onion router.
  - Actually, the negotiated key is used to derive two symmetric keys:
    one for each direction.
  - Preliminary analysis with the NRL protocol analyzer [35] shows
    this protocol to be secure (including perfect forward secrecy)
    under the traditional <<Dolev-Yao>> model.
  - Once Alice has established the circuit (so she shares keys with 
    each OR on the circuit), she can send relay cells.
  - (as an optimization, the first two bytes of the integrity check 
    are zero, so in most cases we can avoid computing the hash). 
  - Thus the “<<break a node and see which circuits go down>>” attack [4]
    is weakened.
  - (usually the last node, but maybe others due to exit policy
    conflicts; see Section 6.2.)
  - Because Tor uses TLS on its links, external adversaries cannot
    modify data. Addressing the insider malleability attack, however,
    is more complex.
  - given that the OP or OR tear down the circuit if they receive a
    bad hash.
  - a circuit’s edges can heuristically distinguish interactive
    streams from bulk streams by comparing the frequency with which
    they supply cells.
  - If enough users choose the same OR-to-OR connection for their cir-
    cuits, that connection can become saturated.
  - These arbitrarily chosen parameters seem to give tolerable
    throughput and delay; see Section 8.
  - This type of anonymity protects against distributed DoS attacks:
    attackers are forced to attack the onion routing network because
    they do not know Bob’s IP address.
  - thus hostnames take the form x.y.onion where x is the
    authorization cookie and y encodes the hash of the public key.
  - Rather than searching exit connections for timing and volume
    correlations, the adversary may build up a database of
    “fingerprints” contain- ing file sizes and access patterns for
    targeted websites. He can later confirm a user’s connection to a
    given site simply by consulting the database. This attack has been
    shown to be effective against SafeWeb [29].
  - On the other hand, an attacker who learns a node’s identity key
    can replace that node indefinitely by sending new forged descrip-
    tors to the directory servers.<<<permanent identity key>>>
  - The best defense here is robustness.
  - Our threat model explicitly assumes directory server operators
    will be able to ﬁlter out most hostile ORs.
  - once we have more experience, and assuming we can resolve the
    anonymity issues, we may partition traffic into two relay cell
    sizes: one to handle bulk traffic and one for interactive traffic.
  - Second, our end-to-end congestion control algorithm focuses on
    protecting volunteer servers from accidental DoS rather than on
    optimizing performance.
  - On the other hand, as our users remain satisﬁed with this
    increased latency, we can address our performance incrementally as
    we proceed with development.
  - [...] we still expect the network to support a few hundred nodes
    and maybe 10,000 users before we’re forced to become more
    distributed.
  - Throughout this paper, we have assumed that end-to-end trafﬁc
    conﬁrmation will immediately and automatically defeat a
    low-latency anonymity system.

*** summary
    1) 
    2) Related work
       
       The first design ever was called Mix-Net. From there, two
       directions diverged. The one was highly anonymized,
       high-latency. The other tried to minimize latency. Among these,
       there are IP, TCP and application-level (e.g. HTTP)
       filters. TOR does a TCP design that is mostly distributed but
       has some fixed directory structure.

    3) Design goals and assumptions

       TOR's design aims to be easy to use by providing deployability,
       usability, flexibility and a simple design. It does not try to
       hide its traffic, normalize protocols, be 100% p2p or prevent
       end-to-end analysis. It is assumed that an adversary can control
       some onion routers and thus analyze or disturb the network.

       1) Threat Model

	  adversaries can
          - observe fraction of traffic
	  - can generate, modify, delete traffic
	  - operate own ORs
	  - compromise fraction of other ORs

	  they are not global.

    4) The TOR design incorporates encapsulation over SSL (link-layer)
       links.  There are two cells types, for control and
       content. Checksumming, QOS-type rate limiting and congestion
       control are implemented.
       + Every OR has a TLS connection to every other OR. They send
	 each other fixed-size cells each encrypted via
	 quickly-changing TLS keys. The connection is signed via
	 mid-term onion keys. The identity is provided via [[permanent
	 identity key]].
       + TORs messages are either command/control/direct (2b circID, 1b
	 cmd, 509b data) messages to the next OR or relay (2b cirdID,
	 1b relay, 2b streamID, 6b digest, 2b len, 1b cmd, 498b data)
	 messages to some other host. The circuit is created
	 recursively using several hops, each of which creates its own
	 symmetric key pair via DH. Each circuit OR can be asked to
	 open a connection out. circuitID changes from hop to hop
       + 
       + when an OR receives a relay message, it attemps to decrypt the
	 relay header and read the digest. If the digest is correct, it
	 is accepted. Else, it is forwarded (or the stream is killed if
	 at the end)
       + for QoS and fairness, a token-bucket mechanism is used and
	 interactive streams get preferential treatment
       + congestion control is handled via two window mechanisms: one
	 for circuit-level, the other for stream-level-throttling
    5) hidden services in TOR are implemented via introduction pionts:
       - bob advertises these, alice creates a rendezvous point, leaves
	 a message at those intro points, bob connects to this
	 rendezvous point
       - both the server and the client can be used unmodified: the
	 server just behind tor, the client just using its onion proxy
       - there was some previous work, also on anonymizing cellphone
	 usage
    6) miscellany
       - dos might be possible but has not been observed yet
       - exit policies: 
	 - thoughts of adding headers
	 - mixed policies
       - directory servers
	 - cached statements at ORs,
	 - more efficient that just propagating messages
    7) attack & defense
       - passive attacks
	 - traffic confirmation attacks outside of design goal, but
	 - fingerprinting might be effective
	 - user content, option distinguishability, timing
	   correlations, size correlations, fingerprinting, latency
       - active attacks
	 - five different keys can be compromised:
	   tls session, tls cert, circuit session, circuit cert, identity
	 - compromise of the OR itself has to be done in tight timeframe
	 - run recipient, eases end-to-end attacks
	   - approached by privoxy
	 - run onion proxy
	   - no solution
	 - DoS non-observed nodes
	   - best defense robustness
	 - Run hostile OR, timing attacks, tagging cells, replace
	   content, replay attacks, smear tor's name, hostile code
	   (altered TOR software)
       - directory attacks
	 - destroy directory server, subvert directory server, subvert
	   majority, encourage dissent, insert hostile OR, do as if
	   working correctly
       - rendezvous attacks
	 - make many request, attack intro, compromise intro, compromise rend
    8) In da wild
       - works for variety of uses
	 - download fast, latency varies greatly
    9) open questions
       - which path length (static/dynamic), which renewal frequency?
       - cascade / hydra /own OR
       - what if some central directory server do not suffice,
	 non-clique aka restricted-route
       - w/o central authority, how to avoid bad nodes
       - anonymity gains from running own OR to lead to recommendation?
    10) future directions
	- scalability, 
	- bandwidth classes [DSL, T1, T3] for ORs
	- incentives, cover traffic, cachin gat exit nodes, better
          directory distribution, further spec review, multisystem
          interoperability, wider-scale deployment
    11) references
*** questions
    - exit policies for .onion
    - safeweb (? was soll das denn sein??)
    - privoxy email default message on plaintext connect
    - "Our threat model explicitly assumes directory server operators
      will be able to ﬁlter out most hostile ORs."
    - caching at exit nodes: Flos Gedanken
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{tor-design,
        title = {{Tor}: The Second-Generation Onion Router},
        author = {Roger Dingledine and Nick Mathewson and Paul Syverson},
        booktitle = {Proceedings of the 13th USENIX Security Symposium},
        year = {2004},
        month = {August},
        www_important = {1},
        www_tags = {selected},
        www_html_url = {https://svn.torproject.org/svn/projects/design-paper/tor-design.html},
        www_pdf_url = {https://svn.torproject.org/svn/projects/design-paper/tor-design.pdf},
        www_section = {Anonymous communication},
      }
    #+END_SRC
** TODO [[./marionette_client.pdf][Marionette Python Client Documentation]]
** TODO [[./cacr2015-08.pdf][Walkie-Talkie: An Effective and Efficient Defense against Website Fingerprinting]]
** TODO [[./cacr2015-09.pdf][On Realistically Attacking Tor with Website Fingerprinting]]
*** summary
*** quotes
    - This is the hardest class to split as there is no noticeable gap
      nor a clear pattern of cells indicating the gap.
** TODO [[./python-doc-howto-doanddont.pdf][Idioms and Anti-Idioms in Python]]
*** quotes
    - You should try to use as few except clauses in your code as you
      can
** [[./wireshark-debian.html][wireshark/trunk/debian/README.Debian]]
*** summary
    capture as root, analyze as user

    or add users to the wireshark-group dpkg-reconfigure ... (see below)
*** quotes
    - The installation method can be changed any time by running:
      dpkg-reconfigure wireshark-common
** TODO [[./python-doc-tutorial.pdf][Python Tutorial]]
*** summary
    0) [@0]

       Python

       - easy

       - powerful

       - high-level data structures

       - dynamic typing

       - \to elegant

       - open source

       - extensible

       - tutorial incomprehensive

       - others: library, reference

    1) WHETTING YOUR APPETITE

       - automate

       - write, compile, test too slow

       - \to has interactive interpreter

       - compact, readable

       - \to very-high-level

       - modular

    2) USING THE PYTHON INTERPRETER

       1) Invoking the Interpreter

	  =python=

	  EOF (C-D) finishes, or =quit=

	  readline adds history features

	  =python -c command=

	  =python -m module=

	  =-i= enters interactive mode afterwards

	  1) Argument Passing

	     - =sys.argv=

	     - no script/arguments: ''

	     - -c: '-c'

	     - -m: module name

	  2) Interactive Mode

	     >>>, resp ...

       2) The Interpreter and Its Environment

	  1) Source Code Encoding

	     "It is possible to use encodings different than ASCII in
             Python source files. The best way to do it is to put one
             more special comment line right after the #! line to
             define the source file encoding:
             #+BEGIN_SRC python
               # -*- coding: encoding -*-
             #+END_SRC

    3) AN INFORMAL INTRODUCTION TO PYTHON

       1) Using Python as a Calculator

	  1) Numbers

	     - floor division: // (default with ints)

	     - ** powers
	       #+BEGIN_SRC python
                 2 ** 7 # 2 to the power of 7
	       #+END_SRC

	     - "In interactive mode, the last printed expression is
               assigned to the variable _."

	     - rounding
	       #+BEGIN_SRC python
                 round(13.0563, 2) # yields 13.05
	       #+END_SRC

	  2) Strings

	     - escape single quote
	       #+BEGIN_SRC python
                 'doesn\'t' # use \' to escape the single quote...
	       #+END_SRC

	     - print erases newline
	       #+BEGIN_SRC python
                 >>> s = 'First line.\nSecond line.' # \n means newline
                 >>> s # without print, \n is included in the output
                 'First line.\nSecond line.'
                 >>> print s # with print, \n produces a new line
                 First line.
                 Second line.
	       #+END_SRC

	     - raw strings
	       #+BEGIN_SRC python
                 >>> print r'C:\some\name' # note the r before the quote
                 C:\some\name
	       #+END_SRC

	     - string combination Strings can be concatenated (glued
               together) with the + operator, and repeated with * :
	       #+BEGIN_SRC python
                 >>> # 3 times 'un', followed by 'ium'
                 >>> 3 * 'un' + 'ium'
                 'unununium'
	       #+END_SRC

	     - combine strings via 'a' 'b':

               "This feature is particularly useful when you want to
               break long strings:"
	       #+BEGIN_SRC python
                 >>> text = ('Put several strings within parentheses '
                 'to have them joined together.')
                 >>> text
                 'Put several strings within parentheses to have them joined together.'
	       #+END_SRC

	     - "Note that since -0 is the same as 0, negative indices
               start from -1."

	     - Slice indices have useful defaults; an omitted first
               index defaults to zero, an omitted second index
               defaults to the size of the string being sliced.
	       #+BEGIN_SRC python
                 >>> word[:2] # character from the beginning to position 2 (excluded)
                 'Py'
                 >>> word[4:] # characters from position 4 (included) to the end
                 'on'
                 >>> word[-2:] # characters from the second-last (included) to the end
                 'on'
	       #+END_SRC

	  3) Unicode Strings

	     - The escape sequence \u0020 indicates to insert the
               Unicode character with the ordinal value 0x0020 (the
               space character) at the given position.
               #+BEGIN_SRC python
                 >>> u'Hello\u0020World !'
                 u'Hello World !'
               #+END_SRC

	  4) Lists

	     - All slice operations return a new list containing the
               requested elements. This means that the following slice
               returns a new (shallow) copy of the list:
	       #+BEGIN_SRC python
                 >>> squares[:]
                 [1, 4, 9, 16, 25]
	       #+END_SRC

	     - replace some values
	       #+BEGIN_SRC python
                 >>> letters[2:5] = ['C', 'D', 'E']
	       #+END_SRC

       2) First Steps Towards Programming

    4) MORE CONTROL FLOW TOOLS

       1) if Statements

	  - if, elif, else

       2) for Statements

	  over iterator, =[:]= for a copy

       3) The range() Function

	  does not include the argument,

	  start, stop, step

       4) break and continue Statements, and else Clauses on Loops

	  - "The break statement, like in C, breaks out of the
            smallest enclosing for or while loop."

	    if loop not broken, =else:= is executed

       5) pass Statements

	  does nothing

	  use cases:

	  - busy-wait

	  - empty class

	  - implement later

       6) Defining Functions

	  - var lookup order:

	    local \to surrounding locals \to global \to builtin

       7) More on Defining Functions

	  different number of parameters

	  1) Default Argument Values

	     #+BEGIN_SRC python
               def func(retries = 4):
	     #+END_SRC

	     if set, takes that, otherwise default

	  2) Keyword Arguments

	     - "When a final formal parameter of the form **name is
               present, it receives a dictionary (see typesmapping)
               containing all keyword arguments except for those
               corresponding to a formal parameter. This may be
               combined with a formal parameter of the form *name
               (described in the next subsection) which receives a
               tuple containing the positional arguments beyond the
               formal parameter list."

	  3) Arbitrary Argument Lists

	     *var sums all others into a tuple

	  4) Unpacking Argument Lists

	     "when the arguments are already in a list or tuple but
             need to be unpacked for a function call requiring
             separate positional arguments. [...] If they are not
             available separately, write the function call with the *
             -operator to unpack the arguments out of a list or tuple:
	     #+BEGIN_SRC python
               >>> args = [3, 6]
               >>> range(*args)
               [3, 4, 5]
	     #+END_SRC

	  5) Lambda Expressions

	     - "Small anonymous functions"

	     - "syntactically restricted to a single expression"

	     - "pass a small function as an argument:"
	       #+BEGIN_SRC python
                 >>> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]
                 >>> pairs.sort(key=lambda pair: pair[1])
                 >>> pairs
                 [(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]
	       #+END_SRC

	  6) Documentation Strings

	     - "The first line should always be a short, concise
               summary of the object’s purpose. [...] This line should
               begin with a capital letter and end with a period."

       8) Intermezzo: Coding Style

	  - 4 space indentation

	  - 79 character lines

	  - blank lines to separate functions, classes, larger code blocks

	  - comments on line of their own if possible

	  - docstrings

	  - spaces right

	  - CamelClass,  function_underscore,  method(self, ...)

	  - ASCII

    5) DATA STRUCTURES

       1) More on Lists

	  0) [@0] all methods

	     - append,

	     - extend

	     - insert

	     - remove

	     - pop

	     - index

	     - count

	     - sort

	     - reverse

	  1) Using Lists as Stacks

	     via append() 
             and pop()

	  2) Using Lists as Queues

	     use collections.deque()

	  3) Functional Programming Tools

	     - filter(function, sequence) returns a sequence
               consisting of those items from the sequence for which
               function(item) is true.

	       #+BEGIN_SRC python
                 >>> def f(x): return x % 3 == 0 or x % 5 == 0
                 ...
                 >>> filter(f, range(2, 25))
                 [3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24]
	       #+END_SRC

	  4) List Comprehensions

	     - list of squares
	       #+BEGIN_SRC python
                 squares = [x**2 for x in range(10)]
	       #+END_SRC
               This is also equivalent to
	       #+BEGIN_SRC python
                 squares = map(lambda x: x**2, range(10))
	       #+END_SRC
               , but it’s more concise and readable.

	     - flatten a list using a listcomp with two 'for'
	       #+BEGIN_SRC python
                 >>> vec = [[1,2,3], [4,5,6], [7,8,9]]
                 >>> [num for elem in vec for num in elem]
                 [1, 2, 3, 4, 5, 6, 7, 8, 9]
	       #+END_SRC

       2) The del statement

	  - remove elements 
	    del a[3]

	  - remove slices
	    del a[3:]

	  - remove whole list
	    del a[:]

	  - remove variable
	    del a

       3) Tuples and Sequences


*** quotes
    - it’s good practice to include docstrings in code that you write,
      so make a habit of it.
    - Thus, global variables cannot be directly assigned a value
      within a function (unless named in a global statement), although
      they may be referenced.
    - Actually, call by object reference would be a better
      description, since if a mutable object is passed, the caller
      will see any changes the callee makes to it (items inserted into
      a list).
    - Falling off the end of a function also returns None.
    - (example in and raw_input
      #+BEGIN_SRC python
        ok = raw_input(prompt)
        if ok in ('y', 'ye', 'yes'):
            return True
      #+END_SRC
    - Now enter the Python interpreter and import this module with the
      following command:
      #+BEGIN_SRC python
      >>> import fibo
      #+END_SRC
    - Finally, the least frequently used option is to specify that a
      function can be called with an arbitrary number of
      arguments. These arguments will be wrapped up in a tuple (see
      Tuples and Sequences). Before the variable number of arguments,
      zero or more normal arguments may occur.
      #+BEGIN_SRC python
        def write_multiple_items(file, separator, *args):
          file.write(separator.join(args))
      #+END_SRC
    - An optional ’:’ and format specifier can follow the field
      name. This allows greater control over how the value is
      formatted. The following example rounds Pi to three places after
      the decimal.
      #+BEGIN_SRC python
        import math
        print 'The value of PI is approximately {0:.3f}.'.format(math.pi)
      #+END_SRC
    - Class definitions play some neat tricks with namespaces, and you
      need to know how scopes and namespaces work to fully understand
      what’s going on.
    - >>> tel = {'jack': 4098, 'sape': 4139}
      >>> tel['guido'] = 4127
    - When *looping through dictionaries*, the key and corresponding
      value can be retrieved at the same time using the iteritems()
      method.
      #+BEGIN_SRC python
        >>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
        >>> for k, v in knights.iteritems():
        ...     print k, v
      #+END_SRC
** TODO [[./Tor Project: Overview.html][Tor: Overview]]
** TODO [[shell:ristretto ./malamud][Malamud:Privacy and Private States]]
*** summary
*** quotes
    - Primo Levi observed that "solitude in a Camp is more precious
      and rare than bread."
** TODO [[./www.ted.com/talks/glenn_greenwald_why_privacy_matters/transcript.html][Greenwald:privacy]]
*** summary
    - youtube videos of people who think they are unobserved, do sth,
      horror on being seen (singing, dancing, nose picking, ...
    - Snowden revealed: US spying on every internet user
    - common thinking: only bad people have a reason for privacy
    - this is self-deprecation
    - about this mentality: people do not believe that. they safeguard
      their privacy (passwords, locks on bedroom and bathroom doors
    - cnet found out stuff about eric schmidt via google, he
      subsequently forbid his employees from talking to cnet
    - zuckerberg bought own house plus all four adjacent houses to
      have a zone of privacy
    - tell people who say they do not care: give me all your
      email/social media-passwords, let me publish whatever I find
      interesting
*** quotes
    - mass, indiscriminate surveillance
    - People can very easily in words claim that they don't value
      their privacy, but their actions negate the authenticity of that
      belief.
    - There are dozens of psychological studies that prove that when
      somebody knows that they might be watched, the behavior they
      engage in is vastly more conformist and compliant.
    - crucial to this design was that the inmates could not actually
      see into the panopticon, into the tower, and so they never knew
      if they were being watched or even when.
*** ref
#+BEGIN_SRC bibtex
     @misc{privacy-g,
       author = "Glenn Greenwald",
       title = "Why Privacy Matters",
       url = "\url{www.ted.com/talks/glenn_greenwald_why_privacy_matters/transcript}",
       note = "[Online; accessed 15-October-2015]"
     }
#+END_SRC
** TODO [[./tor_overview.html][Tor Project: Overview]]
*** summary
    1) Overview

       - tor increases privacy and security

       - series of virtual tunnels

       - censorship circumvention

       - groups
	 - individuals: keep from tracking, connect to blocked
           websites, hidden services allow to publish, socially
           sensitive communication

	 - journalists: communicate with whistleblowers

	 - indymedia, eff recommend Tor, corporations for
           investigating competitors

	 - U.S. Navy: comint, teams,

	 - law enforcement: surveil websites, sting

    2) Why we need Tor

       - f.ex. foreign country, s/o could observe where you are connecting

    3) The solution: a distributed, anonymous network
** MAYBE [[./user_guide_0.16.1.pdf][scikit-learn user guide]]
*** quotes
    - Despite its simplicity, nearest neighbors has been successful in
      a large number of classification and regression problems
    - SVC and NuSVC implement the “one-against-one” approach (Knerr et
      al., 1990) for multi- class classification. If n_class is the
      number of classes, then n_class * (n_class - 1) / 2 classifiers
      are constructed and each one trains data from two classes
    - Note that the LinearSVC also implements an alternative
      multi-class strategy, the so-called multi-class SVM formulated
      by Crammer and Singer, by using the option
      multi_class=’crammer_singer’. This method is consistent, which
      is not true for one-vs-rest classification. In practice,
      one-vs-rest classification is usually preferred, since the
      results are mostly similar, but the runtime is significantly
      less.
** [[./sally/Example1.html]]
*** summary
    - input files example1.cfg reuters.zip
    - sally -c example1.cfg reuters.zip reuters.libsvm
** [[./sally/Example3.html]]
*** summary
    - sally -c example3.cfg jrc.zip jrc.mat
    - load jrc.mat
    - X = [fvec.data]
    - Y = [fvec.label]
    - [Y idx] = sort(Y);
    - K = X(:,idx)' * X(:,idx);
    - imagesc(K);
    - colormap(hot);
    - colorbar;
** TODO [#A] [[./thebook.pdf][INTRODUCTION TO MACHINE LEARNING]] [0/2]
*** TODO summary
    1) Introduction

       often in background

       1) A Taste of Machine Learning

	  applications, formalize

	  1) Applications

	     - page rank

	     - collaborative filtering: amazon recommendations

	     - automatic translation

	     - face recognition vs face verification

	     - named entity recognition: from documents etc some named thing

	     - speech recognition: annotate voice sample

	       - and similarly: handwriting, etc

	     classification: yes/no answer

	  2) Data

	     types of data

	     - Lists

	     - Sets

	     - Matrices

	     - Images

	     - Videos

	     - Trees and Graphs

	     - Strings

	     - Compound structures: combine these

	  3) Problems

	     1) Binary Classification:

		- online learning: instantaneous determination of y for x

		- batch learning: group of stuff

		- transduction: know X' (testing data)

		- active learning: choose X

		- estimation with missing variables

		- covariate shift correction: X and X' from different
                  data sources

		- co-training: related but different problems

		- loss functions

	     2) Multiclass Classification

	     3) Structured Estimation

	     4) Regression

	     5) Novelty Detection: new and unusual observations

       2) Probability Theory

	  1) Random Variables

	     X takes on values in f.ex. \cal X = {1,..., 6} \ni x

	  2) Distributions

	     - discrete values: probability mass function, PMF

	     - continuous values: probability density function, PDF
	       integral: CDF

	     - Pr(a \le X \le b) = \int_{a}^{b} dp(x) = F(b) - F(a)

	     - Quantile: value x' for which Pr(X < x') \le q, Pr(X > x') \le 1-q
	       is /q-quantile/. q=0.5: /median/

	  3) Mean and Variance

	     - mean: E[x] = \sum_{x} xp(x) and \int x dp(x)
	       E[f(x)] =  \int f(x) dp(x)

	     - variance Var[X] = E[(X - E(X))^{2}]
	       Var[f(x)] =  E[(f(X) - E(f(X)))^{2}]

	  4) Marginalization, Independence, Conditioning, and Bayes Rule

	     0) [@0]

		- marginalization, independence, conditioning, iid, bayes
		  rule: see quotes

		- Bayes Rule holds because p(x,y) = p(x|y)p(y) = p(y|x)p(x)

		- "The key consequence of (1.15) is that we may
                  reverse the conditioning between a pair of random
                  variables."

	     1) An Example

		HIV testing,

		- p(T = HIV+ | X = HIV+) = 1

		- p(T = HIV+ | X = HIV-) = 0.01

		- p(X = HIV+) = 0.0015
                  \to  p(X = HIV-) = 0.9985

		- then p(X = HIV+ | T = HIV+) is (shorthand P(x+, t-) etc)
		  = p(t+ | x+) p(x+) / p(t+)

		- p(t+) = p(x+, t+) + p(x-, t+)
		  = p(t+ | x+ ) p(x+) + p(t+ | x-) p(x-)
		  = 1 * 0.0015 + 0.01 * 0.9985

		- back to p(x+ | t+)
		  = 1 * 0.0015 / (1 * 0.0015 + 0.01 * 0.9985)

		- increase test accuracy by further information (f.ex. age)

		- and further, conditionally independent, tests

       3) Basic Algorithms

	  0) [@0]

	     - features X, labels y

	     - /bag of words/: "Assume we have a list of all possible
               words occurring in X, that is a dictionary, then we are
               able to assign a unique number with each of those words
               (e.g. the position in the dictionary). Now we may
               simply count for each document x_{i} the number of times
               a given word j is occurring. This is then used as the
               value of the j-th coordinate of x_{i}."

	  1) Naive Bayes

	     use bayes formula with approximations

	     - replace p(x) by likelihood ratio (adjusted)

	     - replace p(x|y) by independent product of parts (spam: words)

	  2) Nearest Neighbor Estimators

	     - distance to (k-) nearest point(s).

	     - whichever class is closest

	     - distance-dependent

	     - works well if work was put into distance
*** TODO quotes
    - much of the art of machine learning is to reduce a range of
      fairly disparate problems to a set of fairly narrow
      prototypes.
    - A rather related application is collaborative
      filtering. Internet book stores such as Amazon, or video rental
      sites such as Netflix use this information extensively to entice
      users to purchase additional goods (or rent more movies).
    - Many security applications, e.g. for access control, use face
      recognition as one of its components.
    - The overarching theme of learning problems is that there exists
      a nontrivial dependence between some observations, which we will
      commonly refer to as x and a desired response, which we refer to
      as y, for which a simple set of deterministic rules is not
      known. By using learning we can infer such a dependency between
      x and y in a systematic fashion.
    - (ends 1.1.1)
    - One of the challenges in dealing with vectors is that the scales
      and units of different coordinates may vary widely.
    - One way of dealing with those issues in an automatic fashion is
      to normalize the data. We will discuss means of doing so in an
      automatic fashion.
    - In some cases the vectors we obtain may contain a variable
      number of features.
    - (ends 1.1.2)
    - 3-class classification. Note that in the latter case we have
      much more degree for ambiguity.
    - a sequence of (x_{i} , y_{i} ) pairs for which y i needs to be
      estimated in an instantaneous online fashion. This is commonly
      referred to as online learning.
    - know X 0 already at the time of constructing the model. This is
      commonly referred to as transduction.
    - (ends 1.1.3)
    - For more details and a very gentle and detailed discussion see
      the excellent book of [BT03].
    - (ends 1.2.0)
    - Formally, [...] 1 occurs with probability 1/6
    - notational convention [...] use uppercase letters, e.g., X, Y
      etc to denote random variables and lower case letters, e.g., x,
      y etc to denote the values they take.
    - (ends 1.2.1)
    - If the random variable is discrete, i.e., it takes on a finite
      number of values, then this assignment of probabilities is
      called a /probability mass function/ or PMF for short.
    - slightly informal notation p(x) := Pr(X = x)
    - continuous random variable the assignment of probabilities
      results in a probability density function or PDF for short.
    - Closely associated with a PDF is the indefinite integral
      over p. It is commonly referred to as the cumulative
      distribution function (CDF).
    - (ends 1.2.2)
    - if f: R → R is a function, then f(X) is also a random
      variable. Its mean is mean given by

      E[f(X)] := \int f(x)dp(x).
    - (ends 1.2.3)
    - /joint density/ p(x, y).
    - /marginalization/: recover p(x) by integrating out y:
      p(x) = \int dp(x, y).
    - /X and Y are independent/: p(x,y) = p(x) p(y)
    - /iid random variables/: independently and identically distributed
    - /conditional probabilities/:
      p(x|y) := p(x, y) / p(y)
    - /Bayes Rule/: p(y|x) = p(x|y)p(y) / p(x)
    - (ends 1.2.4.0)
    - The physician recommends a test which is guaranteed to detect
      HIV-positive whenever a patient is infected.
    - On the other hand, for healthy patients it has a 1% error
      rate. That is, with probability 0.01 it diagnoses a patient as
      HIV-positive even when he is, in fact, HIV-negative.
    - Moreover, assume that 0.15% of the population is infected.
    - Denote by X and T the random variables associated with the
      health status of the patient and the outcome of the test
      respectively.
    - We are interested in p(X = HIV+|T = HIV+).
    - Note that often our tests may not be conditionally independent
      and we would need to take this into account.
    - p(X = HIV+|T = HIV+) = p(T = HIV+|X = HIV+) p(X = HIV+) / p(T = HIV+)
    - p(T = HIV+) = \sum p(T = HIV+, x) = \sum p(T = HIV+|x)p(x)
    - The corresponding expression yields:

      p(T = HIV+|X = HIV+, A)p(X = HIV+|A) / p(T = HIV+|A)

      Here we simply conditioned all random variables on A in order to
      take additional information into account.
    - What we want is that the diagnosis of T_{2} is independent of that
      of T_{2} given the health status X of the patient. This is
      expressed as
      p(t_{1} , t_{2} |x) = p(t_{1} |x)p(t_{2} |x). (1.16)
    - Random variables satisfying the condition (1.16) are commonly
      referred to as /conditionally independent/.
    - (ends 1.2.4.1)
    - assume that there is sufficiently strong dependence between x
      and y that we will be able to estimate y given x and a set of
      labeled instances X, Y.
    - (ends 1.3.0)
    - estimate of p(y), that is, the probability of receiving a spam
      or ham mail.
    - dispose of the requirement of knowing p(x) by settling for a
      likelihood ratio
      L(x) := p(spam|x) / p(ham|x) = p(x|spam)p(spam) / p(x|ham)p(ham)
    - treat the occurrence of each word in a document as a separate
      test and combine the outcomes in a naive fashion by assuming
      that
      p(x|y) = \Pi_{j=1}^{# of words in x} p(w^{j} |y)
    - p(w|y) can be obtained, for instance, by simply counting the
      frequency occurrence of the word within documents of a given
      class.
    - since we are computing a product over a large number of factors
      the numbers might lead to numerical overflow or underflow. This
      can be addressed by summing over the logarithm of terms rather
      than computing products.
    - need to address the issue of estimating p(w|y) for words w which
      we might not have seen before. One way of dealing with this is
      to increment all counts by 1.
    - perform surprisingly well
    - (ends 1.3.1)
    - Note that nearest neighbor algorithms can yield excellent
      performance when used with a good distance measure.
    - lemma by Johnson and Lindenstrauss [DG03] asserts that a set of
      m points in high dimensional Euclidean space can be projected
      into a O(log m/\epsilon 2 ) dimensional Euclidean space such that the
      distance between any two points changes only by a factor of (1 ±
      \epsilon). [...] The surprising fact is that the projection relies on a
      simple randomized algorithm.
** [[./06vect.pdf][Scoring, term weighting and the vector space model]]
** [[./ch1.pdf][Data Mining]]
** [[shell:firefox ./skl/index.html.4 &][A tutorial on statistical-learning for scientific data processing]]
*** summary
    1) Statistical learning: the setting and the estimator object in scikit-learn

       data: often 2d-array
       estimator: estimates (fit, ..., params at object creation)

    2) Supervised learning: predicting an output variable from high-dimensional observations

       1) Nearest neighbor and the curse of dimensionality

	  toy problem: classifying irises

	  - import numpy as np
	  - from sklearn import datasets
	  - iris = datasets.load_iris()
	  - iris_X = iris.data
	  - iris_y = iris.target
	  - np.unique(iris_y)

	  - k-Nearest neighbors classifier

	    - simplest classifier

	  - The curse of dimensionality

	    the number of points needed scales exponentially with the
            dimension

       2) Linear model: from regression to sparsity

	  1) linear regression

	     fit line X \beta + \epsilon to data points

	  2) Shrinkage

	     problem with few data points per dimension, solve via
             Ridge (uses l_{2}-Norm)

	  3) Sparsity

	     it helps to reduce non-informative variables. Ridge
             reduces, Lasso sets to 0 (good on large datasets), as
             does LassoLARS (few observations)

	  4) Classification

	     for classification-regression, better use sigmoid instead
             of linear (LogisticRegression)

       3) Support vector machines (SVMs)

	  1) Linear SVMs

	     C is regularization parameter:

	     "a small value for C means the margin is calculated using
             many or all of the observations around the separating line
             (more regularization); a large value for C means the
             margin is calculated on observations close to the
             separating line (less regularization)."

	     thus: bigger C, less values considered (always those
             close to separating line)

	     "For many estimators, including the SVMs, having datasets
             with *unit standard deviation* for each feature is
             important to get good prediction."

	  2) Using kernels

	     different fitting functions: linear, poly, rbf

    3) Model selection: choosing estimators and their parameters

       1) Score, and cross-validated scores

	  - score: bigger is better

	  - k-fold cross-validation:

	    - import numpy as np
	    - X_folds = np.array_split(X_digits, 3)
	    - y_folds = np.array_split(y_digits, 3)
	    - scores = list()
	    - for k in range(3):
	      - X_train = list(X_folds)
	      - X_test  = X_train.pop(k)
	      - X_train = np.concatenate(X_train)
	      - y_train = list(y_folds)
	      - y_test  = y_train.pop(k)
	      - y_train = np.concatenate(y_train)
	      - scores.append(svc.fit(X_train, y_train).score(X_test, y_test))

       2) Cross-validation generators

	  X-validation (Kfold, Loo, etc) are available via

	  =sklearn.cross_validation=

	  - "To compute the score method of an estimator, the sklearn
            exposes a helper function:

	    #+BEGIN_SRC python
              cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)
	    #+END_SRC

       3) Grid-search and cross-validated estimators

	  1) Grid-search

	     automatic parameter search

	     #+BEGIN_SRC python
               from sklearn.grid_search import GridSearchCV
               Cs = np.logspace(-6, -1, 10)
               clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)
               clf.fit(X_digits[:1000], y_digits[:1000])
               clf.best_score_
               clf.best_estimator_.C
	     #+END_SRC

	  2) Cross-validated estimators

	     with CV added to name: search automatically

	     (but not svm)

    4) Unsupervised learning: seeking representations of the data

       1) Clustering: grouping observations together

	  when categorization data not at hand

	  1) K-means clustering
	     #+BEGIN_SRC python
               >>> k_means = cluster.KMeans(n_clusters=3)
               >>> k_means.fit(X_iris)
               >>> print(k_means.labels_[::10])
	     #+END_SRC

	     iris: almost correct with 8 clusters, off on border with 3

	  2) Hierarchical agglomerative clustering: Ward

	     hierarchy of clusters

	     - agglomerative (bottom-up) vs divisive (top-down)
*** quotes
    - import pylab as pl
      pl.imshow
*** exercises
**** Exercise digits-knn-linear
     Try classifying the digits dataset with nearest neighbors and a
     linear model. Leave out the last 10% and test prediction
     performance on these observations.

     from sklearn import datasets, neighbors, linear_model

     digits = datasets.load_digits()
     X_digits = digits.data
     y_digits = digits.target
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, neighbors, linear_model
        import numpy as np

        digits = datasets.load_digits()
        X_digits = digits.data
        y_digits = digits.target

        np.random.seed(0)
        indices = np.random.permutation(len(y_digits))
        percent = int(len(y_digits) * 0.9)

        digits_X_train = X_digits[indices[:percent]]
        digits_X_test = X_digits[indices[percent:]]
        digits_y_train = y_digits[indices[:percent]]
        digits_y_test = y_digits[indices[percent:]]

        knn = neighbors.KNeighborsClassifier()
        knn.fit(digits_X_train, digits_y_train)
        knn.score(digits_X_test, digits_y_test)

        lin = linear_model.LogisticRegression()
        lin.fit(digits_X_train, digits_y_train)
        lin.score(digits_X_test, digits_y_test)
      #+END_SRC
**** Exercise iris-svm
     Try classifying classes 1 and 2 from the iris dataset with SVMs,
     with the 2 first features. Leave out 10% of each class and test
     prediction performance on these observations.
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, svm
        import numpy as np

        iris = datasets.load_iris()
        X = iris.data
        y = iris.target

        # normalize std
        s = np.std(X, 0)
        for i in range(4):
            X[:,i] /= s[i]

        np.random.seed(0)
        indices = np.random.permutation(len(y))
        percent = int(len(y) * 0.9)

        X_train = X[indices[:percent]]
        X_test = X[indices[percent:]]
        y_train = y[indices[:percent]]
        y_test = y[indices[percent:]]

        svc = svm.SVC(kernel='linear')
        svc.fit(X_train, y_train)
        print svc.score(X_test, y_test)

        svcr = svm.SVC()
        svcr.fit(X_train, y_train)
        print svcr.score(X_test, y_test)
      #+END_SRC
**** Exercise kfold
     On the digits dataset, plot the cross-validation score of a SVC
     estimator with an linear kernel as a function of parameter C (use
     a logarithmic grid of points, from 1 to 10).
***** Solution
      #+BEGIN_SRC python
        import numpy as np
        from sklearn import cross_validation, datasets, svm

        C_s = np.logspace(-10, 0, 50)

        digits = datasets.load_digits()
        X = digits.data
        y = digits.target

        for C in C_s:
            svc.C = C
            score = cross_validation.cross_val_score(svc, X, y, n_jobs=1)
            print '%f, %f, %f' % (C, np.mean(score), np.std(score))
      #+END_SRC
**** Exercise grid-optimal
     On the diabetes dataset, find the optimal regularization
     parameter alpha.
***** Solution
      #+BEGIN_SRC python
        from sklearn import cross_validation, datasets, linear_model
        import numpy as np

        diabetes = datasets.load_diabetes()
        X = diabetes.data[:150]
        y = diabetes.target[:150]

        lasso = linear_model.Lasso()
        alphas = np.logspace(-4, -.5, 30)

        from sklearn.grid_search import GridSearchCV
        clf = GridSearchCV(estimator=lasso, param_grid=dict(alpha=alphas), n_jobs=-1)
        clf.fit(X, y)
        print clf.score(diabetes.data[150:], diabetes.target[150:])

        # for a in alphas:
        #     lasso.alpha = a
        #     score = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)
        #     print '%f, %s' % (a, np.mean(score))
      #+END_SRC
** [[./skl/working_with_text_data.html][Working With Text Data]]
*** ref
    #+BEGIN_SRC bibtex
      @article{scikit-learn,
        title={Scikit-learn: Machine Learning in {P}ython},
        author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
               and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
               and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
               Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
        journal={Journal of Machine Learning Research},
        volume={12},
        pages={2825--2830},
        year={2011}
      }
    #+END_SRC
** TODO [[./Z5280B.pdf][Dirty Rotten Strategies - How We Trick Ourselves and Others into Solving the Wrong Problems Precisely]]
** TODO [[./Levenshtein.html][Levenshtein-Distanz]]
*** quotes
    - ist die minimale Anzahl von Einfüge-, Lösch- und
      Ersetz-Operationen, um die erste Zeichenkette in die zweite
      umzuwandeln.
*** TODO ref in english
    #+BEGIN_SRC bibtex
      @misc{ wiki:xxx,
        author = "Wikipedia",
        title = "Levenshtein-Distanz --- Wikipedia{,} Die freie Enzyklopädie",
        year = "2014",
        url = "\url{https://de.wikipedia.org/w/index.php?title=Levenshtein-Distanz&oldid=133874990}",
        note = "[Online; accessed 22-Oktober-2015]"
      }
   #+END_SRC
** [[./Cross-validation.leave-one-out_cross-validation.html]]
** TODO [[./2012-jmlr.pdf][Sally: A Tool for Embedding Strings in Vector Spaces]]
** td: dl paper onion routing
** TODO [[./85331-CS.pdf][Improved Inapproximability Results for the Shortest Superstring and Related Problems]]
** TODO [[./EdmanS09.pdf][AS-awareness in Tor Path Selection]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{DBLP:conf/ccs/EdmanS09,
        title = {{AS}-awareness in {T}or path selection},
        author = {Matthew Edman and Paul F. Syverson},
        booktitle = {Proceedings of the 2009 ACM Conference on Computer and Communications
              Security, CCS 2009},
        year = {2009},
        month = {November},
        location = {Chicago, Illinois, USA},
        pages = {380--389},
        editor = {Ehab Al-Shaer and Somesh Jha and Angelos D. Keromytis},
        publisher = {ACM},
        isbn = {978-1-60558-894-0},
        www_section = {Anonymous communication},
        www_pdf_url = {http://www.cs.rpi.edu/~edmanm2/ccs159-edman.pdf},
        www_tags = {selected},
        bibsource = {DBLP, http://dblp.uni-trier.de},
      }
    #+END_SRC
** TODO [#B] [[./ieee-icc15.pdf][A First-Hop Traffic Analysis Attack Against Tor]]
*** summary
    0) [@0] Abstract

       timing-information only

       68% success

    1) INTRODUCTION

       1) [@0]

	  - only timing information

	  - padding defeats size info

	  - packet counting need partitioning

	  - this does not

       2) Related Work
*** quotes
** TODO [[./supersequence.pdf][On the Approximation of Shortest Common Supersequences and Longest Common Subsequences]]
** TODO [[/home/chive/own/tor/doc/read/tor-spec.txt][Tor Protocol Specification]]
*** summary
    0) [@0] Preliminaries: MUST, etc. keywords like RFC
       1) Notation: PK, SK, K, a|b, [a b c], H(m)
	  1) all multibyte values are big-endian
       2) Security: KEY_LEN, PK_ENC_LEN, PK_PAD_LEN, DH_LEN,
          DH_SEC_LEN, HASH_LEN, PAYLOAD_LEN, CELL_LEN(v)
       3) Ciphers
	  - stream: 128bit counter-mode AES with IV=0x0...0
	  - pk: 1024 bit AES with exponent of 65537
	    0aef-mgf1 padding, sha1 digest, label unset
	  - ntor: curve25519
	  - dh generator 2, modulus from rfc2409
	    SHOULD private keys of 320 bits, no reuse
	  - hybrid encryption
    1) System overview: TOR low-latency tcp distributed overlay
       1) Keys and names
	  - long-term *identity* key, signing only
	  - medium-term *onion*-key, keep at least one week after
            advertisement
	  - (short-term TLS *connection* key, rotate at least once a day)
    2) Connections
       - link layer: 3 ways
         1. "certificates up-front"
	    both send two-certificate chain
  	    - init: short-term X.509 cert + self-signed identity X.509
	    - resp: similar
	    - MAY ONLY INCLUDE
	      - TLS_DHE_RSA_WITH_AES_256_CBC_SHA
	      - TLS_DHE_RSA_WITH_AES_128_CBC_SHA
	      - SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA
	      jeder zwei x.509 Zertifikate: eines fuer connection, eines
              fuer identity DARF NUR:
              - TLS_DHE_RSA_WITH_AES_256_CBC_SHA
              - TLS_DHE_RSA_WITH_AES_128_CBC_SHA
              - SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA
	 2. "renegotiation" (tor >= 0.2.0.21)
   	    responder single certificate, 
	    initiator renegotiates
	    - init: no cert
	    - resp: connection cert
	    - ... (continue handshake)
	    - init: renegotiate
	    - MUST HAVE ONE NOT IN ABOVE
	 3. "in-protocol" (tor >= 0.2.3.6-alpha)
	    distinguish vs v2 via one of:
	    - self-signed cert
	    - commonName ends other than ".net"
	    - public key modulus >= 1024 bits
	    then: send VERSIONS cell, ...
       - several security /accessibility features:
         - fixed protocol list
         - fixed choice of response
         - no server without valid certificates
    3) Cell Packet Format
       ...
    4) 
    5) 
       1) 
       2) 
       3) 
       4) 
       5) Routing relay cells
	  checks circID, crypts payload, inspect payload (6.1), work it
    6) Application connections and stream management
       1) Relay cells
       2) 
	  ...
    7) FLow control
       1) Link throttling
*** quotes
   - Tor relays are also identified by "nicknames"; these are specified in
     dir-spec.txt.
   - <<O1>> All implementations MUST support the SSLv3 ciphersuite
     "SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA", and SHOULD support the TLS
     ciphersuite "TLS_DHE_RSA_WITH_AES_128_CBC_SHA" if it is
     available.
   - In all of the above handshake variants, certificates sent in the
     clear SHOULD NOT include any strings to identify the host as a
     Tor relay. In the "renegotiation" and "backwards-compatible
     renegotiation" steps, the initiator SHOULD choose a list of
     ciphersuites and TLS extensions to mimic one used by a popular
     web browser.
   - TLS connections are not permanent.
   - Each client or relay should do appropriate bandwidth throttling to
     keep its user happy. [...]
     The mainline Tor implementation uses token buckets (one for
     reads, one for writes) for the rate limiting.

*** up to
    - 4. Negotiating and initializing connections
    - read to
      - 4.4. AUTHENTICATE cells
** TODO [[./projects.html][Software & Services]]
*** summary
    - Tor Browser

      - "everything you need to safely browse the Internet"

      - requires no installation

    - ...
*** ref
    #+BEGIN_SRC bibtex
     @misc{tor-ecosystem,
       tag = "The Tor Project",
       title = "Software & Services",
       url = "\url{https://www.torproject.org/projects/projects.html.en}",
       note = "[Online; accessed 21-October-2015]"
     }
   #+END_SRC
** MAYBE [[./rijsbergen79_infor_retriev.pdf][INFORMATION RETRIEVAL]]
*** quotes
    |X ∩ Y| / |X ∪ Y| Jaccard's coefficient
** TODO [[./python-doc-howto-logging.pdf][Logging HOWTO]]
*** summary
    1) Basic Logging Tutorial

       log(level[, data])

       1) When to use logging

	  normal output for the user: use =print=, else use logging.level()

	  WARNING is the default level

       2) A simple example

	  only warning and above are shown:

	  import logging
          logging.warning('Watch out!') # will print a message to the console
          logging.info('I told you so') # will not print anything

          WARNING:root:Watch out!

       3) Logging to a file

	  logging.basicConfig(filename='example.log',level=logging.DEBUG)

       4) Logging from multiple modules

	  import in every module

	  call basicConfig in main entry point

       5) Logging variable data

	  %s style, other options work maybe, too

       6) Changing the format of displayed messages

	  #+BEGIN_SRC python
            logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
	  #+END_SRC

       7) Displaying the date/time in messages
*** quotes
** MAYBE [[./python-doc-library.pdf][The Python Library Reference]]
*** quotes
    - doctest
      #+BEGIN_SRC python
        if __name__ == "__main__":
            import doctest
            doctest.testmod()
      #+END_SRC
    - os.walk(top, topdown=True, onerror=None, followlinks=False)

      Generate the file names in a directory tree by walking the tree
      either top-down or bottom-up. For each directory in the tree
      rooted at directory top (including top itself), it yields a
      3-tuple (dirpath, dirnames, filenames).  dirpath is a string,
      the path to the directory. dirnames is a list of the names of
      the subdirectories in dirpath (excluding ’.’ and
      ’..’). filenames is a list of the names of the non-directory
      files in dirpath. Note that the names in the listscontain no
      path components. To get a full path (which begins with top) to a
      file or directory in dirpath, do os.path.join(dirpath, name).
    - str.lstrip( [ chars ] )

      Return a copy of the string with leading characters removed. The
      chars argument is a string specifying the set of characters to
      be removed.
    - os.path.basename(path)

      Return the base name of pathname path.
    - staticmethod(function)

      Return a static method for function. A static method does not
      receive an implicit first argument. To declare a static method,
      use this idiom:
      #+BEGIN_SRC python
        class C(object):
            @staticmethod
            def f(arg1, arg2, ...):
                ...
      #+END_SRC
** MAYBE [[/home/chive/IT-gg/python/scipy-ref-0.15.1.pdf][SciPy Reference Guide]]
** MAYBE [[/home/chive/IT-gg/python/numpy-ref-1.9.1.pdf][NumPy Reference]]
*** quotes
    - numpy.zeros(shape, dtype=float, order=’C’)

      Return a new array of given shape and type, filled with zeros.

    - numpy.array(object, dtype=None, copy=True, order=None,
      subok=False, ndmin=0)

      Create an array.

    - numpy.ma.arange( [ start ] , stop [ , step ] , dtype=None)

      Return evenly spaced values within a given interval.  [...]
      When using a non-integer step, such as 0.1, the results will
      often not be consistent. It is better to use linspace for these
      cases.

    - numpy.linspace(start, stop, num=50, endpoint=True,
      retstep=False, dtype=None)

      Return evenly spaced numbers over a specified interval.
** TODO [#B] [[./tor14design.pdf][Tor: The Second-Generation Onion Router (2014 DRAFT v1)]]
*** summary
    0) [@0] Abstract

       - real-world experiences

       - open problems

    1) Overview

       - Better than original onion routing by:

         - perfect forward secrecy:

           "subsequently compromised nodes cannot decrypt old traffic"

	 - Separation of “protocol cleaning” from anonymity

	   just uses SOCKS for applications to connect. (protocol
           cleaning is done f.ex. by addon or proxy)

	 - No mixing, padding, or traffic shaping (yet):

	   no usable concepts/implementations, high overhead

	 - Many TCP streams can share one circuit:

	   allows for multiple streams to have same circuit (with user
           control)

	   less crypto, less vulnerability (see section 9)

	 - Leaky-pipe circuit topology:

	   traffic can exit at any place in the circuit (how about
           exit node policies?)

	 - Congestion control:

	   end-to-end acks, active research

	 - Directory authorities:

	   instead of flooding the network, trusted nodes provide
           network info

	 - Variable exit policies:

	   exit node operators select which traffic to allow to which
           hosts

	 - End-to-end integrity checking:

	   in addition to crypto

	 - Rendezvous points and hidden services:

	   negotiation of rendezvous points (instead of "reply onions")

	 - Censorship resistance:

	   bridges (unlisted guard nodes) and HTTPS similarity

	 - Modular architecture:

	   - vidalia (control port)

	   - pluggable transports

	   - no OS patches, but only TCP possible

    2) Related work

       - Chaum: Mix-Net

       - Babel, Mixmaster, Mixminion: maximum anonymity, large latency

       - tor low-latency

       - single-hop: anonymizer, etc

       - JonDo: fixed cascades: routes that aggregate traffic

       - PipeNet: multi-hop, weaknesses

       - p2p:

         - tarzan, morphmix, layered encryption

         - crowds: all nodes can read

	 - hordes: crowds with multicast responses

	 - herbivore and P^{5}: crowds with broadcast responses

       - freedom, i2p: circuits all at once

       - cebolla, anonymity network: build in stages

       - circuit-based: which circuit? IP, TCP, HTTP?

       - TCP middle-approach,

         - can transfer all TCP streams

         - avoid TCP-TCP inefficiencies

       - censorship-resistance like eternity, free havfen, publius,
         tangler

    3) Design goals and assumptions

*** quotes
    - most designs protect primarily against traffic analysis rather
      than traffic confirmation
    - distributed-trust, circuit-based anonymizing systems

** [[~/da/git/docs/python-doc-reference.pdf][The Python Language Reference]]
** [[file:sarle/comp.ai.neural-nets%20FAQ,%20Part%201%20of%207:%20Introduction.html][file:~/da/git/docs/sarle/comp.ai.neural-nets FAQ, Part 1 of 7: Introduction.html]]
*** summary
    1) Introduction

       - What is a neural network (NN)?

	 *Artificial* Neural Network:

	 parallelistic learning, experiential learning, asynchronous

       - Where can I find a simple introduction to NNs?

	 see chapter 4

       - Are there any online books about NNs?

*** quotes
    - Some care should be invested into a summary:
      - simple concatenation of all the answers is not enough: instead,
       redundancies, irrelevancies, verbosities, and errors should be
       filtered out (as well as possible)
      - the answers should be separated clearly
      - the contributors of the individual answers should be
        identifiable (unless they requested to remain anonymous [yes,
        that happens])
      - the summary should start with the "quintessence" of the
        answers, as seen by the original poster
      - A summary should, when posted, clearly be indicated to be one
        by giving it a Subject line starting with "SUMMARY:"
      Note that a good summary is pure gold for the rest of the newsgroup
      community
    - If trained carefully, NNs may exhibit some capability for
      generalization beyond the training data, that is, to produce
      approximately correct results for new cases that were not used
      for training.
*** ref
    Sarle, W.S., ed. (1997), Neural Network FAQ, part 1 of 7:
    Introduction, periodic posting to the Usenet newsgroup
    comp.ai.neural-nets, URL: ftp://ftp.sas.com/pub/neural/FAQ.html
** [[file:Curse%20of%20dimensionality%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Curse of dimensionality - Wikipedia, the free encyclopedia.html]]
** [[file:Regularization%20(mathematics)%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Regularization (mathematics) - Wikipedia, the free encyclopedia.html]]
** onion routing project
*** [[~/da/git/docs/onion-routing_pet2000.pdf][Towards an Analysis of Onion Routing Security]]
**** ref
     #+BEGIN_SRC bibtex
       @inproceedings{onion-routing:pet2000,
         title = {{Towards an Analysis of Onion Routing Security}},
         author = {Paul Syverson and Gene Tsudik and Michael Reed and Carl Landwehr},
         booktitle = {Proceedings of Designing Privacy Enhancing Technologies: Workshop on Design
               Issues in Anonymity and Unobservability},
         year = {2000},
         month = {July},
         pages = {96--114},
         editor = {H. Federrath},
         publisher = {Springer-Verlag, LNCS 2009},
         www_important = {1},
         www_tags = {selected},
         www_section = {Anonymous communication},
         www_ps_gz_url = {http://www.onion-router.net/Publications/WDIAU-2000.ps.gz},
       }
     #+END_SRC
*** [[~/da/git/docs/onion-routing_ih96.pdf][Hiding Routing Information]]
**** ref
     #+BEGIN_SRC bibtex
       @inproceedings{onion-routing:ih96,
         title = {{Hiding Routing Information}},
         author = {David M. Goldschlag and Michael G. Reed and Paul F. Syverson},
         booktitle = {Proceedings of Information Hiding: First International Workshop},
         year = {1996},
         month = {May},
         pages = {137--150},
         editor = {R. Anderson},
         publisher = {Springer-Verlag, LNCS 1174},
         www_tags = {selected},
         www_section = {Anonymous communication},
         www_pdf_url = {http://www.onion-router.net/Publications/IH-1996.pdf},
         www_ps_gz_url = {http://www.onion-router.net/Publications/IH-1996.ps.gz},
       }
     #+END_SRC
*** [[~/da/git/docs/onion.pdf][Anonymous Connections and Onio Routing]]
*** [[~/da/git/docs/onion-discex00.pdf][Onion Routing Access Configurations]]
    see also bibtex
** [[~/da/git/docs/timing-fc2004.pdf][Timing Attacks in Low-Latency Mix Systems]]
** [[~/da/git/docs/10.1.1.96.7225.pdf][A Guided Tour to Approximate String Matching]]
*** auch ~/da/git/docs/TR_DCC-1999-005.pdf
** [[~/da/git/docs/mlj12.pdf][Good edit similarity learning by loss minimization]]
** [[/home/chive/IT-gg/rfc7159.json-storage-and_xfer.txt][The JavaScript Object Notation (JSON) Data Interchange Format]]
*** summary
    0) [@0] Abstract

       "JavaScript Object Notation (JSON) is a lightweight, text-based,
       language-independent data interchange format"

    1) Introduction
*** quotes
*** ref
    #+BEGIN_SRC bibtex
      @techreport{rfc7159,
        author="T. Bray",
        title="{The JavaScript Object Notation (JSON) Data Interchange Format}",
        howpublished="Internet Request for Comments",
        TYPE="{RFC}",
        number=7159,
        PAGES = {1-16},
        year=2014,
        month=mar,
        publisher="{RFC Editor}",
        INSTITUTION = "{RFC Editor}",
        url="http://www.rfc-editor.org/rfc/rfc7159.txt",
      }
    #+END_SRC
