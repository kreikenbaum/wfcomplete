#+TODO: TODO | DONE
[[./master.bib]]
** [[./2015-asiaccs.pdf][Arp - Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]]
*** quotes
    - (a) web pages can be easily manipulated to load content from
      untrusted origins and (b) despite encryption, low-latency
      anonymization networks cannot effectively hide the size of
      request-response pairs.
    - A large body of work has studied passive attacks based on
      traffic analysis, most notably website fingerprinting and
      traffic confirmation attacks.
    - First, web pages can often be manipulated to load content from
      untrusted origins, for example, using advertisements or
      user-provided content.
    - deanonymize Tor users in a short period of time.
*** summary
    request packets per JS. they have sizes of 2k, 4k, 6k, 8k = 2bit
    information. Wait after page load, trigger js xmlhttprequest.

    0. [@0]
       deanonymization via traffic injection and identification of
       request-response pairs
    1. user's browser does side-channel communication via
       user-submitted content or advertising, this is detected between
       entry node and onion proxy.
    2. background
       1. low-latency anonymization
       2. active attacker: remote markers via embed (ads) or local (at server)
       3. side-channel (see 3)
    3.
       1. preprocessing:
          1. not ip, but TLS (removes *1 layer* of retransmissions)
          2. empty out, merge adjacent, filter control (discard 512b-cells), merge again, sizes as multiples fo 2k
       2. side channel: messages: 2k, 4k, 6k, 8k bytes, 4 bits,
       3. transmission:
          1. http via xmlhttprequest: with random parameter to avoid caching
          2. use for web page markers, single easy, else 20byte sha1 (hammingdist)
       4. detection: sliding window (faster), svm, see [1]
    4. Evaluation: fixed set vs 60000 vs live
       1. Data Collection via Selenium with Firefox, if not loaded, discard
       2. extracting via Sally, learning via LibSVM, generate, record 50 transmissions, train
       3. closed-world:
          reverse proxy, inject js, delay 30secs, 120secs on slow load
          transmission takes 12-20 secs, compare to fingerprinting, 95%
       4. open-world: 91%, no false positives
       5. users: 31 of 34, no false positives
    5. limitations and defenses
       detect web page markers
       chaff
    6. related
       liberatore: look at traffic
       both ends: better
    7. conclusion
       owie, detect as first countermeasure
** [[./2014-torben.pdf][Arp - Torben: Deanonymizing Tor Communication using Web Page Markers]]
*** quotes
    - As a result, the few known cases of deanonymization of Tor have
      been reported to instead make use of advertisement networks or
      rely on vulnerabilities in browser implementations [29, 30] and
      are thus unrelated to insecurities of Tor in general.
    - As a consequence, there is an urgent need for defenses in
      anonymization services protecting users from active attacks at
      the application layer.
    - A web page[sic: r] marker is implanted using embedded or user-provided
      content, such as an advertisement or an image link. The marker
      induces a traffic pattern visible at the entry node, for
      example, using a chain of HTTP redirects or JavaScript code
      generating HTTP requests.
    - each circuit is only used for 10 minutes until a new circuit is
      created
    - For example, the Walsh-Hadamard code can be used to encode
      messages of length k ≤ 7 as code words of length 2 k with
      maximized minimum Hamming distances.
    - eight different sizes corresponding to the alphabet

      A = {− f(1, 1 ) , . . . , − f(0, 0 ) , f(0, 0 ) , . . . , f(1, 1 )} .
    - # ( x, p ) returns the number of occurrences of the positional
      n-gram p = ( s, i ) between the positions i and i + τ in the
      sequence x.
    - The vector φ ( x ) encodes information about the symbols, their
      order and their position in x—thus reflecting the basic
      properties of a web page marker.
    - This reliability rests on the design of the side channel that
      makes use of atypical request-response pairs for transmitting
      information (Section 3.2).
    - Whether such chaff traffic can be selectively injected to only
      destroy indicative traffic patterns is an interesting question
      for further research.
    - the attacker can expose the web pages a user visits within a
      couple of seconds.
*** summary
    0. [@0] Abstract: torben presented
    1. Introduction

       much research into passive attacks: high false positive,
       changing web content leads to problems, active attacks such as
       patch selection and watermarking

       instead: mostly browser vulnerabilities

       torben introduced (see also [[Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]])
    2. Background
       1. The Tor Network:

          bunch of routers, symmetric keys per hop in the circuit,
          onion encryption, each router only sees neighbors, each
          circuit only used for 10 minutes

       2. Attack Scenario:

          Attacker can insert markers into web page of interest and
          analyze traffic between OP and guard.

    3. A Side-Channel Attack on Tor
       0. [@0]
          - Tor ist low-latency
          - request-response paare sichtbar im TLS Traffic (mit filtern)
          - sollte der gegner die Website beeinflussen koennen
            - via js oder http redirect (andere moeglichkeiten, css?)
            - als direkte beeinflussung oder user content
          - kann er versuchen, ueber diesen side-channel zu kodieren,
            welche websites besucht werden.
          - Schritte
            1. Preprocessing of network traces. (3.1)
            2. Side channel design (3.2)
            3. Transmission of web page markers (3.3).
            4. Detection of web page markers. (3.4?)
       1. Vorverarbeitung:
          merkmal: groesse der kontinuierlich in eine richtung uebermittelten daten
          1. TCP statt IP analyse via tshark
          2. Filtering and Merging TLS Records
             a. filter non-tor-records (\le 100 bytes)
             b. merge continuous to obtain amount of flow (packes sizes random)
             c. filter control cells (512 bytes) and merge again (HTTP
                does not fit into 512 bytes)
             d. normalize sizes, multitudes of 2000 bytes
       2. Side Channel Design
          map two bits q = q_i, q_j to
          q_i, q_j \to ( q_i + 2q_j ) · s + c     (with s, c = 2000)
          = q \cdot s + c
          map four bits to request and response sizes, two bits each.
       3. Transmission

          request: get with "random" parameter of fitting lenth
          response: any page of acceptable size, *any host*

          hash URLs to SHA-1 (optimal when fixed: walsh-hadamard code)
       4. Detection of Web Page Markers

          1. Positional N-grams

             - gelesene Sequenz gegeben (experiment: 100 symbols)

             - A = {2,4,6,8}^2 (Torben-Alphabet, mit minus fuer request)
               |A| = 16

             - S=A^n alle n-gramme von A
               |S| = 16^n

               \to (eigenes) n = 40
               == 2^160

             - positional n-grams:

               P = S \times N,
               mit Element p=(s,i) mit s \in S, i Position von s in Sequenz

             - \varphi bildet von allen n-grammen A^* nach R^{|P|}.
               \varphi(x) \to (#(x,p))_{p \in P}

               Jedes n-gramm (s,i) hat seine Haeufigkeit zwischen i und
               i+\tau als wert

             - \tau is toleranz-parameter

          2. Probabilistic Classification

             - SVM trainiert mit Sequenzen der Marker

             - Riesiger Vektorraum, aber sparse

             - P "nur" multiplikativ mit Fenstergroesse, nicht anders, puh

    4. Evaluation

       mehrere Experimente: cw, ow, users

       cw: unrealistisch, aber haeufig verwendet in website-fingerprinting, vergleich
       ow: 60000 webseiten

       1. Data

          - Selenium WebDriver mit Tor bb

          - wenn nicht load in 3 min, diese seite verwerfen

          - remove similar, vergleich mit fingerprint (die failen)

       2. Detection

          - Sally verwandelt von Netzwerktraces in positional n-grams

          - learning libsvm

          - auf selbem rechner ausser Cai: cluster

          - 100 marker, 50 uebertragungen jeweils gemessen

          - n = 3,

          - the tolerance to τ = 9 and

          - the SVM regularization to C = 0.1

       3. Closed-World Evaluation

          - top 100 seiten je 50 mal

          - jeweils im februar und april 2014

          - js via reverse proxy

          - marker nach 30 \to 120 sec delay

          - transmission time 12-20 secs

          - complete marker: 300 packets, \sim 390000 bytes

          - Vergleich mit Herrmann.., Panchenko.. und Cai.. (mit
            Fingerprints vom Februar)

          - torben imm 95%, die anderen schlechter

          - false classification favors particular markers

       4. Open-World Evaluation

          - 60000 von Alexa (top million \ top 100)

          - few (as before, top 100) with markers
            \to evaluate false positives

          - detect 91% with no false positives

          - reliable,
            due to atypical request-response-pairs

       5. Live Evaluation

          - 4 users, 2 hours each

          - if probability score below threshold of t=0.1, do not select

    5. Limitation and Defenses

       - torben works reliably

       - limitations?

       - detect web page markers: arms race: attackers change params,

       - chaff traffic: "might lower Tor’s overall performance."

    6. Related Work: first early, then active and passive vs low-latency

       1. Attacks on Encrypted Communication

          http pattern of access detectible via tls

          countermeasures fail to address size of data traffic

       2. Passive Attacks against Tor

          - herrmann: ip lengths

          - panchenko: data sent before direction change,

          - cai: ordering w/ displacements

          - wang: tls

          - high false-positives

          - counter: morphing,

       3. Active Attacks against Tor

          - passive: longer period

          - solve: active attack

          - 1: reveal communication path

          - murdoch: similar, but path \to infeasible

          - watermarking: inject specific patterns, inter-packet delays

            - needs to control exit node, tcp level (not app)

    7. Conclusion
** [[./wtfpad-setup.pdf][Arp - Setup der Obfuscation-Tools]]
*** summary
    1. Setup der Bridge

       - einige settings, z.b. ORPort, nickname, contactinfo,
         exitpolicy, bridgerelay, publishserverdescriptor,
         servertransportplugin

       - firewall

    2. Einrichten von obfsproxy auf Port 443

       auf <1024: iptables, orport, servertransportplugin, ...

    3. Aufsetzen von Juarez’ WTFPAD-Tools

       1. Installation: setuptools, psutils, (clienttransportplugin+bridge)

       2. starten: a la sh-skripte (scramblesuit)
*** quotes
    3. [@3] Aufsetzen von Juarez’ WTFPAD-Tools
       - torrc-default:
         - ClientTransportPlugin <DEFENSE> socks5 localhost:4999
         - bridge <DEFENSE> 134.76.96.77:443
       - Clientseite:
         #+BEGIN_SRC sh
           python /usr/lib/python2.7/dist-packages/obfsproxy/pyobfsproxy.py \
           <DEFENSE> socks 127.0.0.1:4999`
         #+END_SRC
       - Serverseite:
         #+BEGIN_SRC sh
           python /usr/lib/python2.7/dist-packages/obfsproxy/pyobfsproxy.py \
           --log-min-severity=debug --data-dir=/tmp/scramblesuit-server <DEFENSE> \
           --dest 127.0.0.1:9001 server 134.76.96.77:42000
         #+END_SRC
*** ref [[file:master.bib::arp-wtf][Arp 2016]]
** [[./fp.pdf][Cai - Touching from a Distance: Website Fingerprinting Attacks and Defenses]]
*** summary
    0. [@0] ABSTRACT

       web-page (!) fingerprinting, 50% regardless of defense scheme

       \to web-site fingerprinting, 90% accuracy

    1. INTRODUCTION

       - "effective attacks against HTTPOS, randomized pipelining, and
         several other defenses."

       - "Even with a 1-to-1 ratio between cover traffic and real
         traffic, our attack could identify the victim’s web page over
         50% of the time."

       - "the first demonstration that application-level defenses,
         such as HTTPOS and randomized pipelining, are not secure."

       - levenshtein-based wf, extended to web sites via hmm

       - others are broken

       - we do better

    2. RELATED WORK

       0. [@0] attack classes

          - identify user

          - identify server

          - identify path

          - user most applicable

       1. Fingerprinting attacks on encrypting tunnels

          beginning: packets sizes

          later: HMMs

       2. Fingerprinting attacks on Tor

          - herrmann et al: multinomial naive bayes,

          - shi et al: cosine similarity

          - panchenko: http-specific with svm

          - reimplementation: 65% success rate, 100 web pages

       3. Proposed traffic analysis defenses.

          - "padding packets, splitting packets into multiple packets,
            and inserting dummy packets"

          - Fu et al: theoretical: constant-rate, fixed-rate

            - random intervals better

          - wright et al: morphing

          - lu et al: morphing extension to distribution of size-ngrams

          - luo et al: HTTPOS:

            - TCP: size and ordering of packets

            - HTTP: multiple possibly overlapping requests, pipelinig,
              extra unnecessary requests, get extra data

            - defeatable by OP

          - Tor: randomized pipelining

            - worse not better

       4. Other related work.

          - Wright et al: HMM protocol classification encrypted TCP

          - White et al HMM partial plaintext of encrypted VoIP

    3. RECOGNIZING WEB PAGES

       - Damerau-Levenshtein edit distance

         - best costs when "transpositions were 20 times cheaper than
           insertions, deletions, and substitutions"

         - size rounding (up)

         - normalization to d(t, t') / min(|t|, |t'|)

         - several worse approaches

           - cells instead of packets

           - knn

           - fixed-length via l_{2}-norm

    4. RECOGNIZING WEB SITES

       - HMM

         - "each web page corresponds to an HMM state, and state
           transition probabilities represent the probability that a
           user would navigate from one page to another."

         - uses classifier for probability

         - web site template for huge pages (like amazon)

         - AJAX: transition between different states

         - *cold* pages: on first visit, vs

         - *warm* pages: with some stuff cached

         - back button as link to warm page

         - one set of usage patterns (or a few distinct, or uniform)

    5. Congestion-Sensitive BUFLO

       - BuFLO with output queue

       - only outgoing, other ends needs CS-BuFLO as well

       - reveals

         - maximum transmission rate T

         - number of transmitted cells B

         - (upstream too)

    6. EVALUATION

       1. Web page classifier

          0. [@0] questions

             - defenses: https, randomized pipelining, padding, morphing

             - other classifiers:herrmann, panchenko

             - if number of web pages goes up?

             - if size of training set goes up?

             - choice of web pages?

             - state of the browser?

          1. Experimental Setup

             - default firefox with Tor

             - "either 20 or 40 traces from each URL"

          2. Attacks and Defenses

             1. data sets

                - none: ssh

                - ssh + httpos

                - tor

                - tor + randomized pipelining

             2. generate defenses

                - ssh + sample-based traffic morphing to flickr.com

                - ssh packet count remove packet size and direction information

                - tor + randomized pipelining + randomized cover traffic

                  only insert 1500 or -1500 at l random positions

                  *weaker than panchenko*

                - tor packet count: as ssh p-c above

             3. Results

                - better in many cases than panchenko

       2. Web site classifier

          1. Experimental Setup

             - facebook:

               - login page, user's home page, "friend profile page"

               - warm and cold of home and profile pages

             - imdb:

               - home page, search results, movie, celebrity

               - warm and cold for each page

             - artificial transition probabilities

             - facebook via fixed path

          2. Results

             - perfect for facebook,

             - still very good for imdb

    7. DISCUSSION

       - "Existing defenses are inadequate."

       - "Traffic analysis can infer user actions through several
         different side channels."

         Panchenko good results. Theirs good "even if all packet size
         information is removed from the trace"

       - "The DLSVM classifier generally outperforms other classifiers."

         - more generic: trace passed "directly into the classifier"

       - "Defenses based on randomized requests and cover traffic are
         not likely to be effective."

         with their special randomized request (random form of l
         \pm 1500)

       - "This attack is practical in real settings."

    8. CONCLUSION

       - HTTPOS, randomized pipelining, traffic morphing were weak

       - new defense

       - this ignores packet sizes

       - web site classifier,

         - sequence of page loads,

         - HMM
*** quotes
    - Our attack converts traces into strings and uses the
      Damerau-Levenshtein distance to compare them.
    - (ends 1)
    - they are a good match for the attacker scenario faced by many
      Tor users today: they use Tor toevade censorship and persecution
      by a government or ISP that wants to know their browsing habits
      and has the ability to monitor their internet connection, but
      cannot easily infiltrate Tor nodes and web servers outside the
      country.
    - (ends 2.0)
    - these edits correspond to packet and request re-ordering,
      request omissions (e.g. due to caching), and slight variations
      in the sizes of requests and responses.
    - a better approach would be to learn optimal costs from the
      training data using the recently-proposed method of Bellet, et
      al.
    - also rounds all packet sizes *up* to a multiple of 600
    - Other normalization factors, such as |t| + |t_{0}| and
      max(|t|, |t_{0}|), yielded worse results.
    - The γ parameter is used to normalize L so that it’s outputs fall
      into a useful range. In our experiments, we found γ = 1 works
      well.
    - We tried representing traces as a sequence of Tor cells instead
      of as a sequence of packets. Classifier performance degraded
      slightly, suggesting that the Tor cells are often grouped into
      packets in the same way each time a page is loaded.
    - neighbor algorithm: to classify trace t, the attacker computed
      t^{∗} = argmin_{t'} L(t, t') over every trace in his database, and
      guessed that t was from the same web page as t^{∗}
    - Finally, we tried using a metric embedding to convert our
      variable-length trace vectors into fixed-length vectors in a
      space using the \ell_{2} - norm, and then used an SVM to classify
      these vectors. This performed substantially worse than the SVM
      classifier with distance-based kernel described above.
    - (ends 3)
    - for each *observation* o ∈ O and *HMM state* s, the probability,
      Pr[o|s], that the HMM generates observation o upon transitioning
      to state s.
    - pages p_{1} and p_{2} can be represented by a single state s only if
      Pr[o|p_{1}] ≈ Pr[o|p_{2}] for all observations o.
    - assumes that users all tend to navigate through a website in the
      same way.
    - ends (4)
    - A (d, ρ, τ ) BUFLO implementation transmits d-byte pack ets
      every ρ milliseconds, and continues this process for at least τ
      milliseconds.
    - (ends 5)
    - if a window had, say, 3 IMDB pages and 3 non-IMDB pages, we
      discarded it from the histogram.
    - (ends 6.2.2)
    - recently proposed randomized pipelining defense
    - has no ordering information
    - (ends 7)
*** ref [[file:./master.bib::ccs2012-fingerprinting][Cai et al. 2012: Touching Distance]]
** [[./ccs14.pdf][Cai - A Systematic Approach to Developing and Evaluating Website Fingerprinting Defenses]]
*** summary
    0. [@0] ABSTRACT

       - systematic analysis of features

       - proven lower bounds of bandwidth cost

       - mathematical framework for open-world given close-world

       - tamaraw, better than BuFLO

    1. INTRODUCTION

       fingerprinting attacks

       - dyer: 80%, which of 128 pages (5)

       - cai: 75% against countermeasures (3)

       - Cai: bundle defenses inffective (13)

       - Luo: HTTPOS (11)

         - Cai: little benefit

       - Wright: traffic morphing (19)

         - Dyer, Cai: little protection

       - Dyer: BuFLO

       - real world vs close-world (14)

       - danger in real world

       - state-of-the-art: only lower bound

       - ideal attacker: websites distinguishable unless exact same
         pattern

       - abstract model:

         - how far from optimal,

       - which traffic features leak most information

       - provably secure: tamaraw

       - evaluate tamaraw with above techniques

    2. WEBSITE FINGERPRINTING ATTACKS

       - cai and chen aim at identifying web sites instead of web
         pages

       - wf explained

         - only encrypted proxy

         - page has characteristic dl/ul traffic pattern

       - two assumptions retained

         - page start noticeable

         - no background traffic (file downloads, music streaming, etc)

    3. FEATURES AND METHODOLOGY

       wf tries to classify by features, defense tries to hide them

       1. Packet Sequences and their Features

          - time and length (positive for outgoing, negative for incoming)

          - unique packet lengths (problem with tor)

            (∃L ∈ P_{\ell} | L \not∈  P'_{\ell}) ∨ (∃L ∈ P_{\ell}' | L \not∈ P_{ell}' )

            exists a length L
            which is in P, but not P'
            or in P', but not P

          - packet length frequency (how often packet length occurs)

            \exists L | n_{L}(P_{l}) \neq n_{L}(P_{l}') \wedge n_{L}(P_{l}) > 0 \wedge n_{L}(P_{l}') > 0

            exists a length L
            which occurs n_L times in P and not n_L times in P'
            and with both occurances greater than 0

          - packet ordering:

            for the multiset of packet lengths M(P)
            M(P) = M(P')
            and P \ne P'

          - interpacket timing:

            two packets cannot be dependent, if their interpacket
            times is less than one RTT

            exists 1 \le i \le min(|P|, |P'|)
            such that the timing t(P_i) \ne t(P'_i)

          - this is a complete feature set (fact 1) (?td: think?)

          - features are rather independent (fact 2) (?)

       2. Comparative Methodology

          - "To determine if a defense is able to hide a feature, we
            apply the defense to two classes, C and C 0 , which differ
            only by that feature. Then, we say that a defense is
            successful in hiding the feature if after applying the
            defense, there is no discernible difference between C and
            C 0."

          - several generators

            1. small changes G_{1}: length + v, upto MTU
            2. large changes G_{2}: length + 1000, upto MTU
            3. length diffusion G_{3}: increased by position i/5, upto MTU
            4. append incoming packets G_{4}: length MTU
            5. append outgoing packets G_{5}: length first outgoing
            6. insert incoming packets G_{6}: length MTU, one per 5 packets
            7. Adjacent Transpositions: "v packets are transposed with
               the previous packet"
            8. Short-Distance Transpositions: v packets are transposed
               with the packet 4 elements ago.
            9. Long-Distance Transpositions: v packets are transposed
               with the packet 19 elements ago.
            10. Delays: Each packet is delayed by a linearly
                increasing amount of time, multiplied by v.

       3. Classification and Experimental Setup

          C = 400 samples of bbc.co.uk
          C' = generator(C)

          200 training, 200 testing

          4 feature classifiers

          - Unique Packet-Lengths: (like jaccard of Liberatore)

          - Packet-Length Frequencies: mean, std of (bytes and
            packets) (incoming and outgoing)

            scored separately, multiplied (like naive bayes of Liberatore)

          - Packet Ordering: each position: length compared to mean of
            all training packet length  (like bissias/liberatore)

          - Interpacket Timing: total elapsed time

          defense applied to each element c and c'

          measured by the differences between C and c' before
          classifier can distinguish

          setup: 100mbps ethernet, mtu 1500, imacros 9.00 firefox
          23.0, tcpdump

    4. COMPARISON OF DEFENSES

          state-of-the-art defenses, simulated

       1. Simulated Defenses

          - Maximum Packet Padding (PadM): pads all to mtu

          - Exponential Packet Padding (PadE): pad to closest power of 2

          - Traffic Morphing (Wr-Morph): mimic target page

          - HTTP Obfuscation (HTTPOS): client-side only, tcp
            advertised windows, http ranges, control sizes of
            outgoing and incoming

            (here: just split packet without extra packets)

          - Background Noise (Pa-Decoy): load decoy in background

            (here: alexa top 800)

          - Buffered Fixed Length Obfuscator (BuFLO): packets at
            fixed intervals with fixed lengths

       2. Comparative Results

          - "The full results are given in Table 3"

          - v from 1 to 180,

            - best feature classifier

            - minimum value v for 55 % accuracy

            - minimum value v for 75 % accuracy

            - * means unable to

          - PadM covers: unique packet lengths and orderings,
            better than PadE

            - both beaten by frequency analysis

          - HTTPOS broken (f.ex. packet ordering)

          - PaDecoy, BuFLO work against Panchenko and frequency attacks

          - Pa-decoy does not completely cover total time (fails
            half the time)

          - BuFLO similar over 10seconds

          - HTTPOS client-only

    5. THEORETICAL FOUNDATIONS

       Model of WF attacks, lower bounds for bandwidth overhead.

       1. Security vs. Overhead Trade-Off

          dissimilarity of websites increases overhead

          offline version

          1. Definitions

             - w: website

             - t: packet trace

             - W: random variable for w (attacker knows distribution)

             - T_{w}^{D}: random variable for t with defense (attacker knows D.)

             - T_{w}: random variable for t without defense

             - A(t) = argmax_{w} Pr[W = w] Pr[T_{w}^{D} = t]
               attacker output (determine website w)

             - D *non-uniformly \epsilon-secure* for W iff Pr[A(T_{W}^{D}) = W] ≤ \epsilon.

             - D *uniformly \epsilon-secure* if max_{W} Pr[A(T_{W}^{D}) = W] ≤ \epsilon.

             - B(t): total number of bytes transmitted in trace t.

             - BWRatio_{D}(W): E[B(T_{W}^{D})] / E[B(T_{W}^{})]
               bandwidth ratio of defense D

          2. Bandwidth Lower Bounds

             - THEOREM 1. Suppose n is an integer. Let W be a
               random variable uniformly distributed over w_{1}, ... ,
               w_{n}, i.e. W represents a closed-world
               experiment. Suppose D is a defense that is
               \epsilon-non-uniformly-secure against A_{S} on
               distribution W. Then there exists a monotonically
               increasing function f from S = {s_{1} , ... , s_{n}} to
               itself such that

               - |f(S)| ≤ \epsilon n
               - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).
               (see also quotes)

               - proof via properties and construction

             - A_{S}(t) = argmax_{w} Pr[B(T_{w}^{D}) = B(t)]

               optimal, looks only at total size

             - "Such an f is equivalent to a partition S_{1}, ... , S_{k}
               of S satisfying k ≤ \epsilon n and minimizing
               \sum_{i=1}^{k} |S_{i}| max_{s \in S_{i}} s.

             - THEOREM 2. Let W be uniformly distributed over w_{1},
               ... , w_{n}, i.e. W represents a closed-world
               experiment. Suppose D is a deterministic defense
               that is uniformly-\epsilon-secure against A_{S} on
               distribution W. Then there exists a monotonically
               increasing function f from S = {s_{1} , ... , s_{n}} to
               itself such that

               - min_{i}|f^{-1}(s_{i})| \ge  1/ \epsilon
               - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).
               (no proof)

       2. From Closed to Open World

          - "researchers need only perform closed-world experiments
            to predict open-world performance."

          - single w^{*}, find out if visited or not

          - construct open-world from closed-world by selecting
            websites w_{2}, ..., w_{n} and determining if A(t) = w^{*

          - compute false-positive rate by (p_{i} probability of w_{i})

          - R_{n} = 1/n \cdot Pr[A(T_{w*}^{D}) = w^{* }] + \sum_{i=2}^{n} Pr[A(T_{wi}^{D}) = w_{i}^{}]
            "the average success rate of A in the closed world"

          ... compute FPR, TPR, TDR (true-discovery rate)

          - algorithm for lower bound \epsilon-secure fingerprinting defense
            against A_{S} attackers

    6. TAMARAW: A NEW DEFENSE

       theoretically provable BuFLO

       1. Design

          1. Strong Theoretical Foundations:

             optimal partitioning and feature hiding against A_{S}
             attackers

          2. Feature coverage:

             not only total size, but all features (except for total
             downstream transmission size)

          3. Reducing Overhead Costs:

             reduces BuFLO's overhead (bandwidth and time)

          differences to BuFLO:

          - 750 bytes, not MTU (most packets)

          - distinguish incoming/outgoing

          - time to next supersequence, not fixed

          Tamaraw as follows:

          - "We denote the packet intervals as ρ_{out} and ρ_{in}
            (measured in s/packet)."

          - "In Tamaraw, however, the number of packets sent in
            both directions are always padded to multiples of a
            padding parameter, L"

       2. Experimental Results

          0. [@0]

             - "our objective in the choice of ρ_{in} and ρ_{out} is to
               minimize overhead."

             - "as ρ in and ρ out increased, size overhead decreased
               while time overhead increased"

             - padm better in some accounts

          1. An Ideal Attacker

             - "evaluate the partitions produced by Tamaraw"

             - "For a partition of size |S|, the attacker can at
               best achieve an accuracy of 1/|S| on each site in
               the partition."

             - capture single instances of top800, reset browser state
               in-between

             - pages indistinguishable if the same packet sizes
               (!except for timing info!)

             - upper bound of success for perfect attacker (via
               deterministic defense)

          2. Closed-world Performance

             much better overhead ratio than BuFLO (configurable)

          3. Open-world Performance

             Much better than against Tor, BuFLO

    7. CODE AND DATA RELEASE

       all available (notes: ask)

    8. CONCLUSIONS

       classify and qualify WF defenses

       tamaraw

    9. ACKNOWLEDGMENTS

       Panchenko talked
*** quotes
    0. [@0] ABSTRACT
       - the Tor project now includes both network- and browser-level
         defenses against these attacks
    1. INTRODUCTION
       - an attacker could infer, with a success rate over 80%, which of
         128 pages a victim was visiting, even if the victim used
         network-level countermeasures.
       - In our ideal attack, two websites are distinguishable unless
         they generate the exact same sequence of network traffic
         observations.
       - BuFLO unnecessarily wastes bandwidth hiding the number of
         upstream packets and does not adequately hide the total number
         of downstream packets.
    2. WEBSITE FINGERPRINTING ATTACKS
       - The structure of a page induces a logical order in its packet
         sequence.
       - This means that the attacker is weak, but is also resource-light
         and essentially undetectable
    3. FEATURES AND METHODOLOGY
       1. Packet Sequences and their Features
          - We indicate the packet length as a positive value if the packet
            is outgoing and as a negative value if it is incoming.
    4. COMPARISON OF DEFENSES
       1. Simulated Defenses
          - Packets are sent at fixed intervals with fixed length, and if no
            data needs to be sent, dummy packets are sent instead.
       2. Comparative Results
          - Pa-Decoy fails to completely cover interpacket timing because it
            only covers the total transmission time roughly half the time
            (i.e., when the decoy page takes longer to load than the desired
            page)
    5. THEORETICAL FOUNDATIONS
       0. [@0]
          - show how to derive open-world performance from closed-world
            experimental results
       1. Security vs. Overhead Trade-Off
          1. Definitions
             - a set of similar websites can be protected with little overhead,
               a set of dissimilar websites requires more overhead.
             - DEFINITION 1. A fingerprinting defense D is *non-uniformly
               \epsilon-secure* for W iff Pr A(T_W^D) = W ≤ \epsilon. Defense D is *uniformly
               \epsilon-secure* for W if max_w Pr A(T_w^D ) = w ≤ \epsilon.

               These are information-theoretic security definitions –
               A is the optimal attacker described above. The first
               definition says that A’s average success rate is less
               than, but it does not require that every website be
               difficult to recognize. The second definition requires
               all websites to be at least \epsilon difficult to
               recognize. All previous papers on website
               fingerprinting attacks and defenses have reported
               average attack success rates in the closed-world model,
               i.e. they have reported non-uniform security
               measurements.
          2. [@2] Bandwidth Lower Bounds
             - Intuitively, f represents a mapping from each website’s
               original size (s_{i}) to the number of bytes that D
               transmits when loading website w_{i}.
       2. From Closed to Open World
          - if the fingerprinting attacker is a government monitoring
            citizens Tor usage, then W would be distributed according to the
            popularity of websites among that nation’s Tor users.
          - Cai, et al., showed that the Alexa top 100 websites were about
            as similar as 100 randomly chosen websites [3], i.e. that the
            most popular websites are not particularly similar to eachother.
          - true-discovery rates for the open-world attack and defense
            evaluations in this paper. Given an open-world classifier, C,
            its true-discovery rate is defined as TDR(C) = Pr[W = w^∗ |
            C(T_W^D) = 1]. Intuitively, the true-discovery rate is the
            fraction of alarms that are true alarms.
    6. TAMARAW: A NEW DEFENSE
       1. Design
          - In our implementations of BuFLO and Tamaraw, we pessimistically
            required that the original logical ordering of the real packets
            must be maintained.
       2. Experimental Results
          0. [@0]
             - A practical implementation could achieve a lower size and time
               overhead as re-ordering is possible for both defenses when
               subsequence is not consequence;
          1. An Ideal Attacker
             - we eliminate the network variability and make the
               defense system deterministic, which, as shown in the
               Appendix, does not reduce the security of the defense.
          2. Closed-world Performance
             - at a size overhead of 130%, there are 553 partitions
               (non-uniform security of 69%) in BuFLO (τ = 9) and 18 partitions
               (non-uniform security of 2.25%) in Tamaraw.
          3. Open-world Performance
             - By showing that the TDR becomes extremely low when attacking
               Tamaraw, even for the first 100 websites, we show that it’s
               extremely low for all websites.
    7. CONCLUSIONS
       - The lower bounds of bandwidth costs are surprisingly low,
         suggesting that it may be possible to build very efficient
         defenses.
*** code
**** [[../sw/attacks/svm.py][svm.py]]
     #+BEGIN_SRC python
       #data is in this format:
       #each data[i] is a class
       #each data[i][j] is a standard-format sequence
       #standard format is: each element is a pair (time, direction)
     #+END_SRC
     - str_to_sinste: helper function, splits string
     - load_one: appends lines to data, returns
     - load_all: appends load_one to data, returns
     - extract: extracts features from data
       - sizemarkers: pad to 300 with 0
       - html size: my naive approach
       - total transmitted: sums up
       - number markers: pads to 300
       - unique packet: unique lengths (-/+)
       - percentage incoming
       - number of packets
     - "main"
       - splits data in test and training
       - saves test and training files
***** problemsmaybe:
      - unique packet no fixed length
**** [[file:~/da/git/sw/attacks/svm-run.py]]
     runs
     - python svm.py i
     - svm-train -c c -g g svm.train svm.model
     - svm-predict svm.test svm.model svm.resultst >> temp-acc
     for i folds from 1 to 10
**** [[file:~/da/git/sw/attacks/svmdotest.rb]]
     cleans up, runs
     - clgen_stratify cltor_matrix 36 40
     - svm-train -t 4 -c 1024
     - svm-predict
*** ref [[file:master.bib::a-systematic][Cai et al. 2014: Systematic Approach Developing]]
** [[./ronathan-heyning.pdf][Cheng - Traffic Analysis of SSL Encrypted Web Browsing]]
*** summary
    0. [@0] Abstract

       - SSL not designed as protection against traffic analysis, tested here

    1. Introduction

       - HTTP lacks encryption, provided by HTTPS

       - gives "false impression of [...] confidentiality"

       - easy to set up: metadata in packets (needed for Internet to work)

         - readily-available tools

       - most files have unique sizes ("Only 10% [...] non-unique")

    2. Traffic Analysis Techniques

       protocol issues, extract data from sniffed traffic, identify
       web pages using this data

       1. Protocol Issues

          - procedure

            1. request html

            2. ip packets with html code

            3. parse html, issue requests for embedded objects

            4. several ip streams with embedded objects

          - user behavior changes fingerprint: cancel request, disable
            images, etc

            - here: images only, assume full download of each

       2. Extracting Information from Sniffed Traffic

          - tcpdump as traffic sniffer

          - separate by ports

          - assume first is HTML (non-parallel)

          - assume others are images

          - caching: if turned on, identify only by HTML

       3. Page Identification

          - (HTML_size, object_size) tuple in DB

          - see if exact match

          - HMM for increased accuracy: (previous, current, next)
            tuples

    3. Defenses

       user should have the option: overhead/security tradeoff

       1. Protocol Modifications

          0. [@0]

             - layer between HTTP and SSL

               - easy to implement, as only need to modify MS-IE and
                 Netscape Navigator

          1. Random padding.

             - to each request add length from uniform distribution

             - SSL supported for block ciphers

          2. Constant size packets

             - pad to full-size packets

             - deterministic relationship web page to traffic sent

          3. Background traffic

             - "selective addition"

             - random enough to not be filtered out

             - pages much larger than others hard to hide

               - only by splitting/delaying, but "disruptive" to viewer

       2. Web site Modifications

          - break big page into smaller hyperlinked pages

          - insert additional data: spaces, comments, metatags, blank images...

          - unlikely to happen

       3. Web Proxies

          - single-hop: alice might sniff at proxy(in and out)

          - rewriting proxies

          - multi-hop

          - currently implemented

    4. Implementation

       - tcpdump, webcopy

       - crude parser for hperlinks and image references

         - ignores al other features (dynamic, java, ...) and external
           hyperlinks

       - simple implementation

    5. Results

       0. [@0]

          - http sites, as https just adds constant size increase

          - spider-accessible, large enough, mix of HTML and images,
            mostly static

          - also rich link structure

       1. Numerical Simulations

          - first browsing by hand to get a feel

          - then select websites by following mainly links

            - two hyperlinks, no outgoing links, or loop(0.8): select
              next at random

            - result: mostly landing pages

          - random padding: uniform to max size

          - next: unex site and link algorithm

          - much harder to trick classifier: needs much more padding (10x)

          - caching (70% for once-visited, 90% for twice) makes it
            much harder again

       2. User Testing

          - users with 2 second pause, browsed unex for 5-10 minutes

          - worked very well (96-94%), also without link algorithm (same)

            - link structure not needed for high accuracy

    6. Discussion and Conclusions

       - real threat

       - several defenses
*** quotes
    3. [@3] Defenses
       0. [@0]
          - defenses almost always impose some degree of overhead in
            terms of computing resources, network bandwidth, or
            latency. Therefore, the user should have the option of
            choosing among different levels of defenses (or no traffic
            analysis defense, for that matter), depending on the desired
            tradeoff between security and performance.
       1. Protocol Modifications
          - traffic analysis defenses for Web browsing should be
            encapsulated in a separate security protocol layer to mediate
            between the HTTP and SSL layers.
    4. Implementation
       - Both the HTML parser and the network traffic parser are
         incomplete due to time constraints.
    5. Results
       1. Numerical Simulations
          - The use of link analysis has the effect of making the
            attack much more resistant to padding, increasing by an
            order of magnitude the amount of padding needed to provide
            the same amount of protection.
          - random padding is much more effective against traffic
            analysis when caching is used, due to the loss of object
            size information.
*** ref [[file:master.bib::ssl-traffic-analysis][Cheng & Avnur 1998: Traffic Analysis SSL]]
** [[./Javascript Closures.prototype_chain.html][Cornford - Javascript Closures]]
*** summary
    1. Introduction

       - closure: free variables with an environment that binds them

       - complicated, easy to misuse, powerful

    2. The Resolution of Property Names on Objects

       0. [@0]

          - native vs. host, built-in \sub native objects

          - undefined: does not remove, but sets to undefined

       1. Assignment of Values

          - create properties by assigning,

            - either .property or ["property"]

       2. Reading of Values

          - object, then prototype chain, until =Object.prototype=

          - else undefined

          - overridden if defined in object

    3. Identifier Resolution, Execution Contexts and scope chains

       1. The Execution Context

          - "All javascript code is executed in an execution context."

          - Global context for sourced scripts in HTML

          - function context for each function call

          - =eval= has its own context, too

          - Activation object

            - not real object

            - hold =arguments= array-like in arguments property

          - scope is =\[\[scope\]\]= of function with Activation added in front

          - variables with Activation object for function's formal parameters

            - if not enough arguments, =undefined=

            - inner function definition

          - set value of =this=

            - if not set \to global object

       2. scope chains and \[\[scope\]\]

          - scope chain is constructed by prepending
            Activation/Variable object to function's [\[scope]]

          - Function object: scope is global object

          - function declarations and function expressions have a scope chain

          - global declaration / expression:

            - global object

            - expression evaluated lated, but still global object

          - inner declaration / expression:

            - scope of outer function, incluing Activation object

          - with

            - sets to scope chain

            - evaluates block

            - does affect function expressions

              - not function declarations

       3. Identifier Resolution

          - up the scope chain

          - function call: first Activation object for formal
            parameters, inner function declaration names or local
            variables, then up scope chain

    4. Closures

       1. Automatic Garbage Collection

          - all objects that are no longer used are freed

            - normally outside of scopes

       2. Forming Closures

          - example: =exampleClosureForm(arg1, arg2)= returning
            =exampleReturned(innerArg)= using =localVar=

          - =var globalVar = exampleClosureForm(2, 4);=

            - object called =globalVar= has a [\[scope]] property

            - scope is
              #+BEGIN_SRC js
                ActOuter1 = {
                    arg1: 2,
                    arg2: 4,
                    localVar: 8,
                    exampleReturned: [...inner function...]
                }
              #+END_SRC

          - execution:

            - new execution context, scope chain:
              =ActInner1-> ActOuter1-> global object=

          - nesting possible

    5. What can be done with Closures?

       "emulate anything"

       1. Example 1: setTimeout with Function References
          #+BEGIN_SRC js
            function callLater(paramA, paramB, paramC){
                return (function(){
                    // do sth with paramA, paramB and paramC
                });
            }
            var functRef = callLater(elStyle, "display", "none");
            setTimeout(functRef, 500);
          #+END_SRC

       2. Example 2: Associating Functions with Object Instance Methods

          attach event listener to DOM object
          #+BEGIN_SRC js
            /* Associates an object instance with an event handler. The returned
               inner function is used as the event handler.  The object instance
               is passed as the - obj - parameter and the name of the method that
               is to be called on that object is passed as the - methodName -
               (string) parameter.
            ,*/
            function associateObjWithEvent(obj, methodName){
                /* The returned inner function is intended to act as an event
                   handler for a DOM element:-
                ,*/
                return (function(e){
                    e = e||window.event;
                    return obj[methodName](e, this);
                });
            }
            /* Creates objects that associate themselves with DOM elements whose
               IDs are passed to the constructor as a string.
            ,*/
            function DhtmlObject(elementId){
                var el = getElementWithId(elementId);
                /* The following block is only executed if the - el - variable
                   refers to a DOM element:-
                ,*/
                if(el){
                    /* assign a function as the element's event handler */
                    el.onclick = associateObjWithEvent(this, "doOnClick");
                    el.onmouseover = associateObjWithEvent(this, "doMouseOver");
                    ...
                }
            }
            DhtmlObject.prototype.doOnClick = function(event, element){
                ... // doOnClick method body.
            }
            DhtmlObject.prototype.doMouseOver = function(event, element){
                ... // doMouseOver method body.
            }
          #+END_SRC

       3. Example 3: Encapsulating Related Functionality

          have a array which is filled on execution with various values

       4. Other Examples

          - crockford: private instance variables

            - extended to private static:
              http://myweb.tiscali.co.uk/cornford/js_info/private_static.html

    6. Accidental Closures

       - if used accidentally, can lead to less efficiency:
         #+BEGIN_SRC js
           var quantaty = 5;
           function addGlobalQueryOnClick(linkRef){
               if(linkRef){
                   linkRef.onclick = function(){
                       this.href += ('?quantaty='+escape(quantaty));
                       return true;
                   };
               }
           }
         #+END_SRC
         creates a function for each call to the function

         - better: assign function ref
         #+BEGIN_SRC js
           var quantaty = 5;
           function addGlobalQueryOnClick(linkRef){
               if(linkRef){
                   linkRef.onclick = forAddQueryOnClick;
               }
           }
           function forAddQueryOnClick(){
               this.href += ('?quantaty='+escape(quantaty));
               return true;
           }
         #+END_SRC

       - same for object methods
         #+BEGIN_SRC js
           function ExampleConst(param){
               this.method1 = function(){
                   ... // method body.
               };
               this.method2 = function(){
                   ... // method body.
               };
               this.method3 = function(){
                   ... // method body.
               };
               this.publicProp = param;
           }
         #+END_SRC
         creates new function objects for each object

         - better:
           #+BEGIN_SRC js
             function ExampleConst(param){
                 this.publicProp = param;
             }
             ExampleConst.prototype.method1 = function(){
                 ... // method body.
             };
             ExampleConst.prototype.method2 = function(){
                 ... // method body.
             };
             ExampleConst.prototype.method3 = function(){
                 ... // method body.
             };
           #+END_SRC
           create the functions just once

    7. The Internet Explorer Memory Leak Problem

       circular references were not cleaned up

       example see quotes
*** quotes
    3. [@3] Identifier Resolution, Execution Contexts and scope chains
       1. The Execution Context
          - running javascript code forms a stack of execution contexts.
    4. Closures
       2. [@2] Forming Closures
          - The ECMAScript specification requires a scope chain to be finite
    5. [@7]
       - If a function object that forms a closure is assigned as, for
         example, and event handler on a DOM Node, and a reference to
         that Node is assigned to one of the Activation/Variable
         objects in its scope chain then a circular reference
         exists. DOM_Node.onevent -> function_object.[[scope]] ->
         scope_chain -> Activation_object.nodeRef -> DOM_Node
** Crockford - on JavaScript - Section 8 Programming Style _ Your Brain
*** quotes
    - Good use of style can help reduce the occurrence of Errors.
    - Avoid forms that are difficult to distinguish from common errors.
    - Make your programs look like what they do.
    - Write in a way that clearly communicates your intent. (that's
      what we should be doing as programmers)
    - Good style is good for your gut.
*** summary
    - gut vs brain: book recommendation
      - advertising
        - smoking
    - jslint
      - comp.lang.javascript mailing list
      - switch fail in jslint
    - with is broken
    - == is broken (M$), always use ===
    - scope good idea, came with algol 60
      - js only has function scope, not block scope
      - in function scope declare var at top of function
    - use \+= 1 instead of ++
** [[./TAIntro-book.pdf][Danezis - Introducing Traffic Analysis]]
*** summary
    1. Introduction

       - TA used by bletchley park@german air force, japan@pearl
         harbour, google, amateurs@CIA

    2. Military roots

       - history: ww1, ww2, iraq, etc

       - military finds info

       - easier to gather, can be done by machines

       - fingerprint f.ex. radio devices, also license plates, etc

       - Signals Intelligence (SIGINT): military branch including TA

       - 3 ways:

         - frequency hopping: hard to jam, but easy to detect
           communication

         - direct sequence spread spectrum: transform
           high-power-low-bw, to low-power-high-bw signal

         - burst communication: very short burst

           - "meteor scatter"

           - needs high availability of communication at other endpoint

    3. Civilian traffic analysis

       - social networks

         - also called power law networks (number of connections
           governed by power laws)

         - resilent vs random failures, easy to disrupt by specific targeting

         - reconfigure when attacked: random connections, and again
           when calm: towards efficiency

       - target groups

         - select specialists, not leaders

         - Saddam Hussein caught by surveilling individuals with close
           ties to him

       - detect islamists, RAF, cannabis farms,

       - social structure similar to hub-spokes of Google PageRank

    4. Contemporary Computer and Communications Security

       0. [@0]

          - jamming \equiv censoring, abuse, spam \equiv deception

          - low-power, not military actors

       1. The Traffic Analysis of SSH

          each character transmits packet, length and timing can
          reveal keywords to HMM

       2. The Traffic Analysis of SSL

          - leaks much information, especially sizes

          - combine with HMM to model link structure, as browsing is
            not random

       3. Web Privacy

          - can see if f.ex. item of competitor's web site is cached

       4. Network Device Identification and Mapping

          - clock skew identifies devices

            - alters with heat, which alters with workload

          - IP-IPID field says how many windows devices

            - ipv6 might use mac address

              - possible to cloak

              - but that would naively reveal that it is cloaked

          - nmap determines many things

            - snort scans for nmap, but that can be circumvented

       5. Detecting Stepping Stones

          detect retransmission of data (attack one host, then the next)

    5. Exploiting Location Data

       - reveal social relationships

       - for some users: predict next move and location from location
         and time

    6. Resisting Traffic Analysis on the Internet

       - started by Chaum

       - 20 years of research

       - Mixmaster and Mixminion for email

       - Tor and JAP for "web browsing"

       - increase latency and traffic volume

       - intersection attacks: find out who is communicating with whom

         - simplest attack: packet counting (into network ==? out of network)

         - template model: "match stream with other streams"

       - infiltrate network

         - predecessor attack: crowds: real sender will appear as
           predecessor more often

       - weaker adversaries

         - low-cost: streams influence one another

         - clock-drift etc as mentioned above

         - (clock-drift with VMs?)

       - other ta helper: timing

       - discover node's role by looking at traffic patterns: weather
         station sends hourly ...

    7. Data Retention

       - data preservation: when crime has happened

         - vs data retention: before crime

       - should be aware of all the stuff that can be extracted to
         protect "law-abiding citizens"

    8. And finally...

       open research field, might circumvent other security measures
*** quotes
    3. [@3] Civilian traffic analysis
       - example is finding a job, where people using ‘far links’ are
         on average more successful, than those who limit themselves
         to their local contacts.
       - defensive strategies is that nodes connect to other random
         nodes in order to get resilience, while connecting according
         to a power law strategy to get efficient routing.
       - It was found to be more effective to arrest the
         ‘specialists’, i.e. those people in the organization that
         have a unique position or skills, that others would find
         difficult to fill.
    4. Contemporary Computer and Communications Security
       2. [@2] The Traffic Analysis of SSL
          - a hidden Markov model can be used to trace the most likely
            browsing paths a user may have taken,. This approach
            provides much faster and more reliable results than
            considering users that browse at random, or web-sites that
            have no structure at all.
    5. Exploiting Location Data
       - So the evidence from these preliminary studies is highly
         suggestive that whatever the wireless medium used, mobile
         phone, wireless LAN or bluetooth, sensitive information about
         your identity, your relations to others and your intentions
         can be inferred merely though traffic analysis.
    7. [@7] Data Retention
       - there is significant scope for drilling down to reveal the
         most private of information about activities, habits,
         interests and even opinions. Storing this data, in an easily
         accessible manner, represents a systemic vulnerability that
         cannot be overstated enough.
*** TODO ref [[file:master.bib::introta][Danezis & Clayton 2007: Introducing Traffic Analysis]]
** [[./4b0fa48670a7269523b1166ad302440876da.pdf][Danezis - k-fingerprinting: a Robust Scalable Website Fingerprinting Technique]]
*** summary
    0. [@0] Abstract

       - better, even against defenses

         - and noisy data

       - (85% vs 0.02%)

       - world size 100'000

       - error rates vary widely

    1. Introduction

       - analyze "order, timing and volume of resources"

       - classification

       - contributions

         - new attack: more accurate and faster

       - feature analysis

       - bigger open world

       - train only small fraction of data

       - uneven error rate

       - tor does not offer additional defenses vs normal browsing

         - distinguish hidden services

    2. Related Work

       1. Website Fingerprinting.

          - Wright et al.'s traffic morphing helped against early
            size-based attacks

          - Panchenko et al.'s decoy reduced accuracy from 55% to 5%

          - Luo et al.'s HTTPOS "was successful in defending against a
            number of classifiers"

          - Dyer et al.'s BuFLO improved upon others with "high
            bandwidth overhead"

            - Cai et al. improved with rate adaptation

            - Nithyanand: Glove groups website traffic, "provides
              information theoretic privacy guarantees and reduces the
              bandwidth overhead by intelligently grouping web traffic
              in to similar sets."

          - Cai et al. improved Panchenko's attack

            - improved by WG (0.95 vs 0.002)

          - Wang et al, kNN open-world (0.85 vs 0.006)

          - WG half-duplex

          - WG practical deployment: parse on 1.5 second-gap

          - Gu et al: multi-tab, 50 websites first @ 75.9%, background @ 40.5%

          - Kwon et al: apply to hidden services: 50 hs, (88% vs 7.8%)

          - Panchenko: CUMUL, huge, suffers from simple defenses

       2. Random Forests.

          - "no need for k-fold cross validation to measure
            k-fingerprinting performance"

    3. Attack Design

       0. [@0]

          - "aims to define a distance-based classifier."

       1. k-fingerprints from random forests

          - each forest gives a leaf identifier for each trace

            - choose k-closest by hamming distance

          - vary k for a TPR/FPR trade-off

            - classify if all k agree

            - majority label also possible

       2. The k-fingerprinting attack

          - select monitored pages

            - collect monitored and some non-monitored traces

            - generate fingerprints

              - (fixed-length)

          - listen to client's browsing

            - generate fingerprint

            - compute k closest training examples by hamming distance

            - say monitored if all k agree that monitored

          - measurements: TPR, FPR, and

            - BDR:  (TPR. Pr(M)) / (TPR. Pr(M) + FPR. Pr(U))
              Bayesian Detection Rate (BDR):

              with Pr(M) = |Monitored| / |Total Pages|,
              and  Pr(U) = 1 − P(M).

              - "probability that the classifier made a correct prediction"

    4. Data gathering

       - normal (Selenium + PhantomJS)

         - 55@30 monitored vs 7000 unmonitored

       - and Tor Browser (Selenium)

         - 55@100 monitored

         - 30@80 hidden services

         - unmonitored: 100'000 top alexa - top 55

       - compare to WG 100@90 + 5000 unmonitored (random from Alexa top 10'000)

       - Nielsen: number of unique websites/month: 89 avg

    5. Feature selection

       - few previous studies

       - randomized forest, use gini coefficient to determine feature
         importances

         - 30 most important features contain most of the info

         - 150 used as it did not increase running time significantly

         - top ~12 really important

         - assign rank as average of 100 experiments

       - feature importances

         - most important (@.. are fixed positions, rest vary)

           1. [@1] number of incoming packets

           2. number of outgoing packets

           3. number of incoming packets as fraction of total

           4. [@4] standard deviation of packet ordering list (how many
              packets of same type before this)

           5. number of outgoing packets as fraction of total

         - other important features:

           - packet ordering incoming/outgoing average/stddev

           - mean of number of outgoing packets in each 20-chunk

           - split number of packets into 20 evenly-sized set
             (alternative), average of this

    6. Attack on Hardened Defenses

       - Wang dataset: 100@90 + 5000@1 background

       - better and faster than both kNN and CUMUL

       - also against many defenses: dummy, tamaraw, buflo, wfpad

       - Juarez' Adaptive Padding works down to 30% accuracy

    7. k-fingerprinting the Wang et al data set

       - train on 60 instances for each of the 100 monitored web pages

       - vary number of unmonitored

         - all as one single class

       - feature numbers 200 and 150

       - classify if all k agree

       - 88% (\pm 1) vs 0.5% (\pm 0.1)

         - better than kNN (85% (\pm 4) vs 0.6% (\pm 0.4)

       - best with training 3500 unmonitored, afterwards does not increase

       - Fingerprint length

         - set k=1 with 4000 unmonitored

         - one fingerprint: 51% vs 90%

         - 20 fingerprints: 87% vs 1.3%

         - does not get much better

    8. Attack evaluation on DS_{Tor}

       55@100 vs 100'000 and 30@80 hidden services vs 100'000

       1. Alexa web pages monitored set

          - (k=2) the more training pages, the better the BDR

            - due to reduction of FPR

       2. Hidden services monitored set

          - lower TPR, but also lower FPR

            - BDR stays very high

    9. Attack evaluation on DS_Norm

       standard encrypted web browsing or through VPN

       1. Attack on encrypted browsing sessions

          - additional features:

            - size transmitted

            - size transmitted statistics of total, incoming and outgoing:

              - average,

              - standard deviation, variance

              - maximum

          - which web page from several websites

            - 55@30 monitored vs 7000 other pages

          - "larger cardinality of world size gives rise to more
            opportunities for incorrect classifications"

          - 0.87 vs 0.004 (training with 4000 unmonitored web pages)

       2. Attack without packet size features

          - TPR - 5%, FPR + 0.1% (closed and open world)

            - "BDR is dominated by the amount of information that can
              be trained upon."

    10. Fine grained open-world false positives on Alexa monitored set of DS_{Tor}

        - some pages are misclassified often

        - removing them leads to more misclassification

        - smart removal: split set into train, test, validation (?)

    11. Attack Summary & Discussion

        1. Attack Summary

           - best results "when training on approximately two thirds
             of the unmonitored web pages"

             - but 2% of data also yields "a very small false positive
               rate"

           - number of fingerprints has "greater influence"

           - robust: similar results for Wang, Kwon, own DS_...

           - possible to select targets with low misclassification
             rates (misclassification is page-dependent)

        2. Computational Efficiency: comparable to kNN, much faster
           than Cai's approach

        3. Discussion

           - big data sets feasible with high BDR and low FPR

           - fast-changing website, news etc, decay faster

    12. Conclusion

        - serious

        - faster and more accurate

        - "twice as large in terms of unique website"[s] as panchenko 2016

        - four datasets

        - small fraction of total data to train

        - remove bad sites
*** quotes
    1. Introduction
       - Traditional encryption obscures only the content of
         communications and does not hide metadata such as the size
         and direction of traffic over time. Anonymous communication
         systems obscure both content and metadata, preventing a
         passive attacker from observing the source or destination of
         communication.
    4. [@4] Data gathering
       - By including website visits to trending topics we argue that
         this diminishes the ability to properly measure how effective
         a website fingerprinting attack will perform in general.
    8. [@8] Attack evaluation on DS_{Tor}
       1. Alexa web pages monitored set
          - an attacker needs to train on less than 10% of the entire
            dataset to have nearly 70% confidence that classifier was
            correct when it claims to have detected a monitored page.
*** ref [[file:master.bib::kfingerprint][Hayes & Danezis]]
**** TODO check which version to cite
** [[../sw/js/jasondavies_bloomfilter.js%20%C2%B7%20GitHub.html][Davies - bloomfilter.js]]
*** summary
    - bloom filter using Fowler–Noll–Vo hash function
    - creation
      #+BEGIN_SRC js
        var bloom = new BloomFilter(
          32 * 256, // number of bits to allocate.
          16        // number of hash functions.
        );
      #+END_SRC
    - adding querying
      #+BEGIN_SRC js
        // Add element to the filter.
        bloom.add("foo");

        // Test if an item is in our filter.
        bloom.test("foo"); // Returns true if an item is probably in the set,
        bloom.test("blah"); // false if an item is definitely not in the set.
      #+END_SRC
    - serialisierung
      #+BEGIN_SRC js
        // Serialisation.
        var array = [].slice.call(bloom.buckets),
            json = JSON.stringify(array);

        // Deserialisation.
        var bloom = new BloomFilter(array, 3);
      #+END_SRC
      - better: typed array for performance (=array= is used directly)
** [[./challenges.pdf][Dingledine - Challenges in deploying low-latency anonymity]]
*** summary
    0. [@0] Abstract

       social challenges and technical issues for a low-latency
       anonymity network

    1. Introduction

       TOR has grown

    2. Background

       Tor design, properties, compare to other

       1. Tor, threat models, and distributed trust

          0. [@0]

             - forward privacy: hides connections

             - location-hidden services, directory servers, circuit, exit policies

          1. Threat models and design philosophy

             - tries to maximize anonymity given practicality and
               usability as fixed

             - weaker threat model: does not defend against global
               observer

             - there are known intra-network and end-do-end anonymity
               breaking attacks

             - "Tor only attempts to defend against external observers
               who cannot observe both sides of a user’s connections."

          2. Distributed trust

             - "mutually distrustful users" possible

             - increases anonymity for every party involved

               - as otherwise, f.ex. government would be identifyable

             - built-in encryption and authentication for "enclave
               approach"

       2. Related work

          - Mixmaster and Mixminion: high-delay, high-security

          - single-hop: single point of failure, easy for eavesdropper

          - JAP: web browsing

          - Freedom network: transport IP packets, out of money:
            collect users' money

          - Tarzan, MorphMix: p2p, not fielded, latter very similar to Tor

    3. Social challenges

       Tor's image impacts its users' security

       1. Communicating Security

          - Perceived security by others is a value to the user, as it
            attracts more users, which provides a bigger anonymity set

          - how to communicate this? (JAP "anonymity meter" inaccurate)

       2. Reputability and perceived social value

          - better to have reputable users than irreputable

            - public support, political climate

            - want to attract them, also OR-operators

          - better to have more diverse groups than only reputable users (?)

          - good uses often kept private (privacy network)

       3. Sustainability and incentives

          - need to keep OR-operators happy

          - bandwidth limiting, also per billing cycle

       4. Bandwidth and file-sharing

          - problem because it reduces useful traffic "bandwidth"

            - and because of legal implications

          - technically difficult to block

          - little used, because of bandwidth

       5. Tor and blacklists

          - keep abusers from making tor banned

          - example: wikipedia ip-based blocking

          - example: freenode IRC labelled all Tor-IP-users as
            "anonymous users", stopped problem

    4. Design choices

       technical issues

       1. Transporting the stream vs transporting the packets

          - numerous IP-leve challenges:

            - TCP fingerprinting,

            - application scrubbing,

            - dns leaks and redesign name space

            - unclear crypto (TLS?)

            - even TCP needs tuning of network params

            - exit policies for IP packets harder

       2. Mid-latency

          - some need to resist traffic correlation attacks

          - protect only some transactions

          - usability might suffer

          - (again) allow clients to "label certain circuits as mid-latency"

       3. Enclaves and helper nodes

          - running your own OR

          - need to increase default hop length (entry is itself
            "sensitive")

          - defend against end-to-end attacks

          - "helper node" to defend against this triangulation attack

          - randomized path length helps, but network (was?) still too
            small

       4. Location-hidden services

          - hard to completely anonymify

          - hot-swap hidden services would solve some problems, but
            hard to design see paper (angelos...)

          - also used to connect to your private network via ssh

          - increased robustness if used with dual-IP

       5. Location diversity and ISP-class adversaries

          - how much can ISP see?

          - best to use Tier-1 ISPs "such as AT&T and Abovenet"

          - many open questions

            - global adversary which knows algorithm

            - how to select nodes? (too much data to send)

       6. The Anti-censorship problem

          - tor well suited due to distribution: can volunteer IP

          - how to distribute: trust metric?

    5. Scaling

       how to scale to millions of users

       1. Incentives by Design

          - social: good thing to do

          - useful: deniability, need network to be up

          - easy: rate limiting, packaging, configurable exit policies

          - direct incentives possible:

            - money-based systems have often failed

            - maybe tit-for-tat: better service gives you better service

              - could create anonymity problems, requires further study

       2. Trust and discovery

          - state: sign up new nodes manually (?still?)

          - development direction depends on threat model

            - if small: scale each single nodes' bandwidth

            - else: make adding nodes to Tor easier

       3. Measuring performance and capacity

          - designers want to know much, but this is an anonymity
            system

          - self-reported bandwidth (?changed now?

       4. Non-clique topologies

          - possible solution: split network

          - danezis proposed expander graphs, but for high-latency nets

          - backbone approach: main routers well-connected

    6. The Future

       1. volunteer-based will work

       2. needs better protocol-aware proxies

       3. need reputation for begin good

       4. architecture scale to meet demand
*** quotes
    1. Introduction
       - Tor is an overlay network for anonymizing TCP streams over the Internet
    2. Background
       1. Tor, threat models, and distributed trust
          1. Threat models and design philosophy
             - Because of our strategy, Tor has a weaker threat model
               than many designs in the literature.
             - Tor does not attempt to defend against a global observer.
          2. Distributed trust
             - Our defense lies in having a diverse enough set of
               nodes to prevent most real-world adversaries from being
               in the right places to attack users, by distributing
               each transaction over several nodes in the network.
               [...]
               No organization can achieve this security on its
               own. If a single corporation or government agency were
               to build a private network to protect its operations,
               any connections entering or leaving that network would
               be obviously linkable to the controlling
               organization. The members and operations of that agency
               would be easier, not harder, to distinguish.
    3. Social challenges
       0. [@0]
          - In particular, the Tor project’s *image* with respect to its
            users and the rest of the Internet impacts the security it can
            provide.
       1. Communicating security
          - Usability for anonymity systems contributes to their
            security, because usability aﬀects the possible anonymity
            set [1, 4].
       2. Reputability and perceived social value
          - Thus, reputability is an anonymity issue [...]
    4. [@4] Design choices
       2. [@2] Mid-latency
          - A trade-off might be worthwhile even if we could only
            protect certain use cases, such as infrequent
            short-duration transactions.
       3. Location diversity and ISP-class adversaries
          - The key insight from their paper is that while we
            typically think of a connection as going directly from the
            Tor client to the first Tor node, actually it traverses
            many different ASes on each hop.
*** ref [[file:master.bib::challenges][Dingledine et al.: Challenges]]
** [[./tor14design.pdf][Dingledine - Tor: The Second-Generation Onion Router (2014 DRAFT v1)]]
*** summary
    0. [@0] Abstract

       - real-world experiences

       - open problems

    1. Overview

       - Better than original onion routing by:

         - perfect forward secrecy:

           "subsequently compromised nodes cannot decrypt old traffic"

         - Separation of “protocol cleaning” from anonymity

           just uses SOCKS for applications to connect. (protocol
           cleaning is done f.ex. by addon or proxy)

         - No mixing, padding, or traffic shaping (yet):

           no usable concepts/implementations, high overhead

         - Many TCP streams can share one circuit:

           allows for multiple streams to have same circuit (with user
           control)

           less crypto, less vulnerability (see section 9)

         - Leaky-pipe circuit topology:

           traffic can exit at any place in the circuit (how about
           exit node policies?)

         - Congestion control:

           end-to-end acks, active research

         - Directory authorities:

           instead of flooding the network, trusted nodes provide
           network info

         - Variable exit policies:

           exit node operators select which traffic to allow to which
           hosts

         - End-to-end integrity checking:

           in addition to crypto

         - Rendezvous points and hidden services:

           negotiation of rendezvous points (instead of "reply onions")

         - Censorship resistance:

           bridges (unlisted guard nodes) and HTTPS similarity

         - Modular architecture:

           - vidalia (control port)

           - pluggable transports

           - no OS patches, but only TCP possible

    2. Related work

       - Chaum: Mix-Net

       - Babel, Mixmaster, Mixminion: maximum anonymity, large latency

       - tor low-latency

       - single-hop: anonymizer, etc

       - JonDo: fixed cascades: routes that aggregate traffic

       - PipeNet: multi-hop, weaknesses

       - p2p:

         - tarzan, morphmix, layered encryption

         - crowds: all nodes can read

         - hordes: crowds with multicast responses

         - herbivore and P^{5}: crowds with broadcast responses

       - freedom, i2p: circuits all at once

       - cebolla, anonymity network: build in stages

       - circuit-based: which circuit? IP, TCP, HTTP?

       - TCP middle-approach,

         - can transfer all TCP streams

         - avoid TCP-TCP inefficiencies

       - censorship-resistance like eternity, free haven, publius,
         tangler

    3. Design goals and assumptions

       0. [@0]

          - Goals

            - Deployability: cheap, rather legal, easy to implement

            - Usability: more usable by more applications \to more users
              \to higher security

            - Flexibility: specified, replaceable

            - Simple design: kiss: well-understood, accepted approaches

            - Resistant to censorship: both by IP and protocol fingerprinting

          - Non-goals

            - Not peer-to-peer: "still has many open problems"

            - Not secure against end-to-end attacks:

              "Some approaches, such as having users run their own onion
              routers, may help;"

            - No protocol normalization: needs to be added via Privoxy f.ex.

       1. Threat Model

          adversary like [[*%5B%5B./tor-design.pdf%5D%5BTor:%20The%20Second-Generation%20Onion%20Router%5D%5D][Tor: The Second-Generation Onion Router*]]

    4. The Tor Design

       0. [@0]

          - atop TLS

          - onion routers

            - TLS connection to other ORs

            - 2 (+1) keys

              - long-term identity: signs router descriptor and TLS
                certificates

              - short-term onion key: decrypt circuit requests,
                negotiates keys

          - onion proxy

            - fetch directories

            - establish circuits

            - handle connections from users

       1. Cells

          - fixed size vs variable size

            - versions, vpadding, certs, auth_challenge, authenticate,
              autorize

          - command vs relay vs relay_early

            - relay: streamid, digest, length

       2. TLS details

          - previously, TLS handshake identified Tor

          - nowadays, in-TLS handshake using /versions/ cell

       3. Circuits and streams

          Tor constructs circuits preemptively, routes several
          application streams via them

          except if the user signals that she wants a separation

          - Constructing a circuit

            1. Alice to Bob: create e_{bob}(g^x)

            2. Bob to Alice: created hash(K = g^xy), g^y)

            3. Alice to Carol via Bob: relay extend g^{x_2}

            4. Bob to Carol: create e_{carol}(g^{x_2})

            5. Carol to Bob: created (...)

            6. Bob to Alice: relay extended

            also /create fast/ possible instead of create, which
            relies on TLS security and avoids the RSA overhead

          - Relay cells

            Cells sent forward from the host

            - if digest valid, this OR is meant, process instructions

            - else send on

            - leaky circuit

            - /destroy/ and /relay truncate/

       4. Choosing nodes for circuits

          0. [@0]

             - (bandwidth / capabilities) weighted distribution

             - bandwidth measured, distributed by consensus

          1. Guard nodes

             increased (little) risk of more deanonymization,
             decreases (bit higher) risk of some deanonymization

          2. Avoiding duplicate node families in the same circuit

             - attack: control entry and exit node

             - defense: avoid both from /16, also from (mutual) families

       5. Opening and closing streams

          - create or select circuit

          - use last hop or intermediate as exit

          - /relay begin/ with random /streamID/

          - /relay connected/

          - client sends TCP with /relay data/

          - SOCKS problems

            - DNS data leak

          - firefox problems

            - cookies, DOM storage

            - TLS session IDs

            - browser characteristics

            - plugins

            - privoxy weak against HTTPS

          - /relay teardown/ \sim TCP RST

          - /relay end/ ~ TCP FIN (allows TCP half-closed conns)

       6. Integrity checking on streams

          - both ends use SHA1 updated on each cell with the contents
            of four bytes

          - allows some faster attacks than correlation

            - need to improve

       7. Rate limiting and fairness

          - token bucket based

          - prefers interactive services (lowest total
            exponentially-decaying number of cells)

       8. Congestion control

          0. [@0]

             - might allow attack

          1. Circuit-level throttling:

             - packaging window: back to OP

             - delivery window: outside

             - initialized at, say, 1000, decremented on each packet

             - refilled after /relay sendme/ cell

          2. Stream-level throttling:

             - end-to-end

             - refilled only when number of bytes pending to be
               flushed <= 10 * cell_size

    5. Rendezvous Points and hidden services

       0. [@0]

          - protects against DoS

            - attackers have to attack Tor first

          - design

            - access-control: control who can connect (and who cannot)

            - robustness: long-term, even if router goes down

            - smear-resistance

            - application-transparency

          - avoid finding out even if bob is online

       1. Rendezvous points in Tor

          - bob: generate key, select introduction points IP, advertise, connect

          - alice: select rendezvous point RP (with ID, dh-part), tell IP

          - bob: connect to RP, (with ID, other dh-part, has session key)

          - rp connects both, alice: /relay begin/

          - introduction points can be DoSed \to select many

       2. Integration with user applications

          - seamlessly via OP: virtual =.onion= domain, resolved to
            hidden service

       3. Previous rendezvous work

          - ISDN

          - mobile phones

          - Goldberg: manual hunt down location, no dh, volunteers spared work

    6. Other design decisions

       1. Denial of service

          - several possibilities, none yet seen in the wild

          - start: TLS, harder for server, for DOS

            - defense: puzzle or limit number of create cells

          - as DoS amplifier, create long path, one cell per hop

            - defense: limit to 8 via relay_early cells (only 8 sendable)

          - attack ORs network links or hosts

       2. Exit policies, node history, and abuse

          - choose between  open / restricted / private exit and middleman

       3. Directory Servers

          - list of all (reachable) ORs

            - checked

          - create consensus by voting

       4. The Tor controller protocol

          - alternative to config file and log output

          - ASCII-based messages

          - control also path selection etc

    7. Attacks and Defenses

       1. Passive Attacks

          1. Observing user traffic patterns: ?on/off when?

          2. Observing user content: ?

          3. Option distinguishability: torrc options

          4. End-to-end timing correlation: hard to defend, maybe via private OR

          5. End-to-end size correlation: simple counting, but: leaky pipe

          6. Website fingerprinting: in design goal, database (see quote)

       2. Active attacks

          1. Compromise keys

             - tls session key: see encrypted traffic

             - circuit session key: unwrap one layer encryption

             - tls private key: impersonate

             - identity key: replace

          2. iterated compromise

             march down circuit, compromise, but short lifetime

          3. run a recipient: create traffic patterns, other compromising

          4. run op: compromise all

          5. DoS non-controlled nodes: defense robustness

          6. run hostile OR: correlate end-to-end, guard nodes
             concentrate vulnerabilities

          7. introduce timing into messages

          8. tagging attacks

          9. replace contents of unauthenticated protocols AND ATTACK

          10. replay attacks: impossible

          11. smear attacks

          12. distribute hostile code: signed releases

          13. block access: bridges

       3. Directory attacks

          1. destroy server

          2. own server: tie-braking vote

          3. own majority of servers: include/exclude any node you want

          4. encourage dissent: sow distrust, split into camps

          5. have hostile OR listed

          6. have non-working OR seem as working

       4. Attacks against rendezvous points

          1. Make many introduction requests

          2. attack introduction point (disrupt)

          3. compromise introduction point: flood bob

          4. compromise rendezvous point: no more effect than other OR

    8. Early experiences: Tor in the Wild

       - slow growth

       - various protocols web aim irc anonymous email recipient, ssh, ftp, kazaa

       - 80 % of down, 40% of upstream used

         - maybe later one packet size for bulk, one for interactive traffic

    9. Open Questions in Low-latency Anonymity

       - grow beyond directory servers?

       - how long paths?

       - padding etc to defeat end-to-end correlation

    10. Future Directions

        - Scalability: Clique topology scaleable? better see sec 9

        - Bandwidth classes: DSL | T1 | T3 as in MorphMix

        - Incentives: more than publicity and (?more privacy?)

        - Cover traffic: currently ommitted (link-level + long-range):
          provable protection

        - Caching at exit nodes: improve anonymity, yet weakens forward security

        - Better directory distribution: (currently?) every 15 minutes
          dl entire network

        - Further specification review: byte-level needs external review

        - Multisystem interoperability: unify specification and
          implementation of MorphMix and Tor

        - Wider-scale deployment
*** quotes
    - most designs protect primarily against traffic analysis rather
      than traffic confirmation
    - distributed-trust, circuit-based anonymizing systems
    - (ends 2)
    - adding unproven techniques to the design threatens
      deployability, readability, and ease of security analysis.
    - like all practical low-latency systems, Tor does not protect
      against such a strong adversary [a global passive adversary]
    - (ends 3)
    - some control cells are variable length, where the ability of an
      attacker to detect their presence doesn’t affect security.
    - Most traffic passes along these connections in fixed-size
      cells. (A few cell types, notably those used for connection
      establishment, are variable-sized.)
    - To determine that this newer version of the link protocol
      handshake is to be used, the initiator avoids using the exact
      set of ciphersuites used by early Tor versions, and the Tor
      responder uses an X.509 certificate unlike those generated by
      earlier versions of Tor. This may be too clever for Tor’s own
      good; we mean to eliminate it once every supported version of
      Tor supports this version of Tor’s link protocol.
    - (ends ?4.2?)
    - This circuit-level handshake protocol achieves unilateral entity
      authentication (Alice knows she’s handshaking with the OR, but
      the OR doesn’t care who is opening the circuit — Alice uses no
      public key and remains anonymous)
    - Preliminary analysis with the NRL protocol analyzer [33] shows
      this protocol to be secure (including perfect forward secrecy)
      under the traditional Dolev-Yao model.
    - (ends 4.3.Constructing a circuit)
    - As mentioned above, if the first and last node in a circuit are
      controlled by an adversary, they can use traffic correlation
      attacks to notice that the traffic entering the network at the
      first hop matches traffic leaving the circuit at the last hop,
      and thereby trace a client’s activity with high
      probability. Research on preventing this attack has not yet come
      up with any affordable, effective defense suitable for use in a
      low-latency anonymity network.
    - (ends 4.4.2)
    - (begins 9)
    - Recent work on long-range padding [31] shows promise.
    - (ends 9)
**** [[file:master.bib::tor2014][Dingledine et al. 2014: Tor]]
** [[./oakland2012-peekaboo.pdf][Dyer - Peek-a-Boo, I Still See You: Why Efficient Traffic Analysis Countermeasures Fail]]
*** summary
    0. [@0] Abstract

       - coarse features

       - analysis of TA countermeasures

         - standardized in TLS, SSH, IPsec

         - wright

    1. INTRODUCTION

       - wf is a problem

       - most defenders try to eliminate just one feature

       - 9 defenses, 7 attacks, two datasets

       - tested countermeasures ineffective

       - hiding length is not sufficient

       - Coarse information is unlikely to be hidden efficiently.

       - Relevance to other settings: real-world attacks harder, yet
         wf defenses should cover this

       - did not try Camouflage and HTTPOS

    2. EXPERIMENTAL METHODOLOGY

       - herrmann and levine datasets

       - each classifier, each defense

       - select privacy set k \le n of websites

       - for each websites, 20 traces, 16 of which training, 4 test

       - many trials

       - downloadable python code http://www.kpdyer.com/

    3. TRAFFIC CLASSIFIERS

       supervised learning, testing and training data

       1. Liberatore and Levine Classifier

          - naive Bayes

          - direction and length of packets

       2. Herrmann et al. Classifier

          - multinomial naive Bayes

          - normalized counts

          - tf + cosine

       3. Panchenko et al. Classifier

          - SVM

          - panchenko's parameters C=2**17, \gamma=2**-19

          - remove ACKs

          - number of features

    4. COUNTERMEASURES

       three types: explicitly allowed in SSH/TLS/IPSEC, other
       padding-based, and distribution-oriented

       1. SSH/TLS/IPSec-Motivated Countermeasures

          1. session padding: all packets with same fixed length

          2. random padding: each packet with some other padding

          in reality: *look at plaintext data*, here: assume all text
          fits in 255 bytes \to more noise, overestimate efficacy

       2. Other Padding-based Countermeasures

          1. Linear: all increased to min(nearest multiple of 128, MTU)

          2. all increased to min(nearest power of two, MTU)

          3. mice-elephants length \le 128 ? 128 : MTU

          4. all to MTU

          5. random sampling of (MTU-length) and added

       3. Distribution-based Countermeasures

          wright: match other web page

          1. Direct target sampling

             - algo

               1. sample target packet length l' from target distribution

                  - if this length l <= l', pad to l'

                  - else send l' bytes, set l=l-l', resample until done

             - here: target distribution chosen randomly

          2. Traffic morphing

             similar to direct target sampling, but with convex
             optimization to minimize overhead

       4. Overhead

          - "almost no correlation between overhead and level of
            confidentiality provided"

    5. EXISTING COUNTERMEASURES VERSUS EXISTING CLASSIFIERS

       Liberatore, Herrmann, Panchenko each vs nine countermeasures

       1. Comparing the Datasets

          - worst: no countermeasure

          - some bugs in Liberatore dataset (tiny traces)

            - lead to worse classifier accuracy

       2. Comparison of Classifiers

          - P best for all most all

            - graphics with classifier groups, world size (x), accuracy (y)

       3. Comparison of Countermeasures

          - here: single value per session random padding is better
            than per-packet ( "averages out" )

          - pad-to-MTU works worse than some other (leaks direction info)

          - Session Random was better than pad-to-MTU

            - less overhead

          - DTS works best

    6. EXPLORING COARSE FEATURES

       0. [@0]

          3 coarse features

          - total transmission time (TIME),

          - total bandwidth (BW)

          - traffic "bustiness" (VNG)

       1. Total Time

          - simplest measure

          - some accuracy

          - same as against no padding at all

       2. Total Per-Direction Bandwidth

          - works, is "more robust" than LL and H

          - padding changes only little bit of bw

       3. Variable n-gram

          - tuples (sum up, sum down, sum up, sum down, ...) for
            bursts: how much data in the same direction

          - achieves P classifier's performance

       4. Combining Coarse Features: the VNG++ Classifier

          - combines three above

          - P has more fine-grained features, and more complicated ML algo

          - P-NB: panchenko with naive Bayes ML: on par (slightly
            better without padding), way worse for session 255

       5. Discussion

          - bandwidth a problem

          - bursts (VNG, panchenko-style) too

    7. BuFLO: BUFFERED FIXED -LENGTH OBFUSCATOR

       0. [@0]

          - fixed data rate, fixed time length, fixed length packets

          - stops after fixed time even if website continues

          - setting this time lower leads to better classification

       1. BuFLO Description

          - variables d: size of fixed-length packets,

            - \rho: rate at which to send packets (in milliseconds)

            - \tau: minimum amount of time for which to send packets

          - (this yields approximation of the webpage size :-( ---
            while sending, keeps on, then finishes)

       2. Experiments

          - assumptions: implementable, start detectable

          - several ranges: accuracy from 27.3% (1000, 40, 0) to 5.1%
            (1500, 20, 10000) (each (d, \rho, \tau))

       3. Observations about BuFLO

          - weaknesses:

            - yields some size and time data if at \tau

              - buffer is still full, or

              - website is still sending

            - enhances (!) timing-based classifier for low data rate

    8. RELATED WORK

       - Schneier et al citing Yee

       - Cheng et al applied this

       - Sun et al similar setting, more thorough (jaccard

       - Hintz simple attack (safeweb)

       - Bissias et al: weaker adversary could observe ssh tunnel,
         length, direction, timing)

       - Liberatore et al: HTTP via SSH inferred from lengths and
         directions of unordered packets, countermeasures

       - Herrmann: MNB, big datasets

       - Panchenko: SVM

       - Wright: traffic morphing

       - Panchenko and LU: countermeasures

    9. CONCLUDING DISCUSSION

       - "several simplifying assumptions":

         - knows privacy set,

         - trains and tests on same traffic as generated

         - no effects like caching, parallel loading, etc

       - does not need packet lengths

       - engine does not matter that much

       - privacy-set size does not matter much (quoth he)
*** quotes
    0. [@0] Abstract
       - in the context of website identification, it is unlikely that
         bandwidth-efficient, general-purpose TA countermeasures can ever
         provide the type of security targeted in prior work.
    1. INTRODUCTION
       - This implies that any effective countermeasure must produce
         outputs that consume indistinguishable amounts of bandwidth.
    5. [@5] EXISTING COUNTERMEASURES VERSUS EXISTING CLASSIFIERS
       1. Comparing the Datasets
          - The fraction of traces that have short duration, particularly
            ones that are clearly degenerate (≤ 10 packets)
    6. [@7] BuFLO: BUFFERED FIXED -LENGTH OBFUSCATOR
       - whether any measure can work, even prohibitively inefficient
         ones.
*** ref [[./master.bib::oakland2012-peekaboo][Dyer et al. 2012: Peek Boo I]]
** [[./ieee-icc15.pdf][Feghhi - A First-Hop Traffic Analysis Attack Against Tor]]
*** summary
    0. [@0] Abstract

       timing-information only

       68% success

    1. INTRODUCTION

       0. [@0]

          - only timing information

          - padding defeats size info

          - packet counting need partitioning

          - this does not

       1. Related Work

          - Hintz: SafeWeb

            - sequential page fetches

            - port/direction/size observed

            - 75% success rate

          - Bissias:

            - also sequential

            - size/direction/time observed

            - 23% for 100 pages, 40% for fewer

       2. ANATOMY OF A WEB PAGE FETCH

          - assume padding

          - direction/timing observed

          - enough: uplink traffic timestamps

          - how web page fetched

            - third-party: new delay (TCP handshake)

            - AJAX: large inter-arrival times \to signature

            - number of fin/finack/ack dependent on "number of
              distinct locations"

       3. COMPARING SEQUENCES OF PACKET TIMESTAMPS

          how to compare different-length sequences

          1. Derivative Dynamic Time Warping

             - warping path (see quotes)

             - cost function C: sum

             - with cost per single difference given by derivative

          2. F -Distance Measure

             with two paths and a warping path, sum the stretches
             where only one of them increments (non-parallel), divide
             by total length of both paths

       4. DE - ANONYMISING WEB FETCHES OVER TOR

          0. [@0]

             - 20 health/ 20 finance websites a 100 "fetches"

             - watir-webdriver script

             - Firefox 21.0

          1. Hardware/Software Setup

             3 ghz core 2 duo, 2 gb ram, ubuntu 12.04 lts

          2. Classifying Measured Timestamp Sequences

             - K-NN with F-distance

             - better than naive Bayes

             - top 5 per web page to represent

          3. Randomised Routing

             - "Abrupt, substantial changes in the mean RTT are
               evident"

          4. Classification Performance

             - 67.7% on 40 sites a 100 samples

             - 93% without Tor

             - K=1 is best here

          5. Finding a web page within a sequence of web requests

             - 3 out of 5:

             - find

               - stream,

               - cut by 10,

               - use 3-instance exemplar to match,

               - analyze using all

             - 69% success with position \pm 65 packets

       5. SUMMARY AND CONCLUSIONS

          - "The attack makes use only of packet timing information on
            the uplink"

          - effective (68% accuracy on 40 sites)
*** quotes
    - define a *warping path* p to be a sequence of pairs,

      {(p_{k}^{i}, p_{k}^{j})}, k = 1, 2, ··· ,l with (p_{k}^{i}, p_{k}^{j}) ∈ V
      := {1, ... , n}× {1, ..., m}

      satisfying boundary conditions

      p^{i}_{1} = 1 = p^{j}_{1} ,

      p^{i}_{l} = n, p^{j}_{l} = m

      and step-wise constraints

      (p^{i}_{k+1} , p^{j}_{k+1} ) ∈ V p^{i}_{k} ,p^{j}_{k} := {(u, v) :
        u ∈ {p^{i}_{k} , p^{i}_{k + 1}} ∩ {1, . . . , n},
        v ∈  {p^{j}_{k} , p^{j}_{k + 1}} ∩ {1, . . . , n}},
      k = 1, · · · , l−1.
    - where D_{t}(i) = (t_{i} - t_{i^{-}}) + (t_{i^{+}} - t_{i^{-}}) / 2,
      i^{-} = max{i-1, 1} and
      i^{+ }= min{i+1, |t|}
    - (ends 3.1)
    - This suggests using the fraction of the overall warping path
      which is parallel to the x- or y-axes as a distance measure,
      which we refer to as the *F-distance*.
    - define κ 1 := 0 < κ 2 < · · · < κ r−1 < κ r := l such that for
      each s = 1, · · · , r − 1
      a) either p ik 1 = p ik 2 ∀k 1 , k 2 ∈ {κ s + 1, · · · , κ s+1 }
         or p jk 1 = p jk 2 ∀k 1 , k 2 ∈ {κ s + 1, · · · , κ s+1 } and
      b) either κ s+1 = l or condition (a) is violated for some k 1 ,
         k 2 ∈ {κ s , · · · , κ s+1 + 1} i.e. each subsequence is
         maximal.
    - define the *F-distance* measure between timestamp sequences t and
      t′ , namely:

      φ(t, t') := \sum_{s=1}^{r−1} (κ_{s+1} − (κ_{s} + 1)) / n+m (4)
    - (ends 3)
    - congestion window growth slows with increasing RTT.
    - (ends 4.D (=4.4))
*** ref
** [[./authorsversion-ccsw09.pdf][Herrmann - Website Fingerprinting: Attacking Popular Privacy Enhancing Technologies with the Multinomial Naïve-Bayes Classifier]]
*** summary
    0. [@0] ABSTRACT

       - attack privacy-enhancing technologies via text-mining
         techniques

       - closed-world: 97% success

    1. INTRODUCTION

       - PET (=privacy enhancing technology) website fingerprint
         attack

       - by local ISP, local admin, secret services

       - multinomial naive bayes

    2. SCENARIO

       - between user and PET, records traffic, can link IP to victim

       - passive, local, external attacker

       - training phase: fingerprints for all (or set of observed) websites

       - testing phase: measure user traffic, compare to fingerprints

    3. RELATED WORK

       - HTTP traffic analysis

         - Mystry, Cheng: determine URLs via encrypted SSL (single web
           server)

           - not feasible anymore: pipelining and multiple
             simultaneous connections

         - Hintz, Sun: HTTP proxy with SSL

           - library of histograms of sizes of transferred files

           - Sun: 100000 websites, Jaccard, 75% with FPR 1.5

         - Bissias, Liberatore: improved, patterns in IP packets

           - Liberatore

             - neglects timing information and order,

             - compare packet size histograms via Jaccard coefficient and Naive
               Bayes

             - with kernel density estimation:

             - 73%

             - padding schemes evaluated: ip padding foils attack

         - Kiraly: Traffic Flow Confidentiality (IPSEC extension:
           padding and packet clocking)

           - not against WF

         - Wright: Traffic Morphing: "thwart statistical traffic
           analysis algorithms by efficiently modifying traffic of a
           website in a way so that it looks like another one."

    4. METHODOLOGY

       1. Analysed Systems

          - single-hop

            - OpenSSH: offers SOCKS proxy, multiplexing, flow control

            - OpenVPN: raw IP packets (routing mode)

            - CiscoVPN: ESP via UDP

            - Stunnel: TCP and TLS handshakes for each connection

            - later also WiFi via WPA (same category)

          - multi-hop

            - Tor: short-lived circuit

            - JonDonym: static cascade

            - I2P not included: performance/stability & used mostly for
              inter-I2P-communication

       2. Research Assumptions

          assumptions very good for adversary
          
          1. knows PET type

          2. knows all pages = closed-world

          3. similar internet access like victim

          4. knows browser and configuration

          5. browser configured easily (no caching, no prefetching, no
             querying for updates)

          6. victim requests single pages one at a time

       3. Modelling the Classification Problem

          - data mining problem: classification

          - Attributes: number of packets of a certain size (multiset)

       4. Known Website Fingerprinting Techniques

          1. Jaccard’s Classifier

             s_{AB} = |A \cap B| / |A \cup B|

             60% in Liberatore/Levine

          2. Naïve Bayes Classifier with Kernel Density Estimation

             naive bayes, better for padded, worse for unpadded than
             jaccard

       5. Our Novel Website Fingerprinting Method

          text mining techniques

          1. Multinomial Naïve Bayes (MNB) Classifier

             - text mining

               - spam

             - tf-idf similar to packet frequency

             - different from naive bayes

          2. Application of Text Mining Transformations

             - averaging the number of texts (f.ex. ACKs) via tf

             - averaging total word occurrences via idf

             - normalising lengths via cosine transform

    5. EVALUATION

       0. [@0]

          - Weka with own Jaccard-classifier

          - single hop easily deanonymized, multi-hop "some protection"

       1. Data Collection and Sampling

          - school data: real users, 2000 domain names from log file
            by frequency, filtered to 775 (real domains)

          - setup

            - script based on firewatir

              - and javascript shell

            - ff 2.0

            - start tcpdump before

            - aborts after 90 seconds

            - restart browser after 775 URLs download

          - 2 (tor) to 17 (cisco) instances per day

          - testing data (48h),

          - training data from (48h + \Delta) later time (48h)

          - \to stratified

          - corrected resampled paired t-test

       2. Performance of the MNB Classifier

          0. [@0]

             - comparison to other OpenSSH-fingerprinting
               attacks. This relates to other systems as well.

             - accuracy: found/total

          1. Influence of Transformations

             best result for (only) TF with normalization

          2. Size of Training Set

             4 training instances ("good compromise between necessary
             resources and achievable accuracy.")

          3. Robustness

             quite robust to changes over time

             also adjusts to changes in content: concept drift

       3. Comparison of Website Fingerprinting Methods Against OpenSSH

          - with transformation (tf + normalisation + cosine),
            significantly better

          - also faster for training and testing

       4. Attacking Popular PETs Using the MNB Classifier

          - single-hop all above 94% with tf-normalization

          - multi-hop JonDonym 19.97, Tor 2.96% (normalization only)

          - better for top k (3/10) classes

          - multi-hop worse than theoretic, might be vulnerable

    6. DISCUSSION

       - setup constant, might change for different Operating Systems etc

       - caching decreased success from 96.65 % (with caching
         disabled) to 91.70 %

       - false alarm avoidance comes at a great cost: with ~1.4% false
         alarms, tp falls to 40% (78 interesting sites of 775)

    7. CONCLUSION

       - Multinomial Naïve Bayes

       - "operates on the frequency distribution of IP packet sizes"

       - "increased performance is mainly due to the normalisation of
         the packet size frequency vectors"
*** quotes
   0. [@0] Abstract
      - influence of the browser cache on accuracy.
   2. [@2] SCENARIO
      - The attack consists of two phases: in the training phase the
        attacker creates traffic fingerprints for a large number of
        sites (or for a small set of interesting sites) and stores them
        together with the site URLs in a database. In the testing phase
        the attacker records the encrypted traffic of the user, creates
        fingerprints of small traffic chunks and tries to match them
        with records in the database.
   4. [@4] METHODOLOGY
      1. Analysed Systems
         - Tor is based on the idea of Onion Routing [10], i. e. the Tor
           clientwraps the data packets in multiple layers of encryption,
           which are “peeled off” as packets are relayed over multiple
           onion routers.
      2. Research Assumptions
         - the browser cache has only a moderate impact on the accuracy in
           our sample
      3. Modelling the Classification Problem
         - Note that our instances closely resemble the typical document
           representation in the domain of text mining, where instances are
           represented by term frequency vectors.
      4. Known Website Fingerprinting Techniques
         1. Jaccard’s Classifier
            - Jaccard’s coefficient is a similarity metric for sets [31],
              which is often used for unsupervised learning tasks.
         2. Naïve Bayes Classifier with Kernel Density Estimation
            - [The Naive Bayes Classifier] naïvely assumes independence of
              attributes, which is often not the case for real-world problems.
            - operates directly on multiset instances,
   5. Our Novel Website Fingerprinting Method
      2. [@2] Application of Text Mining Transformations
         - biased towards classes which contain many packets and/or packets
           with high frequencies [...] problem is addressed by a sublinear
           transformation of the frequencies:

           f^{∗}_{x_{j}} = log(1 + f_{x_{j}}).

           This is referred to as *term frequency (TF) transformation*.
         - some packet sizes (e. g. with the size of the MTU) are part of
           every instance and do not confer much information [...] is
           alleviated using the *inverse document frequency (IDF)
           transformation* [...]

           f^{*}_{x_{j}} = f_{x_{j}} · log (n / df_{x_{j}})
         - normalising the lengths [...] by applying cosine normalisation
           to the attribute vectors, i. e. the transformed frequencies are
           divided by the Euclidean length of the raw vectors:

           f^{norm}_{x_{j}}= (f^{* }_{x_{j}} / ||(f^{* }_{x_{1}}, ... f^{*}_{x_{n}})|| )
      4. [@4] Attacking Popular PETs Using the MNB Classifier
         - From an information-theoretic viewpoint, even the multi-hop
           systems do not protect perfectly, though: the accuracies found
           for them are well above the accuracy achievable by randomly
           guessing the class without any context knowledge (1/775 ≈
           0.13%).
         - top k classes from the list of predicted classes (sorted in
           descending order by class membership probability). If the actual
           class was among the list of predicted classes, the test instance
           was counted as correctlyclassified, otherwise as incorrectly
           classified. For k = 3 and k = 10 the accuracy values for Tor
           increase to 16.69 % and 22.13 %, respectively, for JonDonym they
           increase to 31.70 % and 47.53 %.
   6. DISCUSSION
      - dependent to some degree on the operating system, the type
        of the Internet connection and the browser and its
        configuration. We therefore expect that the accuracy of
        website fingerprinting attacks is degraded in case training
        and testing instances are not recorded in the same
        environment.
      - footnote to /false alarms/: Please note that the term /false
        positives/ is intentionally not used here, as it is used to
        convey another meaning in classical data mining.
   7. CONCLUSION
      - the development and implementation of efficient
        countermeasures becomes an important task for the PET
        community.
*** vocabulary
    - website fingerprinting: learn the identity, i. e. the URLs, of
      websites that are downloaded over an encrypted tunnel by
      comparing the observed traffic to a library of previously
      recorded fingerprints.
*** ref
    [[file:~/da/docs/master.bib::ccsw09-fingerprinting][Herrmann et al. 2009: Website]]
** [[./hintz02.pdf][Hintz - Fingerprinting Websites Using Traffic Analysis]]
*** summary
    0. [@0] Abstract

       Attack to find out whether user is visiting certain websites,
       even though he uses an encrypted proxy.

       Plus discussion: better attack and defenses

    1. Introduction

       With normal encryption, metadata is visible.

       With one-hop proxies, metadata is discoverable.

       There are several defenses.

    2. Definition of Traffic Analysis

       sender, receiver, amount of data transferred (ssl does not try
       to obfuscate)

    3. SafeWeb

       - anonymizing one-hop proxy with some content-rewriting

       - JS and SSL-based

    4. Fingerprinting Websites

       - HTML \to images, stylesheets

       - separate TCP connection and port

       - many combinatorical possibilities to uniquely identify each page

    5. The Real World Threat

       - easy to distinguish

       - little https vs other traffic, thus easier

    6. Implementing a Fingerprinting Attack

       - analyse log files for occurring sizes (data received per port)

       - compare these sizes*counts with other sizes*counts

       - same site: smallest number of exact matches = 21, typically 75%

       - different sites: biggest number = 2, most often 1, 0, at most 6%

       - service shut down before large scale tests were possible

    7. Improving the Attack

       so far small-scale, for large-scale, improvements would be

       1. Analyzing the Order of Transmissions

          - also order HTML-embedded

          - create several fingerprints (for different web browsing programs)

       2. Improving Creation of Fingerprints

          - always some noise

          - maybe from different computers, adding sizes

       3. Expanding Fingerprints to Entire Websites

          - f.ex. by getting all sizes of all files on the website
            (cnn.com f.ex.)

       4. Improving Matching

          - range matching (adding ranges after exact matches) created
            more false positives

    8. Protecting Against Fingerprinting

       several ways

       1. Adding Noise to Traffic

          0. [@0]

             - possible by proxy

             - SafeWeb already reformats HTML to point back to SafeWeb

          1. Modify Sizes of Connections

             - modify sizes of files (comments to HTML, jpg, f.ex.)

             - increase amount of bandwith

          2. Add Extra Fake Connections

             - f.ex. by embedding 1x1 sized pictures

               - drawbacks: rendering, high amount of bandwidth

       2. Reduce Number of Files Transferred

          - "quick and easy solution": disable graphics, etc

            - reduced number of files by 3/4

       3. Transfer Everything in One Connection

          - f.ex. one tarball

          - (or HTTP/1.1)

    9. Conclusion

       - applicable to other services

         - f.ex. SSH: determine size of password
*** quotes
    - The process of monitoring the nature and behavior of traffic,
      rather than its content, is known as traffic analysis.
    - Websites that are purely HTML and do not reference any other
      files, such as graphics, would probably not have a unique
      fingerprint.
    - Although the government would have a huge amount of traffic to
      analyze, https traffic comprises only a very small portion of
      all Internet traffic. (5)
*** ref [[file:master.bib::hintz02][Hintz 2002: Fingerprinting Websites Using Traffic Analysis]]
** [[./guide_libsvm.pdf][Hsu - A Practical Guide to Support Vector Classification]]
*** summary
    0. [@0] Abstract

       SVM cookbook

    1. Introduction

       0. [@0]

          - separate into training and testing sets

          - training set instance:

            - "target value" = class label

            - "attributes" = features or observed variables

          - goal: produce model which predicts target values of test data
            given only its attributes

          - four basic kernels (other developed)

            - linear

            - polynomial

            - radial basis: exp(-γ || x_{i}- x_{j} ||^{2} )

            - sigmoid

       1. Real-World Examples

          data by users

       2. Proposed Procedure

          - transform data for input

          - scale

          - with rbf:

            - find C,\gamma by cross-validation

            - train whole training set

          - test

    2. Data Preprocessing

       1. Categorical Feature

          - use m numbers to represent a m-category attribute
            one is one, others are zero

            +: more stable

       2. Scaling

          +: avoid attributes in greater numeric ranges dominating
          those in smaller numeric ranges

          +: avoid numerical difficulties

          how: linearly scale to [-1, +1] or [0,1]

          same scale for training and testing (which might be [-1.1,
          +0.8] for testing)

    3. Model Selection

       1. RBF Kernel

          - includes linear kernel

          - sigmoid similar for certain parameters, yet sometimes invalid

          - polynomial has more hyperparameters

          - fewer numerical difficulties: goes to 0

          - large features: linear kernel

       2. Cross-validation and Grid-search

          - high training accuracy not useful \to cross-validation

          - avoids overfitting better

          - grid-search: all pairs of e.g.

            - \gamma \in {2^{-15}, 2^{-13}, ..., 2^{3}}
            - C \in {2^{3}, ..., 2^{-13}, 2^{-15}}

          - advantages: parallelizable, better feeling

          - first coarse grid, then finer grid

    4. Discussion

       - many features \to select which ones to use

    5. Appendix

       A) Examples of the Proposed Procedure

          there are automated scripts easy.py and grid.py

          first scale, then grid, then test \to better, automatic with scripts

       B) Common Mistakes in Scaling Training and Testing Data

          - use the same scaling factors

            $ ../svm-scale -l 0 -s range4 svmguide4 > svmguide4.scale
            $ ../svm-scale -r range4 svmguide4.t > svmguide4.t.scale

       C) When to Use Linear but not RBF Kernel

          RBF \ge linear only after searching (C, \gamma) space

          1. Number of instances << number of features

             linear kernel 98.6111 vs rbf kernel 97.2222

          2. Both numbers of instances and features are large

             liblinear faster and more accurate

          3. Number of instances >> number of features

             use liblinear -s 2, way faster than default -s 1
*** ref [[file:master.bib::Hsu10apractical][Hsu et al. 2016]]
** [[./article-2456.pdf][Juarez - A Critical Evaluation of Website Fingerprinting Attacks]]
*** summary
    0. [@0] ABSTRACT

       many WP papers do not use practical scenarios: browsing habits,
       location, version tbb,

    1. INTRODUCTION

       old studies did less about localization, tbb version and
       browsing habits, this addresses

       - evaluates these assumptions

       - what defeats the accuracy

       - how to reduce false positive rates

       - adversary's cost

    2. WEBSITE FINGERPRINTING

       find out which site or page is visited from network traffic only

       - first within single website

       - then within set of websites

       - then hintz's safeweb anonymizing web proxy ++

       - then Herrmann: 3% success

       - Shi 50% for 20 pages, Panchenko 54% for Herrmann's dataset

       - cai et al, wang and goldberg: over 90%, *100 pages*

    3. MODEL

       passive local attack, targeted vs non-targeted

       1. Assumptions

          listed by papers that explicitly mention assumptions

          client-side, adversary, web assumptions

          - client:

            closed world: user may only visit certain pages, or only
            certain pages from a set are searched for

            browsing behavior: users only have one tab open at a time,
            sequential browsing

          - websites:

            (?) all websites are built using templates

            localized versions: but language of webpage is determined
            by exit node (really?)

          - adversary:

            page load parsing: page load start/stop are detectable

            no background traffic: tor separable from other traffic

            replicability: adversary can replicate user's setup (tbb
            version, OS, network connection)

    4. EVALUATION

       some assumptions distort the model

       1. Datasets

          Alexa top sites and ALAD

       2. Data collection

          - tbb with selenium

          - dumpcap

          - tor configured via stem

          - circuit renewal to 600000 (? cf. wang/goldberg)

          - disable UseEntryGuards

          - batches: page 4 times, 5-10 batches of data per time

          - 5 seconds before each crawl, 5 second pauses between each visit

          - round-robin, hours apart

          - two physical, three cloud-based virtual machines

          - Linux Container based virtualization

          - disabled OS updates (how about time, claws updates?)

          - one crawler per machine at a time

          - average CPU load low

       3. Methodology

          - control crawl : default value

          - test crawl: value of interest

          - less controllable: time and tor-path-selection

            - k-fold cross-validation and

            - minimizing time gap control-to-test

          - compared other papers

          - chose the faster of the two by W[32]

          - also own decision tree with panchenko "merkmale"

       4. Time

          website fingerprints decay as time goes on: 50% after 9 days

       5. Multitab browsing

          decays a lot, halved when only one of them counts as success

          delays (0.5, 3, 5 sec) matter very little

       6. Tor Browser Bundle Versions

          2.4.7 dissimilar to others

          3.5 similar to 3.5.2.1

          accuracy greatest for NumEntryGuards = 1, UseEntryGuards = 1

          lowest for UseEntryGuards = 0, +2% for NumEntryGuards =3 (default)

       7. Network

          differences in where the puter is matter greatly: backbone
          gives different pattern

       8. The importance of false positives

          - Open-world:

            4 top sites vs 32710 other sites.

          - The base rate fallacy

            If there is a low chance that the user visits the
            fingerprinted websites, then the occurrence of false
            positives relative to true positives rises.

          - User’s browsing habits

            three random users from ALAD, 100 URLs each

            tried to match everything, failed

    5. CLASSIFY-VERIFY

       probabilistic SVM

       with rejection if P_1 or P_1 - P_2 lower than threshold

       1. Evaluation and result

          this combination greatly decreases the number of false
          positives

    6. MODELING THE ADVERSARY’S COST

       1. Data collection cost:

          data D = n (training pages) \cdot m (versions) \cdot i (instances)

          collection cost col(D)

       2. Training cost:

          train(D, F(=features), C(=classifier)) = D \cdot c

       3. Testing cost:

          Test data T = v (=victims) \cdot p (=pages /victim /day)

          test = col(T) + test(T, F, C)

       4. Updating cost:

          update(D, F, C) / d(=days until change)

       5. Total cost:

          init(D,F,C,T) = col(D) + train(D,F,C) + col(T) + test(T,F,C)

          cost(D,F,C,T,d) = init(D,F,C,T) + update(D,F,C)/d

    7. CONCLUSION AND FUTURE WORK

       practical scenarios
*** quotes
    - The main objective of an adversary in a typical WF scenario is
      to identify which page the user is visiting.
    - Wang and Goldberg concluded that sites that change in size are
      hard to classify correctly
    - Over 50% sites were pages other than the front page
    - Classifiers designed for WF attacks are based on features
      extracted from the length, direction and inter-arrival times of
      network packets, such as unique number of packet lengths or the
      total bandwidth consumed.
    - In most cases, classifier W performed better than the others.
    - the accuracy drops extremely fast over time.
    - We observe a dramatic drop in the accuracy for all the
      classifiers with respect to the accuracy obtained with the
      control crawl
    - This might imply that the specific learning model is not as
      important for a successful attack as the feature selection.
    - The average page load for the test crawl for the 5 second gap
      experiment is 15 seconds, leaving on average 30% of the original
      trace uncovered by the background traffic. Even in this case,
      the accuracy with respect to the control crawl drops by over
      68%.
    - In practice, many TBB versions coexist, largely because of the
      lack of an auto-update functionality. (*new versions include updater*)
    - Even though we fix the entry guard for all circuits in a batch,
      since we remove the Tor data directory after each batch, we
      force the entry guard to change. On the other hand, allowing Tor
      to pick a different entry guard for each circuit results in a
      more balanced distribution because it is more likely that the
      same entry guards are being used in each single batch, thus
      there is lower variance across batches. We must clarify that
      these results are not concluding and there may be a different
      explanation for such difference in standard deviation.
    - the accuracy drop between the crawls training on Leuven and
      testing in one of the other two locations is relatively greater
      than the accuracy drop observed in the experiments between
      Singapore and New York. Since the VM in Leuven is located within
      a university network and the other two VMs in data centers
      belonging to the same company
    - One possible reason for low TPR is due to the effect of inner
      pages.
    - Bayesian detection rate [...] is defined as the probability that
      a traffic trace actually corresponds to a monitored webpage
      given that the classifier recognized it as monitored.[...]

      P (M | C)
      = [P (C | M) P (M)] / [P (M) P (C | M) + P (¬M) P (C | ¬M)]
    - The results show that the BDR doubles when we use the
      Classify-Verify approach.
    - 10-fold cross-validation, where a threshold is determined by
      using 90% of the data and then tested on the remaining 10%.
    - train with different localized versions
    - When each of these assumptions are violated, the accuracy of the
      system drops significantly, and we have not examined in depth
      how the accuracy is impacted when multiple assumptions are
      violated.
    - it seems that the non-targeted attack is not feasible given the
      sophistication level of current attacks.
    - We believe that further research on evaluating the common
      assumptions of the WF literature is important for assessing the
      practicality and the efficacy of the WF attacks.
*** ref [[file:master.bib::ccs2014-critical][Juarez et al. 2014: Critical Evaluation Website Fingerprinting Attacks]]
** [[./wfpadtools-spec.txt][Juarez - WFPadTools Protocol Specification]]
*** summary
    0. [@0] Preliminaries: KEYWORDS, MUST etc as in RFC 2119

    1. Overview

       - pluggable transport

       - link padding-based wf countermeasure

       - between OP and bridge, discarded

    2. WFPadTools protocol

       1. ...

          - between SOCKS and TCP/obfsproxy

          - wrapped into WFPadTools messages

          - types: data/dummy and control

            - data: |total_length |payload_length |flags [|payload]
              [|padding] |

            - control: |header |opcode |args_length [|args]
              [|payload] |

              - opcode signals padding, data, control op, last control

       2. Primitives

          - send_padding: request n dummy messages after t second delay

          - app_hint: start of session

          - burst_histo: delay probability distribution after data

          - gap_histo: delay probability distribution after dummy

          - total_pad, payload_pad, batch_pad: parse, do (see [3] for details)

       3. Histograms

          - cover "interval values from 0 to infinity"

       1. [@1] Handshaking

          - could send distributions in handshaking, then start-command, then pad
*** quotes
    - [3] https://gitweb.torproject.org/user/mikeperry/torspec.git/blob/refs/heads/multihop-padding-primitives:/proposals/ideas/xxx-multihop-padding-primitives.txt
** [[./1512.00524v1.pdf][Juarez - WTF-PAD: Toward an Efficient Website Fingerprinting Defense for Tor]]
*** TODO [#F] order quotes
*** summary
    0. [@0] Abstract

       - lightweight defense

       - tool to evaluate of pluggable transports

    1. INTRODUCTION

       - wf attacks mostly successful in lab

       - avoiding time delays crucial

       - contributions

         - other defenses unsuitable

         - framework for link-padding implementations

         - lightweight defense

         - realistic evaluation

    2. WEBSITE FINGERPRINTING (WF)

       how wf works

       1. Attacks

          - infer from traffic which websites visited

          - real world applicability questionable

       2. Defenses

          - High-level (application) defenses:  HTTPOS,
            randomized pipelining

          - supersequence etc and traffic morphing: require large database

          - low-level (network): padding, morphing, BuFLO (constant
            rate), Tamaraw Cs-BuFLO, high latency addon

    3. SYSTEM MODEL

       threat and network model

       realistic defense requirements

       1. Adversary Model

          here: bridge to connect

          open-world: adversary tries to detect whether client
          downloads "one of a small set of target pages"

       2. Defense Model

          - assume trusted bridge

       3. Defense Requirements

          - Effectiveness: prevent WF ("need only provide this
            protection in a realistic setting")

          - Usability: delay minimized, average bw acceptable

          - Efficiency: not excessive bandwidth

          - No server-side cooperation

          - No databases: big files hard to distribute

    4. FRAMEWORK

       - provide wfpadtools as pt for obfsproxy

         - minimally extended (wrapped)

       - evaluate "broad range of traffic analysis defenses."

       - can do

         - crawl traces with defense enabled

         - apply defense to traces

         - simulate defense

       - parts: crawler, replayer, simulator

    5. ADAPTIVE PADDING

       no latency padding, 50% overhead often sufficed

       1. Design Overview

          0. [@0] 

             - burst and gap mode,

             - random bursts of traffic in burst mode,

             - in gap mode, waiting

          1. AP algorithm.

             - S

             - (on arrival) Burst mode U, sample time

               - (on delay without packet) Gap mode G, sample time

               - (on packet receipt) resample time

             - (on send) Gap mode G, sample time

               - (on expired) send packet

               - (on infinite sample) to U

             - histogram bins exponentially

               - better bin web traffic

               - last to \infty

             - tokens in bin,

               - select token randomly

               - when expires, remove token

               - when arrives, remove arrival time bin token

          2. Burst mode.

             - if inter-arrival time longer than typical, go to G state

          3. Gap mode

             - "*within* a burst in traffic collected for a large sample
               of sites."

             - (rather see timing...)

       2. WTF-PAD

          - send / receive padding

          - send histograms via control messages

          - beginning of transmission

          - soft stopping as with adaptive padding

       3. Interarrival time distribution

          - interarrival times are 1/2 less than 1 msec

          - sample on setup

          - when gap: measure

       4. Tuning bandwidth overhead versus security

          - move lognormal inter-arrival distribution to the left

            - increases overhead, decreases discoverability

          - do not use self-similarity

          - set \mu' based on percentile, \sigma' somehow

          - apply only to H_U

          - at some point, augments real traffic

    6. EVALUATION

       1. Data

          - real traces

          - top 35000 alexa, ten batches, batch: all page four times

          - negative incoming, positives outgoing

       2. Methodology

          - simulated defense

          - knn

          - also other attacks

       3. Results

          - different percentile values: 0.5 to 0.01

          - normal "better" than log-normal

    7. REALISTIC SCENARIOS

       1. Open-world evaluation

          - wang: leave-one-out

            - here: k-fold, with k=4..10

          - wang: 1 instance in training set for unmonitored

          - more monitored: worse results, converge < 1%

          - ROC by varying neighbors

       2. Multi-tab evaluation

          - Scenario 1: train single tab, test multi-tab

          - Scenario 2: detect number of open tabs, train on multi-tab
            instances

          - evaluate 1:

            - tab1: top 100 sequentially

            - tab2: delay 0.5..5, uniformly from top 100

            - 10-fold cross-validation

            - "the defense does not impact significantly the TPR of
              the classifier."

          - evaluate 2:

            - dataset: single- and multi-tab

            - classifier very good (~89%)

            - defense reduces to ~15%

          - 1st much page better detected than 2nd, the better 2nd
            detection the higher the inter-tab delay

    8. DISCUSSION

       less bandwidth, similar security

       1. Suitability for Tor

          - attack within threat model

          - this uses little bandwidth: 60%

          - that much is most often also free at entry and middle nodes

       2. Open Issues in Realistic Scenarios

          - multi-tab not a real model

          - website distribution of Tor users

       3. Limitations of WTF-PAD

          - not implemented in Tor

          - histograms critical

            - currently bandwidth proposed to be determined by bootstrapping

    9. CONCLUSION

       - no delay

       - multi-tab

       - pluggable transport framework
*** quotes
    - lightweight defenses that provide a sufficient level of security
      against website fingerprinting
    - a tool for evaluating the traffic analysis resistance properties
      of Tor’s Pluggable Transports
    - (ends 0)
    - bandwidth and latency increases come at a cost to usability and
      deployability,
    - While the study does not dismiss WF attacks as impractical, it
      argues that effective defenses may be feasible with bandwidth
      and latency overheads that are much lower than what is required
      in artificial lab conditions.
    - the Tor network has spare bandwidth on its entry edges
    - We have dubbed this new defense Website Traffic Fingerprinting
      Protection with Adaptive Defense (WTF-PAD).
    - (ends 1)
    - the WF problem is treated as a supervised classification problem
    - Our own findings with their [W&G] classifier (see Section VI),
      however, indicate that these open-world findings may be incorrect.
    - (ends 2)
    - equivalent for a client connecting directly to Tor without a
      bridge, but the bridge-based model fits our framework.
    - hard to justify the large-scale deployment of WFcountermeasures
      with potentially heavy overheads that would significantly impact
      the quality and usability of the network as a
      whole. Implementing WF defenses as PTs instead allows
      researchers to evaluate these defenses outside the laboratory
      without introducing excessive overheads in the Tor network.
    - We argue that a defense against WF in Tor should satisfythe
      following requirements:
    - confound classifiers by reducing inter-class variance and
      increasing the intra-class variance
    - The defense should be implemented without requiring a large
      database profiling many websites.
    - (ends 3+4)
    - with a 50% padding rate, the accuracy of end-to-end timing-based
      traffic analysis is significantly degraded.
    - Adaptive Padding (AP) works differently. It does not delay
      application data; rather, it sends it immediately.
    - pads the gaps between data packets so that the interarrival
      packet timings follow a certain distribution, rather than
      imposing a constant rate.
    - (ends 5.0)
    - until transmission starts and it *receives* a real packet
    - (ends 5.1)
    - a fake burst interrupted by a real burst (right).
    - While we are in a burst, the delays we sample from H_{U} will not
      expire until we find an inter-arrival time that is longer than
      typical within a burst, which will make the delay expire and
      trigger the G state.
    - (ends 5.2)
    - In burst mode, the algorithm essentially assumes there is a
      burst of real data and consequently waits for a longer period
      before sending any padding. In gap mode, the algorithm assumes
      that there is a gap between bursts and consequently aims to add
      a fake burst of padding with short delays between packets.
    - In this paper, we follow Shmatikov and Wang and define a burst
      in terms of bandwidth. In particular, a burst is a sequence of
      packets that has been sent in a relatively short time.
    - It also adds padding randomly to some real bursts.
    - burst mode may randomly switch to gap mode in the middle of a
      burst, leading to a fake burst in the middle of a real burst
    - (ends AP algorithm..Beginning of transmission)
    - WTF-PAD does not require an explicit mechanism to conceal the
      total time of the transmission.
    - Inter-arrival times in seconds (*log scale*)
    - the interarrival times follow a log-normal distribution.
    - (ends 5.4)
    - simulate WTF-PAD on the traces of our dataset.
    - used the Kolmogorov-Smirnov test to evaluate the goodness of
      fit.
    - Pareto and Beta distributions seemed to best fit our
      inter-arrival time distributions
    - use normal and log-normal distributions for the sake of
      simplicity,
    - (ends 6)
    - In both multi-tab scenarios, we found that k-NN could detect the
      background page (the first tab) with much greater accuracy than
      the foreground page (the second tab).
    - Additionally, adding significant delay before loading the
      foreground page increases the accuracy of classifying it. The
      likely cause of these findings is that the first few packets in
      a page often hold important features.
    - (ends 7)
    - only the remaining 60% of traffic needs to be covered by this
      defense.
    - distribution of inter-arrival times depends on the average
      bandwidth of the client. We have built our statistical model
      based on the inter-arrival times observed in our
      dataset. However, AP lacks a systematic method to set the
      optimal configuration for each client. We propose to have
      WTF-PAD bootstrap these values during installation.
    - (ends 8)
*** ref [[file:master.bib::wtfpad][Juarez et al. 2015: WTF PAD]]
** [[./HTTP Traffic Model_v1 1 white paper.pdf][Lee - A NEW TRAFFIC MODEL FOR CURRENT USER WEB BROWSING BEHAVIOR]]
*** summary
    0. [@0] ABSTRACT

       - squid log analyzer

       - well-matching http model and generator

    1. INTRODUCTION

       - previous data limited

       - need for privacy-aware data gathering

         - +: constant updates of data

       - new model

    2. RELATED WORKS

       - empirical: web page

       - web request

       - based on TCP/IP headers (problem: top level vs embedded)

       - distinguish user clicks

    3. HTTP TRAFFIC MODELING OVERVIEW

       - on and off

       - boundary: web request

         - 1 second: user can blick before that

         - user action: not possible to distinguish

         - referer: can be used for both cases, is optional

    4. MODELING METHODOLOGY

       squid logs

       1. Traffic Parsing

          - browser cache not modeled, unclear (but also irrelevant)

          - cache files: time logged to end of sending (not receipt) \to algo

          - privacy filtered (url out, except for file extension)

          - separated by client-IP

          - each "http session" has parameters computed

          - fitted with lognormal, weibull, exponential, gamma,
            generalized pareto,

            - selected by log-likelihood and finite parameters

       2. Response Arrival Time

          - algorithm for time, dependent on MTU < L < 10*MTU

       3. HTTP Session

          - browsing session, after about 2:45 hrs

       4. Web-Request

          - session = series of Web-Requests

          - Web-Request: HTML + [embedded]+

            - next HTML ends

            - parallelism consciously ignored

          - embedded IAT, parsing time bounded by 5 minutes (TCP session)

          - HTML by Content-type || file-ending: html/htm, cgi, asp,
            sml, stm, php, '/'

          - see quotes-table

    5. MODEL AND ANALYSIS RESULT

       1. Model Parameters

          0. [@0] details about specific parameters in detail

          1. /HTML object size/ and /Embedded object size/

             - embedded bigger than html

          2. Number of embedded objects

             - same as before, maybe due to cacheing

          3. Parsing time: between HTML *arrival* and first object *arrival*

          4. Embedded Object Interarrival time: between object *arrivals*

          5. Reading time: arrival last object to arrival new HTML object

          6. HTML object IAT: between two html objects (validation param)

          7. request size: size of http requests

       2. Caching Model

          - identified by http 304

          - only 190 bytes on average

          - 16% cached of all requests

          - 1.5%of html average size

          - model number of consecutive hits

    6. VALIDATION

       - test via HTML IAT (only)

    7. CONCLUSIONS

       - changes in user behavior

         - video streaming increases embedded object size

         - less IAT reading times: high speed networks, caching, proxies, ...

       - no packet data, thus no timing,

         - believe to be fair
*** quotes
    - the actual packet inter-arrival time (IAT) is governed by
      specific conditions that existed in the internet when the trace
      was collected and are unique in that context.
    - (ends 1+2)
    - Typically, HTTP traffic is modeled as an ON/OFF source with the
      ON state corresponding to the request and download of the
      objects and the OFF state corresponding to the inactive time.
    - It is possible that the ON state lasts over multiple Web-Request
      periods when the downloading of the last embedded object and the
      next HTML object overlaps.
    - (ends 3)
    - Table 2: Analysis Result of HTTP Model Parameters
      | Parameters         | Mean            |   S.D. | Best Fit              |
      |--------------------+-----------------+--------+-----------------------|
      | <18>               | <15>            |        | <21>                  |
      | HTML Object size   | 11872 (Max 2MB) |  38306 | Truncated Lognormal (μ=7.90272, σ=1.7643) |
      | Embedded ObjectSize | 12460 (Max 6MB) | 116050 | Truncated Lognormal (μ=7.51384, σ 2.17454) |
      | Number of Embedded objects | 5.07 (Max 300)  |        | Gamma (κ=0.141385, θ=40.3257) |
      | Parsing Time       | 3.12 (median 0.30)(max 300 sec) |  14.21 | Truncated Lognormal (μ=-1.24892, σ=2.08427) |
      | Embedded Object IAT | 0.83            |    8.4 | Weibull (α=0.2089, β=0.376) |
      | Reading Time       | 39.70 (max 10,000 sec) | 324.92 | Lognormal (μ=-0.495204, σ=2.7731) |
      | Request Size       | 318.59          | 179.46 | Uniform (350 Bytes)   |
*** questions
*** ref: [[file:master.bib::newtrafficmodel][Lee & Gupta 2007]]
** [[/home/chive/import/pakdoc/rfc1928.socks5.txt][Leech - SOCKS Protocol Version 5]]
*** summary
    1. Introduction

       - firewall traversal with authentication

       - does not forward ICMP

       - both TCP and UDP

    2. Existing practice

       - SOCKS Version 4: unsecured, TCP-based

       - extends to include UDP, strong authentication, domain-name, IPv6

    3. Procedure for TCP-based clients

       1. client opens connection to SOCKS port,

       2. authentication negotiation

    4. Requests

       - connect OR bind OR udp associate

       - address type

       - address

       - port

    5. Addressing

       domain name has as first octet the name octets

    6. Replies

       ...
*** quotes
    - Compliant implementations MUST support GSSAPI and SHOULD support
      USERNAME/PASSWORD authentication methods.
** [[./Liberatore_2006.pdf][Liberatore - Inferring the Source of Encrypted HTTP Connections]]
*** summary
    0. [@0] ABSTRACT

       - two traffic analysis techniques

       - work to identify sites at internet scale

    1. INTRODUCTION

       - traffic analysis to get information from PETs (Tor, SSH
         tunnels, IPSec, ESP, WPA-ll)

       - Contributions: large trace set, high accuracy, padding as
         defense, profile all front pages with 13GB of storage

    2. RELATED WORK

       - Wright: 15,16: vulnerable to passive logging and intersection

       - Hintz, Sun 12, Bissias 1: profiling attacks based on object sizes

       - Fu 8: dummy packets, Levine 10: dropping packets, 13: partial-path ct

       - here: study packet length, not object length

    3. MODEL

       0. [@0]

          - how to capture

          - Jaccard's coefficient and naive Bayes

       1. Observer Model

          - listen at encrypted OpenSSH-SOCKS tunnel

          - observe packet lengths (interarrival times also possible)

          - able to distinguish start/end of requests

          - record database not necessarily at same connectionb

       2. Profiling Methods

          - trace + attributes (direction, length): instance, has class (url)

          - similarity metric: Jaccard's coefficient (see [[(liberatore-1)]])

            - attribute to set if in majority of instances

            - normalize S(X, Y): divide by \sum_{Y \in U}S(X, Y) (U: training sets)

          - naive Bayes classifier

    4. DATA COLLECTION

       0. [@0]

          - large dataset

          - representing real-world users

          - collected like real attacker

       1. Initial Data Collection

          - use real traffic

          - strip lots

          - select top 2000 (64% of ~100000)

       2. Page Retrieval

          - GNU/Linux, Firefox 1.5, OpenSSH 4.2p1 (links to each) as
            SOCKS proxy

            - FF: disabled caching, update checks, etc

          - tcpdump 3.9.4, libpcacp 0.9.4, first 68 bytes of each packet

    5. EVALUATION

       0. [@0]

          - jaccard vs bayes, jaccard has 60% accuracy

            - scales well

          - gather training data before or afterwards

          - countermeasure evaluation: 145% overhead yields accuracy drop to 8%

       1. Experiment Setup

          - as in section 4

          - top-2000, every six hours, for two months, 480000 samples

          - vary training, testing, \Delta, number of sites

          - measures: k-identifiability and k-accuracy see quotes

       2. Classifier Performance

          - test and training: each full day, one extra day apart

            - n=1000 sites

          - more training: better accuracy, lower \Delta: higher accuracy

          - k of course improves performance

          - accuracy seems to scale down logarithmically with number of
            sites (scalable)

       3. Forensics Feasibility

          - collect traffic traces after you saw traffic

            - train data *after* test data (negative \Delta)

          - accuracy reduced by "less than 3%"

       4. Explaining Performance

          - huge amount of information (137 bits) per unpadded trace

          - if conscious about this problem, padding defeats this

       5. Countermeasure Effectiveness

          - measures:

            - linear: pad to next mutiple of 50

            - exponential: pad to next multiple of two (or MTU)

            - mice/elephant: pad to either 100 or 1500

            - MTU: pad all to MTU (150% overhead)

          - Bayes performs quite well even for maximal padding

    6. DISCUSSION

       - easy to identify

       - anonymity systems should pad (f.ex. Tor)

         - but Tor might be vulnerable to timing analysis

       - law enforcement might benefit from database/tools

       - log-linear scales well to web

       - dynamic content uses templates: easy to update

       - static content updated less frequently

    7. CONCLUSION AND FUTURE WORK

       - next steps

         - steps: interdependent classifier

         - more padding: non-deterministic might be better

         - more traces

         - use timing data, but depends on location
*** quotes
    1. INTRODUCTION
       - Encrypted tunnels thwart the legitimate gathering of evidence
         by authorized law enforcement.
    3. [@3] MODEL
       2. [@2] Profiling Methods
          - For two sets X and Y , *Jaccard’s coefficent* S is defined as:
            #+BEGIN_SRC latex
              S(X, Y) = |X \cap Y| / |X \cup Y| (ref:liberatore-1)
            #+END_SRC
    5. [@5] EVALUATION
       1. Experiment Setup
          - The *k-identifiability* for an instance is defined as 1 if
            the actual class of the instance is in the top k of the
            predicted class list, as ordered by probability estimate
            (predicted classes with the same estimate are ordered in
            an arbitrary but fixed manner) or 0 if the actual class is
            not in the top k. The *k-accuracy* for an experiment is
            the average of the k-identifiability value for each
            instance in the test set.
*** ref [[file:master.bib::Liberatore:2006][Liberatore & Levine 2006: Inferring Source Encrypted HTTP Connections]]
** [[./torspec/address-spec.txt][Mathewson - Special Hostnames in Tor]]
*** summary
    1. Overview

       - most often, lets exit node resolve domain

       - special hostnames

    2. .exit

       - [hostname.]name-or-digest.exit

       - use router name-or-digest to exit the tor network (disabled)

       - may cache IP for hostname

    3. .onion

       - digest.onion

       - "first eighty bits of SHA1 hash of identity key" (base32)

       - use hidden service, see rend-spec

    4. .noconnect

       - string.noconnect

       - disabled in 0.2.2.1-alpha

       - close connection
*** ref [[file:master.bib::address-spec][Mathewson 2011: Special Hostnames Tor]]
** [[./10.1.1.10.5823.pdf][Mistry - Quantifying Traffic Analysis of Encrypted Web-Browsing]]
*** summary
    0. [@0] Abstract

       - for example banking

       - see which websites a user visits just by packet size

    1. Introduction

       - ssl encrypts but does not obscure

       - scenarios

         - what is done at oneline banking, which type of transaction

         - buying or selling shares

         - which books at oneline bookstore

       - tools

         - taffic sizes on path

         - can browse site beforehand

       - only one account at those sites caveat

       - ssl: v2 and v3

    2. Methodology

       0. [@0] (procedure)

          1. gather information

             1. record sizes of traffic (with tcp header stripped)

             2. locality information: which page reachable from which other page

          2. record and filter client requests

             - separate requests by port number

             - recognize ssl handshake

               - within handshake: client sends requests with

               - response sizes of each request

          3. match in \pm 40 bytes increments

       1. Choice of web sites

          - https://www.isaac.cs.berkeley.edu: static, few images, v2

          - https://www.index.berkeley.edu: also generated, many images, login,v3

          - https://banking.wellsfargo.com: mostly generated, login, v2

          - https://orders.datek.com: user login, many generated, v3

       2. Test cases

          - no images, no cache

          - images, no cache

          - images, cache

          - images, warm cache

    3. Results

       1. ISAAC

          - a, b exact or approximate for all pages

          - c only when download, d not recognized

       2. INDEX

          - some (entry) consistently mismatched, rest good

       3. Wells Fargo Banking Site

          - some not reconized: varying greatly,

          - some with locality recognized

          - some consistently

       4. Datek Stock Brokerage Site

          - greatly size-varying mismatches, rest good

       5. Some Interesting Observations

          - SSLv2 and v3 recognizable by packet sizes

          - dynamic pages (Wells Fargo) invalidated caching defense, B=C=D

          - D - warm cache: most pages not transferred

    4. Conclusions

       - works on most cases

       - does not work when size varies greatly

    5. Further Work

       - more functionality

         - automatic creation of size-database (\to website fingerprints)

         - real-time traffic analysis by integrating sniffer and analysis

       - further traffic analysis

         - more client variability

         - locality necessary?

         - weaken by adding large random padding

         - ipsec:

           - encrypt port

           - simultaneous multiple connections
*** quotes
   3. [@3] Results
      - traffic analysis identified all pages fairly accurately. We
        did not have to use any locality information for recognizing
        any of the pages on this site.

*** ref: [[file:master.bib::quantifying][Mistry & Raman 1998: Quantifying Traffic Analysis]]
** [[./ndss16-wfp-internet-scale.pdf][Panchenko - Website Fingerprinting at Internet Scale]]
*** summary
    0. [@0] Abstract

       - faster

       - more accurate

       - still inherently (?) problematic

    1. INTRODUCTION

       - anonymity services for censorship circumvention and freedom
         of expression

       - local attacker one of the weakest adversaries

       - traffic analysis, infer information about the content by
         observing data flow

       - novel attack: "cumulative behavioral representation"

       - 300,000 webpages, not only index pages

       - alexa plus others, twitter links, exit node

       - webpage very hard, website may be possible

       - contributions

         1. novel attack

         2. dataset

         3. page vs site

    2. BACKGROUND

       - /instance/ trace of a single website

       - closed vs open world

       - background set vs foreground set

       - website vs web page

       - attacker: passive observer, monitor link or compromised entry node

    3. RELATED WORK

       96 schneier

       1. Traffic Analysis on Encrypted Connections

          - 98 quantifying first, 2001 hintz

          - sun jaccard, all http object sizes

          - bissias: IP packet sizes + IAT

          - lu: also packet ordering

       2. WFP in Anonymization Networks

          - herrmann first, panchenko worked, dyer,

          - cai website, juarez,

          - kwon: hidden service differentiable, yet not among them

       3. Countermeasures against WFP

          - liberatore: padding

          - morphing

          - panchenko: padding

          - fixed rate

          - walkie-talkie

          - application-level: randomized pipelining + HTTPOS

    4. DATA SETS

       0. [@0]

          - only index pages

          - did not see link-based web structure

       1. Data sets provided by Wang et al.

          - wang13: 100@40

          - wang14: 100@90 + 9000@1 from top 10000

          - both removed some cells (SENDME) (see Improved...)

          - rerecord with SENDME etc as ALEXA100

       2. RND-WWW: An Unbiased Random Sample of the World Wide Web

          several sources, combined as RND-WWW: >120000 unique web pages

          1%@40, 99%@1

          1. Twitter: live access to randomly-chosen tweets

          2. Alexa-one-click: alexa top 20000 plus one link on main page

          3. Google: most frequently used keywords from Australia,
             ...; random selection of link from first to fifth page

          4. Googling at random: 20000 english terms from dictionary,
             select as 3

          5. Censored in China: as of greatfire.org

       3. Monitoring Real Tor Traffic

          - TOR-Exit

          - referer of links (otherwise also images etc)

          - ~210000 pages, ~65000 unique, of which ~45000 only once

       4. Website-Collections

          - WEBSITES

          - 20 popular websites from sevral categories

          - 50 links per site

          - 90 instances of index, 15 of each subpage

    5. EXPERIMENTAL SETUP

       0. [@0]

          - wlog, attack collects traces using same anonymity service as target

          - matches to victim, using statistical methodds

       1. Data Collection

          - tbb 3.6.1

          - tcpdump, chickenfoot, iMacros, Scriptish, stem (tor control)

       2. Data Extraction and Processing

          - different layers: TCP or TLS or cell (see sec 7.2)

          - remove faulty by http error

          - remove outliers by quantiles (5%)

            - of sum of incoming packet sizes =: I

            - formula
              Q1 − 1.5(Q3 − Q1) < I < Q3 + 1.5(Q3 − Q1).

    6. A NOVEL WEBSITE FINGERPRINTING ATTACK

       - contrary to wang: abstract representation

       - layer used for data extraction? (seems like tor cell)

       - cumulative sum of packet sizes, sampled at equidistant points

       - 100 points yields good accuracy/efficiency tradeoff =: CUMUL

       - libsvm with rbf kernel, 10-fold cross-validation

       - parameters: c = 2^11 , ... , 2^17 and γ = 2^−3 , ... , 2^3

    7. EVALUATION AND DISCUSSION

       A. [@0]

          - optimal layer of extraction

          - optimize parametrization

          - compare to knn

          - attack scenarios

          - realistic attack for web site fingerprinting

       A. [@A] Layers of Data Extraction

          - idea: tls allows for start/end recognition

          - TCP or TLS or CELL (w/o SENDME)

            - cell layer is best

          - removing SENDME effect varies around plus-minus 0

          - better to add cell size than 1 per cell

       B. Optimizing Feature Sampling

          - number of features increses granularity and time

          - optimal trade-off for around n=100

            - cell better than TCP better than TLS

       C. Comparison with State of the Art

          compare knn with this (CUMUL)

          1. closed-world

             - bit better

             - marginal gains

             - differences in cross-validation:

               - knn: 60 train, 30 test

               - cumul: 10-fold cross-validation

          2. Open World

             - more knn computation

               - no difference in accuracy

             - two-class vs multi-class

               - two-class: monitored or not

               - multi: which monitored one

               - knn: different yahoos as different sites

             - knn (modified) took much longer than CUMUL (with
               increasing background set size)

               - CUMUL grows better

       D. Webpage Fingerprinting at Internet Scale

          - single webpage classifiers

          - filtering background

            - either filtered background (only those not on same site)

            - or unfiltered (all)

          - precision and recall

            - instead of TPR/FPR

            - see base rate fallacy

            - eve wants to find group which may have visited: recall

              - wants to find one user who has visited: precision

          - both fall drastically as the number of pages grows

            - "this attack does not scale"

       E. Detection of Websites

          - closed-world worse

          - open-world stabilizes better

    8. CONCLUSION AND FUTURE WORK

       - better and faster than previous

       - website fp might work, web page did not really scale

       - not staying on same page evaluated, further work

       - published "datasets and tools"
*** quotes
    3. [@3] RELATED WORK
       3. [@3] Countermeasures against WFP
          - Note that we have a disjoint dataset for tuning the
            features and optimizing the SVM parameters.
    7. [@7] EVALUATION AND DISCUSSION
       C. Comparison with State of the Art
          - TABLE II: Accuracy of both classifiers for the WANG14
            dataset (all values in %).
            |                      | 90 Instances | 40 Instances |
            |----------------------+--------------+--------------|
            | k-NN (3736 features) |        90.84 |        89.19 |
            | CUMUL (104 features) |        91.38 |        92.03 |
          - TABLE III: Results for the open-world scenario of both
            classifiers using the WANG14 dataset (all values in %).
            |     | multi- | class |  two- | class |
            |     |   k-NN | CUMUL |  k-NN | CUMUL |
            |-----+--------+-------+-------+-------|
            | TPR |  89.61 | 96.64 | 90.59 | 96.92 |
            | FPR |  10.63 |  9.61 |  2.24 |  1.98 |
*** ref [[file:master.bib::panchenko2][Panchenko et al. 2016: Website Fingerprinting Internet Scale]]
** [[./acmccs-wpes11-fingerprinting.pdf][Panchenko - Website Fingerprinting in Onion Routing Based Anonymization Networks]]
*** summary
    0. [@0] Abstract

       local website fingerprinting based on volume, time & direction

    1. INTRODUCTION

       most attacks need some additionaly knowledge, f.ex. seeing both
       ends,

       - between OP and guard node easily observable

       - and gives 80% (73% open) against JAP and 55% against Tor

       - f.ex. ensure that censored /pages/ are not viewed

       - studies influence of supposed features,

       - propose camouflage

    2. RELATED WORKS

       - Hintz coined "website fingerprinting" in 2002 (paper)

       - 1996 Wagner/Schneier

       - 1998 Berkeley project about SSL traffic analysis

       - Bissias: IP packet sizes and inter-arrival times

       - Liberatore and Levine:
         - OpenSSH tunnels,
         - Jaccard + naive Bayes classifier,
         - consider only packet size of transmitted data,
         - neglect timing and order information

       - Wright: morphing as countermeasure

       - Herrmann:
         - multinomial naive Bayes classifier
         - OpenSSH, OpenVPN, Stunnel, Cisco IPsec-VPN, JAP, Tor
         - 90%, 90%, 90%, 90%, 20%, 2.95%

    3. DATA SETS

       0. [@0]

          - Firefox modification: No JS, Flash, Java, Cache

          - Scripting Chickenfoot

          - Data from Herrmann et al and Open-World-Dataset

       1. Closed-World Dataset

          - incoming size as positive, outgoing as negative

          - only fully-loaded pages: users reload else \to load time to 600s

          - "20 instances per website from our list of 775 sites"

       2. Open-World Dataset

          - Alexa top 1000000,

          - three censored: sexually explicit, top, random from alexa

       3. Countermeasure Dataset

          - applied to closed-world (more difficult to camouflage ==
            easier to detect)

          - if hampered, then also in open-world

          - at the same time load a random website

          - open: attacker has not seen the user's normal pattern

          - separate dataset for tuning features and optimizing SVM

    4. A NEW APPROACH

       0. [@0] features, machine learning, compare to bayes, improve via SVM

       1. Features

          facilitate subsequent classification, describes most relevant

          Without Packets Sized 52: no ACKs

          - *Size Markers* of uninterrupted flow (except ACK), grouped
            by 600 bytes

          - *HTML Marker*: size of html document (first uninterrupted flow)

          - *Total Transmitted Bytes*: grouped by 10000 bytes

          - *Number Markers*: number of uninterruped packet flow in
            direction, grouped by 1,2,3-5,6-8,9-13,14

          - *Occurring Packet Sizes*: grouped by 2, in/out

          - *Percentage Incoming Packets* grouped by 5%

          - *Number of Packets*, in/out grouped by 15

          not working

          - incoming/outgoing packets,

          - leaving out frequent/rare sizes,

          - including TLS/SSL record sizes,

          - leaving empty TLS records,

          - preserving packet order,

          - rounding packet sizes,

          - rounding packet frequencies

       2. Support Vector Classification

          Uses SVM instead of Bayes classifier

          SVMs try to separate the points via a hyperplane, maximizing
          the distance between the closes instances (= support
          vectors) and the plane

          He uses a radial basis function kernel with parameters C=2^{17}
          and \gamma=2^{-19}.

          Finding these was the longest computation time.

    5. EXPERIMENTS AND RESULTS

       1. Experiments

          closed-world: ten-fold stratified cross-validation

          open-world: sufficient amount of data, not necessary

          how:

          - five censored, Sex Exp, Alexa Top, Alexa Random:
            35 instances as training, 25 as test

          - uncensored:
            4000 at random from top 1000000 for training,
            1000 for test, disjoint from training

          - 20 times measured, each with new uncensored

       2. Results
          0. [@0]
             - Closed-World: recognition rates of 54.61% Tor, 80 % JAP

             - Open-World: true positive rate of up to 73%

          1. Results on Closed-World Dataset

             - Final Result via checking if really loaded and removal
               of redirects

             - JAP premium cascades worse in WF than free cascades

          2. Results on Open-World Dataset
             1. Experiment 1
                - 5 censored pages, 35 instances each, 4000 uncensored
                  pages, 1 each

                - test: 1000 which differ from the 4000

                - top ranked most easy to distinguish

             2. Experiment 2

                - 5 censored, 20 training and 2 testing

                - uncensored variable: tp and fp both fall with more
                  examples of uncensored sites (measured up to 4000)

             3. Experiment 3

                - censored from whole of alexa, varying number, 35
                  instances each

                - unsensored, 4000, 1 instance each

                - the more censored pages, the less clear the
                  classifier: fp rises, less impact on tp

             4. Experiment 4

                - 5 censored, varying instance numbers

                - 4000 uncensored, 1 instance each

                - the more instances, the clearer, converges at about 35

             5. Summary:

                - Your ISP could find out what you do online

    6. COUNTERMEASURES

       - padding works rather bad

       - camouflage: load randomly chosen web page simultaneously

       - used in both training and testing

       - to 3% where close to random would be optimal

    7. CONCLUSION AND FUTURE WORK

       Website Fingerprinting is possible in Tor and JAP, camouflage hampers.

       Next:

       detect: additional feature selection, active content, embedded
       links, analyse specific webpages,

       deter: browser plug-in, user feedback per page, parallel camouflage

*** quotes
    0. [@0] Abstract
       - We first define features for website fingerprinting solely based
         on volume, time, and direction of the traffic.
       - Finally, we show preliminary results of a proof-of-concept
         implementation that applies camouflage as a countermeasure to
         hamper the fingerprinting attack. For JAP, the detection rate
         decreases from 80% to 4% and for Tor it drops from 55% to about
         3%.
    1. INTRODUCTION
       - Several attacks against anonymization networks have been
         discovered, e.g., [6, 17, 19, 18], most notable the traffic
         confirmation attack.
       - Totalitarian regimes such as China or Iran usually do not have
         control over the communication party located in western
         countries precluding a traffic confirmation attack.
       - this attack is very realistic and anonymization networks must by
         all means be secure with respect to local attacks.
    2. RELATED WORKS
       - Instead of transforming websites, we obfuscate the page by
         loading another page in parallel.
    3. DATA SETS
       0. [@0]
          - In practice an attacker first retrieves a certain amount of
            relevant web pages by himself as training data for
            fingerprinting, using the anonymization network that he assumes
            his victim uses as well.
       1. Closed-World Dataset
          - the victim retrieves only web pages from the predefined
            set and the attacker knows the set of possible web pages.
          - For each fetch of a URL, we store the sizes of packets in the
            observed order while recording incoming packets as positive,
            outgoing ones as negative numbers.
    5. [@5] EXPERIMENTS AND RESULTS
       0. [@0]
          - We once more achieve alarming detection rates motivating the
            need for additional countermeasures for anonymization networks
       1. Experiments
          - the data is split into n evenly large parts, the
           /folds/. Then, the entire process of training and testing
           is repeated n times, using one of the n folds as test data
           and the remaining n − 1 folds as training data in
           turn. The results are averaged and therefore more solid
           and meaningful.
       2. [@2] Results
          - we disregard web pages with a redirect statement and add the
            URLs of the final web pages instead.
    6. COUNTERMEASURES
       - Successful countermeasures should decline the detection rates of
         all web pages to a level that is almost similar to random guess
         and at the same time cause only little performance losses.
       - We expect even better obfuscation for additional background
         pages as it will be more challenging for the attacker to extract
         the original statistics from the merged packets. Still, it has
         to be explored whether more sophisticated statistical measures
         can achieve this extraction.
    7. CONCLUSION AND FUTURE WORK
       - We achieved a surprisingly high true positive rate of up to
         73% for a false positive rate of 0.05%
       - This countermeasure can be simply implemented in the form of
         a browser plug-in.
*** ref [[file:master.bib::panchenko][Panchenko et al. 2011: Website]]
** [[../sw/p/classifier/README.txt][Panchenko - classifier/README.txt STEPS]]
*** summary
    - after installation (python) of tldextract and natsort, evaluate via
      #+BEGIN_SRC sh
        python eval.py -ffg Foreground_40I_100Cum_1125_TLS -fbg Background_100Cum_111884_TLS -rmtmp -webpage -dirin ../data/ -dirout ../output/ -svm ../libsvm-3.20/ # open-world (filtered link)
        python eval.py -ffg Foreground_40I_100Cum_1125_TLS -fbg Background_100Cum_111884_TLS -rmtmp -website -dirin ../data/ -dirout ../output/ -svm ../libsvm-3.20/ # open-world (filtered domain)
        python eval.py -fcw FILENAME -rmtmp -dirin ../data/ -dirout ../output/ -svm ../libsvm-3.20/ # closed-world
      #+END_SRC
** [[./critique.html][Perry - A Critique of Website Traffic Fingerprinting Attacks]]
*** summary
    0. [@0] A Critique of Website Traffic Fingerprinting Attacks

       - closed vs open world

       - pages vs websites

    1. Pinning Down the Adversary Model

       - leave rest of traffic unmolested, only block specific traffic

       - identifying users visiting small set

       - recognizing every single web page

    2. Theoretical Issues: Factors Affecting Classification Accuracy

       factors

       1. "size of the hypothesis space"

       2. "accuracy of feature extraction and representation"

       3. "size of the instance space"

       4. "number of training examples"

       huge world, which leads to problems

    3. Practical Issues: False Positives Matter. A Lot.

       - 100 pages per user with fpr=0.2 \to 18% probability (=18% of
         userbase) will be accused

         - 1000 pages: 86.5% of userbase

       - P(Censored|Classified) = 0.10 vs P(~Censored|Classified) = 0.90

       - IDS failed in reality

    4. Practical Issues: Multipliers of World Size are Common

       - even 1000000 pages are way too few

       - caching/ js/ adblock each multiply different variations

       - non-web vs web (f.ex. twitter query similar to xmpp/irc)

    5. Literature Review: Andriy Panchenko et al

       - good: see quote about good parts

       - could be better:

         - false positives property of specific sites (multiple runs)

         - lacks realistic adversary model

    6. Literature Review: Kevin P. Dyer et al

       - good: source, good definitions, buflo

       - improveable: small closed world,

         - little analysis of defenses (helped against which feature, etc)

         - lacked which sites suffer misclassification

         - lacked "standard ways of evaluating the contribution of
           features to accuracy under various conditions"

    7. Literature Review: Xiang Cai et al

       - good:

         - low-resource congestion-sensitive tunable buflo variant

         - automatic feature detection

       - improveable:

         - edit-distance well suited to closed-world, (also to open?)

         - defenses could be more mentioned and better-analyzed

         - HMMs are user-specific (?), f.ex. twitter usage varies widely

    8. Literature Review: Tao Wang and Ian Goldberg

       - improveable:

         - use current browser

         - bigger world size

         - have varying target sizes

    9. Concluding Remarks: Suggestions for Future Work

       - light-weight might suffice

       - defenses need re-test, new defenses welcome

       - attacks: please evaluate feature importances

       - do contact them

    10. Concluding Remarks: Dual Purpose Defenses

        - might also help against end-to-end dragnet

          - especially if traffic to inside of Tor network

          - ?? but also if against different hosts (different circuits) ??
*** quotes
    0. [@0]
       - defenses were "broken" and dismissed primarily by taking a
         number of shortcuts that ignore basic properties of machine
         learning theory and statistics in order to enable their claims
         of success
    2. [@2] Theoretical Issues: Factors Affecting Classification Accuracy
       - First, for the same world size and classifier technique,
         every work that examined both the open and closed worlds
         reports much higher accuracy rates for the open world than
         for the closed world
    4. [@4] Practical Issues: Multipliers of World Size are Common
       - in Tor Browser, either restarting the browser or using the
         "New Identity" button causes all of this caching state to be
         reset.
    5. [@5] Literature Review: Andriy Panchenko et al
       - All of the other papers to date simply omit one or more of
         the following: data on feature extraction, the contribution
         of individual features to accuracy, the number of training
         examples, the effect of target size on both the open and
         closed world, the effect of target page types on accuracy,
         and the effects of world size on accuracy.
    11. [@11] Comments
        - Tor browser could have option for users that an image has a
          50% probability of being downloaded automatically and the
          rest can be downloaded by clicking. This is a hybrid option
          between allowing and not allowing images. Or the probability
          is user adjustable between 10% and 90% but always same for
          one website in one session.
*** ref [[file:master.bib::critique][Critique Website Traffic Fingerprinting Attacks]]
** [[./experimental.html][Perry - Experimental Defense for Website Traffic Fingerprinting]]
*** summary
    - prior: belief that all was well (failed attempts)
    - panchenko: showed that not
    - disagree with background fetch: additional traffic
    - first attempt at mitigation: enable http pipelining and
      randomize pipeline size, request further research
    - other: http to spdy-translation && obfsproxy
*** quotes
    - Despite these early results, whenever researchers tried naively
      applying these techniques to Tor-like systems, they failed to
      come up with publishable results (meaning the attack did not
      work against Tor), due largely to the fixed 512 byte cell size,
      as well as the multiplexing of Tor client traffic over a single
      TLS connection.
    - We create it as a prototype, and request that future research
      papers do not treat the defense as if it were the final solution
      against website fingerprinting of Tor traffic.
    - However, the defense could also be improved. We did not attempt
      to determine the optimal pipeline size or distribution, and are
      relying on the research community to tweak these parameters as
      they evaluate the defense.
    - as these translations are potentially fragile as well as
      labor-intensive to implement and deploy, we are unlikely to take
      these measures without further feedback from and study by the
      research community.
    - We would love to hear feedback from the research community about
      these approaches, and look forward to hearing more results of
      future attack and defense work along these and other avenues.
*** ref: [[file:master.bib::experimental][Perry 2011: Experimental Defense Website Traffic Fingerprinting]]
** [[./ShWa-Timing06.pdf][Shmatikov - Timing analysis in low-latency mix networks: attacks and defenses]]
*** summary
    0. [@0] Abstract

       Analyze correlation attacks on low-latency mix networks.

       previous defenses ineffective or huge latency or huge overhead

    1. Introduction

       - low-latency mix networks

       - path establishment not researched

       - realistic traffic model based on HTTP traces

       - adaptive padding

         - fill statistically unlikely gaps

         - also against active attackers

       - compare to constant rate dummy traffic

    2. Related work

       - Venkatraman: mathematical model, defense requiring complete knowledge

       - Timmerman: traffic masking: assure certain profile via cover
         traffic and artificial delays

       - Berthold: high-latency, intermediate dummy, global,

         - CONTRA: not fine-grained

       - Rennhard: artificially delay traffic

         - CONTRA: latency

       - Fu: constant rate is vulnerable to analysis of inter-packet
         arrivals,

         - SOLUTION: variable inter-packet intervals

       - defensive-dropping: drop at intermediate, same constant rate,

         - CONTRA: latency

       - Overlier: trusted entry node,

         - PRO: combines with this

    3. Model and metrics

       1. Network

          sender, path of N (small: {2,3}, large: {5..8}) mixes, destination

          like tarzan

          (end-to-end TCP?)

       2. Timing analysis

          - attacker measures inter-packet intervals

          - active: also impose unique timing signature

          - simple model: ignores TCP effect of drops and bursts

       3. Defense metric.

          - adversary: correlate packet counts

          - crossover rate: see "timing...systems"

       4. Negative impact on network performance.

          - metric: padding ratio

          - metric: extra delay

    4. Adaptive padding

       create statistic traffic, send if no packet in next drawn
       sample time

       1. Data structures: bins, indexed exponentially

       2. Adaptive padding algorithm

          1. take token

             - if packet arrives before time, remove token for packet
               arrival time, replace token into bin

             - else, send dummary after token's time

          2. until token buckets empty

       3. Destroying natural fingerprints

          - 2 modes

            - burst: packet received, take expected high-duration

            - gap: dummy sent, take expected low-duration

    5. Experimental evaluation

       0. [@0] 

          - 4 schemes

            - undefended

            - defensive dropping

            - defensive dropping variant with constant rate cover traffic

            - adaptive padding

          - exponential distribution

       1. Attack model

          - observes 60 seconds

          - does cross-correlation for 1-second-steps

          - sets threshold t to equals FPR and FNR

          - defender's goal: high t

       2. Evaluation results

          - constant rate dummies and no cover fails

          - defensive dropping works by upping other streams' correlation

          - adaptive padding works by lowering this stream's correlation

       3. Short paths

          2-3 mixes

          - defensive dropping works worse

          - adaptive padding works better

    6. Active attacks

       0. [@0]

          - impose timing signature

          - attacker cannot create or replay packets

       1. Artificial gaps

          - drops of several packets in a row

            - recognizable with defensive dropping

            - but not with adaptive padding

       2. Artificial bursts

          - queue packets, then send all at once

            - defensive dropping crossover rate drops to 0

            - adaptive padding lower, the longer (.21 with 15-second-queue)

    7. Comparison with constant-rate defenses

       compare to constant-rate defenses

       - latency is prohibitive: especially in low-latency mix networks

         maybe for non-interactive applications

       - Everybody must send at the same constant rate: see section
         5.2 most flows must use this, in contrast to adaptive padding

       - There is no “right” constant rate: f.ex. bursts require high
         rate which might congest the network

       - Defense fails against active attacks: see section 6

       - Defense fails at high traffic rates: if higher rate than
         fixed, most packets are real, which "is vulnerable to basic
         timing analysis"

       - Defense reveals presence of real traffic: sparse is padding,
         dense is real traffic

    8. Creation and management of dummy packets

       1. simplest: creation by mix, but nonforwardeable

       2. pre-computed on generation and times of quiet

          - con: stream ciphers

       3. Malicious clients and servers.

         bad, a.p. requires server cooperation, TODO: reread

       4. Reverse routes.

          === Server shares keys with all route mixes, but there is an
          attack

    9. Challenges and future directions

       - only some attacks looked at

       - difficult to hide that there is traffic

       - the more padding the later in the chain

         - detectable
*** quotes
    1. Introduction
       - Many mix networks are specifically intended to provide anonymity
         against attackers who control the communication medium.
       - Traffic analysis is an especially serious threat for low-latency
         mix networks because it is very difficult to hide statistical
         characteristics of the packet stream and satisfy the stringent
         latency requirements imposed by interactive applications.
       - The standard measure of success is the /crossover error rate/, at
         which the attacker’s false positive rate is equal to his false
         negative rate.
       - (Maximum crossover error rate is 0.5, which corresponds to
         random guessing.)
    2. Related work
       - recognized as a serious threat to low-latency mix networks, but
         few defenses have been proposed to date.
    3. Model and metrics
       - An active attacker can also impose his own unique timing
         signature (by dropping packets or introducing artificial bursts)
         onto the flow he is interested in, and then attempt to identify
         this signature on other network links.
    4. Adaptive padding
       0. [@0]
          - samples from the statistical distribution of inter-packet
            intervals. If the next packet arrives before the chosen interval
            expires, it is forwarded and a new value is sampled. To avoid
            skewing resulting intervals towards short values, the
            distribution is modified slightly to increase the probability of
            drawing a longer interval next time.
          - rough statistical distribution of inter packet intervals for a
            “normal” flow [...] can be pre-computed from traffic
            repositories such as NLANR [21]
       1. Data structures
          - increasing the range represented by each bin exponentially with
            the bin index works well.
    5. Experimental evaluation
       0. [@0] 
          - We do not model TCP acknowledgements, re-transmissions,
            exponential backoff in response to dropped packets, and other
            TCP features that may be exploited by a sophisticated attacker.
        1. Attack model
          - We conjecture that long-term attacks cannot be prevented without
           assuming that some senders or mixes emit cover traffic in
           perfect synchrony, which cannot be achieved by any real system.
         - The attacker chooses t [threshold for correlation r(d)] so
           that the false positive and false negative rates are
           equal. This is the attacker’s crossover error rate. High
           crossover rate means that the defense is effective.
    9. [@9] Challenges and future directions
       - We made the standard, unrealistic assumption that all
         connections start at the same time. In reality, attacks based
         on correlating start and end times of connections may prove
         very successful.
       - it is very difficult to hide connection start and end times
         using dummy traffic because the mix network handles dummy and
         real packets differently
*** ref [[file:master.bib::ShWa-Timing06][Shmatikov & Wang 2006: Timing Analysis Low]]
** [[./Tor%20Project:%20obfsproxy.html][Tor - obfsproxy]]
*** summary
    - transform traffic between client and bridge
    - uses pluggable transports for transformation
*** quotes
*** [[file:master.bib::tor-obfsp][Tor 2016]]
** [[./Tor%20Project:%20Pluggable%20Transports.html][Tor - Pluggable Transports]]
*** summary
    - random IPs by volunteers etc protect against filtering by IP/port
    - DPI can filter for traffic patterns
    - pluggable transports defend against that
    - list of pluggable transports
*** quotes
*** ref [[file:master.bib::tor-pt-web][Inc. 2016: Pluggable Transports]]
** [[./torproject.orgSLASHaboutSLASHtorusers.html][Tor - Who uses Tor?]]
*** summary
    1. Inception

       - U.S. Navy

       - primary purpose: "protecting government communications"

    2. Normal people use Tor

       - protect privacy, communication, children (IP-address),

       - do research f.ex. tibet,

       - skirt surveillance, circumvent censorship

    3. Militaries use Tor

       - field agents

       - hidden services

       - intelligence gathering

    4. Journalists and their audience use Tor

       - readers and publishing and research

    5. Law enforcement officers use Tor

       - online surveillance

       - sting operations

       - anonymous tips

    6. Activists & Whistleblowers use Tor

       - report injustices without repercussions

       - regain privacy

    7. High & low profile people use Tor

       - banker with diverse clientele

       - living in poverty

    8. Business executives use Tor

       - report security risks and internal malfeasance

       - evaluate competition and investments

    9. Bloggers use Tor

       - avoid problems for legal things

    10. IT Professionals use Tor

        - audit firewalls, disallowed content, services

        - access resources

        - mitigate outage

    11. end

        - send stories
*** ref
    [[file:master.bib::who-uses-tor][Project td: Who Tor]]
** [[./paper-ssl-revised.pdf][Wagner - Analysis of the SSL 3.0 protocol]]
*** summary
    0. [@0] Abstract

       some minor flaws, yet easily corrected, good stuff

    1. Introduction

       cryptographic security of SSL 3.0

       background, possible attacks, cryptographic protection, high-level view

    2. Background

       SSL consists of record layer and (connection) layer

       SSLv2 had key weaknesses, end deletion, and protocol
       degradation

    3. The record layer

       standard crypto problems, ok

       1. Confidentiality: eavesdropping

          lots of known plaintext, but should be ok

       2. Confidentiality: traffic analysis

          possible to determine request length, response lengths,
          determine which URL was visited

          supports random padding for block ciphers, but not stream
          ciphers (more common)

       3. Confidentiality: active attacks

          cut-and-pasted exchanges blocks of ciphertext, trying to
          leak the plaintext

          short-block-attacks determine the last plaintext block: see
          when the ack is returned

       4. Message authentication

          uses (old) HMAC, but still HMAC

       5. Replay attacks

          includes sequence number in MACed data

       6. The Horton principle

          is all the meaning validated?

          (SSLCiphertext.ProtocolVersion is not), but in general, yes

       7. Summary

          ok, minor concerns

    4. The key-exchange protocol

       better, but some scars

       1. Overview of the handshake flow

          exchange data, compute secret, authenticate sent messages

       2. Ciphersuite rollback attacks

          negotiation, change cipher spec, finished

       3. Dropping the change cipher spec message

          in authentication-only mode, the change cipher spec-message
          can be untransmitted by the adversary, which allows her to
          always strip the authentication part

          if weak encryption is used, this might allow for a online
          key search, with 4-12 (stream - block) bytes of known
          plaintext

       4. Key-exchange algorithm rollback

          middleman tells each different ciphersuites, as this is not
          protected by hash

          (horton principle violated)

       5. Anonymous key-exchange

          specification unclear in what should be signed in anonymous
          mode

       6. Version rollback attacks

          Mallory might exchange version 3 for version 2 session
          initiation to exploit the weaknesses of SSLv2. There is a
          proposed defense, which sets some padding bytes to fixed
          values.

          There might be the danger of session resumption leading to
          use of v2. (room for further examination)

       7. Safeguarding the master secret

          A nonce is hashed with the master secret on every session
          resume. Mallory can get a bit number of data thus hashed.

          Replay attacks might work for that, too.

       8. Diffie-Hellman key-exchange

          good idea, watch out to avoid server trapdooring

       9. The alert protocol

          signify problems, mostly tear down the connection

       10. MAC usage

           should consistently use HMACs

       11. Summary

           some weaknesses in implementations possible

    5. Conclusion

       passive only recommendation: padding to avoid get request
       length analysis

       active: change cipher spec dropping and
       KeyExchangeAlgorithm-spoofing

       good step, minor patches recommended
*** quotes
    - We conclude that, while there are still a few technical wrinkles
      to iron out, on the whole SSL 3.0 is a valuable contribution
      towards practical communications security.
    - The SSL record layer provides confidentiality, authenticity, and
      replay protection over a connection-oriented reliable transport
      protocol such as TCP.
    - The only change to SSL’s protection against passive attacks
      worth recommending is support for padding to stop traffic
      analysis of GET (v5)
    - Diffie-Hellman is the only public key algorithm known which can
      efficiently provide perfect forward secrecy
    - To avoid server-generated trapdoors, the client should be careful
      to check that the modulus and generator are from a fixed public
      list of safe values.
*** ref [[file:master.bib::SSL][Wagner & Schneier 1997: Analysis SSL]]
** [[./cacr2014-05.pdf][Wang - Effective Attacks and Provable Defenses for Website Fingerprinting]]
*** summary
    0. [@0] Abstract

       effective for seldomly visited pages

       85% tpr vs 0.6% fpr

    1. Introduction

       tor, ssh tunnels, vpn, ipsec are vulnerable to website
       fingerprinting

       contributions:

       - better attack

       - large open-world setting

       - best defense: supersequences over anonymity sets

    2. Basics

       1. Website Fingerprinting on Tor

          two assumptions:

          - clear start and end of trace

          - no other activity

       2. Classification

          kNN is multi-modal: different settings yield different
          traces for the same page

    3. Related Work

       HTTP/1.0 (resource lengths)

       \to HTTP/1.1, VPN, SSH-Tunnel (packet lengths)

       \to TOR (padded packet lengths)

       1. Resource length attacks

          HTTP/1.0: each resource a separate tcp connection

       2. Unique packet length attacks

          HTTP/1.1: combined in tcp connection, yet packets length
          distinguishable

       3. Hidden packet length attacks

          extract features:

          - burst patterns

          - main document size

          - ratio incoming/outgoing

          - total packet counts

          use SVN

          Dyer: less features, n-grams

          Cai: edit distance of packet sequences, modified kernel of SVM

          W&G: modified edit distance algo

       4. Defenses

          simulatable vs non-simulatable

          - simulatable: transform packet sequence, does not look at
            contents, cheaper

          - non-simulatable: in-browser, access to client data

          deterministic vs random

          - deterministic: always outputs the same sequence on the
            same input

          - random: can differ
            |-----------------+-------------------------+-----------------|
            |                 | random                  | deterministic   |
            |-----------------+-------------------------+-----------------|
            | simulatable     | morphing & panchenko    | padding & BuFLO |
            | non-simulatable | Tor's packet reordering | parts of HTTPOS |

    4. Attack

       k-NN, large feature set with weight adjustment

       1. k-NN classifier

          points with classes, "lowest distance chosen"

          lots of features, weighted & learned distance

          similar to SVM

       2. Feature set

          diverse:

          - general features:

            - *total size*

            - *total time*

            - *number of incoming and outgoing packets*

          - *unique packet lengths*: 1 if a size occurs, 0 if not, for
            each size incoming and outgoing (useless on Tor (?),
            similar to Liberatore and Herrmann)

          - *packet ordering*: number of packets before each, number
            of incoming between this and last outgoing packet (burst,
            see Cai)

          - *Concentration of outgoing packets*: number of outgoing in
            30-packet-chunks (non-overlapping span)

          - bursts: sequence with no two adjacent incoming packets,

            - *maximum burst length*

            - *mean burst length*

            - *number of bursts*

          - *initial lengths*: length of first 20 packets

          pads with special character X for empty values, such that
          d(X, y) == d(y, X) := 0

       3. Weight initialization and adjustment

          R rounds of learning

          focus on one point P_{train} \in S_{train}, do two steps

          1. weight recommendation

             1. compute distances to all other P' \in S_{train}

             2. take closest k_{reco} points in S_{train} as S_{good}
                and closest k_{reco} points in all other classes S' as S_{bad}

             3. with d(P, S) := { d(P, s) | s \in S } define:

                - d_{maxgood_i} := max({d_{f_i}(P_{train}, P) | P \in S_{good} }) and compute:

                - n_{bad_i} := |{P' \in S_{bad} | d_{f_i}(P_{train}, P') \le d_{maxgood_i} }|

                  number of classes "closer" by feature than worst good candidate
                  the closer to k_{reco}, the worse

          2. weight adjustment

             1. for features worse than the best,
                reduce by \Delta w_{i} = w_{i} \cdot 0.01

             2. other features, afterwards, increase equally such that
                d(P_{train}, S_{bad}) remains the same

             3. both:

                - \Delta w_i \cdot n_{bad_i}/k_{reco}

                - multiply by overall badness 0.2 + N_bad/k_{reco} with
                  N_{bad} = |{P' \in S_{bad} | d(P_{train}, P') \le d_{maxgood} }|

          3. best results for k_{reco} = 5

          4. random vector between 0.5 and 1.5

    5. Attack evaluation

       better than all others

       1. Attack on Tor

          - 90 instances each of 100 sensitive pages

          - 1 instance each of 5000 non-monitored pages

          - regular circuit resetting, no caches and time gaps between
            multiple loads of the same page

          - weight adjustment: 6000 rounds, 100 pages, 60 instances
            (each instance once)

          - only classified if all k neighbors agree, varying 1 \le k \le 15

          - W&G has 10x higher FPR

          - accuracy levels off after 800 rounds of weight adjustment

          - 0.1 CPU seconds to test one instance,

       2. Training confidence

          - FPR good for k=6, TPR good for k=2 (|C_{0}| = 500)

       3. Attack on Other Defenses

          - evaluated defenses:

            - traffic morphing,

            - HTTPOS split,

            - Panchenko decoy,

            - BuFLO

          - implemented as simulations.

    6. Defense

       Tamaraw++

       supersequences (provably best in simulatable, deterministic
       class)

       approximation of optimal strategy

       1. Attacker’s upper bound

          - Attacker: given observation (packet sequence p), find class C(p)

          - trains on the same data

          - optimal strategy: find class that occurs the most often

          - with possibility set Q(p) := {C_{1}, C_{2}, ...} classes with
            the same observation p define

            Accuracy Acc(p):= |{C ∈ Q(p)|C = C_{max }}| / |Q(p)|

          - non-uniform accuracy:= mean of accuracies Acc(p) (p \in S_{test})

          - uniform accuracy:= maximum of accuracies Acc(p) (p \in S_{test})

          - defense with optimal uniform accuracy leads to optimal
            non-uniform accuracy

       2. Optimal defense

          - bandwidth-optimal, simulatable defense

          - packet sequence as sequence of +1/-1

          - anonymity set: set of packet sequences p_{i} with D(p_{i}) the
            same

       3. Anonymity set selection

          - client cannot always choose freely:

            - before page load

          - levels of information

            1. no information: all have to map to single set

               solution: single supersequence

            2. sequence end information: when is the query ended

               solution: single supersequence with stopping points

            3. class-specific: class is clear, but f.ex. multi-modal
               mode is not

            4. full: prescience

          - clustering to find anonymity sets

            - except in level 1 or 2: one set

            - level: cluster by distance for prefixes p',q' of length
              min(|p|,|q|):
              2 |f_{SCS}(p', q')| - |p'| - |q'|

              step 2: stopping points by prefix

            - level 4: only by distance with whole p,q

       4. SCS approximation

          - NP-hard problem

          - approximation algo:

            - counters c_{i} for each packet sequence p_{i} of n, init at 1

            - if p_{i}[c_{i}] outgoing for more than n/4-ths, add outgoing,
              increment c_{i} where outgoing

            - else, add incoming, increment c_{i} where incoming

          - cannot have bounded error

    7. Defense evaluation

       best: only two supersequences, supersequence is way better,
       also than tamaraw, as it has uniform accuracy

    8. Discussion

       1. Realistically applying an attack

          attacker's assumption: start/end is clearly defined and
          trace is from a single page load

       2. Realistic consequences of an attack

          - many sensitive pages are among the top-100

          - if local and temporal area is known, identifying get way
            easier

       3. Reproducibility of our results

          attack, defense, other attacks & defenses and data available

    9. Conclusion

       - pages multi-modal

       - adjusting distance weights

       - knn very fast

       - performs well

       - powerful against all known defenses

       - provable defense

       - better overhead, same security level
*** quote
    - We found that our new attack is much more accurate than previous
      attempts, especially for an attacker monitoring a set of sites
      with low base incidence rate.
    - Privacy technologies are becoming more popular: Tor, a
      low-latency anonymity network, currently has 500,000 daily users
      and the number has been growing [21].
    - Only with a provably effective defense can we be certain that
      clients are protected against website
      fingerprinting. (*certainty* necessary?)
    - a training and testing time that is orders of magnitude lower
      than the previous best.
    - Tor developers remain unconvinced that website fingerprinting
      poses a real threat.
    - An attacker can deal with multi-modal data sets by gathering
      enough data to have representative elements from each mode.
    - Random defenses (noise) have the disadvantage that choosing a
      good covering is not guaranteed,
    - Implementation of random defenses must be careful so that noise
      cannot be easily distinguished from real packets.
    - We then train the attack to focus on features which the defense
      fails to cover and which therefore remain useful for
      classification.
    - The k-NN classifier needs a distance function d for pairs of
      packet sequences. The distance is non-trivial for packet
      sequences.
    - (ends 4.1)
    - the general features are amongst the strongest indicators of
      whether or not two packet sequences belong to the same mode of a
      page,
    - The total number of features is close to 4,000 (3,000 of which
      are just for the unique packet lengths).
    - (ends 4.2)
    - Note that we are not claiming these particular choices of
      parameters and constants yield an optimal attack
    - Our list of 100 monitored pages was compiled from a list of
      blocked web pages from China, the UK, and Saudi Arabia.
    - After |C_{0} | > 2500 [non-monitored pages], we could not see a
      significant benefit in adding more elements.
    - if the base incidence rate of the whole sensitive set is 0.01
      (99% of the time the client is visiting none of these pages),
      and our new classifier claims to have found a sensitive site,
      the decision is correct at least 80% of the time, the rest being
      false positives.
    - The testing time amounts to around 0.1 CPU seconds to classify
      one instance for our classifier and around 800 CPU seconds for
      Wang and Goldberg’s classifier, and 450 CPU seconds for that of
      Cai et al.
    - almost all of the graph in Figure 1 can be drawn only by varying
      k with |C_{0}| = 5000, suggesting that it is advantageous for the
      attacker to have a large number of non-monitored training pages.
    - Then, we must determine the SCS of all the packet sequences in
      the anonymity set. This is in general NP-hard. [13]
    - In fact, it is known that any polynomial-time approximation
      algorithm of shortest common supersequences cannot have bounded
      error [13].
    - It is possible that a clever clustering strategy for class-level
      information could achieve lower bandwidth overheads.
    - the start of a packet sequence generally contains around 3 times
      more outgoing packets than the rest of the sequence. If the user
      is accessing a page for which she does not have a current
      connection (i.e. most likely the user is visiting a page from
      another domain), then the user will always send one or two
      outgoing connections (depending on the browser setting) to the
      server, followed by acceptance from the server, followed by a
      GET request from the main page, and then by data from the
      server. This particular sequence is easily identifiable.
    - On Tor, users are discouraged from loading videos, using
      torrents, and downloading large files over Tor, which are types
      of noise that would interfere with website fingerprinting.
*** questions
    - features perfectly covered by a defence (such as unique packet
      lengths in Tor) will always have n_{bad_i} = k_{reco} , its maximum
      possible value.

      why *always*?

    - Then, accuracy is computed over the remaining 30 instances each,
      on which we perform all-but-one cross validation.

    - As we work with Tor cells, in the following a packet sequence
      can be considered a sequence of -1’s and 1’s (downstream and
      upstream packets respectively),

      so timing information is omitted?
*** ref [[file:master.bib::effective][Wang et al. 2014: Effective Attacks Provable]]
** [[./webfingerprint-wpes.pdf][Wang - Improved Website Fingerprinting on Tor]]
*** summary
    0. [@0] Abstract

       - new attack

       - new data gathering methodology

       - removal of useless cells, views Tor layer, not TCP/IP

    1. INTRODUCTION

       - evaluate metrics

       - contributions:

         - data gathering: modify tor browser

         - new data processing

           - not at TCP/IP level, but above

           - remove SENDME cells

         - New website fingerprinting metrics:

           - dynamic content is loaded last and varies only incoming

           - new metric

    2. RELATED WORK

       Defenses not implemented because of network load and ineffective

       1. Non-distance-based methods

          - only sizes and occurrence counts as features

          - liberatore: naive bayes (normal frequency)

          - herrmann: multinomial naive bayes, more successful, only
            3$ accuracy on Tor

       2. Distance-based methods

          - liberatore: jaccard-based, without occurence, less
            effective than naive bayes

          - panchenko: svm, several features

          - cai: "optimal string alignment distance", also for
            websites instead of web pages via Hidden Markov Models

       3. TOR

          0. [@0]

             - anonymization service

             - volunteers

             - if wf works, not anonymizing

             - defense

          1. Circuits and Streams

             - guard nodes rather fixed

             - middle and exit nodes

             - varying bandwidths per node

             - SENDME cells like ACKS

               - should be removed

          2. Defenses

             - tor browser defense

               1. enable pipelining

               2. randomize request order

               3. randomize pipeline size

             - cai found to fail, this evluates, too

       4. DATA COLLECTION AND PROCESSING

          1. Collecting Data

             0. [@0]

                different clients, different traces

             1. Circuit Construction

                - should control circuit construction manually (?if
                  done in sequence?)

             2. Timing

                - should train on sites several hours apart (not
                  feasible in real-time because of training time)

             3. Localization

                - problem of redirects to localized site
                  (f.ex. google.ca)

                - problem of localized content (f.ex. advertising)

             4. Our Methodology

                - tor controller

                - 100@40 sites

                - batch: 4 instances

                - maximum circuit duration

                - removed similar sites (google.de|be|... unified)

                - try to get localized sites (yahoo.de)

                - remove trace if size is less than 20% of median

                - disable cacheing

          2. Processing Data

             0. [@0]

                - "After processing, each traffic instance is
                  represented as a sequence of positive and negative
                  integers."

             1. TCP/IP packet instances

                - length field

                - cai and panchenko rounded sizes by increments of 600

             2. Tor cell instances

                - TCP/IP layer info has problems f.ex. with retransmit

                - round down to closest multiple of 512, divide by 512

                - "will produce the number of Tor cells in the record."

             3. Removing SENDMEs

                - guess the p2-th cell to be a sendme after p1 start cells

                - p1=45, p2=40 gave TPR .62, FPR .057

       5. CLASSIFICATION

          0. [@0]

             - monitored vs others (open world)

             - site vs rest (closed world)

          1. Support Vector Machine

             - find separating hyperplane

             - cost function if cannot be foun

             - lift to higher dimension to find hyperplane there

             - ovo as ovm was not better

             - which metric to use?

          2. Distance-based Metrics

             1. Optimal String Alignment Distance

                - transposition limited to originally adjacent
                  elements

                - cai: lower cost for transpositions

             2. Damerau-Levenshtein Distance

                - all transpositions allowed

                - implementation: also elements not adjacent with
                  deletion an insertion

             3. Removing Substitutions

                these do not correspond to realistic scenarios as
                deletions, insertions and transpositions do

             4. Different costs for Incoming/Outgoing Packets

                make changes to outgoing packets more costly

             5. Varying transposition cost

                - reduce transposition costs the later an element occurs

                - at beginning, always HTML page

                - then simultaneous connections

                - cost = (1 - 0.9 min(i/m, j/n))^2

             6. Fast Levenshtein-like distance

                efficient new levenshtein algorithm in O(m+n)

          3. Post-processing

             - need values in [0,1] with 0 greatest similarity

             - as Cai: transform via

               - D'(s 1 , s 2 ) = D(s 1 , s 2 ) / min(|s 1 |, |s 2 |), and

               - K(s 1 , s 2 ) = e^{ −D'(s 1 ,s 2 )^2}

       6. EXPERIMENTAL RESULTS

          1. Setting

             - "embarassingly parallel"

             - no alterations to tbb except for disabled cache

             - set UseEntryGuards to 0 to disable (otherwise limited
               test validity)

             - iMacros

          2. Closed-World Results

             0. [@0]

                - avoid overfitting by setting the costs once

                - vary SVM cost param from 4^0 to 4^10, choose best for
                  each metric

                - 91% for SENDME removed and combined OSAD (appendix D)

                - Cai: 88%

                - fast: 283 vs 608000 CPU seconds, 70%

             1. Cai’s Data

                similar results, thus analysis seems robust

          3. Open-World Results

             - alexa 101 to 1000 as background

               - 860 successfully loaded

               - 96.9% recall

          4. Evaluation of Tor’s Pipelining and Order Randomization
             Defense

             - no significatn difference between results,

             - slight better detection with defense enabled

       7. DISCUSSION

          1. Training set size and selection

             - circuit rotation while fetching has quite an impact

             - number of training set: the more instances the better

               - OSAD outperformed

             - open-world background-set: the more training sites the
               better (for given background set size)

          2. Testing set size and selection

             - Alexa's top100 to compare with Panchenko and Cai

             - 10 sites very high accuracy

               - 20: 92.3%

               - 100: 90.9%

             - little decrease as number of sites increases

          3. Difficult sites and Defenses

             problems arose from

             - Localization

             - frequently updated content

             - slides: "the number of slide images that are loaded
               depended on the total load time"

             - randomized content: recommendations based on prefs

             how web designers can help anonymize

       8. CONCLUSION

          - realistically capture data

            - modify Tor/ TBB

          - cells better,

            - better without SENDME (?where?)

          - better results:

            - recall 95%

            - fpr < 0.2%

          - compare to previous: 35% fewer mistakes

          - justify use of Alexa top-100 (closed-world) and top-1000
            (open world)
*** quotes
    1. INTRODUCTION
       - [@?] dynamic content such as advertisements may cause a
         variation in incoming packets but not outgoing packets, and
         dynamic content is often loaded last in a website.
    4. [@4] DATA COLLECTION AND PROCESSING
       1. Collecting Data
          1. Circuit Construction
             - the top 1% of relays comprised 29% of the total
               bandwidth of Tor and the top 20% of relays comprised
               93% of the bandwidth.
    7. [@7] DISCUSSION
       1. Training set size and selection
          - the attacker can increase their open-world success rate by
            training on more non-monitored sites.
*** ref [[file:master.bib::wpes13-fingerprinting][Wang & Goldberg 2013: Improved Website Fingerprinting Tor]]
** [[./cacr2015-08.pdf][Wang - Walkie-Talkie: An Effective and Efficient Defense against Website Fingerprinting]]
*** summary
    0. [@0] ABSTRACT

       other defenses: too much time: 1/2 minute wait

       here: half-duplex

       9% time, 32% bw \to 5 % fpr

       9% time (?), 55% bw \to 10%fpr

    1. INTRODUCTION

       - tor breakable by wf

       - walkie-talkie: half-duplex defense, against perfect attacker

    2. RELATED WORK

       0. [@0]

          1998 wf first mentioned

       1. Attacks

          evolution

       2. Defenses

          0. [@0]

             - limited: against specific WF attack

             - general: against perfectly accurate attacker

          1. Limited defenses

             - Wright's morphing

               - addressed Liberatore and Levine's attack (relies on
                 unique packet lengths)

               - randomly padded packets

               - broken by effective's packet ordering attack

             - Luo's HTTPOS

               - broken by touching and effective

             - Tor's randomised pipelining

               - updated in critique

               - both versions ineffective as of [[file:master.bib::ccs2012-fingerprinting][Cai et al. 2012:
                 Touching Distance]], effective and "improved wf on tor"

          2. General defenses

             - "different web pages should, with sufficient
               likelihood, produce the exact same packet sequence."

             - Dyer: BuFLO: fixed, constant rate, dummy packets

               - padded upto 10 seconds

             - Cai: Tamaraw

             - Wang: Supersequence

       3. Moving Forward

          - Problems:

            - Overhead:

              - time 170% to 240%

              - bandwdith 90% to 180%

            - Congestion:

              - not congestion-aware

          - Solution: half-duplex communication

    3. IMPLEMENTING HALF-DUPLEX COMMUNICATION

       0. [@0]
          - half-duplex:

            - bursts by each side

            - small time overhead

            - no bandwidth overhead

            - easy to implement

       1. How browsers work

          - HTTP: client connects, retrieves or sends data

          - Firefox (other browsers similar): connectionmanager uses
            up to fixed maximum of connections, sends requests via
            these

          - for half-duplex: modify connection manager

       2. Implementing half-duplex communication

          - walkie: idle

          - talkie: sent data, receiving replies

          - proof:

            - client sends only when no active connections, \to server not talking

            - server talking: client sends no data \to client not talking

       3. Optimistic data

          - sending TCP connection request and GET at the same time

            - one less RTT

            - reduces number of bursts, thus padding

       4. Other Implementation Details

          - pipelining disabled

          - tls: delay, but not a weakness

          - speculative connections: disabled

    4. PADDING

       0. [@0]

          - assume padding of data

          - half-duplex reduces further to number of packets in each
            burst [+timing?]

            to burst pair (incoming number, outgoing number)

          - perfect attacker can detect except for collision

       1. Deterministic Padding

          - round_Y(a) up to rounding set Y

          - pad number of bursts to round_Y(bursts)

          - overhead-optimal rounding set by algorithm

          - randomized test 2 \le |Y| \le 9

       2. Random padding

          - client and cooperator send dummy packets when request has
            started

          - adding from uniform distribution to real burst

          - adding from (test uniform and normal)

    5. EVALUATION

       0. [@0]

          test against known attacks

          expect perfect attacker, which can do perfect table lookup

       1. Evaluation of deterministic defenses

          - "The objective of the defense is to maximize collisions."

          - "To classify any sequence belonging to c, the attacker’s
            optimal strategy (optimizing accuracy) is to guess that
            all elements in the collision set belong to the class with
            the highest rate of occurrence in that set. The attacker’s
            overall accuracy over c is therefore max_{i} n_{i} /|c|.

       2. Evaluation of random defenses

          - determine maximum probability instead of "highest rate of
            occurrence"

       3. Overhead

          - bandwidth in %: dummy / real packets

          - time

    6. RESULTS

       1. Data Collection

          - modified tor browser

          - top 100 a 90 as monitored

          - next 10000 as non-monitored

          - drop \le 50 cells

          - no padding

       2. Walkie-Talkie results

          - 3 ways of adding bursts

            - nofake: none

            - normal: normal distribution

            - uniform: uniform distribution

            - and also deterministic padding

          - perfect attacker

          - uniform padding best

          - overhead 35% \to fpr 5%, 55% \to fpr 10%

       3. Comparison with previous defenses

          morphing (other form) and padding (more traffic)

          1. Morphing

             - time overhead 8.5%

               - rtt dominates

             - more realistic than supersequence/tamaraw

          2. Padding

             - wt has way less time overhead and much less bandwidth
               overhead than tamaraw/supersequence

       4. Performance against known attacks

          - none comes close to theoretical maximum

          - different amounts of padding (\to bandwidth) (none, low, high)

    7. ALTERNATIVE DEFENSES

       without generality (only against kNN), without ease-of-use
       (only select sites), without decentralization (with database)

       1. Without generality

          - half-duplex with inserted dummy packets works well against
            knn

          - should be general defense against specific attack, not
            specific defense, as in this point

       2. Without ease of use

          select some pages to shield

          for each page: supersequence of most traces, then popular
          page which is supersequence of this supersequence

          failure rate 4% \pm 3%

       3. Without decentralization

          - save packet sequences for each page

          - computer supersequences in advance

          - 5% \to only 90% accuracy

            - yet some pages undefended

    8. DISCUSSION

       1. reproducibility

          code downloadable

       2. timing

          left out

          1. no research has shown benefits

          2. different network conditions

          3. changes in circuit changes avery x minutes

          4. many users with different access speeds pose a problem

    9. Conclusion

       - easy-to use efficient,

       - tamaraw, supersequence too much

       - better when conditions relaxed
*** quotes
    - design goals:
      1. General: The defense succeeds against all possible
         classification attacks by causing collisions, where the
         defense outputs the same packet sequence given input packet
         sequences from different web pages. Even a perfect attacker
         would not be able to tell which page it came from.
      2. Easy to use: The client does not need to configure the
         defense. The defense is ready to use out of the box, and can
         be deployed incrementally as it does not depend on other
         clients using the same defense.
      3. Decentralized: The defense should not require some central
         server, with a shared database, to operate. We want to match
         the decentralized model of anonymity networks.
    - (ends 1)
    - Half-duplex communication reduces the attacker’s potential
      feature set to only a series of burst traffic sizes, which makes
      WF difficult.
    - (ends 2/3?)
    - The client adds outgoing dummy packets, while a cooperator adds
      incoming dummy packets.
    - Due to the low base rate of visits to each single web page, a
      client protecting her page accesses against an attacker may only
      need a false positive rate over 5% [18,21] even if the
      attacker’s true positive rate was 100%.
    - (ends 4.0-6.1)
    - If the attacker categorized a monitored page as a different
      monitored page, we count it as a false positive
    - (ends 6.2)
    - designing a defense that specifically defeats the
      state-of-the-art attack is a flawed approach.
    - (ends 7.1)
*** questions
    - Further, we assume this encrypted channel pads all packets to a
      fixed size so as to remove packet size as a possible
      feature. (Note that Tor delivers all data in fixed-size cells.)

      How about other cells? Are they irrelevant?

      Probably, as they are only the versions, tls, etc

      Maybe not, handshake etc...
    - client reduces the attacker’s possible feature set even further,
      to simply counts for thenumber of packets in each burst of
      traffic.

      How about timing data?
*** ref [[file:master.bib::wang2015walkie][Wang & Goldberg 2015: Walkie Talkie]]
** [[./notes][Wang - notes]]
*** quotes
    - While we did some editing and customization to Firefox to enable
      data collection, in the newest version of Tor Firefox this is no
      longer necessary. It is possible to run it using just
      <torbrowserfolder>/firefox <sitename>, and we recommend this.
** [[../sw/w/knn/README][Wang - README in knn.zip]]
*** quotes
    1. run
       - python fextractor.py, compile flearner.cpp and run it.
    2. fextractor.py
       - each line is a time (float), then \t, then a positive or
         negative number (postive: outgoing packet length or cell,
         negative: incoming packet length or cell).
    3. flearner.cpp
       - Most interesting variables are near the top or the start of
         the main function. Accuracy output as true positive rate and
         then false positive rate.
** [[./Intercepting Page Loads - Mozilla | MDN.html][wbamberg - Intercepting Page Loads]]
*** summary
    1. The Easy Way: Load Events

       - listen to load event

         - +: "easy"

         - -: cannot intercept, abort, etc

    2. HTTP Observers

       - better keep in non-chrome objects

       - one instance sees each request and each response

       - stay effective

    3. WebProgressListeners

       - in chrome

       - attach to each tab

       - tons of listeners, with filters

    4. XPCOM Solutions

       - Document Loader Service

         - nsIDocumentLoader: "global web progress listener"

         - Content Policy: cleaner-looking code, allows =shoudLoad=

           - does not show redirects
** [[./Security best practices in extensions - Mozilla | MDN.html][wbamberg - Security best practices in extensions - Mozilla | MDN]]
*** summary
    0. [@0]

       some strict guidelines, some recommendations

    1. Web content handling

       - best: little privileges: https://developer.mozilla.org/En/Displaying_web_content_in_an_extension_without_security_issues

       - privileges depend on origin

         - website: only that site

         - chrome: all

         - file: all files

       - restrict via

         #+BEGIN_SRC js
           frame.docShell.allowImages = false;
           frame.docShell.allowJavascript = false;
           frame.docShell.allowPlugins = false;
         #+END_SRC

       - Components.utils.evalInSandbox()

    2. The sidebar: a use case

       - iframe with _type="content"_ attribute sandboxes, blocks callbacks

    3. Using eval() in an extension: don't

    4. Sandboxed HTTP connections: prevents cookies, etc, see:

    5. Handling of logins and passwords: Login Manager

    6. APIs and other data handling

       - https

       - JSON: see https://developer.mozilla.org/En/Using_native_JSON

       - certificates MUST NOT be self-signed

    7. Remote JavaScript and Content: not in chrome context, use sandbox

    8. evalInSandbox: look at doc, see also http://arantius.com/misc/greasemonkey/script-compiler

    9. Third-party JavaScript:

       - namespace

       - trusted source

    10. Conclusion: also http://blog.mozilla.com/security/
** [[./topranked.html][Web - Does Alexa have a list of its top-ranked websites?]]
*** url https://support.alexa.com/hc/en-us/articles/200449834-Does-Alexa-have-a-list-of-its-top-ranked-websites-
*** summary
    top 1m sites at http://s3.amazonaws.com/alexa-static/top-1m.csv.zip

    updated daily
** [[./Add-ons_Extension Signing - MozillaWiki.html][Web - Add-ons/Extension Signing - MozillaWiki]]
*** summary
    - all extensions need to be signed (no themes, dictionaries, etc)

    - since Firefox 47

    - via API: http://olympia.readthedocs.org/en/latest/topics/api/signing.html

    - or jpm: https://developer.mozilla.org/en-US/Add-ons/SDK/Tools/jpm#jpm_sign
*** ref [[file:master.bib::addon-signing][Mozilla 2016: Add Extension Signing]]
** [[./chickenfoot.html][Web - Chickenfoot]]
*** summary
    - script firefox

    - no longer developed
*** ref [[file:master.bib::chickenfoot][2015: Chickenfoot]]
** [[./JavaScript Garden.html][Wetzel - JavaScript Garden]]
*** summary
    1. Objects

       1. Object Usage and Properties

          0. [@0]

             =null= and =undefined= are not objects, others are coerced

          1. Objects as a Data Type

             - hash maps

          2. Accessing Properties

             - [] or .

          3. Deleting Properties

             - only per =delete=, setting to undefined or null keeps the key

          4. Notation of Keys

             - older engines: quote keywords as string, others: unquoted ok

       2. The Prototype

          0. [@0]

             - prototypal inheritance:

             - object as class

          1. Property Lookup

             - prototype chain, up and up until =null=

          2. The Prototype Property

             - you can set =MyObject.prototype = new OtherObject()=

               - primitives are ignored

          3. Performance

             - impact for high-up objects or iteration

          4. Extension of Native Prototypes

             - "monkey patching"

             - only to backport newer-ES features

          5. In Conclusion

             - efficiency

             - no monkeypatch

       3. hasOwnProperty

          0. [@0]

             - defined in object itself or not?

               - only thing not traversing prototype chain

          1. hasOwnProperty as a Property

             - can break (but then, you can always override Object.hasOwnProp

          2. In Conclusion

             - recommended for =for ... in= loop

       4. The for in Loop

          - in operator: object has property, array has *key* (not value)

            - [3,4] has =1 in [3,4]=, but not =3 in [3,4]=

          - =for in= goes up the prototype chain

            - except if =Object.defineProperty= is used with
              =enumerable= set to =false=

          - library code needs to make sure via =hasOwnProperty=

          - In Conclusion: always for ECMA <= 3. In ECMA 5, you can
            defineProperty and omit the hasOwnProperty in for-in

    2. Functions

       1. Function Declarations and Expressions

          - can be used like any other objects

          - pass as (anonymous) parameters to other function as callback

          - *function Declaration* is hoisted = moved to top, available
            in whole program
            #+BEGIN_SRC js
              function foo() {}
            #+END_SRC

          - *function Expression* is defined at runtime
            #+BEGIN_SRC js
              var foo = function() {};
            #+END_SRC

            - can be named with name available inside function

       2. How this Works

          special name, 5 different objects bound

          1. The Global Scope

             - in global scope: to global object

               - except for strict mode: undefined

          2. Calling a Function

             - global object

               - except for strict mode: undefined

          3. Calling a Method

             - to the containing object

          4. Calling a Constructor

             to newly created object

          5. Explicit Setting of this

             #+BEGIN_SRC js
               function foo(a, b, c) {}

               var bar = {};
               foo.apply(bar, [1, 2, 3]); // array will expand to the below
               foo.call(bar, 1, 2, 3); // results in a = 1, b = 2, c = 3
             #+END_SRC

             =this= explicitly set to bar

          6. Common Pitfalls

             - in method definitions, =this= refers to global object, solution:
               #+BEGIN_SRC js
                 Foo.method = function() {
                     var test = function() {
                         // this now refers to Foo
                     }.bind(this);
                     test();
                 };
               #+END_SRC

          7. Assigning Methods

             aliasing as =var test = someObject.methodTest;= has the
             same problem

       3. Closures and References

          0. [@0]

             "One of JavaScript's most powerful features"

          1. Emulating private variables

             using closures
             #+BEGIN_SRC js
               function Counter(start) {
                   var count = start;
                   return {
                       increment: function() {
                           count++;
                       },
                       get: function() {
                           return count;
                       }
                   }
               }
             #+END_SRC
             =count= is hidden from the outside

          2. Why Private Variables Work

             - =count= referenced by =increment= and =get=, but
               otherwise unavailable

             - scopes cannot be assigned or referenced

          3. Closures Inside Loops

             - often a problem: variable is referenced, thus always
               the same (f.ex. with =setTimeout=)

          4. Avoiding the Reference Problem

             1. anonymous wrapper (see also 2.6)
                #+BEGIN_SRC js
                  for(var i = 0; i < 10; i++) {
                      (function(e) {
                          setTimeout(function() {
                              console.log(e);
                          }, 1000);
                      })(i);
                  }
                #+END_SRC

             2. return a function from the anonymous wrapper
                #+BEGIN_SRC js
                  for(var i = 0; i < 10; i++) {
                      setTimeout((function(e) {
                          return function() {
                              console.log(e);
                          }
                      })(i), 1000)
                  }
                #+END_SRC

             3. additional argument to setTimeout
                #+BEGIN_SRC js
                  for(var i = 0; i < 10; i++) {
                      setTimeout(function(e) {
                          console.log(e);
                      }, 1000, i);
                  }
                #+END_SRC

             4. using .bind
                #+BEGIN_SRC js
                  for(var i = 0; i < 10; i++) {
                      setTimeout(console.log.bind(console, i), 1000);
                  }
                #+END_SRC

       4. The arguments Object

          0. [@0]

             - in every function scope

             - array-like, not array

          1. Converting to an Array

             slow, avoid in performance-critical parts
             #+BEGIN_SRC js
               Array.prototype.slice.call(arguments);
             #+END_SRC

          2. Passing Arguments

             recommended:
             #+BEGIN_SRC js
               function foo() {
                   bar.apply(null, arguments);
               }
               function bar(a, b, c) {
                   // do stuff here
               }
             #+END_SRC

             you can also use =call= and =apply= together to turn
             methods into normal functions

          3. Formal Parameters and Arguments Indices

             setting the parameter also changes =arguments= (except
             for strict mode)

          4. Performance Myths and Truths

             using =arguments.callee= may make things slower, as
             functions can no longer be inlined

       5. [@5] Constructors

          1. Basics

             - any function with =new=

             - sets =this= and Object.prototype to prototype

             - returns =this= if no obect given

          2. Factories

             - return object, =new= unimportant, allows for information hiding

             - does not affect prototype

          3. Creating New Objects via Factories

             - uses more memory (no prototypes)

             - inheritance harder

             - "contrary to spirit of the language"

          4. In Conclusion

             depends on application, but prototypes have many benefits, too

       6. Scopes and Namespaces

          0. [@0]

             - only function scope (except for js6)

             - only global namespace

          1. The Bane of Global Variables

             always use =var= unless you *explicitly* want outer
             scope

          2. Local Variables

             only =var= and function parameters (and let and const)

          3. Hoisting

             - function declarations and var statements get /hoiste/
               to the top of their scope
               #+BEGIN_SRC js
                 bar();
                 var bar = function() {};
                 var someValue = 42;

                 test();
                 function test(data) {
                     if (false) {
                         goo = 1;
                     } else {
                         var goo = 2;
                     }
                     for(var i = 0; i < 100; i++) {
                         var e = data[i];
                     }
                 }
               #+END_SRC
               becomes
               #+BEGIN_SRC js
                 // var statements got moved here
                 var bar, someValue; // default to 'undefined'

                 // the function declaration got moved up too
                 function test(data) {
                     var goo, i, e; // missing block scope moves these here
                     if (false) {
                         goo = 1;
                     } else {
                         goo = 2;
                     }
                     for(i = 0; i < 100; i++) {
                         e = data[i];
                     }
                 }

                 bar(); // fails with a TypeError: bar still 'undefined'
                 someValue = 42; // assignments not affected by hoisting
                 bar = function() {};

                 test();
               #+END_SRC
               var inside the for-loop, in if-statement and in
               for-declaration all get hoisted

          4. Name Resolution Order

             - var

             - then parameter

             - then function name

             - repeat in outer scope

          5. Namespaces

             several ways to avoid polluting the global namespace

             unnamed functions somehow

          6. In Conclusion

             best to use anonymous wrapper

    3. Arrays

       1. Array Iteration and Properties

          0. [@0]

             avoid for in

          1. Iteration

             - best for performance is the classic for loop
               #+BEGIN_SRC js
                 var list = [1, 2, 3, 4, 5, ...... 100000000];
                 for(var i = 0, l = list.length; i < l; i++) {
                     console.log(list[i]);
                 }
               #+END_SRC

               - half as fast without caching =l=

             - notes: also [].forEach(callback(element, index, array))

          2. The length Property

             - can be used to truncate array or make it sparse

          3. In Conclusion

             - bet performance with plain for loop (with =length= caching)

       2. The Array Constructor

          1. 

             - use =[]= instead

             - except for repeating f.ex. a string without =for=
               #+BEGIN_SRC js
                 new Array(count + 1).join(stringToRepeat);
               #+END_SRC

          2. In Conclusion

             use [] almost all the time

    4. Types

       1. Equality and Comparisons

          two ways to do

          1. The Equality Operator

             - ==

             - type corecion

             - bad for performance

          2. The Strict Equality Operator

             - ===

             - no type coercion, hardens code

             - better performance

          3. Comparing Objects

             both compare object identity

          4. In Conclusion

             - use only ===

             - if you need type coercion, use it explicitly

       2. The typeof Operator

          broken

          1. The JavaScript Type Table

             is hard to grok (typeof 'ab' !== typeof new String('ab'))

          2. The Class of an Object

             determine via =Object.prototype.toString=
             #+BEGIN_SRC js
               function is(type, obj) {
                   var clas = Object.prototype.toString.call(obj).slice(8, -1);
                   return obj !== undefined && obj !== null && clas === type;
               }
             #+END_SRC

          3. Testing for Undefined Variables

             test via
             #+BEGIN_SRC js
               typeof foo !== 'undefined'
             #+END_SRC
             "This is the only thing typeof is actually useful for."

          4. In Conclusion

             use Object.prototype.toString to test type

       3. The instanceof Operator

          only useful for custom-made objects
          #+BEGIN_SRC js
            function Foo() {}
            function Bar() {}
            Bar.prototype = new Foo();
            new Bar() instanceof Bar; // true
            new Bar() instanceof Foo; // true
            // This just sets Bar.prototype to the function object Foo,
            // but not to an actual instance of Foo
            Bar.prototype = Foo;
            new Bar() instanceof Foo; // false
          #+END_SRC

       4. Type Casting

          type coercion "whereever possible": best use strict equality

          1. Constructors of Built-In Types

             behave differently when used with =new= and without
             #+BEGIN_SRC js
               new Number(10) === 10;     // False, Object and Number
               Number(10) === 10;         // True, Number and Number
               new Number(10) + 0 === 10; // True, due to implicit conversion
             #+END_SRC
             =Number= etc without new do type conversion

          2. Casting to a String

             prepend with ='' +=
             #+BEGIN_SRC js
               '' + 10 === '10'; // true
             #+END_SRC

          3. Casting to a Number

             prepend unary +:
             #+BEGIN_SRC js
               +'10' === 10; // true
             #+END_SRC

          4. Casting to a Boolean

             !!

    5. Core

       1. Why Not to Use eval

          0. [@0]

             - =eval= changes locally-scoped variables

               - except if not called by name =eval=, then globals

             - avoid it

          1. eval in Disguise

             - setTimeout takes string as arg, evals it

          2. Security Issues

             - evaluates **any string**

          3. In Conclusion

             never be used, code using should be questioned
             (workings, performance, security)

             (except maybe REPL-loops)

       2. undefined and null

          two values for nothing, use =undefined=

          1. The Value undefined

             used for uninitialized variables, and much more

          2. Handling Changes to the Value of undefined

             not necessary in strict mode

          3. Uses of null

             non in production, can be replaced

       3. Automatic Semicolon Insertion

          parser needs semicola, inserts if it encouters error with
          certain rules, retries

          1. How it Works

             - example with (group stuff) on new line

             - can lead to errors

          2. Leading Parenthesis
             #+BEGIN_SRC js
               log('testing!')
               (options.list || []).forEach(function(i) {})
             #+END_SRC
             without semicolon insertion yields `TypeError` because
             log does not return a function

          3. In Conclusion

             recommended to always use them

       4. The delete Operator

          if =DontDelete= attribute is set, impossible to delete

          1. Global code and Function code

             - elements (var or function) of global and function scope
               have DontDelete set, so undeleteable
               #+BEGIN_SRC js
                 var a = 1; // DontDelete is set
                 delete a; // false
                 a; // 1
               #+END_SRC

             - no longer true in firefox

          2. Explicit properties

             - those can be deleted normally

             - use this to delete global vars:
               #+BEGIN_SRC js
                 var GLOBAL_OBJECT = this;
                 GLOBAL_OBJECT.a = 1;
                 a === GLOBAL_OBJECT.a; // true - just a global var
                 delete GLOBAL_OBJECT.a; // true
                 GLOBAL_OBJECT.a; // undefined
               #+END_SRC

          3. Function arguments and built-ins

             DontDelete also set (?) for function arguments and
             built-in properties, f.ex. .length

          4. Host objects

             unpredictable (what are these)

          5. In conclusion

             recommended only for "explicitly set properties on normal
             objects"

    6. Other

       1. setTimeout and setInterval

          - js asynchronous: schedule function to run

          - no exact scheduling guaranteed (fn might block)

          - =this= is global object on execution

       2. Stacking Calls with setInterval

          - discouraged

       3. Dealing with Possible Blocking Code

          - call setTimeout() in function after blocking code

       4. Manually Clearing Timeouts
          #+BEGIN_SRC js
            var id = setTimeout(foo, 1000);
            clearTimeout(id);
          #+END_SRC

       5. Clearing All Timeouts

          - no fixed method

          - simple: clear all up to 1000

          - often monotonically increasing, then
            #+BEGIN_SRC js
              var biggestTimeoutId = window.setTimeout(function(){}, 1), i;
              for(i = 1; i <= biggestTimeoutId; i++) {
                  clearTimeout(i);
              }
            #+END_SRC
            "works on all major browsers"

          - better keep track of IDs, clear individually

       6. Hidden Use of eval

          - if string as first parameter, uses =eval= in global scope

       7. In Conclusion

          - avoid string use

          - avoid setInterval, as "its scheduler is not blocked by
            executing JavaScript."
*** quotes
    6. Other
       - Note: As setTimeout takes a function object as its first
         parameter, a common mistake is to use setTimeout(foo(),
         1000), which will use the return value of the call foo and
         not foo. This is, most of the time, a silent error, since
         when the function returns undefined setTimeout will not raise
         any error.
** [[./tf-idf.html][Wiki - term frequency–inverse document frequency]]
*** quotes
    - tf–idf is the product of two statistics, term frequency and
      inverse document frequency. Various ways for determining the
      exact values of both statistics exist.
** TODO [#A] vorlesungsnotizen sv2/3 :hannover_laptop:
*** [#A] zu stufen der mustererkennung
** TODO [#A] [[./duda-etal-00.pdf][Duda - Pattern Classification]]
*** TODO [#A] chapter 1 upto 1.5
*** TODO [#B] skim ugly duckling theorem and previous
*** summary
    1. Introduction

       humans can easily detect complex patterns with ease

       1. Machine Perception

          machines should try to learn, as it facilitates many things

       2. An Example

          0. [@0]

             - differentiate sea bass from salmon

             - first attempt: size, classifies, but many failures

             - second attempt: lightness, classifies better, still
               some errors

             - third attempt: combined lightness and width, a few
               errors, but you do not want to overclassify

             - general: the less model, the more data needed

          1. Related fields

             - hypothesis testing: *one* hypothesis, here: select from
               several

             - image processing: output is image, pattern
               classification loses information

             - associative memory (return gestalt image): here loses a
               lot of information, as output is only a few bits == the
               class

       3. The Sub-problems of Pattern Classification

          issues in pattern classification

          1. Feature Extraction

             feature extraction is domain-dependent, classification is
             less

          2. Noise

             - randomness due to world and sensors

             - how to distinguish?

          3. Overfitting

             - fit too tightly to training data

             - how to solve generally

          4. Model Selection

             - how to choose the features by which to distinguish
               classes

          5. Prior Knowledge

             - much of stuff to influence model, f.ex. voice, or here lightness

          6. Missing Features

             - f.ex. fish occluded by other fish, width unclear

             - set to 0, or to average is "provably non-optimal"

             - how to set it

          7. Mereology

             - groups and parts

             - how to incorporate just the right parts into the
               element to classify, examples POLOPONY and BEATS

          8. Segmentation

             - separate fish images

             - even harder in phoneme-speech detection: sklee and
               skloo have different /sk/

          9. Context

             - order: all salmons before might sway classifier

             - place: "jeetyet"? in cafeteria as"'did you eat yet?"

          10. Invariances

              - things that should not affect the classifier,
                f.ex. the rotation of the fish == invariant to rotation

                - able to translate/rotate to use the same

              - more complicated for speech recognition: invariant to time

              - handwriting: invariant to writing speed, pen width, ...

              - which invariances are present? how to train the
                system?

          11. Evidence Pooling

              - how to combine input from different classifiers

              - example: 10 experts, one says fish is sick, might be
                right

          12. Costs and Risks
*** quotes
    1. Introduction
       2. [@2] An Example
          - The overarching goal and approach in pattern
            classification is to hypothesize the class of these
            models, process the sensed data to eliminate noise (not
            due to the models), and for any sensed pattern choose the
            model that corresponds best.
          - These features (or, more precisely, the values of these
            features) are then passed to a classifier that evaluates
            the evidence presented and makes a final decision as to
            the species.
       3. The Sub-problems of Pattern Classification
          1. Feature Extraction
             - boundary between feature extraction and classification
               proper is somewhat arbitrary: an ideal feature extractor
               would yield a representation that makes the job of the
               classifier trivial; conversely, an omnipotent classifier
               would not need the help of a sophisticated feature
               extractor. The distinction is forced upon us for
               practical, rather than theoretical reasons.
          2. Noise
             - noise very general terms: any property of the sensed
               pattern due not to the true underlying model but
               instead to randomness in the world or the sensors.
          3. Overfitting
             - While an overly complex model may allow perfect
               classification of the training samples, it is unlikely
               to give good classification of novel patterns — a
               situation known as /overfitting/.
    2. Bayesian Decision Theory
       8. [@8] Error Bounds for Normal Densities
          3. [@3] Signal Detection Theory and Operating Characteristics
             - the Bayes error rate — the most important property of
               any classifier.
*** ref [[file:master.bib::duda][Duda et al. 2000: Pattern Classification]]
** TODO [#A] [[./Sensitivity and specificity - Wikipedia.fpr_etc.html]]
*** summary
    0. [@0]
       
       - sensitivity == tpr == recall: proportion of correctly
         identified positives, avoiding of false negatives

       - specitivity == tnr: proportion of correctly identified
         negatives, avoiding of false positives

    1. Definitions

       0. [@0]

          - true positive (TP) == hit

          - true negative (TN) == correct rejection

          - false positive (FP) == false alarm, Type 1 error

          - false negative (FN) == miss, Type 2 error

          - sensitivity == hit rate == recall: TPR = TP / P = TP / (TP+FN)

          - specitivity == true negative rate: TNR = SPC = TN/N = TN/(TN+FP)

          - precision == positive predictive values: PPV = TP / (TP+FP)

          - fall-out: FPR = FP/N = FP / (FP+TN) = 1-SPC

          - accuracy: ACC = (TP+TN) / (TP+FP+FN+TN)

          - Informedness: TPR + SPC -1

          - Markedness: PPV + NPV -1

       1. Application to screening study

          TP: sick people identified as sick, FP: healthy id'ed as sick, ...

       2. Confusion matrix
*** quotes
    0. [@0]
       - Sensitivity and specificity are statistical measures of the
         performance of a binary classification test, also known in
         statistics as classification function
    1. Definitions
       1. Application to screening study
          - In general, Positive = identified and negative = rejected
       2. Confusion matrix
          - The four outcomes can be formulated in a 2×2 /contingency
            table/ or /confusion matrix/
** TODO [#A] [[./201101-Evaluation_JMLT_Postprint-Colour.pdf][Powers - Evaluation: from precision, recall and f-measure to roc, informedness, markedness & correlation]]
*** summary
    0. [@0] Abstract

       - commonly used measures are biased

         - needs to understand that bias

       - new measures, and relation to existing ones

    1. INTRODUCTION

       - recall, precision, f-measure biased: ignore negative sample
         handling

       - medical: roc, tpr, fpr

       - behavioural: specitivity, sensitivity

       - alternate, still biased: Rand Accuracy, Cohen Kappa

    2. THE BINARY CASE

       0. [@0]

          - UPPER CASE (TP/FP ...) for counts, lower case (tp/fp ...)
            for probabilities

          - assumption: no value is 0, dichotomous (two-valued) model

       1. Recall & Precision, Sensitivity & Specitivity
*** quotes
    2. [@2] THE BINARY CASE
       - On the other hand =tp=, =fp=, =fn=, =tn= and =rp=, =rn=, and
         =pp=, =pn= refer to the joint and marginal probabilities
** TODO [#A] [[./user_guide_0.16.1.pdf][Pedregosa - scikit-learn user guide]]
*** TODO [#A] 6.3. Model evaluation: quantifying the quality of predictions
*** TODO [#B] 42.21. sklearn.metrics: Metrics: sklearn.metrics.auc, see [[file:~/da/da.org::*open%20world][open world]]
*** TODO [#B] 4.13.5 Tree-based feature selection
    feature_importances: gini or obb-score, see todo
*** summary
    1. AN INTRODUCTION TO MACHINE LEARNING WITH SCIKIT-LEARN

       1. Machine learning: the problem setting

          - supervised learning

            - classification: classes

            - regression: continuous variables

          - unsupervised learning:

            - clustering: similar examples within the data

            - distribution of data: density estimation

          - training set and testing set:

            - training: learn properties

            - testing: test properties

       2. Loading an example dataset

          - from sklearn import datasets

          - digits = datasets.load_digits()

          - digits.data: features

          - digits.target: class

          - digits.images[0] (here)

       3. Learning and predicting

          - =estimator= offers =fit(X, y)= and =predict(T)=

          - from sklearn import svm

          - clf = svm.SVC(gamma=0.001, C=100.)

          - clf.fit(digits.data[:-1], digits.target[:-1])

          - clf.predict(digits.data[-1])

       4. Model persistence

          - from sklearn.externals import joblib

          - joblib.dump(clf, 'filename.pkl')

          - clf2 = joblib.load('filename.pkl')

            followed by clf2.predict(...)

    2. A TUTORIAL ON STATISTICAL-LEARNING FOR SCIENTIFIC DATA
       PROCESSING

       1. Statistical learning: the setting and the estimator object
          in scikit-learn

          data: often 2d-array
          estimator: estimates (fit, ..., params at object creation)

       2. Supervised learning: predicting an output variable from
          high-dimensional observations

          1. Nearest neighbor and the curse of dimensionality

             toy problem: classifying irises

             - import numpy as np
             - from sklearn import datasets
             - iris = datasets.load_iris()
             - iris_X = iris.data
             - iris_y = iris.target
             - np.unique(iris_y)

             - k-Nearest neighbors classifier

               - simplest classifier

             - The curse of dimensionality

               the number of points needed scales exponentially with the
               dimension

          2. Linear model: from regression to sparsity

             1. linear regression

                fit line X \beta + \epsilon to data points

             2. Shrinkage

                problem with few data points per dimension, solve via
                Ridge (uses l_{2}-Norm)

             3. Sparsity

                it helps to reduce non-informative variables. Ridge
                reduces, Lasso sets to 0 (good on large datasets), as
                does LassoLARS (few observations)

             4. Classification

                for classification-regression, better use sigmoid instead
                of linear (LogisticRegression)

          3. Support vector machines (SVMs)

             1. Linear SVMs

                C is regularization parameter:

                "a small value for C means the margin is calculated using
                many or all of the observations around the separating line
                (more regularization); a large value for C means the
                margin is calculated on observations close to the
                separating line (less regularization)."

                thus: bigger C, less values considered (always those
                close to separating line)

                "For many estimators, including the SVMs, having datasets
                with *unit standard deviation* for each feature is
                important to get good prediction."

             2. Using kernels

                different fitting functions: linear, poly, rbf

       3. Model selection: choosing estimators and their parameters

          1. Score, and cross-validated scores

             - score: bigger is better

             - k-fold cross-validation:

               - import numpy as np
               - X_folds = np.array_split(X_digits, 3)
               - y_folds = np.array_split(y_digits, 3)
               - scores = list()
               - for k in range(3):
                 - X_train = list(X_folds)
                 - X_test  = X_train.pop(k)
                 - X_train = np.concatenate(X_train)
                 - y_train = list(y_folds)
                 - y_test  = y_train.pop(k)
                 - y_train = np.concatenate(y_train)
                 - scores.append(svc.fit(X_train, y_train).score(X_test, y_test))

          2. Cross-validation generators

             X-validation (Kfold, Loo, etc) are available via

             =sklearn.cross_validation=

             - "To compute the score method of an estimator, the sklearn
               exposes a helper function:

               #+BEGIN_SRC python
                 cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)
               #+END_SRC

          3. Grid-search and cross-validated estimators

             1. Grid-search

                automatic parameter search

                #+BEGIN_SRC python
                  # previously created a SVM named svc
                  from sklearn.grid_search import GridSearchCV
                  Cs = np.logspace(-6, -1, 10)
                  clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)
                  clf.fit(X_digits[:1000], y_digits[:1000])
                  clf.best_score_
                  clf.best_estimator_.C
                #+END_SRC

             2. Cross-validated estimators

                with CV added to name: search automatically

                (but not svm)

       4. Unsupervised learning: seeking representations of the data

          1. Clustering: grouping observations together

             when categorization data not at hand

             1. K-means clustering
                #+BEGIN_SRC python
                  from sklearn import cluster, datasets
                  iris = datasets.load_iris()
                  X_iris = iris.data
                  y_iris = iris.target
                  k_means = cluster.KMeans(n_clusters=3)
                  k_means.fit(X_iris)
                  print(k_means.labels_[::10])
                #+END_SRC

                iris: almost correct with 8 clusters, off on border with 3

             2. Hierarchical agglomerative clustering: Ward

                hierarchy of clusters

                - agglomerative (bottom-up)

                  "This approach is particularly interesting when the
                  clusters of interest are made of only a few
                  observations. When the number of clusters is large, it
                  is much more computationally efficient than k-means."

                - divisive (top-down)

                  "For estimating large numbers of clusters, this
                  approach is both slow (due to all observations starting
                  as one cluster, which it splits recursively) and
                  statistically ill-posed."

             3. Connectivity-constrained clustering

                - cluster depending on adjacency

                - f.ex. used to find separating lines in lena picture

             4. Feature agglomeration

                - defeat curse of dimensionality by merging "similar
                  features"

          2. Decompositions: from a signal to components and loadings

             decompose signal X into components C and loadings C

             1. Principal component analysis: PCA

                - extracts combinations which explain "maximum
                  variance" (see quotes)

             2. Independent Component Analysis: ICA

                - might work better for analysis of non-gaussian data
                  comparison (example see quotes)

       5. Putting it all together

          1. Pipelining

             - combined estimators
               #+BEGIN_SRC python
                 from sklearn import linear_model, decomposition, datasets
                 from sklearn.pipeline import Pipeline
                 #...
                 pca = decomposition.PCA()
                 pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])
                 # prediction
                 n_components = [20, 40, 64]
                 Cs = np.logspace(-4, 4, 3)
                 estimator = GridSearchCV(pipe,
                 dict(pca__n_components=n_components,
                 logistic__C=Cs))
                 estimator.fit(X_digits, y_digits)
               #+END_SRC

          2. Face recognition with eigenfaces

             0. [@0] imports
                #+BEGIN_SRC python
                  from sklearn.cross_validation import train_test_split
                  from sklearn.metrics import classification_report
                  from sklearn.metrics import confusion_matrix
                #+END_SRC

             1. get and prepare data
                #+BEGIN_SRC python
                  X_train, X_test, y_train, y_test = train_test_split(
                      X, y, test_size=0.25)
                #+END_SRC

             2. do PCA

             3. apply PCA to data

             4. train svm
                #+BEGIN_SRC python
                  clf = GridSearchCV(SVC(kernel='rbf', class_weight='auto'), param_grid)
                  clf = clf.fit(X_train_pca, y_train)
                #+END_SRC
                ('auto' is deprecated, use 'balanced' instead)

             5. evaluate
                #+BEGIN_SRC python
                  y_pred = clf.predict(X_test_pca)
                  print(classification_report(y_test, y_pred, target_names=target_names))
                  print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))
                #+END_SRC

          3. Open problem: Stock Market Structure

             predict variation in Google stock price

       6. Finding help

          1. The project mailing list

             "If you encounter a bug with scikit-learn or something
             that needs clarification in the docstring or the online
             documentation, please feel free to ask on [[http://scikit-learn.sourceforge.net/support.html][the Mailing
             List]]"

          2. Q&A communities with Machine Learning practitioners

             - http://metaoptimize.com/qa

    4. [@4] SUPERVISED LEARNING

       1. Generalized Linear Models

          y ˆ (w, x) = w 0 + w 1 x 1 + ... + w p x p

          1. Ordinary Least Squares
             #+BEGIN_SRC python
               from sklearn import linear_model
               clf = linear_model.LinearRegression()
               clf.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
             #+END_SRC

          2. Ridge Regression

             penalty term for size of parameters: min ||Xw − y||^2 + α||w||^2

             #+BEGIN_SRC python
               clf = linear_model.Ridge (alpha = .5)
             #+END_SRC

             evaluate alpha:
             #+BEGIN_SRC python
               clf = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
               clf.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
               clf.alpha_
             #+END_SRC

             also has CV method

          3. Lasso

             for sparse parameters

             #+BEGIN_SRC python
               clf = linear_model.Lasso(alpha = 0.1)
               clf.fit([[0, 0], [1, 1]], [0, 1])
               clf.predict([[1, 1]])
             #+END_SRC

          4. Elastic Net

             - sparse model like Lasso with regularization Ridge

             - correlated features
               #+BEGIN_SRC python
                 from sklearn.linear_model import ElasticNet
                 enet = ElasticNet(alpha=alpha, l1_ratio=0.7)
                 y_pred_enet = enet.fit(X_train, y_train).predict(X_test)
               #+END_SRC

          5. Multi-task Lasso

             - simultaneously evaluate n lassos
               #+BEGIN_SRC python
                 from sklearn.linear_model import MultiTaskLasso, Lasso
                 rng = np.random.RandomState(42)
                 # Generate some 2D coefficients with sine waves with random frequency and phase
                 n_samples, n_features, n_tasks = 100, 30, 40
                 n_relevant_features = 5
                 coef = np.zeros((n_tasks, n_features))
                 times = np.linspace(0, 2 * np.pi, n_tasks)
                 for k in range(n_relevant_features):
                     coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))
                 X = rng.randn(n_samples, n_features)
                 Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)
                 coef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])
                 coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_
               #+END_SRC

          6. Least Angle Regression

             - high-dimensional data
               #+BEGIN_SRC python
                 from sklearn import linear_model
                 clf = linear_model.Lars(n_nonzero_coefs=1)
                 clf.fit([[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111])
               #+END_SRC

          7. LARS Lasso

             point 6) with Lasso
             #+BEGIN_SRC python
               clf = linear_model.LassoLars(alpha=.1)
               clf.fit([[0, 0], [1, 1]], [0, 1])
             #+END_SRC

          8. Orthogonal Matching Pursuit (OMP)

             #+BEGIN_SRC python
               from sklearn.linear_model import OrthogonalMatchingPursuit
               from sklearn.linear_model import OrthogonalMatchingPursuitCV
               omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)
               omp.fit(X, y)
               # plot the noisy reconstruction with number of non-zeros set by CV
               omp_cv = OrthogonalMatchingPursuitCV()
               omp_cv.fit(X, y_noisy)
             #+END_SRC

             - arg min ||y − Xγ||^{2}_{2} subject to ||γ||_{0} ≤ n_{nonzero_coefs}

          9. Bayesian Regression

             1. Bayesian Ridge Regression

               #+BEGIN_SRC python
                 from sklearn import linear_model
                 X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
                 Y = [0., 1., 2., 3.]
                 clf = linear_model.BayesianRidge()
                 clf.fit(X, Y)
               #+END_SRC
                - adapts, but costly (see quotes)

             2. Automatic Relevance Determination - ARD

       11. [@11] Ensemble methods

           0. [@0]

              - combine estimators

              - averaging methods: concurrently: bagging methods,
                forests of randomized trees

              - boosting methods: subsequently

           1. Bagging meta-estimator

              - several instances on subsets of data (and features)

                - combine those

              - method to reduce variance

              - difference of drawing subsets: just so, with
                replacement, subsets of features, both features and
                samples
              #+BEGIN_SRC python
              from sklearn.ensemble import BaggingClassifier
              from sklearn.neighbors import KNeighborsClassifier
              bagging = BaggingClassifier(KNeighborsClassifier(),
              max_samples=0.5, max_features=0.5)
              #+END_SRC

           2. Forests of randomized trees

              based on decision trees, perturb-and-combine, multi-output

              1. Random Forests

                 - sample drawn with replacement

                 - each node chooses best of a random subset of features

                 - this implementation averages probabilitic
                   prediction of trees, instead of adding the votes

              2. Extremely Randomized Trees

                 - difference: randomly draw threshholds for each
                   feature and use the best of these

                   - use whole dataset to bootstrap

              3. Parameters

                 - best to cross-validate parameters

                   - n_estimators, max_features, max_depth, min_samples_split

              4. Parallelization

                 possible, but might not give linear speedup

              5. Feature importance evaluation

                 - depth of feature relates to rank

                 - averaging over several trees reduces variance

              6. Totally Random Trees Embedding

                 - encodes the leaves

                 - maybe used by k-forest

                 - example see 11.11.8 "Hashing feature transformation
                   using Totally Random Trees"

           3. AdaBoost

       12. Multiclass and multilabel algorithms

           change ways in which multi-class classification is
           approached

           1. Multilabel classification format

              - use MultiLabelBinarizer to switch set to 0/1 labels
                #+BEGIN_SRC python
                  from sklearn.preprocessing import MultiLabelBinarizer
                  Y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]
                  MultiLabelBinarizer().fit_transform(Y)
                #+END_SRC

           2. One-Vs-The-Rest

              - efficient and interpretable
                #+BEGIN_SRC python
                  from sklearn import datasets
                  from sklearn.multiclass import OneVsRestClassifier
                  from sklearn.svm import LinearSVC
                  iris = datasets.load_iris()
                  X, y = iris.data, iris.target
                  OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X)
                #+END_SRC

              - can also use multi-label

           3. One-Vs-One

    6. [@6] Model selection and evaluation

       1. Cross-validation: evaluating estimator performance

          - example usage of
            #+BEGIN_SRC python
              X_train, X_test, y_train, y_test = cross_validation.train_test_split(
                  iris.data, iris.target, test_size=0.4, random_state=0)
            #+END_SRC

       2. Grid Search: Searching for estimator parameters

          0. [@0]

             - for *hyperparameters*: need to be set explicitly, example:
               C, kernel, gamma for SVM

          3. [@3] Tips for parameter search

             - specify explicit metric (default is accuracy_score)

               - good for unbalanced classification

             - split train and test set

             - exploit parallelism

          4. Alternatives to brute force parameter search

             3. [@3] (? - p.278) Out of Bag Estimates

                - for bagging methods: they leave out part of the data

                - use that to estimate scores

    7. Dataset transformations

       3. [@3] Preprocessing data

          make data "more suitable for downstream estimators"

          1. Standardization, or mean removal and variance scaling

             - set to zero mean and unit variance
               #+BEGIN_SRC python
                 X_scaled = preprocessing.scale(X)
               #+END_SRC

             - save values to apply to test data later
               #+BEGIN_SRC python
                 scaler = preprocessing.StandardScaler().fit(X)
                 scaler.transform(X)
               #+END_SRC

             - Scaling features to a range

               instead of fixed variance, set to range

               - initialize: =min_max_scaler = preprocessing.MinMaxScaler()=

               - train: =X_train_minmax = min_max_scaler.fit_transform(X_train)=

             - test: =X_test_minmax = min_max_scaler.transform(X_test)=
    11. [@11] Examples

        18. [@18] Model Selection

            4. [@4] Receiver Operating Characteristic (ROC) with cross
               validation

               - TPR on y-axis, FPR on x-axis

               - several measurements, average

               - area-under-curve important measure

               - make 2-class
                 #+BEGIN_SRC python
                   # make 2-class: exclude third class
                   X, y = X[y != 2], y[y != 2]

                   cv = StratifiedKFold(y, n_folds=6)
                   classifier = svm.SVC(kernel='linear', probability=True)
                 #+END_SRC

               - compute tpr, fpr
                 #+BEGIN_SRC python
                   for i, (train, test) in enumerate(cv):
                       probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
                       fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
                       roc_auc = auc(fpr, tpr)
                 #+END_SRC

            8. [@8] Receiver Operating Characteristic (ROC)

               - top left point ideal, larger area under curve (AUC)
                 better, steeper better

               - typically binary classification, extendable by
                 binarizing or micro-averaging

*** quotes
    2. [@2] A tutorial on statistical-learning for scientific data processing
       1. Statistical learning: the setting and the estimator object
          in scikit-learn
          - import pylab as pl
            pl.imshow
       4. [@4] Unsupervised learning: seeking representations of the data
          2. [@2] Decompositions: from a signal to components and loadings
             - Create a signal with only 2 useful dimensions, analyse
               #+BEGIN_SRC python
                 x1 = np.random.normal(size=100)
                 x2 = np.random.normal(size=100)
                 x3 = x1 + x2
                 X = np.c_[x1, x2, x3]
                 from sklearn import decomposition
                 pca = decomposition.PCA()
                 pca.fit(X)
                 PCA(copy=True, n_components=None, whiten=False)
                 print(pca.explained_variance_)
                 pca.n_components = 2
                 X_reduced = pca.fit_transform(X)
                 X_reduced
               #+END_SRC
             - Combine signals
               #+BEGIN_SRC python
                 time = np.linspace(0, 10, 2000)
                 s1 = np.sin(2 * time) # Signal 1 : sinusoidal signal
                 s2 = np.sign(np.sin(3 * time)) # Signal 2 : square signal
                 S = np.c_[s1, s2]
                 S += 0.2 * np.random.normal(size=S.shape) # Add noise
                 S /= S.std(axis=0) # Standardize data
                 # Mix data
                 A = np.array([[1, 1], [0.5, 2]]) # Mixing matrix
                 X = np.dot(S, A.T) # Generate observations
                 ica = decomposition.FastICA() # Compute ICA
                 S_ = ica.fit_transform(X) # Get the estimated sources
                 A_ = ica.mixing_.T
                 np.allclose(X, np.dot(S_, A_) + ica.mean_)
               #+END_SRC
    4. [@4] Supervised learning
       1. Generalized Linear Models
          2. [@2] Ridge Regression
             - Here, α ≥ 0 is a complexity parameter that controls the
               amount of shrinkage: the larger the value of α, the
               greater the amount of shrinkage and thus the
               coefficients become more robust to collinearity.
          3. [@3] Lasso
             - The alpha parameter controls the degree of sparsity of
               the coefficients estimated.
          9. [@9] Bayesian Regression
             - The advantages of Bayesian Regression are:
               - It adapts to the data at hand.
               - It can be used to include regularization parameters
                 in the estimation procedure.
               The disadvantages of Bayesian regression include:
               - Inference of the model can be time consuming.
       4. [@4] Support Vector Machines
          1. Classification
             1. Multi-class classification
                - SVC and NuSVC implement the “one-against-one”
                  approach (Knerr et al., 1990) for multi- class
                  classification. If =n_class= is the number of
                  classes, then =n_class * (n_class - 1) / 2=
                  classifiers are constructed and each one trains data
                  from two classes
                - Note that the LinearSVC also implements an
                  alternative multi-class strategy, the so-called
                  multi-class SVM formulated by Crammer and Singer, by
                  using the option
                  =multi_class=’crammer_singer’=. This method is
                  consistent, which is not true for one-vs-rest
                  classification. In practice, one-vs-rest
                  classification is usually preferred, since the
                  results are mostly similar, but the runtime is
                  significantly less.
             2. Scores and probabilities
                - When the constructor option =probability= is set to
                  =True=, class membership probability estimates (from
                  the methods =predict_proba= and =predict_log_proba=)
                  are enabled.
          5. [@5] Tips on Practical Use
             - Avoiding data copy: For SVC, SVR, NuSVC and NuSVR, if
               the data passed to certain methods is not C-ordered
               contiguous, and double precision, it will be copied
               before calling the underlying C implementation. You can
               check whether a give numpy array is C-contiguous by
               inspecting its =flags= attribute.
             - *it is highly recommended to scale your data*
             - In SVC, if data for classification are unbalanced
               (e.g. many positive and few negative), set
               =class_weight=’auto’= and/or try different penalty
               parameters C.
       6. [@6] Nearest Neighbors
          0. [@0]
             - Despite its simplicity, nearest neighbors has been successful in
               a large number of classification and regression problems
          2. [@2] Nearest Neighbors Classification
             - RadiusNeighborsClassifier [...] For high-dimensional parameter
               spaces, this method becomes less effective due to the so-called
               “curse of dimensionality”.
          4. [@4] Nearest Neighbor Algorithms
             - Currently, algorithm = ’auto’ selects ’kd_tree’ if k < N/2 and
               the ’effective_metric_’ is in the ’VALID_METRICS’ list of
               ’kd_tree’. It selects ’ball_tree’ if k < N/2 and the
               ’effective_metric_’ is not in the ’VALID_METRICS’ list of
               ’kd_tree’. It selects ’brute’ if k >= N/2. This choice is based
               on the assumption that the number of query points is at least
               the same order as the number of training points, and that
               leaf_size is close to its default value of 30.
          6. [@6] Approximate Nearest Neighbors
             - There are many efficient exact nearest neighbor search
               algorithms for low dimensions d (approximately 50). However
               these algorithms perform poorly with respect to space and query
               time when d increases. These algorithms are not any better than
               comparing query point to each point from the database in a high
               dimension (see Brute Force). This is a well-known consequence of
               the phenomenon called “The Curse of Dimensionality”. There are
               certain applications where we do not need the exact nearest
               neighbors but having a “good guess” would suffice. When answers
               do not have to be exact, the LSHForest class implements an
               approximate nearest neighbor search.
       9. [@9] Naive Bayes
          0. [@0]
             - although naive Bayes is known as a decent classifier, it
               is known to be a bad estimator, so the probability outputs
               from predict_proba are not to be taken too seriously.
          2. [@2] Multinomial Naive Bayes
       10. Decision Trees
           - DecisionTreeClassifier [...]
             #+BEGIN_SRC python
               from sklearn import tree
               X = [[0, 0], [1, 1]]
               Y = [0, 1]
               clf = tree.DecisionTreeClassifier()
               clf = clf.fit(X, Y)
             #+END_SRC
       11. Ensemble methods
           2. [@2] Forests of randomized trees
              - As other classifiers, forest classifiers have to be fitted with
                two arrays: a sparse or dense array X of size [n_samples,
                n_features] holding the training samples, and an array Y of size
                [n_samples] holding the target values (class labels) for the
                training samples:
                #+BEGIN_SRC python
                  from sklearn.ensemble import RandomForestClassifier
                  X = [[0, 0], [1, 1]]
                  Y = [0, 1]
                  clf = RandomForestClassifier(n_estimators=10)
                  clf = clf.fit(X, Y)
                #+END_SRC
              - The best parameter values should always be
                cross-validated.
       12. Multiclass and multilabel algorithms
           3. [@3] One-Vs-One
              - This is because each individual learning problem only
                involves a small subset of the data whereas, with
                one-vs-the-rest, the complete dataset is used
                n_classes times.
    5. Unsupervised learning
       3. [@3] Clustering
          2. [@2] K-means
             - The KMeans algorithm clusters data by trying to separate
               samples in n groups of equal variance, minimizing a
               criterion known as the inertia <inertia> or
               within-cluster sum-of-squares. This algorithm requires
               the number of clusters to be specified.
    6. MODEL SELECTION AND EVALUATION
       1. Cross-validation: evaluating estimator performance
          - Learning the parameters of a prediction function and
            testing it on the same data is a methodological mistake
       2. Grid Search: Searching for estimator parameters
          3. [@3] Tips for parameter search
             - When evaluating the resulting model it is important to
               do it on held-out samples that were not seen during the
               grid search process: it is recommended to split the
               data into a development set (to be fed to the
               GridSearchCV instance) and an evaluation set to compute
               performance metrics.
    11. [@11] Examples
        18. [@18] Model Selection
            3. [@3] Confusion matrix
               - Example of confusion matrix usage to evaluate the
                 quality of the output of a classifier on the iris
                 data set.
                 #+BEGIN_SRC python
                   from sklearn.metrics import confusion_matrix
                   def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):
                       plt.imshow(cm, interpolation='nearest', cmap=cmap)
                       plt.colorbar()
                       tick_marks = np.arange(len(iris.target_names))
                       plt.xticks(tick_marks, iris.target_names, rotation=45)
                       plt.yticks(tick_marks, iris.target_names)
                       plt.tight_layout()
                   # Compute confusion matrix
                   cm = confusion_matrix(y_test, y_pred)
                   np.set_printoptions(precision=2)
                   print(cm)
                   plt.figure()
                   plot_confusion_matrix(cm)
                   # Normalize the confusion matrix by row (i.e by the number of samples
                   # in each class)
                   cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
                 #+END_SRC
            8. [@8] Receiver Operating Characteristic (ROC)
               - ROC curves typically feature true positive rate on
                 the Y axis, and false positive rate on the X
                 axis. This means that the top left corner of the plot
                 is the “ideal” point - a false positive rate of zero,
                 and a true positive rate of one.
               - The “steepness” of ROC curves is also important,
                 since it is ideal to maximize the true positive rate
                 while minimizing the false positive rate.
               - Python source code: plot_roc.py
                 #+BEGIN_SRC python
                   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)#, random_state=0)
                   # Learn to predict each class against the other
                   classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True))#, random_state=random_state))
                   y_score = classifier.fit(X_train, y_train).decision_function(X_test)
                   # Compute ROC curve and ROC area for each class
                   fpr = dict()
                   tpr = dict()
                   roc_auc = dict()
                   for i in range(n_classes):
                       fpr[i], tpr[i], _ = roc_curve(y_test[:, i],
                                                     y_score[:, i])
                       roc_auc[i] = auc(fpr[i], tpr[i])
                   # Plot of a ROC curve for a specific class
                   plt.figure()
                   plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])
                   plt.plot([0, 1], [0, 1], 'k--')
                   plt.xlim([0.0, 1.0])
                   plt.ylim([0.0, 1.05])
                   plt.xlabel('False Positive Rate')
                   plt.ylabel('True Positive Rate')
                   plt.title('Receiver operating characteristic example')
                   plt.legend(loc="lower right")
                   plt.show()
                 #+END_SRC
    12. Frequently Asked Questions
        12. [@12] How do I deal with string data (or trees, graphs...)?
            - Firstly, many estimators take precomputed
              distance/similarity matrices, so if the dataset is not too
              large, you can compute distances for all pairs of
              inputs. If the dataset is large, you can use feature
              vectors with only one “feature”, which is an index into a
              separate data structure, and supply a custom metric
              function that looks up the actual data in this data
              structure. E.g., to use DBSCAN with Levenshtein distances:
              #+BEGIN_SRC python
                >>> from leven import levenshtein
                >>> import numpy as np
                >>> from sklearn.cluster import dbscan
                >>> data = ["ACCTCCTAGAAG", "ACCTACTAGAAGTT", "GAATATTAGGCCGA"]
                >>> def lev_metric(x, y):
                ...
                i, j = int(x[0]), int(y[0])
                # extract indices
                ...
                return levenshtein(data[i], data[j])
                ...
                >>> X = np.arange(len(data)).reshape(-1, 1)
                >>> X
                array([[0],
                [1],
                [2]])
                >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2)
                ([0, 1], array([ 0, 0, -1]))
              #+END_SRC
              (This uses the third-party edit distance package leven.)
              Similar tricks can be used, with some care, for tree
              kernels, graph kernels, etc.
    19. [@19] 0.14
        2. [@2] API changes summary
           - ... =decision_function= method. This will return the
             distance of each sample from the decision boundary for
             each class, as long as the underlying estimators
             implement the decision_function method.
    42. [@42] Reference
        5. [@5] sklearn.cross_validation: Cross Validation
           7. [@7] sklearn.cross_validation.StratifiedKFold
              - class sklearn.cross_validation.StratifiedKFold(y,
                n_folds=3, indices=None, shuffle=False,
                random_state=None)

                Stratified K-Folds cross validation iterator.

                Provides train/test indices to split data in train
                test sets.

                This cross-validation object is a variation of KFold
                that returns stratified folds. The folds are made by
                preserving the percentage of samples for each class.
        9. [@9] sklearn.ensemble: Ensemble Methods
           9. [@9] sklearn.ensemble.RandomForestClassifier
              - apply(X)

                Apply trees in the forest to X, return leaf indices.
                - Parameters X : array-like or sparse matrix, shape =
                  [n_samples, n_features]

                  The input samples. Internally, it will be converted to
                  dtype=np.float32 and if a sparse matrix is provided to
                  a sparse csr_matrix.
                - Returns X_leaves : array_like, shape = [n_samples,
                  n_estimators]

                  For each datapoint x in X and for each tree in the
                  forest, return the index of the leaf x ends up in.
        13. [@13] sklearn.grid_search: Grid Search
            0. [@0]
               - User guide: See the Grid Search: Searching for
                 estimator parameters section for further details.
            1. sklearn.grid_search.GridSearchCV
               - class sklearn.grid_search.GridSearchCV(estimator,
                 param_grid, scoring=None, loss_func=None,
                 score_func=None, fit_params=None, n_jobs=1, iid=True,
                 refit=True, cv=None, verbose=0,
                 pre_dispatch=‘2*n_jobs’, error_score=’raise’)
               - Parameters:
                 - verbose : integer
                   - Controls the verbosity: the higher, the more messages.
        21. [@21] sklearn.metrics: Metrics
            2. [@2] Classification metrics
               - sklearn.metrics.roc_curve(y_true, y_score,
                 pos_label=None, sample_weight=None)

                 Compute Receiver operating characteristic (ROC)

                 Returns
                 - fpr : array, shape = [>2] [...]

                   Increasing false positive rates such that element i
                   is the false positive rate of predictions with
                   score >= thresholds[i].
                 - tpr : array, shape = [>2]

                   Increasing true positive rates such that element i
                   is the true positive rate of predictions with score
                   >= thresholds[i].
                 - thresholds : array, shape = [n_thresholds]
            3. [@3] Regression metrics
               - sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None)
                 [...]
                 #+BEGIN_SRC python
                   from sklearn.metrics import r2_score
                   y_true = [3, -0.5, 2, 7]
                   y_pred = [2.5, 0.0, 2, 8]
                   r2_score(y_true, y_pred) # 0.948...
                   y_true = [[0.5, 1], [-1, 1], [7, -6]]
                   y_pred = [[0, 2], [-1, 2], [8, -5]]
                   r2_score(y_true, y_pred)
                 #+END_SRC
        23. [@23] sklearn.multiclass: Multiclass and multilabel classification
            2. [@2] sklearn.multiclass.OneVsRestClassifier
               - class sklearn.multiclass.OneVsRestClassifier(estimator, n_jobs=1)
                 - [...]
                 - Parameters:
                   - [...]
                   - n_jobs : int, optional, default: 1

                     The number of jobs to use for the computation. If
                     -1 all CPUs are used. If 1 is given, no parallel
                     computing code is used at all, which is useful
                     for debugging. For n_jobs below -1, (n_cpus + 1 +
                     n_jobs) are used. Thus for n_jobs = -2, all CPUs
                     but one are used.
        25. [@25] sklearn.neighbors: Nearest Neighbors
            2. [@2] sklearn.neighbors.KNeighborsClassifier
               - class
                 sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,
                 weights=’uniform’, algorithm=’auto’,
                 leaf_size=30, p=2, metric=’minkowski’,
                 metric_params=None, **kwargs) [...]

                 metric : string or DistanceMetric object
                 (default = ‘minkowski’) the distance metric to
                 use for the tree. The default metric is
                 minkowski, and with p=2 is equivalent to the
                 standard Euclidean metric. See the documentation
                 of the Distance-Metric class for a list of
                 available metrics.
            10. [@10] sklearn.neighbors.DistanceMetric
                - “braycurtis” | BrayCurtisDistance | sum(|x - y|)
                  / (sum(|x|) + sum(|y|))
        29. [@29] sklearn.pipeline: Pipeline
            1. sklearn.pipeline.Pipeline
               - class sklearn.pipeline.Pipeline(steps) Pipeline
                 of transforms with a final
                 estimator. Sequentially apply a list of
                 transforms and a final estimator. Intermediate
                 steps of the pipeline must be ‘transforms’, that
                 is, they must implement fit and transform
                 methods. The final estimator only needs to
                 implement fit. The purpose of the pipeline is to
                 assemble several steps that can be
                 cross-validated together while setting different
                 parameters. For this, it enables setting
                 parameters of the various steps using their
                 names and the parameter name separated by a
                 ‘__’, as in the example below.

                 Parameterssteps: list :List of (name, transform)
                 tuples (implementing fit/transform) that are
                 chained, in the order in which they are chained,
                 with the last object an estimator.

                 Examples
                 #+BEGIN_SRC python
                   >>> from sklearn import svm
                   >>> from sklearn.datasets import samples_generator
                   >>> from sklearn.feature_selection import SelectKBest
                   >>> from sklearn.feature_selection import f_regression
                   >>> from sklearn.pipeline import Pipeline
                   >>> # generate some data to play with
                   >>> X, y = samples_generator.make_classification(
                   ...
                   n_informative=5, n_redundant=0, random_state=42)
                   >>> # ANOVA SVM-C
                   >>> anova_filter = SelectKBest(f_regression, k=5)
                   >>> clf = svm.SVC(kernel='linear')
                   >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
                   >>> # You can set the parameters using the names issued
                   >>> # For instance, fit using a k of 10 in the SelectKBest
                   >>> # and a parameter 'C' of the svm
                   >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
                   ...
                   Pipeline(steps=[...])
                   >>> prediction = anova_svm.predict(X)
                   >>> anova_svm.score(X, y)
                   0.77...
                 #+END_SRC
        30. sklearn.preprocessing: Preprocessing and Normalization
            7. [@7] sklearn.preprocessing.MinMaxScaler
               - class
                 sklearn.preprocessing.MinMaxScaler(feature_range=(0,
                 1., copy=True)

                 Standardizes features by scaling each feature to a
                 given range.

                 This estimator scales and translates each feature
                 individually such that it is in the given range on
                 the training set, i.e. between zero and one.
            14. [@14] sklearn.preprocessing.label_binarize
                - sklearn.preprocessing.label_binarize(y, classes,
                  neg_label=0, pos_label=1, sparse_output=False,
                  multilabel=None)

                  Binarize labels in a one-vs-all fashion
                  #+BEGIN_SRC python
                    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])
                    array([[1], [0], [0], [1]])
                  #+END_SRC
        34. [@34] sklearn.svm: Support Vector Machines
            - decision_function(X)

              Distance of the samples X to the separating hyperplane.
        35. sklearn.tree: Decision Trees
            - class sklearn.tree.DecisionTreeClassifier[...]

              feature_importances_ : array of shape = [n_features]

              The feature importances.
    47. [@47] Chapter 47. How to optimize for speed
        3. [@3] Profiling Python code
           - It is however still interesting to check what’s happening
             inside the _nls_subproblem function which is the hotspot if
             we only consider Python code: it takes around 100% of the
             cumulated time of the module. In order to better understand
             the profile of this specific function, let us install
             line-prof and wire it to IPython:
             #+BEGIN_SRC sh
               pip install line-profiler
             #+END_SRC

    48. somewhere
                #+BEGIN_SRC python
                  from sklearn.ensemble import ExtraTreesClassifier
                  [...]
                  # Build a forest and compute the feature importances
                  forest = ExtraTreesClassifier(n_estimators=250,
                  random_state=0)
                  forest.fit(X, y)
                  importances = forest.feature_importances_
                #+END_SRC
*** ref [[file:master.bib::scikit-learn][Pedregosa et al. 2011: Scikit]]
*** exercises
**** Exercise digits-knn-linear
     Try classifying the digits dataset with nearest neighbors and a
     linear model. Leave out the last 10% and test prediction
     performance on these observations.

     from sklearn import datasets, neighbors, linear_model

     digits = datasets.load_digits()
     X_digits = digits.data
     y_digits = digits.target
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, neighbors, linear_model
        import numpy as np

        digits = datasets.load_digits()
        X_digits = digits.data
        y_digits = digits.target

        np.random.seed(0)
        indices = np.random.permutation(len(y_digits))
        percent = int(len(y_digits) * 0.9)

        digits_X_train = X_digits[indices[:percent]]
        digits_X_test = X_digits[indices[percent:]]
        digits_y_train = y_digits[indices[:percent]]
        digits_y_test = y_digits[indices[percent:]]

        knn = neighbors.KNeighborsClassifier()
        knn.fit(digits_X_train, digits_y_train)
        knn.score(digits_X_test, digits_y_test)

        lin = linear_model.LogisticRegression()
        lin.fit(digits_X_train, digits_y_train)
        lin.score(digits_X_test, digits_y_test)
      #+END_SRC
**** Exercise iris-svm
     Try classifying classes 1 and 2 from the iris dataset with SVMs,
     with the 2 first features. Leave out 10% of each class and test
     prediction performance on these observations.
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, svm
        import numpy as np

        iris = datasets.load_iris()
        X = iris.data
        y = iris.target

        # normalize std
        s = np.std(X, 0)
        for i in range(4):
            X[:,i] /= s[i]

        np.random.seed(0)
        indices = np.random.permutation(len(y))
        percent = int(len(y) * 0.9)

        X_train = X[indices[:percent]]
        X_test = X[indices[percent:]]
        y_train = y[indices[:percent]]
        y_test = y[indices[percent:]]

        svc = svm.SVC(kernel='linear')
        svc.fit(X_train, y_train)
        print svc.score(X_test, y_test)

        svcr = svm.SVC()
        svcr.fit(X_train, y_train)
        print svcr.score(X_test, y_test)
      #+END_SRC
**** Exercise kfold
     On the digits dataset, plot the cross-validation score of a SVC
     estimator with an linear kernel as a function of parameter C (use
     a logarithmic grid of points, from 1 to 10).
***** Solution
      #+BEGIN_SRC python
        import numpy as np
        from sklearn import cross_validation, datasets, svm

        C_s = np.logspace(-10, 0, 50)

        digits = datasets.load_digits()
        X = digits.data
        y = digits.target

        for C in C_s:
            svc.C = C
            score = cross_validation.cross_val_score(svc, X, y, n_jobs=1)
            print '%f, %f, %f' % (C, np.mean(score), np.std(score))
      #+END_SRC
**** Exercise grid-optimal
     On the diabetes dataset, find the optimal regularization
     parameter alpha.
***** Solution
      #+BEGIN_SRC python
        from sklearn import cross_validation, datasets, linear_model
        import numpy as np

        diabetes = datasets.load_diabetes()
        X = diabetes.data[:150]
        y = diabetes.target[:150]

        lasso = linear_model.Lasso()
        alphas = np.logspace(-4, -.5, 30)

        from sklearn.grid_search import GridSearchCV
        clf = GridSearchCV(estimator=lasso, param_grid=dict(alpha=alphas), n_jobs=-1)
        clf.fit(X, y)
        print clf.score(diabetes.data[150:], diabetes.target[150:])

        # for a in alphas:
        #     lasso.alpha = a
        #     score = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)
        #     print '%f, %s' % (a, np.mean(score))
      #+END_SRC
** TODO PART [#B] [[./thebook.pdf][Smola - INTRODUCTION TO MACHINE LEARNING]]
*** [#B] sec 3.3 Constrained Optimization, cont p.128 / 136
*** [#B] parts: classifiers for thesis: knn, svm, (bayes?, random forest?)
*** TODO summary
    1. Introduction

       often in background

       1. A Taste of Machine Learning

          applications, formalize

          1. Applications

             - page rank

             - collaborative filtering: amazon recommendations

             - automatic translation

             - face recognition vs face verification

             - named entity recognition: from documents etc some named thing

             - speech recognition: annotate voice sample

               - and similarly: handwriting, etc

             classification: yes/no answer

          2. Data

             types of data

             - Lists

             - Sets

             - Matrices

             - Images

             - Videos

             - Trees and Graphs

             - Strings

             - Compound structures: combine these

          3. Problems

             1. Binary Classification:

                - online learning: instantaneous determination of y for x

                - batch learning: group of stuff

                - transduction: know X' (testing data)

                - active learning: choose X

                - estimation with missing variables

                - covariate shift correction: X and X' from different
                  data sources

                - co-training: related but different problems

                - loss functions

             2. Multiclass Classification

             3. Structured Estimation

             4. Regression

             5. Novelty Detection: new and unusual observations

       2. Probability Theory

          1. Random Variables

             X takes on values in f.ex. \cal X = {1,..., 6} \ni x

          2. Distributions

             - discrete values: probability mass function, PMF

             - continuous values: probability density function, PDF
               integral: CDF

             - Pr(a \le X \le b) = \int_{a}^{b} dp(x) = F(b) - F(a)

             - Quantile: value x' for which Pr(X < x') \le q, Pr(X > x') \le 1-q
               is /q-quantile/. q=0.5: /median/

          3. Mean and Variance

             - mean: E[x] = \sum_{x} xp(x) and \int x dp(x)
               E[f(x)] =  \int f(x) dp(x)

             - variance Var[X] = E[(X - E(X))^{2}]
               Var[f(x)] =  E[(f(X) - E(f(X)))^{2}]

          4. Marginalization, Independence, Conditioning, and Bayes Rule

             0. [@0]

                - marginalization, independence, conditioning, iid, bayes
                  rule: see quotes

                - Bayes Rule holds because p(x,y) = p(x|y)p(y) = p(y|x)p(x)

                - "The key consequence of (1.15) is that we may
                  reverse the conditioning between a pair of random
                  variables."

             1. An Example

                HIV testing,

                - p(T = HIV+ | X = HIV+) = 1

                - p(T = HIV+ | X = HIV-) = 0.01

                - p(X = HIV+) = 0.0015
                  \to  p(X = HIV-) = 0.9985

                - then p(X = HIV+ | T = HIV+) is (shorthand P(x+, t-) etc)
                  = p(t+ | x+) p(x+) / p(t+)

                - p(t+) = p(x+, t+) + p(x-, t+)
                  = p(t+ | x+ ) p(x+) + p(t+ | x-) p(x-)
                  = 1 * 0.0015 + 0.01 * 0.9985

                - back to p(x+ | t+)
                  = 1 * 0.0015 / (1 * 0.0015 + 0.01 * 0.9985)

                - increase test accuracy by further information (f.ex. age)

                - and further, conditionally independent, tests

       3. Basic Algorithms

          0. [@0]

             - features X, labels y

             - /bag of words/: "Assume we have a list of all possible
               words occurring in X, that is a dictionary, then we are
               able to assign a unique number with each of those words
               (e.g. the position in the dictionary). Now we may
               simply count for each document x_{i} the number of times
               a given word j is occurring. This is then used as the
               value of the j-th coordinate of x_{i}."

          1. Naive Bayes

             use Bayes Rule with approximations

             - replace p(x) by likelihood ratio (adjusted)

             - replace p(x|y) by independent product of parts (spam: words)

          2. Nearest Neighbor Estimators

             - distance to (k-) nearest point(s).

             - whichever class is closest

             - distance-dependent

             - works well if work was put into distance

          3. A Simple Classifier

             1. use means of classes

             2. compute distance of point to means

             3. extension: line boundary \to geometric boundary

          4. Perceptron

             - online learning

             - "data points x t ∈ R d , and labels y t ∈ {±1}"

               - if wrong, updates w by x_{t}y_{t}

               - kernelizeable: x replaceable by \phi(x)

             - converges if convergeable

             - margin describes how thin the border is

             - novikoff's theorem gives perceptron's accuracy bounded
               by margin (+ data set diameter)

             - proof by two limits per step, induction, cauchy-schwartz

             - perceptron was used in (a)nn

             - here, more complex feature maps and "associated kernel"s

          5. K-Means

             - cluster

             - start with k /prototype vector/s,

             - approximate classes

             - approximate vectors

             - repeat

    2. Density Estimation

       1. Limit Theorems

          0. [@0] Casino

             - example: 100 die (1w6) tosses at casino yields only 11 6es
               (instead of 17, as expected)

             - (^{m}_{n}) = m! / n!(m-n)! different sequences with
               proportions n="6" and m="not 6" respectively

             - only 11 '6'es for 100 die tosses: \sim 7.0%

               probability[X \le 11] = \sum_{i=0}^{11} (^{100}_{i}) (1/6)^{i}  (5/6)^{100-i}

          1. Fundamental Laws

             - weak law: irgendwann ist summe durch \epsilon beschraenkt

             - strong law: wkt der konvergenz ist 1

             - central limit theorem: id vars converge to (var-sth)
               normal distribution (all - \mu)

             - slutsky: continuous functions can be pulled into E

             - delta method: see later

          2. The Characteristic Function

             - Fourier transform

             - Characteristic functions

             - Sum of random variables and convolutions

               - Proof via Y=z-x, joint distribution p(z, x)

             - proofs of 1)

             - moments \to function

          3. Tail Bounds

             - Gauss inequality: proof via

               - 1 \le x/\epsilon (auf [\epsilon, \infty))

               - 1/\epsilon herausziehen, auf [0, \infty) erweitern

               - auf dem Intervall ist x durschnittlich kleiner \mu

          4. An Example

             - assume that all production steps are iid, have the same \mu_{B}

             - X_{i} for wafer i

             - X̄_{m} the average of the yields of m wafers using process
               ’B’

             - interested in is the accuracy \epsilon for which the
               probability

               δ = Pr(| X̄_{m} − μ_{B} | > \epsilon) satisfies δ ≤ 0.05.

             - solve

               - chebyshev: 40000

               - hoeffding: 738

               - central limit theorem: 55

       2. Parzen Windows

          1. Discrete Density Estimation

             estimate by actual occurrences in sample data

             problem: some numbers might not appear

             solution: laplace smoothing

          2. Smoothing Kernel

             - "proper density estimation."

             - parzen windows estimate (see quotes)

             - different kernels

          3. Parameter Estimation

             - width of kernel function is important, shape less

             - to maximize likelihood, sum over log-likelihood to
               avoid float overflow

             - maximizing likelihood on all of data leads to
               overfitting

             - avoid by using a Validation Set, n-fold
               Cross-validation or a Leave-one-out Estimator

          4. Silverman’s Rule

             - locality-dependent kernel width

               r_{i} = c/k \sum_{x \in kNN(x_i)} ||x - x_{i}||

             - locality-dependent parzen rule

               p\circ(x) = 1/m \sum_{i=1}^{m} r^{-d}h(x - x_{i}/r_{i})

          5. Watson-Nadaraya Estimator

             - non-parametric data

       3. Exponential Families

          0. [@0]

             - "Gaussians, Poisson, Gamma and Wishart distributions all
               form part of the exponential family"

             - "lead to convex optimization problems"

          1. Basics

             - Defined by

               p(x; θ) := p_{0}(x) exp (<φ(x), θ> − g(θ)) . (2.40)

             - with g(\theta) weighting term to get density function

             - \varphi map from parameters to statistics

          2. Examples

             Many distributions can be seen as member of the exponential family

             - Gaussian

             - Multinomial

             - Poisson

             - Beta

    3. Optimization

       3. [@3] Constrained Optimization

          0. [@0]

             - minimize J(w)

               - given constraints c_i(w) \le 0 or e_j(w) = 0 for some i,j

             - normal solution to find w with ||\nablaJ(w)||=0 does
               ignore these

          1. Projection Based Methods

             find projection to constraint set (for simple constraint
             set)

          2. Lagrange Duality

             - add weighted sums to [[(iml-3.101)]] for constraints, see
               [[(iml-3.107)]]

             - continue at p.128/136

    4. Online Learning and Boosting

       Not all data available at start, new is added

       2. [@2] Weighted Majority

          - no consistent expert

          - aim: "minimize regret"

    6. [@6] Kernels and Function Spaces

       all algorithms which use dot product =<x, x'>= can be
       made to use a kernel function =k(x, x')=

       1. The Basics

          0. [@0]

             - let *X* be the input space,

             - $k: X \times X \to \mathbb{R}$ function with

               k(x, x') = <\Phi(x), \Phi(x')>

               with \Phi being a /feature map/ from X to dot product
               space \cal H.

             - Advantages

               1. hyperplane suffices to characterize in \cal H

               2. no need to explicitly compute \Phi

               3. input space can be any set (strings, trees, graphs, etc)

          1. Examples

             1. Linear Kernel

                - k(x, x') = <x, x'> = \sum_{i} x_{i}x_{i}'

                - simplest, easy to compute: O(n) or less for sparse vectors

             2. Polynomial Kernel

                - easy to compute, as k(x,x') = <\Phi(x), \Phi(x')> = (<x, x'>)^d

    7. Linear Models

       0. [@0]

          - hyperplane [[(iml-7.1)]] divides space H in two halves

            - decision boundary

            - maximize margin [[(iml-7.2)]]

       1. Support Vector Classification

          0. [@0]

             - given point-class tuples {(x1, y1), ..., (xm, ym)} with xi
               \in H, yi \in {-1, 1}

             - find linear boundary such that <w, xi> + b \ge 0 if yi == 1,
               else \le (iff y_i(<w, xi> +b) \ge 1)

               - maximizing the margin

                 - equivalent to minimize: 1/2 ||w||^2

             - hard margin: data linearly separable, else

             - soft margin: y_i(<w, xi> +b) \ge 1 - \xi_i

               - see [[(iml-7.5)]]

             - lagrangian form to accomodate constraints

               - solve minimization for that

          1. A Regularized Risk Minimization Viewpoint

             - possible to remove \xi from optimization problem, see [[(iml-7.12)]]

             - SVC as regularized risk minimizer with binary hinge loss

             - compute

               - primal: d-dimensional problem (d dimension of x)

               - (Fenchel) dual: m-dimensional problem

                 - use at kernel trick or when d >> m

          2. An Exponential Family Interpretation
*** TODO quotes
    1. Introduction
       1. A Taste of Machine Learning
          0. [@0]
             - much of the art of machine learning is to reduce a range of
               fairly disparate problems to a set of fairly narrow
               prototypes.
          1. Applications
             - A rather related application is collaborative
               filtering. Internet book stores such as Amazon, or video rental
               sites such as Netflix use this information extensively to entice
               users to purchase additional goods (or rent more movies).
             - Many security applications, e.g. for access control, use face
               recognition as one of its components.
             - The overarching theme of learning problems is that there exists
               a nontrivial dependence between some observations, which we will
               commonly refer to as x and a desired response, which we refer to
               as y, for which a simple set of deterministic rules is not
               known. By using learning we can infer such a dependency between
               x and y in a systematic fashion.
          2. Data
             - One of the challenges in dealing with vectors is that the scales
               and units of different coordinates may vary widely.
             - One way of dealing with those issues in an automatic fashion is
               to normalize the data. We will discuss means of doing so in an
               automatic fashion.
             - In some cases the vectors we obtain may contain a variable
               number of features.
          3. Problems
             - 3-class classification. Note that in the latter case we have
               much more degree for ambiguity.
             - a sequence of (x_{i} , y_{i} ) pairs for which y i needs to be
               estimated in an instantaneous online fashion. This is commonly
               referred to as online learning.
             - know X 0 already at the time of constructing the model. This is
               commonly referred to as transduction.
       2. Probability Theory
          0. [@0]
             - For more details and a very gentle and detailed discussion see
               the excellent book of [BT03].
          1. Random Variables
             - Formally, [...] 1 occurs with probability 1/6
             - notational convention [...] use uppercase letters, e.g., X, Y
               etc to denote random variables and lower case letters, e.g., x,
               y etc to denote the values they take.
          2. Distributions
             - If the random variable is discrete, i.e., it takes on a finite
               number of values, then this assignment of probabilities is
               called a /probability mass function/ or PMF for short.
             - slightly informal notation p(x) := Pr(X = x)
             - continuous random variable the assignment of probabilities
               results in a probability density function or PDF for short.
             - Closely associated with a PDF is the indefinite integral
               over p. It is commonly referred to as the cumulative
               distribution function (CDF).
          3. Mean and Variance
             - if f: R → R is a function, then f(X) is also a random
               variable. Its mean is mean given by
               E[f(X)] := \int f(x)dp(x).
          4. Marginalization, Independence, Conditioning, and Bayes
             Rule
             0. [@0]
                - /joint density/ p(x, y).
                - /marginalization/: recover p(x) by integrating out y:
                  p(x) = \int dp(x, y).
                - /X and Y are independent/: p(x,y) = p(x) p(y)
                - /iid random variables/: independently and identically distributed
                - /conditional probabilities/:
                  p(x|y) := p(x, y) / p(y)
                - /Bayes Rule/: p(y|x) = p(x|y)p(y) / p(x) (ref:bayes_rule)
             1. An Example
                - The physician recommends a test which is guaranteed to detect
                  HIV-positive whenever a patient is infected.
                - On the other hand, for healthy patients it has a 1% error
                  rate. That is, with probability 0.01 it diagnoses a patient as
                  HIV-positive even when he is, in fact, HIV-negative.
                - Moreover, assume that 0.15% of the population is infected.
                - Denote by X and T the random variables associated with the
                  health status of the patient and the outcome of the test
                  respectively.
                - We are interested in p(X = HIV+|T = HIV+).
                - Note that often our tests may not be conditionally independent
                  and we would need to take this into account.
                - p(X = HIV+|T = HIV+) = p(T = HIV+|X = HIV+) p(X = HIV+) / p(T = HIV+)
                - p(T = HIV+) = \sum p(T = HIV+, x) = \sum p(T = HIV+|x)p(x)
                - The corresponding expression yields:

                  p(T = HIV+|X = HIV+, A)p(X = HIV+|A) / p(T = HIV+|A)

                  Here we simply conditioned all random variables on A in order to
                  take additional information into account.
                - What we want is that the diagnosis of T_{2} is independent of that
                  of T_{2} given the health status X of the patient. This is
                  expressed as
                  #+BEGIN_SRC latex
                    p(t_{1} , t_{2} |x) = p(t_{1} |x)p(t_{2} |x). (ref:iml-1.16)
                  #+END_SRC
                - Random variables satisfying the condition [[(iml-1.16)]] are
                  commonly referred to as /conditionally independent/.
       3. Basic Algorithms
          0. [@0]
             - assume that there is sufficiently strong dependence between x
               and y that we will be able to estimate y given x and a set of
               labeled instances X, Y.
          1. Naive Bayes
             - use [[(bayes_rule)]] to estimate p(y), f.ex. the
               probability of receiving a spam or ham mail.
             - dispose of the requirement of knowing p(x) by settling for a
               likelihood ratio

               L(x) := p(spam|x) / p(ham|x) = p(x|spam)p(spam) /
               p(x|ham)p(ham)
             - treat the occurrence of each word in a document as a separate
               test and combine the outcomes in a naive fashion by assuming
               that

               p(x|y) = \Pi_{j=1}^{# of words in x} p(w^{j} |y)
             - p(w|y) can be obtained, for instance, by simply
               counting the frequency occurrence of the word within
               documents of a given class.
             - since we are computing a product over a large number
               of factors the numbers might lead to numerical
               overflow or underflow. This can be addressed by
               summing over the logarithm of terms rather than
               computing products.
             - need to address the issue of estimating p(w|y) for
               words w which we might not have seen before. One way
               of dealing with this is to increment all counts
               by 1.
             - perform surprisingly well
          2. Nearest Neighbor Estimators
             - Note that nearest neighbor algorithms can yield
               excellent performance when used with a good distance
               measure.
             - lemma by Johnson and Lindenstrauss [DG03] asserts
               that a set of m points in high dimensional Euclidean
               space can be projected into a O(log m/\epsilon 2 )
               dimensional Euclidean space such that the distance
               between any two points changes only by a factor of
               (1 ± \epsilon). [...] The surprising fact is that the
               projection relies on a simple randomized algorithm.
          3. A Simple Classifier
             - In general, we may pick *arbitrary maps* φ : X → H mapping the
               space of observations into a feature space H, as long as the
               latter is endowed with a dot product (see Figure 1.21). This
               means that instead of dealing with <x, x'> we will be dealing
               with <φ(x), φ(x')>.
          4. Perceptron
             - a user might want to have instant results whenever a new e-mail
               arrives and he would like the system to learn immediately from
               any corrections to mistakes the system makes.
             - the Perceptron is a linear classifier
             - if ŷ_{t} \neq y_{t} the weight vector is updated as
               w ← w + y_{t} x_{t} and b ← b + y_{t}
             - If the dataset (X, Y) is linearly separable, then the Perceptron
               algorithm eventually converges and correctly classifies all the
               points in X.
             - Definition 1.6 (*Margin*) Let w ∈ R d be a weight vector and let
               b ∈ R be an offset. The margin of an observation x ∈ R d with
               associated label y is

               \gamma(x, y) := y (<w, x> + b) (1.27)

               Moreover, the margin of an entire set of observations X with
               labels Y is

               γ(X, Y) := min_{i} γ(x_{i} , y_{i}) (1.28)
             - Geometrically speaking (see Figure 1.22) the margin measures the
               distance of x from the hyperplane defined by {x| <w, x> + b = 0}.
             - Theorem 1.7 (*Novikoff’s theorem*) Let (X, Y) be a dataset with
               at least one example labeled +1 and one example labeled −1. Let
               R := max_{t} ||x_{t}||, and assume that there exists (w^{∗}, b^{∗}) such
               that ||w^{∗}|| = 1 and γ_{t} := y_{t} (<w^{∗}, x_{t}>) + b^{∗} ) ≥ γ for
               all t. Then, the Perceptron will make at most (1+R^{2})(1+(b^{*})^2)/γ^{2}
               mistakes.
             - a similar bound can be shown for Support Vector Machines [Vap95]
          5. K-Means
             - define prototype vectors μ_{1} , ... , μ_{k} and an indicator vector
               r_{ij} which is 1 if, and only if, x_{i} is assigned to cluster j. To
               cluster our dataset we will minimize the following *distortion
               measure*, which minimizes the distance of each point from the
               prototype vector:

               J(r, μ) := 1/2 \sum_{i=1}^{m} \sum_{j=1}^{k} r_{ij} ||x_{i} - \mu_{j}||^{2}

               where r = {r_{ij}}, μ = {μ_{j}}
             - adapt a two stage strategy
               - Stage 1 Keep the μ [prototype vectors] fixed and determine r.[...]
                 The solution for the i-th data point x_{i} can be found by setting:

                 r_{ij} = 1 if j = argmin_{j'} ||x_{i} − μ_{j'}||^{2} (1.30)

                 and 0 otherwise.
               - Stage 2 Keep the r fixed and determine μ. [...]  [J] can be
                 minimized by setting the derivative with respect to μ_{j} to be
                 0:

                 \sum_{j=1}^{m} r_{ij} (x_{i} - \mu_{j}) = 0 for all j. (1.31)

                 Rearranging obtains \mu_{j} = \sum_{i} r_{ij}x_{i} / \sum_{i} r_{ij} (1.32)

                 Since \sum_{i} r_{ij} counts the number of points assigned to cluster
                 j, we are essentially setting μ_{j} to be the sample mean of the
                 points assigned to cluster j.
               - Two issues [...]
                 1. sensitive to the choice of the initial cluster centers μ.
                 2. hard assignment of every point to a cluster center.
    2. Density Estimation
       1. Limit Theorems
          0. [@0]
             - Before crying foul you decide that some mathematical analysis
               is in order.
             - limit theorems [...] tell us by how much averages over a set
               of observations may deviate from the corresponding
               expectations and how many observations we need to draw to
               estimate a number of probabilities reliably.
          1. Fundamental Laws
             - Theorem 2.1 (Weak Law of Large Numbers) Denote by X_{1} , ... ,
               X_{m} random variables drawn from p(x) with mean μ = E_{X_{i}} [x_{i}]
               for all i. Moreover let

               X̄_{m} := 1/m \sum_{i=1}^{m} X_{i} (2.2)

               be the empirical average over the random variables X_{i} . Then
               for any \epsilon > 0 the following holds

               lim_{m \to \infty} Pr (|X̄_{m} − μ| ≤ \epsilon = 1). (2.3)
             - Theorem 2.2 (Strong Law of Large Numbers) Under the conditions
               of Theorem 2.1 we have Pr(lim_{m→∞} X̄_{m} = μ) = 1.
             - Theorem 2.3 (Central Limit Theorem) Denote by X_{i} independent
               random variables with means μ_{i} and standard deviation σ_{i}. Then

               Z_{m} := m [ \sum_{i=1}^{m} \sigma_{i}^{2} ]^{- 1/2} [ \sum_{i=1}^{m} X_{}_{i} - \mu_{i}^{} ] (2.4)

               converges to a Normal Distribution with zero mean and unit
               variance.
             - Theorem 2.4 (Slutsky’s Theorem) Denote by X_{i} , Y_{i} sequences of
               random variables with X_{i} → X and Y_{i} → c for c ∈ R in
               probability. Moreover, denote by g(x, y) a function which is
               continuous for all (x, c). In this case the random variable
               g(X_{i}, Y_{i}) converges in probability to g(X, c).
             - Theorem 2.5 (Delta Method) Assume that X_{n} ∈ R^{d} is
               asymptotically normal with a_{n}^{-2}(X_{n} - b) \to N(0, \Sigma) for a_{n}
               → 0. Moreover, assume that g : R^{d} → R^{l} is a mapping which is
               continuously differentiable at b. In this case the random
               variable g(X_{n}) converges

               a_{n}^{-2}(g(X_{n}) - g(b)) \to N(0, [∇_{x} g(b)]Σ[∇_{x} g(b)]^{T}) (2.5)
          2. The Characteristic Function
             - The Fourier transform plays a crucial role [...] in
               statistics. For historic reasons its applications to
               distributions is called *the characteristic function*.
             - Definition 2.6 (*Fourier Transform*) Denote by f : R^{n} →
               C a function defined on a d-dimensional Euclidean
               space. Moreover, let x, ω ∈ R^{n} . Then the Fourier
               transform F and its inverse F^{-1} are given by

               F [f](ω) := (2π)^{−d/2} \int_{R^{n}} f(x) exp(-i<\omega, x>)dx (2.7)

               F^{-1} [g](ω) := (2π)^{−d/2} \int_{R^{n}} g(\omega) exp(i<\omega, x>)d\omega (2.8)
             - Definition 2.7 (*Characteristic Function*) Denote by
               p(x) a distribution of a random variable X ∈ R^{d}. Then
               the characteristic function φ_{X} (ω) with ω ∈ R^{d} is given
               by

               φ_{X} (ω) := (2π)^{d/2} F^{−1} [p(x)] = \int exp(i <ω, x>)dp(x). (2.9)
             - Theorem 2.8 (*Sums of random variables and convolutions*)
               Denote by X, Y ∈ R two independent random variables. Moreover,
               denote by Z := X + Y the sum of both random variables. Then
               the distribution over Z satisfies p(z) = p(x) ◦
               p(y). Moreover, the characteristic function yields:

               φ_{Z} (ω) = φ_{X}(ω)φ_{Y}(ω). (2.10)
             - ∇^{n}_{ω} φ_{X}(0) = i^{−n} E_{X} [x^{n}]. (2.12)

             - if we know the moments of a distribution we are able to
               reconstruct it directly since it allows us to reconstruct its
               characteristic function.
          3. Tail Bounds
             - Theorem 2.9 (*Gauss-Markov*) Denote by X ≥ 0 a random variable
               and let μ be its mean. Then for any \epsilon > 0 we have

               Pr(X ≥ \epsilon) ≤ \mu / \epsilon (2.13).

               This means that for random variables with a small mean, the
               proportion of samples with large value has to be small.
             - Theorem 2.10 (*Chebyshev*) Denote by X a random variable with
               mean μ and variance σ^{2} . Then the following holds for \epsilon > 0:

               Pr(|x − μ| ≥ \epsilon) \le \sigma^{2} / \epsilon^{2} (2.14)
             - Theorem 2.12 (*Hoeffding*) Denote by X_{i} iid random variables
               with bounded range X_{i} ∈ [a, b] and mean μ. Let X̄_{m} := m^{−1}
               \sum_{i=1}^{m} X_{i} be their average. Then the following bound holds:

               Pr(|X̄_{m} − μ| > \epsilon) ≤ 2 exp(-2m\epsilon^{2} / (b-a)^{2}) (2.15)
       2. Parzen Windows
          1. Discrete Density Estimation
             - The convergence theorems discussed so far mean that we can use
               empirical observations for the purpose of density estimation.
             - Example 2.4 (Curse of Dimensionality) Assume that X = {0, 1} d
               , i.e. x consists of binary bit vectors of
               dimensionality d. As d increases the size of X increases
               exponentially, requiring an exponential number of observations
               to perform density estimation. For instance, if we work with
               images, a 100 × 100 black and white picture would require in
               the order of 10^{3010} observations to model such fairly
               low-resolution images accurately. This is clearly utterly
               infeasible — the number of particles in the known universe is
               in the order of 10^{80} . Bellman [Bel61] was one of the first to
               formalize this dilemma by coining the term ’curse of
               dimensionality’.
             - (ends 2.2.1)
          2. Smoothing Kernel
             - p\circ(x) = 1/m \sum_{i=1}^{m} r^{-d}h(x - x_{i}/r)

               This expansion is commonly known as the /Parzen
               windows/ estimate.
             - h(x) = (2π)^{−1/2} e^{−1/2 x^2} /Gaussian kernel/
             - h(x) = 3/4 max(0, 1 − x^2) /Epanechnikov kernel/
             - For practical purposes the Gaussian kernel (2.28) or the
               Epanechnikov kernel (2.30) are most suitable.
             - /compact support/ [...] for any given density estimate at
               location x we will only need to evaluate terms h(x_{i} − x) for
               which the distance ||x_{i} − x|| is less than r.
          3. Parameter Estimation
             - choose r such that the /log-likelihood of the data/ is
               maximized. It is given by

               log \Pi_{j=0}^m p(x_{j}) = −m log m + \sum_{j=1}^m log (m \cdot p\circ(x_{j}))
          5. [@5] Watson-Nadaraya Estimator
             - Provided that we are able to compute density estimates
               p(x) given a set of observations X we could appeal to
               Bayes rule to obtain
               #+BEGIN_SRC latex
                 p(y|x) (ref:iml-2.38)
                 = p(x|y)p(y)/p(x)
                 = ((m_{y} / m) \cdot 1/m_{y} \sum_{i:y_i=y} r^{-d} h(x_{i}-x/r) / 1/m \sum_{i=1}^{m} r^{-d} h(x_{i}-x/r))
               #+END_SRC
             - For binary classification (2.38) can be simplified
               [...] the function

               f (x)
               = \sum_{i} y_{i} h(x_{i}-x/r) / \sum_{i} h(x_{i}-x/r)
               = \sum_{i} y_{i} (h(x_{i}-x/r) / \sum_{i} h(x_{i}-x/r))
               =: \sum_{i} y_{i} w_{i}(x) (2.39)

               can be used to achieve the same goal since

               f (x) > 0 ⇐⇒ p(y = 1|x) > 0.5
       3. Exponential Families
          2. [@2] Examples
             - The reason why many discrete processes follow the
               Poisson distribution is that it can be seen as the
               limit over the average of a large number of Bernoulli
               draws
    3. Optimization
       3. [@3] Constrained Optimization
          2. [@2] Lagrange Duality
             - Many machine learning problems involve constraints, and
               can often be written in the following canonical form:
               #+BEGIN_SRC latex
                 (ref:iml-3.101)
                 min_w J(w)                      (ref:iml-3.101a)
                 s.t. c_i(w) \le 0 for i \in I       (ref:iml-3.101b)
                      e_i(w) = 0 for i \in E       (ref:iml-3.101c)
               #+END_SRC
             - Lagrangian
               #+BEGIN_SRC latex
                 L(w, α, β) = J(w) + \Sigma_{i \in I} α_i c_i (w) + \Sigma_{j \in E} β_i e_i (w)
                 (ref:iml-3.107)
               #+END_SRC
               for \alpha_i \ge 0, \beta_i \in \mathbb R. [...] refer to \alpha
               (respectively \beta) as the Lagrange multipliers associated
               with the inequality (respectively equality) constraints
             - call \alpha and \beta dual feasible if and only if \alpha_i \ge 0 and
               \beta_i \in \mathbb R
             - The Lagrangian [[(iml-3.107)]] of [[(iml-3.101)]] satisfies

               max_{\alpha \ge 0, \beta) L(w, \alpha, \beta)
               = {J(w) if w is feasible,
                  \infty    else

               In particular, if J∗ denotes the optimal value of
               [[(iml-3.101)]], then

               J∗ = min_{w} max_{\alpha>0, \beta} L(w, α, β).
             - Lagrange dual function
               #+BEGIN_SRC latex
               D(α, β) = min_{w} L(w, α, β)           (ref:iml-3.109)
               #+END_SRC
    6. [@6] Kernels and Function Spaces
       - the design of a good kernel underlies the success of machine
         learning in many applications.
    7. Linear Models
       0. [@0]
          - A *hyperplane* in a space H endowed with a dot product <·, ·>
            is described by the set
            #+BEGIN_SRC latex
              {x ∈ H | <w, x> + b = 0} (w \in H, b \in \mathbb R) (ref:iml-7.1)
            #+END_SRC
            where w ∈ H and b ∈ R.
          - Given a set X = {x_1, ..., x_m}, the *margin* is the distance
            of the closest point in X to the hyperplane [[(iml-7.1)]].
            #+BEGIN_SRC latex
              min_{i=1,...,m} \frac{|<w, x_i> + b|}{\|w\|}. (ref:iml-7.2)
            #+END_SRC
       1. Support Vector Classification
          0. [@0]
             - penalize large ξ_i [...] via the following modified
               optimization problem:
               #+BEGIN_SRC latex
                 min_{w, b, \xi} 1/2 ||w||^2 + C/m \sum_{i=1..m} \xi_i
                 s.t. y_i(<w, x_i>+b) \ge 1 - \xi_i for all i
                      \xi_i \ge 0                               (ref:iml-7.5)
               #+END_SRC
          1. A Regularized Risk Minimization Viewpoint
             - eliminate \xi_i from [[(iml-7.5)]], and write it as the
               following unconstrained optimization problem:
               #+BEGIN_SRC latex
                 min_{w,b} 1/2 ||w||^2 + C/m \sum_{1=1..m} max(0, 1−y_i (<w, x_i> + b)) (ref:iml-7.12)
               #+END_SRC

    100) [@100] Appendix
         3. [@3] Loss Functions
            1. Loss Functions
               - a contingency table as follows:
                 |--------+------+------|
                 |        | y> 0 | y< 0 |
                 |--------+------+------|
                 | y'> 0  | T +  | F +  |
                 | y' < 0 | F −  | T −  |
                 |--------+------+------|
                 In this table, we denote by m_{+ } = T_{+ }+ F_{}_{+} and m_{- } =
                 T_{- }- F_{}_{-} the numbers of positives and negative labels
                 in y, respectively.
*** ref [[file:master.bib::iml][Smola & Vishwanathan 2008: Introduction Machine Learning]
** TODO [#B] [[./10.1.1.20.2080.bloom_orig.pdf][Bloom - Space/Time Trade-offs in Hash Coding with Allowable Errors]]
*** summary
    0. [@0] Abstract

       - set membership hash-coding

       - reduce amount of space by allowing some errors

    1. Introduction

       - new hash-coding method

         - "suggested for applications in which the great majority of
           messages to be tested will not belong to the given set"

       - reduce space by allowing error rate

       - extension: might allow problems to be hash-coded in main
         memory where previously space was lacking

    2. A Sample Application

       - hyphenation

         - easy rules in 90% of cases

         - 10% need dictionary lookup

       - dictionary too large to fit in core

         - expensive to look up

       - hash fits in core with some allowable errors

         - error: says lookup is needed when it is not

    3. A Conventional Hash-Coding Method

       - h fields of b+1 bits

         - hash has b bits

         - 1 bit to check whether cell is empty

       - hash message, create /hash address/ \le b depending on message

         - if its cell is empty, store hash

         - else, generate next hash address somehow, until empty cell found

       - test message: hash, go through cells til hash matches (in) or
         empty (out)

    4. Two Hash-Coding Methods with Allowable Errors

       - version 1: as error-free, but more smaller cells

         - message transformed to code, codes may overlap

         - as before, first bit == 1 signifies cell is in use

       - version 2 (current bloom method): bit field, set bits of hash to 1

         - half the bits 1: problematic to add new ones

    5. Computational Factors

       - error rate: (accepted - member) / (total - member)

       - Space: number of bits

       - Time: how many single bits to access (simplification)

    6. Analysis of the Conventional Hash-Coding Method
*** quotes
    4. [@4] Two Hash-Coding Methods with Allowable Errors
       - The increased a priori likelihood of each bit accessed being
         a 1 outweighs the effect of adding the additional bit to be
         tested when half the bits in the hash field are 1 and half
         are 0, as will be shown later in this paper.
    5. [@6] asd
       - The hash area has N bits and is organized into h cells of b +
         1 bits each, of which n cells are filled with the n messages
         in M. Let \varphi represent the fraction of cells which are
         empty. Then
         #+BEGIN_SRC latex
           \varphi = {h \over n}
         #+END_SRC
*** ref [[file:master.bib::Bloom70space/timetrade-offs][Bloom 1970: Space Time Trade]]
** TODO [#B] PART -s 1,5,7 [[./McGrawHill_-_Machine_Learning_-Tom_Mitchell.pdf][Mitchell - Machine Learning]]                   :ML:
*** [#B] sec 8.2: KNN
    aka read up knn links (mitchell)
*** summary
    1. INTRODUCTION

       0. [@0]

          - programming computers to learn

       1. WELL-POSED LEARNING PROBLEMS

          - definition: see quotes

          - fits many examples from simple (db query optimizer) to
            complex (robot car driver, checkers play algorithm)

       2. DESIGNING A LEARNING SYSTEM

          0. [@0]

             - checkers

               - performance measure: percent of games won in world tournament

          1. Choosing the Training Experience

             - direct feedback vs indirect feedback

               - best moves, vs

               - sequence of game moves

             - learner control over learning sequence

               - teacher selects, vs learner asks, vs learner explores

             - examples vs reality

               - training set complete enough?

             Decision: plays against itself (+: no trainer, as much
             data as time permits)

             - Example
               A checkers learning problem:
               - Task T: playing checkers
               - Performance measure P: percent of games won in the world tournament
               - Training experience E: games played against itself
               In order to complete the design of the learning system, we must now choose
               1. the exact type of knowledge to be,learned
               2. a representation for this target knowledge
               3. a learning mechanism
          2. Choosing the Target Function

             - 1st idea: maps state to next move

             - 2nd idea (better to compute): maps board state B to real number

               - won\to 100, lost \to -100, tie \to 0

               - compute back from every state, assuming optimal
                 strategy of both

               - nonoperational: hard to compute

               - operational approximation V^{\tilde} : B \to {real numbers}

          3. Choosing a Representation for the Target Function

             - possibilities: table with values, neural network,
               polynomial based on board state, ...

             - here, simple choice: linear function with weights

               d(x) = w0 + w1 x1 + ... + w6 x6

               with weights as number of b/w pieces/kings/threatened by other

          4. Choosing a Function Approximation Algorithm

             training data, f.ex ((0, 1, 0, 0, 0, 0), -100) (black won)

             1. Estimating training values

                - simple approach: assign value of successor (next
                  state that can be played by this side)

                  #+BEGIN_EXAMPLE
                  V_train(b) = V^{\tilde}(successor(b)) (ref:1.1)
                  #+END_EXAMPLE

             2. Estimating the Weights

                - minimize the squared error \sum (V_train - V^{\tilde})^2

                - method: least mean squares, for each instance (b, V_train(b))

                  w_i = w_i + \eta (V_train(b) -v^{\tilde}(b)) x_i

          5. The Final Design

             four parts

             1. Performance System: must solve task (f.ex. playing checkers)

                - input: new problem (new game)

                - output: solution (game trace)

             2. Critic:

                - input solution (game trace)

                - output: training examples of target function (similar to [[(1.1)]])

             3. Generalizer

                - input: training examples

                - output: estimate of target function (f.ex. via LMS)

             4. Experiment Generator: practice problems that maximize learning

                - input: hypothesis/target function

                - output: game start state

             summary

             - for good performance the solution should be modelable
               similar to the linear approach

             - and depend mostly on the six features used for learning

       3. PERSPECTIVES AND ISSUES IN MACHINE LEARNING

          learning as searching the hypothesis space

          1. Issues in Machine Learning

             - which algorithms to use?

             - "how much training data is sufficient"?

             - how much can prior knowledge help? (perfect and approximate)

             - how to choose next training experience?

             - what functions to learn? (automatable?)

             - automatically change representation to better
               learn target ufnction?

       4. HOW TO READ THIS BOOK

          - any sequence

          - best first read chapters 1 and 2

          - (interesting to me)

            2. [@2] inductive bias, ordering of hypotheses

            5. [@5] estimation theory, central limit theorem

            6. bayesian

            7. weighted majority: combine multiple methods

       5. SUMMARY AND FURTHER READING

          - ml useful in

            - data mining (large dbs)

            - domains poorly understood by humans (face recognition)

            - adapt to changing conditions (stock, "adapt to changing reading
              interests of individuals")

          - many domains

          - well-defined: task, performance metric, source of training
            experience

          - learning as searching the hypothesis space

    2. CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING

       concept learning: "acquiring the definition of a general
       category given a sample of positive and negative training
       examples of the category."

       search a predifined set of hypotheses for those that fit best

       1. INTRODUCTION

          - "Concept learning. Inferring a boolean-valued function from
            training examples of its input and output."

       2. A CONCEPT LEARNING TASK

          0. [@0]

             - enjoysports: given sky, airtemp, humidity, wind, water,
               forecast, predict if aldo enjoys water sports that day

             - simple representation: conjuction of constraints

          1. Notation

             - set of Instances X

             - target concept c: X \to {0, 1}

             - training example (x \in X, c(x))

               - positive if c(x) = 1, negative if c(x) = 0

             - hypothesis h \in H, h: X \to {0, 1}

               - aim: h = c for x \in X

          2. The Inductive Learning Hypothesis

             "Any hypothesis found to approximate the target function
             well over a sufficiently large set of training examples
             will also approximate the target function well over other
             unobserved examples." (?stupid?)

       3. CONCEPT LEARNING AS SEARCH

          0. [@0]

             - distinct instances: possible combinations

             - syntactically different instances: possible
               combinations with \emptyset and ? on each place

             - semantically different instances: \emptyset always classifies
               as false, thus 1+ possible combinations with ? on each place

          1. General-to-Specific Ordering of Hypotheses

             - order hypotheses by moregeneral-than-or-equal-to

               - superset of hypotheses (see quotes)

               - is a partial ordering

               - algorithm based on this (see quotes)

       4. FIND-S: FINDING A MAXIMALLY SPECIFIC HYPOTHESIS

          - start with (\empty, ..., \empty)

          - first example as new hypothesis

          - generalises to h \to (Sunny, Warm, ?, Strong, Warm, Same)

          - ignores all negative examples as they should not match

            - assumption: H contains correct hypothesis and no
              contradictions

          - gives (one) most specific hypothesis that fits all

       5. VERSION SPACES AND THE CANDIDATE-ELIMINATION ALGORITHM

          0. [@0]

             - candidate-elimination gives all fitting results

               - as with find-s: problem with noisy data

          1. Representation

             - consistent hypothesis: fits all examples (def see quotes)

             - version space: all hypotheses in H consistent with
               training examples D (def see quotes)

          2. The LIST-THEN-ELIMINATE Algorithm

             - list all, eliminate non-fitting

             - guaranteed to terminate (on finite H), simple

             - takes too long

          3. A More Compact Representation for Version Spaces

             - save only the most general and most specific consistent
               hypotheses (see defs quotes, and
               moregeneral-than-or-equal-to)

             - all instances in between are equal to the version space
               (theorem see quotes)

          4. CANDIDATE-ELIMINATION Learning Algorithm

       7. [@7] INDUCTIVE BIAS

          1. A Biased Hypothesis Space

             hypothesis space might be limited in advance, precluding
             perfect solution

          2. An Unbiased Learner

             - if the full hypothesis space is allowed, it gets very big

             - ...

    5. [@5] EVALUATING HYPOTHESES

       0. [@0]

          - dealing with limited training data

            - generalize accuracy to all data

            - one outperforming other on training, does it also
              outperform on whole data

            - how to best learn hypothesis and estimate its accuracy

       1. MOTIVATION

          problems with limited data

          - bias in the estimate: tends to some non-optimal solution,
            especially for a "very rich hypothesis space,"

          - variance in the estimate: even if unbiased, the less data,
            the higher the variance of results

       2. ESTIMATING HYPOTHESIS ACCURACY

          0. [@0]

             - setting: n elements drawn with probability given by D from
               population X

             - estimate accuracy of hypothesis h given accuracy over n elements

             - how correct is this estimate?

          1. Sample Error and True Error

             - sample error: misclassification rate on sample (see quotes)

             - true error: misclassification rate on whole
               distribution (see quotes)

             - how good an approximation is the sample error?

          2. Confidence Intervals for Discrete-Valued Hypotheses

             - sample error and number of samples estimate true error
               (see quotes)

             - this works well if error not to close to 0 or 1 and n
               big enough (see quotes)

       3. BASICS OF SAMPLING THEORY

          1. Error Estimation and Estimating Binomial Proportions

             - measure sample error \to experiment with random outcome

             - follows Binomial distribution

               - for np(1-p) \ge 5, you can use normal distribution

          2. The Binomial Distribution

             - weird coin with probability p for heads.

               - How many heads r after n tosses

               - r/n estimates p

             - corresponds to estimation of error:

               - p to error_{D}(h)

               - r/n to error_{S}(h)

             - general: base experiment with binary outcome repeated n times

          3. Mean and Variance

             - mean

             - variance, standard distribution

          4. Estimators, Bias, and Variance

             - error_{S}(h) is an estimator for error_{D}(h): random
               variable to estimate "underlying population"

             - estimation bias is how much an estimator is wrong on
               average

               - with bias = 0, it is called an /unbiased estimator/ for p

               - hypothesis h and sample S must be chosen independently

               - /estimation bias/ is a number, vs /inductive bias/ a
                 set of estimations

             - if several unbiased estimators exist, best choose that
               with the smallest variance

             - error standard deviation grows with p, shrinks with
               \sqrt(n) (see quote)

          5. Confidence Intervals

             - confidence interval: contains param p with prob N% (see
               quotes)

             - approximate error_{D} with error_{S}

             - approximate binomial with normal distribution

             - z-tables for normal distribution: z_{N} is "half the width
               of the interval measured in standard deviations"

             - use normal-binomial approximation for n \ge 30 or np(1-p)\ge5

          6. Two-sided and One-sided Bounds

             - with error, one-sided bound is often sufficient:

             - 100(1-a)% two-sided confidence interval implies
               100(1-a/2)% one-sided c.i. (symmetry of N)

       4. A GENERAL APPROACH FOR DERIVING CONFIDENCE INTERVALS

          (which param, esti, pdf, interval bounds) see quotes

          1. Central Limit Theorem

             - n iid variables Y1, ..., Yn (independent, identically
               distributed)

               - Y'_n := 1/n \sum_{i=1}^n Y_i

               - n \to \infty: mean(Y'_n) \to mean(Y)

                 - and std \to \sigma/\sqrt{n}

               - can use normal distribution to estimate, for n \ge 30

       5. DIFFERENCE IN ERROR OF TWO HYPOTHESES

          0. [@0]

             - evaluate two hypotheses h_1, h_2, tested on samples S_1,
               S_2 with sizes |S_1| = n_1, |S_2| = n_2

             - estimate difference d = error_D(h_1) - error_D(h_2)
               between true errors by d^ = error_{S_1}(h_1) - error_{S_2}(h_2)

               - difference of normal distributions is normal \to d^ assumed normal

               - variance of d^ is sum of variances of error_{S_{i}}(h_i) [[(mi5.12)]]

             - single sample S \to smaller variance (no variance of S_1 \neq S_2)

          1. Hypothesis Testing

             - compare hypotheses h1, h2

               - error_{s_1}(h_1) = 0.30

               - error_{s_1}(h_2) = 0.20

             - observed difference of d^ = .10

               - probability that h1 is still better than h2?

                 1. compute variance as of [[(mi5.12)]]

                 2. look up z-value from table, "halve" for one-sided

                 3. see that: 95% confidence level.

       6. COMPARING LEARNING ALGORITHMS

          0. [@0]

             - compare learning algorithms L1, L2 instead of hypotheses

             - find out if error of L1 <, >, or = error of L2

               - use training set S_0 and test set T_0

             - algorithm: average of cross-validated error difference (see quotes)

             - confidence interval \delta \pm t_{N, k-1} s_{\delta}

               - t_{N, k-1} similar to z_N, goes to z_N as (k-1) \to \infty

               - s_{\delta} = \sqrt{1 / (k(k-1)) \sum_{i=1}^k (\delta_i - \delta)^2}

             - note: same set, different to hypothesis testing

          1. Paired t Tests

             first reading: skip or skim this section

          2. Practical Considerations

             - assumption of 1): iid normal variables

               - invalid for limited data

             - either cross-test: split data into test sets, train on rest

             - or randomly draw at least 30 elements, use rest as training

               - problem: not independnt any more

               - advantage: resample as often as you want, reduce variance

       7. SUMMARY AND FURTHER READING

          main points:

          - estimate confidence interval as of [[(5.1)]]

          - general: identify parameter
            \to get estimator for it
            \to is random variable
            \to check confidence interval

          - estimation bias: estimators mean is different from parameter mean

          - variance of estimator: decreases with size of sample

          - compare two learning algos: cross-validate difference of error rates

          - assumptions:

            - approximate binomial distribution with normal distribution + variance

            - and assume distribution fixed

    7. [@7] COMPUTATIONAL LEARNING THEORY

       0. [@0]

          - which conditions enable/forbid successful learning?

          - when is success assured?

       1. INTRODUCTION

          - classes of problems? (difficult/easy)

          - how much data needed? (necessary/sufficient)

          - smart sampling vs random sampling

          - computational complexity of classes

          - estimate number of errors made before learning target

          - how much sample and computation complexity and mistake
            bound is necessary (see quotes)

       2. PROBABLY LEARNING AN APPROXIMATELY CORRECT HYPOTHESIS

          binary noise-free training data, can be extended somewhat

          1. The Problem Setting

             - X all possible instances (f.ex. people = { {young,
               short}, {old, tall}})

             - C target concepts, c \in C: X \to {0,1} (f.ex. people who
               are skiers)

             - D distribution over X (f.ex. largest sports store
               switzerland)

             - learner L draws x from D, tests with other x from D

          2. Error of a Hypothesis

             - (see quotes, see also ch.5)

             - true error of hypothesis: how often wrong re distribution/class

             - training error: how much of training data classified wrongly

             - re sample error: same, but statements assumed
               independent of training data and h

          3. PAC Learnability

             - error bounded by \epsilon

             - sucess rate bounded by \delta (suceed in at least 1-\delta cases)

             - C pak-learnable (see quote) if in fixed max error rate
               and failure rate, learnable in polynomial(\epsilon, \delta, n,
               size(c))

             - assumptions:

               - H contains small-error hypothesis

       3. SAMPLE COMPLEXITY FOR FINITE HYPOTHESIS SPACES

          0. [@0]

             - how many training samples needed until perfectly fitting
               hypothesis is found

             - assumption: *consistent learners*: can learn perfect
               hypothesis

             - bound number of hypotheses needed to probably (p > 1-\delta)
               learn an approximate (error \epsilon) hypothesis: m (see quotes)

          1. Agnostic Learning and Inconsistent Hypotheses

    8. INSTANCE-BASED LEARNING

       0. [@0]

          - store training examples, do not generalize until
            classification time ("lazy")

          - advantage: target function can be estimated locally

       1. INTRODUCTION

          - learning === storing all training samples

          - classifying === retrieve (some) samples, approximate (locally)

          - disadvantage: can have high cost for classifying

          - sections:

            2. [@2] knn,

            3. locally weighted regression,

            4. rbf-networks: mix between local-data and nn

            5. case-based reasoning: symbolic representation,
               knowledge-based inference

            6. general differences lazy vs eager learning

       2. k-NEAREST NEIGHBOR LEARNING

          0. [@0]

             - points in \R^n

             - discrete-valued: see what closest points classify as
               (based on euclidean distance), classify to majority

               - with k==1, just closest point

             - decision boundary: convex polygons around points (k==1?)

             - real-valued: use mean of k closest points

          1. Distance-Weighted NEAREST NEIGHBOR Algorithm (p.245)
*** quotes
    1. INTRODUCTION
       1. WELL-POSED LEARNING PROBLEMS
          - Definition: A computer program is said to learn from
            experience E with respect to some class of tasks T and
            performance measure P, if its performance at tasks in T,
            as measured by P, improves with experience E.
    2. CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING
       3. [@3] CONCEPT LEARNING AS SEARCH
          - Definition: Let hj and hk be boolean-valued functions
            defined over X. Then hj is *moregeneral-than-or-equal-to* hk
            (written h_{j} \ge_{g} h_{k} ) if and only if

            (\forall x \in X)[(hk(x) = 1) \to (hj(x) = 1)]
          - FIND-S Algorithm.
            1. Initialize h to the most specific hypothesis in H
            2. For each positive training instance x
               - For each attribute constraint a_i in h
                 - If the constraint a_i is satisfied by x
                   - Then do nothing
                   - Else replace a_i in h by the next more general
                     constraint that is satisfied by x
            3. Output hypothesis h
       5. [@5] VERSION SPACES AND THE CANDIDATE-ELIMINATION ALGORITHM
          1. Representation
             - A hypothesis h is *consistent* with a set of training
               examples D if and only if h(x) = c(x) for each example
               (x, c(x)) in D.

               Consistent(h, D) \equiv (\forall(x, c(x)) \in D) h(x) = c(x)
             - The *version space*, denoted VS_{H, D} with respect to
               hypothesis space H and training examples D, is the
               subset of hypotheses from H consistent with the
               training examples in D.

               VS_{H, D} \equiv {h \in H | Consistent(h, D)}
          3. [@3] A More Compact Representation for Version Spaces
             - Definition: The *general boundary* G, with respect to
               hypothesis space H and training data D, is the set of
               maximally general members of H consistent with D.

               G = {g \in H | Consistent(g, D)
                    \wedge (\not\exists g' \in H) [(g' >_{g} g) \wedge Consistent(g', D)]]
             - Definition: The *specific boundary* S, with respect to
               hypothesis space H and training data D, is the set of
               minimally general (i.e., maximally specific) members of
               H consistent with D.

               S = {s \in H | Consistent(s, D)
                    \wedge (\not\exists s' \in H) [(s >_{g} s') \wedge Consistent(s', D)]]
             - Theorem 2.1. *Version space representation
               theorem*. Let X be an arbitrary set of instances and
               let H be a set of boolean-valued hypotheses defined
               over X. Let c : X \to {O, 1) be an arbitrary target
               concept defined over X, and let D be an arbitrary set
               of training examples {<x, c(x)>). For all X, H, c, and
               D such that S and G are well defined,

               VS_{H, D} = {h \in H | (\exists s \in S, g \in G)(g \ge_{g} h \ge_{g} s)}
    5. [@5] EVALUATING HYPOTHESES
       1. ESTIMATING HYPOTHESIS ACCURACY
          1. Sample Error and True Error
             - Definition: The *sample error* (denoted error_{S}(h)) of
               hypothesis h with respect to target function f and data
               sample S is

               error_{S}(h) \equiv 1/n \sum_{x \in S} \delta(f(x), h(x))

               Where n is the number of examples in S, and the
               quantity \delta(f(x), h(x)) is 1 if f(x) \neq h(x) , and 0
               otherwise
             - Definition: The *true error* (denoted error_{D}(h)) of
               hypothesis h with respect to target function f and
               distribution D, is the probability that h will
               misclassify an instance drawn at random according to D.

               error_{D}(h) \equiv Pr_{x \in D}[f(x) \neq h(x)]
          2. Confidence Intervals for Discrete-Valued Hypotheses
             - [confidence interval z_N = 1.96 for 95% confidence interval]
               error_{S} \pm z_{N} \sqrt(error_{S}(h)(1- error_{S}(h)) / n)          (ref:5.1)
             - n \cdot error_{S}(h) (1-error_{S}(h)) \ge 5
       3. [@3] BASICS OF SAMPLING THEORY
          4. [@4] Estimators, Bias, and Variance
             - The *estimation bias* of an estimator Y for an arbitrary
               parameter p is

               E[Y] - p
             - In general, given r errors in a sample of n
               independently drawn test examples, the standard
               deviation for errors(h) is given by

               \sigma_{errors_{S}(h)} = \sigma_{r} / n = \sqrt{p(1-p) / n} (5.8)
             - which can be approximated by substituting r/n =
               errors_{S}(h) for p
          5. Confidence Intervals
             - An N% *confidence interval* for some parameter p is an
               interval that is expected with probability N% to
               contain p .
             - if a random variable Y obeys a Normal distribution with
               mean \mu and standard deviation \sigma , then the measured
               random value y of Y will fall into the following
               interval N% of the time

               \mu \pm z_{N }\sigma

               Equivalently, the mean \mu will fall into the following
               interval N% of the time

               y \pm z_{N} \sigma
       4. A GENERAL APPROACH FOR DERIVING CONFIDENCE INTERVALS
          0. [@0]
             1. Identify the underlying population parameter p to
                be estimated, for example, error_{D}(h).
             2. Define the estimator Y (e.g., error_{S}(h)). It is
                desirable to choose a minimum-variance, unbiased
                estimator.
             3. Determine the probability distribution D_{Y} that governs
                the estimator Y, including its mean and variance.
             4. Determine the N% confidence interval by finding
                thresholds L and U such that N% of the mass in the
                probability distribution D_{Y} falls between L and U.
          1. Central Limit Theorem
             - *Central Limit Theorem*. Consider a set of independent,
               identically distributed random variables Y1 ... Yn,
               governed by an arbitrary probability distribution with
               mean p and finite variance \sigma^2. Define the sample mean,
               Y'_n := 1/n \sum_{i=1}^n Y_i. Then as n \to \infty, the distribution
               governing

               (Y'_n - \mu) / (\sigma / \sqrt{n})

               approaches a Normal distribution, with zero mean and
               standard deviation equal to 1.
       5. DIFFERENCE IN ERROR OF TWO HYPOTHESES
          0. [@0]
             - d^ \equiv error_{S_1}(h_1) - error_{S_2}(h_2)
             - \sigma_{d^}^{2} \approx
               [error_{S_1}(h_1) (1 - error_{S_1}(h_1))] /n_1
               + [error_{S_2}(h_2) (1 - error_{S_2}(h_2))] /n_2         (ref:mi5.12)
       6. COMPARING LEARNING ALGORITHMS
          - Learning procedure
            1. Partition the available data D0 into k disjoint subsets
               T1, T2, ..., Tk of equal size, where this size is at
               least 30.
            2. for i in range(1,k+1):
               S_i = D0 \ Ti
               h_A = L_A(S_i)
               h_B = L_B(S_i)
               \delta_i = error_{Ti}(h_A) - error_{Ti}(h_B)
            3. Return the value \delta, where

               \delta = 1/k \sum_{i=1}^k \delta_i
    7. [@7] COMPUTATIONAL LEARNING THEORY
       1. INTRODUCTION
          - *Sample complexity*. How many training examples are needed
            for a learner to converge (with high probability) to a
            successful hypothesis?
          - *Computational complexity*. How much computational effort
            is needed for a learner to converge (with high
            probability) to a successful hypothesis?
          - *Mistake bound*. How many training examples will the
            learner misclassify before converging to a successful
            hypothesis?
       2. PROBABLY LEARNING AN APPROXIMATELY CORRECT HYPOTHESIS
          *PAC*: probably approximately correct model
          2. [@2] Error of a Hypothesis
             - Error of a Hypothesis
               - The *true error* (denoted error_{D}(h)) of hypothesis h
                 with respect to target concept c and distribution D is
                 the probability that h will misclassify an instance
                 drawn at random according to D.

                 error_{D}(h) \equiv Pr_{x \in D} [c(x) \neq h(x)]
          3. PAC Learnability
             - classes of target concepts that can be reliably learned
               from a reasonable number of randomly drawn training
               examples and a reasonable amount of computation.
             - Consider a concept class C defined over a set of
               instances X of length n and a learner L using
               hypothesis space H . C is *PAC-learnable* by L using H
               if for all c \in C, distributions D over X, \epsilon such that 0
               < \epsilon < 1/2, and \delta such that 0 < \delta < 1/2, learner L will
               with probability at least (1 - \delta) output a hypothesis h
               \in H such that error_{D}(h) \le \epsilon, in time that is polynomial
               in 1/\epsilon, 1/\delta, n, and size(c).
       3. SAMPLE COMPLEXITY FOR FINITE HYPOTHESIS SPACES
          - The reason is that in most practical settings the factor
            that most limits success of the learner is the limited
            availability of training data.
          - Consider a hypothesis space H, target concept c, instance
            distribution \cal D, and set of training examples D of c. The
            version space VS_{H,D} is said to be *\epsilon-exhausted* with
            respect to c and \cal D, if every hypothesis h in VS_{H,D} has
            error less than \epsilon with respect to c and \cal D.

            (\forall h \in VS_{H, D}) error_{\cal D}(h) < \epsilon
          - *\epsilon-exhausting the version space*. If the hypothesis space H
            is finite, and D is a sequence of m \ge 1 independent
            randomly drawn examples of some target concept c, then for
            any 0 \le \epsilon \le 1, the probability that the version space VS_{H,
            \cal D} is not \epsilon-exhausted (with respect to c) is less
            than or equal to |H| e^{-\epsilon m}
          - m \ge 1/\epsilon (ln |H| + ln (1/\delta))
    8. INSTANCE-BASED LEARNING
       1. INTRODUCTION
          - conceptually straightforward approaches to approximating
            real-valued or discrete-valued target functions
          - has significant advantages when the target function is
            very complex, but can still be described by a collection
            of less complex local approximations.

*** ref [[file:master.bib::mitchell][Mitchell: Machine Learning]]                                       :ML:
** TODO [#B] [[./guyon03a.pdf][An Introduction to Variable and Feature Selection]]
** TODO PART [#B] [[./minion-design.pdf][Mixminion: Design of a Type III Anonymous Remailer Protocol]]
*** TODO at least dummy problem
*** summary
    0. [@0] Abstract

       - remailer with reply addresses

       - encryption, directory servers for key discovery, real
         internet

    1. Overview
*** quotes
*** ref [[file:master.bib::minion-design][Danezis et al. 2003: Mixminion]]
** TODO [#B] [[~/da/git/docs/onion.pdf][Anonymous Connections and Onion Routing]]
*** summary
    0. [@0] Abstract

       - "Onion Routing is an infrastructure for private
         communication over a public network.

       - Eavesdropping and traffic analysis are hampered by /onion
         routing/.

    1. Introduction

       - traffic analysis extracts data from who talks to whom

       - side effect of onion routing: protect against this

       - offer connection akin to TCP streams (and ATM AAL5)

    2. Onion Routing Overview

       0. [@0]

          - instead of direct socket connection, connect through onion routers

          - hides connection details

            - hide identifying information from stream yourself to
              hide from recipient

            - data looks different from hop to hop

            - each onion router can only identify previous and next
              router on connection

       1. Operational Overview

          - /application proxy/ sends formatted messages to /onion proxy/ (=OP)

          - OP constructs /onion/ and passes it to /entry funnel/

          - passed on via /onion router/ s (OR), until it reaches
            /exit funnel/, then to responder

          - /onion/ contains route and key seed material at each layer

          - data looks different at each layer

          - each OR keeps track of non-expired

          - routing on top of TCP/IP

       2. Configurations

          routers are neighbors if they are connected, proxies provide access

          1. Firewall Configuration

             - one firewall/OR at local network border, inside
               physically secure

               - filters traffic (governmental/corporate policy)

               - should route data between other onion routers

               - if non-Tor web server f.ex. as endpoint, data stream
                 needs to be anonymized

          2. Remote Proxy Configuration

             - "onion proxy" on separate host

             - need to trust both host and receiving side

          3. The Customer-ISP Configuration

             - customer runs own onion router

             - ISP cannot see whom customer connects to ("relieve
               [...] responsibility")

             - ISP might run onion router of his own: make identifying
               harder

    3. Empirical Data

       - invited to experiment with 13-node network

       - data based on one SUN machine with 5 ORs

       - main overhead is due to encryption

       - HTTP pipelining might further reduce costs

    4. Threat Model

       - marker attack: connections have "some distinguished marker"

       - timing attack: timing signature, cooperating attackers along
         route

       - if "uniformly busy", both of those two are "ineffective"

       - compromised routers: add marker, throttle traffic, easier

    5. Onion Routing Specifics

       1. Onion Routing Proxies

          - aim: no change to applications

            - solution: proxies

              - either use proxified applications

              - or work around this

          - application proxy: provides data and
            =dns-name-or-ip-addr:port= tuple to onion proxy

          - onion proxy: defines route, forwards data to entry funnel

          - entry funnel: "multiplexes connections"

          - example: http GET, modified, forwarded

            - proxy removes deanonymizing data, f.ex. cooies

       2. Implementation

          phases: network setup, connection setup, data, closing

       3. Application Proxy

          - to onion proxy (each 8 bit): 
            | version | protocol | retry count | addr format |

       4. Onion Proxy

          - accept or reject request

          - if acc, builds onion, routes data

       5. Onions
*** quotes
    1. Introduction
       - Web based shopping or browsing of public databases should not
         require revealing one's identity.
       - Our implementation of anonymous connections, onion routing,
         provides protection against eavesdropping as a side effect.
    2. Onion Routing Overview
       1. Operational Overview
          - Therefore, an anonymous connection is as strong as its
            strongest link, and even one honest node is enough to
            maintain the privacy of the route.
    4. [@4] Threat Model
       - Our goal is to prevent traffic analysis, not traffic
         confirmation.
*** ref [[file:master.bib::anonymous-connections][G. et al. 1998: Anonymous]]
** TODO [#B] [[./oakland2013-parrot.pdf][The Parrot is Dead: Observing Unobservable Network Communications]]
*** summary
    0. [@0] Abstract

       - censorship led to censorship-resistant communication

       - these fail to achieve unobservability

       - how to do it right

    1. INTRODUCTION

       - to avoid censorship-blocking, tor looks more like other
         protocols

       - mostly easy to distinguish from the things they are mimicking

       - approach is flawed: hard to mimick sophisticated protocol
         like skype

       - solution: higher on protocol stack: tunnel via real skype

    2. UNOBSERVABILITY BY IMITATION

       - Skype: VoIP

         - supernode: resource-rich public IP computer, forwards NAT
           traffic

       - IETF VoIP

         - SIP: session initiation

           - users registered with VoIP provider

         - RTP transmits info, RTCP controls

           - with encryption SRTP/SRTCP, uses key exchange protocol
             f.ex. MIKEY

    3. PARROT CIRCUMVENTION SYSTEMS

       1. SkypeMorph

          - skype text message to derive key material

          - skype ringing to initiate comm

          - the UDP over pre-selected high UDP ports

       2. StegoTorus

          - traffic hidden in voip or http traffic

       3. CensorSpoofer

          - standalone

          - spoofes IP-address UDP traffic for web downstream

          - email f.ex. for upstream (which web sites

    4. ADVERSARY MODELS

       A. Capability classification

          - passive attack: just observe packets

          - active attack: insert data into existing traffic

          - proactive attack: initiate session/traffic

       B. Knowledge classification

          - local adversary: some computers, like WiFi, company, ISP

          - 
*** quotes
** TODO [#B] PART [[./Masterthesis.pdf][Malte Hübner - Automatische Erkennung von anormalen Android-Anwendungen]]
*** TODO ch. 2
*** summary
    2. [@2] Grundlagen

       machinelles Lernen und Android

       1. Methoden zur Anomalieerkennung

          ml, anomalieerkennung, eingabedaten, einbettung daten in
          vektorraeume

          1. Anomalieerkennung als Teilgebiet des maschinellen Lernens

             - maschinelles lernen: maschinell + lernen + def mitchell

               - algorithmus m + modell \Theta + vorhersagefkt f_{\theta}

               - anwendungsgebiete (texterkennung, dna-analysen)

               - verwandt: ki, data mining, statistik

             - Anomalieerkennung (cont p.7)

               - normale Android-Programme sind gutartig

               - Ziel: Abweichen feststellen

               - Ueberwachtes Lernen: Klassen der instanzen werden mit
                 angegeben

                 - m: (X \times Y)^n \to \Theta

                 - f_{\Theta} : X \to Y

               - Unueberwachtes Lernen: Klassen erkennen

                 - m: X^n \to \Theta

                 - f_\Theta: X \to Y

          2. Eingabedaten für SVMs und andere Methoden in Eingabe-,
             Vektor- und Merkmalsräumen

             0. [@0]

                - input: Merkmal: "eine wesentliche Eigenschaft"

                  - numerisch, oder

                  - strukturiert

                - weiter: Eingaberaum \cal X: (Menge aller)
                  Merkmalskombinationen

                - Umwandlung: \Phi: \cal X \to \cal H (Merkmalsraum - Vektorraum)

             1. Normierung

                - deutsch: normierung, englisch: scaling/data normalization

                - \mu to 0, \sigma to 1 (Standardnorm)

                - auf Intervall [0, 1] (Min-Max-Norm)

          3. Kernel-Trick

             - \Phi muss manchmal unendlichen Vektorraum darstellen, schwierig

             - Exkurs - Lineare Trennbarkeit und die Two-Class SVM

               - (?falls nicht annähernd linear trennbar?): \Phi
                 Projektion in "höherdimensionalen Vektorraum"

                 - dadurch Reduktion der Komplexität

               - Kern ist Skalarprodukt auf Eingaberaum, einfacher als
                 \Phi vollständig zu definieren

               - k(x, x') = <\Phi(x), \Phi(x')> = \sum_{i=1}^N \phi_i(x)\phi_i(x') (1 \le N \le \infty)
                 mit \Phi = (\phi_1, ..., \phi_N): X \to H

               - Kerne:

                 - RBF k(x,y) = exp(-||\phi(x)-\phi(y)||^2 / 2 \sigma^2)

                 - linear k(x,y) = <\phi(x), \phi(y)>

                 - polynomiell k(x,y) = (<\phi(x), \phi(y)> + \omega)^d

          4. Anomalieerkennung mit der One-Class Support-Vector-Machine

             - SVM: geometrische ML-Algos: Struktur beschreiben

               - one-class: hyperkugel

                 - formal: min, i.v.m. cond

                 - Schlupfvariablen

                 - \nu-SVC

                 - duale Form, Lagrange-Multiiplikatoren, KKT-Bedingung

                 - entscheidungsfunktion

             - Über- oder Unteranpassung (Over- and Underfitting)
*** quotes
*** ref: [[file:master.bib::malte][Hübner 2014: Automatische Erkennung Android Anwendungen]]
** TODO [#B] [[./voydock.pdf][Voydock - Security Mechanisms in High-Level Network Protocols]]
*** ref
V.L. Voydock and S.T. Kent. Security Mechanisms in High-Level Network Protocols. ACM Computing Surveys (CSUR), 15(2):135–171, 1983.
** TODO [#B] [[./crowds.pdf][Reiter - Crowds: Anonymity for Web Transactions]]
*** ref [[file:master.bib::crowds:tissec][Reiter & Rubin 1998: Crowds]]
** TODO [#B] [[./LiuSIGIR2010.pdf][Liu - Understanding web browsing behaviors through Weibull analysis of dwell time]]
*** ref: C. Liu, R. White, and S. Dumais. Understanding web browsing
      behaviors through Weibull analysis of dwell time.  In
      Proceedings of the 33rd international ACM SIGIR Conference,
      pages 379–386, 2010.
** TODO [#B] [[./Newman etal2003.pdf][Newman - Metrics for Traffic Analysis Prevention]]
*** ref [[file:master.bib::newman:pet2003][Newman et al. 2003: Metrics Traffic Analysis Prevention]]
** TODO [#B] [[./george-thesis.pdf][Danezis - Better Anonymous Communications]]
*** TODO [#B] skim
*** ref [[file:master.bib::george-thesis][Danezis 2004: Better Anonymous Communications]]
** TODO PART [#B] Schölkopf - Learning with Kernels                      :ML:
*** [#B] chapter 7.4 (p.200)
*** [#C] chapter 6: optmization
*** summary
    1. A Tutorial Introduction

       1. Data Representation and Similarity

          - patterns do not exist in dot product space, need to map
            them there via map \Phi

            - pattern space X

            - dot product / feature space H

          - pro:

            - dot product as similarity measure,

            - geometric interpretation,

            - free to choose mapping, vary similarities

       2. A Simple Pattern Recognition Algorithm

          - algo

            1. find geometric mean of each class in H

            2. find middle between these

            3. create orthogonal hyperplane through this middle point

          - check if new point is left or right of hyperplane
            (?middle?)

          - simple equates (with other conditions) to Bayes classifier

       3. [#C] Some Insights From Statistical Learning Theory

    2. [@2] Kernels

       0. [@0]

          1. why kernels

          2. which kernels yield \Phi, how to construct?

          3. examples, properties

          4. dissimilarity kernels

       1. Product Features

          - sometimes effective to classifiy more than just linear data

          - huge dimensions of H if computed

            - yet can use kernel to compute directly

            - easy example:
              #+BEGIN_SRC latex
                \langle \Phi(x),  \Phi(x') \rangle
                = [x]_1^2[x']_1^2 + [x]_2^2[x']_2^2 + 2[x]_1[x]_2[x']_1[x']_2
                = \langle x, x'\rangle^2
              #+END_SRC

              - always nth power easily taken by nth power of dot product

              - all powers up to d-th: k(x, x') = (<x, x'>+1)^d

              - scaling factor so that all values in [-1, 1]

       2. The Representation of Similarities in Linear Spaces

          0. [@0]

             - which kernels have mappings

          1. Positive Definite Kernels

             - gram matrix, positive (semi-)definite matrix/kernel: see quotes

             - actually positive semidefinite, but used like that here

             - dot product is kernel

             - kernels are not necessarily dot product: non-linear

             - but cauchy-schwarz transfers, see [[(kernels-2.19)]]

          2. The Reproducing Kernel Map

             - every pd kernel can create a feature map, and vice
               versa

             - kernel trick (see quotes): possible to replace any
               kernel with another

               - default input domain and dot product replaced by
                 others

       3. Examples and Properties of Kernels

          - polynomial, gaussian,

          - sigmoid
            #+BEGIN_SRC latex
              k(x, x') = tanh(\kappa \langle x, x'\rangle + \varphi)
            #+END_SRC

          - inhomogenous polynomial, B_n-Spline of Odd Order

          - invariance of kernels: invariance to rotation:

            k(x, x') = k(Ux, Ux') if U^{T} = U^{-1}

          - RBF kernels: k(x, x') = f(d(x, x'))
            with d: metric, f: \R^+_0\to\R^b+_0

            - all mappings in same part of feature space

            - gaussian kernel produces feature space of infinite
              dimension if domain has infinite cardinality (as of
              [[(kernels-2.74)]])

       5. [@5] Summary

          - kernel trick crucial

          - basis is always linear space

          - also for dissimilarities (ch2.4): use instead of metrics

    7. [@7] Pattern Recognition

       0. [@0]

          - detail SVC method

          - contents

            1. separating hyperplanes

            2. how big to choose margin

            3. optimal margin hyperplane

            4. kernel trick: hyperplane in nonlinear feature space

            5. slack variables: outliers and noise

            6. multi-class SV

            7. variations to SV

            8. experiments and applications

       1. Separating Hyperplanes

          - hyperplane (see [[(kernels-7.2)]]) which separates input data

          - if this works, linearly separable (see quotes)

       2. The Role of the Margin

          - how far next point to hyperplane

          - if margin big: intuition that classifies well

            - "proof": assume test points around training points with
              random offset up to r>0, if r < \rho, then all points are
              classified correctly

            - margin error bound theorem: the bigger the margin while
              the smaller \|w\| and \|w|, the lower the chance of
              misclassification.

            - also hardware inaccuracies better to be glossed over

       3. Optimal Margin Hyperplanes

          - given /set of examples/ (x_1, y_1), ..., (x_m, y_m), x_i \in
            H, \y_i \in {\pm 1}

          - find /decision function/ f_{*w*,b}(*x*) = sgn(\lang *w* , *x*\rang +b) with
            f_{*w*,b}(x_i) = y_i

          - solve the /primal optimization problem/, cf. [[(kernels-7.11)]]

          - translate to dual problem, cf [[(kernels-7.17)]]

            - maximize w.r.t. \alpha, minimized w.r.t. w and b

          - alternate formulation: hyperplane bisects shortest line
            between convex hulls of both classes of training points

       4. Nonlinear Support Vector Classifiers

       8. [@8] Experiments

          1. Digit Recognition Using Different Kernels

             - scaling avoids roundoff errors

             - comparison SVC kernels: most important to choose
               correct parameters, not which method to choose

             - cross-validation: split training set

               - but may be problematic, because parameters on
                 training and complete may be different...

             - parameter estimation approaches

               - parameters which previously worked well

               - set \nu-SVC error to rate of previous other methods

               - mimic this for C-SVC: start with high C, lower until
                 error closes to 5%

                 - problem: more training runs

               - estimate error from sphere, maximum length, etc

               - theoretical tools such as VC bounds (see sec 5.5)

             - comparison to others on USPS digits: virtual SVC worked
               best on specific set

          2. Universality of the Support Vector Set
*** quotes
    1. A Tutorial Introduction
       0. [@0]
          - [...] introduce kernels informally as similarity measures
            that arise from a particular representation of patterns
       1. Data Representation and Similarity
          - For reasons that will become clear later (cf Remark 2.16),
            the function /k/ is called a /kernel/ [359, 4, 42, 62,
            223].
       2. A Simple Pattern Recognition Algorithm
          - The hyperplane will then only depend on a subset of
            training patterns called /Support Vectors/.
    2. [@2] Kernels
       0. [@0]
          - kernel arises as a similarity measure that can be thought
            of as a dot product in a so-called feature space.
       2. [@2] The Representation of Similarities in Linear Spaces
          1. Positive Definite Kernels
             - Given a function k: X^2 \to \K (where \K = \C or \K = \R) and
               patterns x_1, ..., x_m \i X, the m \times m matrix K with
               elements

               K_{ij} := k(x_i, x_j)

               is called the /Gram matrix/ (or /kernel matrix/) of k
               with repect to x_1, ..., x_m.
             - A complex m \times m matrix K satisfying

               \sum_{i,j} c_i \overline{c_i} K_{ij} \ge 0

               for all c_i \in \C is called /positive (semi-!)
               definite/. Similarly, a real symmetric m \times m matrix K
               satisfying [the above] for all c_i \in \R is called
               /positive (semi-!) definite/.
             - Let X be a nonempty set. A function k on X \times X which
               for all m \in \N and all x_1, ... x_m \in X gives rise to a
               positive definite Gram matrix is called a /positive
               definite (pd) kernel/. Often, we shall refer to it
               simply as a /kernel/.
             - If k is a positive definite kernel and x_1, x_2 \in X, then
               #+BEGIN_SRC latex
                 |k(x_1, x_2|^2 \le k(x_1, x_1) \cdot k(x_2, x_2) (ref:kernels-2.19)
               #+END_SRC
          2. The Reproducing Kernel Map
             - Given an algorithm which is formulated in terms of a
               positive definite kernel k, one can construct an
               alternative algorithm by replacing k by another
               positive definite kernel \overline{k}
             - /any/ algorithm that only depends on dot products,
               i.e., any algorithm that is rotationally inariant, can
               be kernelized [479, 480].
       3. Examples and Properties of Kernels
          - Suppose that x_1, ..., x_m \subset X are distinct points and \sigma
            \neq 0. The matrix K given by
            #+BEGIN_SRC latex
              K_{ij} := exp \left ( - { \|x_i - x_j\|^2 \over 2 \sigma^2 } )
              (ref:kernels-2.74)
            #+END_SRC
            has full rank. In other words, the points \Phi(x_1), ...,
            \Phi(x_m) are linearly independent (provided no two x_i are
            the same). They span an m-dimensional subspace of H.
    6. [@6] Optimization
       3. [@3] Constrained Problems
          1. Optimality Conditions
             - Some of the most important sufficient criteria are the
               Kuhn-Tucker[fn::An earlier version is due to
               Karush [283]. This is why often one uses the
               abbreviation KKT (Karush-Kuhn-Tucker) rather than KT to
               denote the optimality conditions.] saddle point
               conditions[312].
    7. Pattern Recognition
       1. Separating Hyperplanes
          - The pair (w, b) \in H \times \R is called a /canonical/ form of
            the hyperplane [...] with respect to x_1, ..., x_m \in H, if
            it is scaled such that
            #+BEGIN_SRC latex
              min_{i=1,...,m} |\langle w, x_i \rang +b| =1 (ref:kernels-7.2)
            #+END_SRC
          - they are oriented differently; they correspond to two
            /decision functions/,
            #+BEGIN_SRC latex
              f_{w, b}: H \to {\pm 1}
                       x \mapsto f_{w,b}(x) = sign(\langle w, x \rangle +b) (ref:kernels-7.3)
            #+END_SRC
          - solution f_{w, b} which /correctly classifies/ the labelled
            examples (x_i, y_i) \in H \times {\pm 1}; in other words, which
            satisfies f_{w, b}(x_i) = y_i for all i (in this case, the
            training set is said to be /separable/)
       2. The Role of the Margin
          - For a hyperplane {x \in H \mid \langw, x \rang +b = 0}, we call
            #+BEGIN_SRC latex
              \rho_{(*w*, b)}(*x*, *y*) := y(\lang *w*, *x*\rang> + b) / \| *w* \| (ref:kernels-7.4)
            #+END_SRC
            the /geometrical margin of the point (x, y) \in H \times
            {\pm1}/. The minimum value
            #+BEGIN_SRC latex
            \rho_{(*w*, b)} := min_{i=1,...,m} \rho_{(*w*, b)}(x_i, y_i) (ref:kernels-7.5)
            #+END_SRC
            shall be called the /geometrical margin of (x_1, y_1),
            ... (x_m, y_m)/. If the latter is omitted, it is
            understood that the training set is meant.
          - perceptron algorithm [439] is one of the simplest
            conceivable iterative procedures for computing a
            separating hyperplane.
          - Consider the set of decisdion functions f(x) = sgn( \lang *w*,
            *x*, \rang with \| *w* \| \le \Lambda and \| *x* \| \le R, for some R, \Lambda
            > 0. Moverover let \rho > 0, and \nu denote the fraction of
            training examples with margin smaller than \rho / \| *w* \|,
            referred to as the /margin error/.

            For all distributions P generating the data, with
            probability at least 1 - \delta over the drawing of the m
            training patterns, and for any \rho > 0 and \delta \in (0, 1), the
            probability that a test pattern odrawn from P will be
            misclassified is bounded from above, by
            #+BEGIN_SRC latex
              \nu + \sqrt{{c \over m} \left( {R^2 \Lambda^2 \over \rho^2} ln^2 m +
                ln(1 / \delta)  (ref:kernels-7.7)
            #+END_SRC
       3. Optimal Margin Hyperplanes
          - minimize_{*w* \in H, b \in \R} \tau(*w*) = 1/2 \| *w* \|^2   (ref:kernels-7.10)
            subject to
            #+BEGIN_SRC latex
              y_i(\lang x_i, *w* \rang+b) \ge 1 for all i=1,...,m (ref:kernels-7.11)
            #+END_SRC
          - the Lagrangian
            #+BEGIN_LaTeX
            L({\bf w}, b, {\bf \alpha})
            =  {1 \over 2} \| {\bf w} \|^2
            - \sum_{i=1}^m \alpha_i(y_i(\lang {\bf x}_i, {\bf w} \rang) +b) -1)
            (ref:kernels-7.12)
            #+END_LaTeX
            with Lagrange multipliers \alpha_i \ge 0.
          - The patterns *x*_i for which \alpha_i >0 are called /Support
            Vectors/. [...] they lie examctly on the margin.
          - The expectation of the number of Support Vectors obtained
            durin gtaining on a training set of size m, divided by m,
            is an /upper bound/ on the expected probability of test
            error of the SVM trained on training sets of size m-1.
          - dual form of the optimization problem
            maximize_{*\alpha* \in \R^m}
            #+BEGIN_SRC latex
              W({\bf \alpha}) \sum_{i=1}^m \alpha_i
              - 1/2 \sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j \langle {\bf x}_i, {\bf x}_j \rangle
              (ref:kernels-7.17)
            #+END_SRC
            subject to \alpha_i \ge 0, i = 1,..., m
            and \sum_{i=1}^m \alpha_i y_i = 0.
          - [decision function]
            #+BEGIN_SRC latex
              f(*x*) = sgn (\sum_{i=1}^m \alpha_i y_i \langle *x*, *x*_i \rangle +b)
              (ref:kernels-7.20)
            #+END_SRC
       8. [@8] Experiments
          - typically, the smaller set will require a slightly
            stronger regularization
*** ref [[file:master.bib::kernels][Bernhard & Smola 2002: Learning Kernels]]
** TODO [#B] PART [[/usr/share/doc/texlive-doc/generic/pgf/pgfmanual.pdf][Tantau - The TikZ and PGF Packages]]
*** TODO [#B] look through for basic pgf understanding, usage, ...
*** summary
    7. [@7] Guidelines on Graphics

       0. [@0]

          - general guidelines for scientific papers

          - Edward Tufte

          - break rules, but be *aware* that you are breaking them

       1. Planning the Time Needed for the Creation of Graphics

          - as much time as for text of the same length

          - revise graphics repeatedly

       3. [@3] Linking Graphics With the Main Text

          - stand-alone happens to "fill" pages

          - if so:

            - caption to make self-clear

            - add as much context as possible

            - reference in text

            - spell out, f.ex. "Figure 2.1"

              - periods are precious

       4. Consistency Between Graphics and Text

          - just as with text: same font, same line width, create at right size

       5. Labels in Graphics

          - readable: same font, same style

          - in place

          - maybe grey out unimportant labels

       6. Plots and Charts

          - consider using a table instead, if comparing a few data
            points

          - use 3d effects only if it makes sense

          - use appropriate colors: bright for emphasis

       7. Attention and Distraction

          - /avoid distractions/ and *steer attention*

          - constrasts attract the eye

          - differentiate curves in plots by colors, not dashing patterns

          - no background color, images, no clip art
*** quotes
    7. [@7] Guidelines on Graphics
       3. [@3] Linking Graphics With the Main Text
          - Do not feel afraid of a 5-line caption.
       6. [@6] Plots and Charts
          - The first question you should ask yourself when creating a
            plot is: Are there enough data points to merit a plot? If
            the answer is “not really,” use a table.
** TODO [#B] [[./Vorratsdatenspeicherung – Wikipedia.html][Wikipedia - Vorratsdatenspeicherung]]
*** summary
    0. [@0]

       - meist: Telekommunikation-Vorratsdatenspeicherung

       - ohne Anfangsverdacht oder konkrete Gefahr

       - 2006 EU-Richtilinie, 2008 Gesetz, 2010 BVerfG, 2014 EuGR, 2015 Gesetz

    1. Hintergrund

       - ursprünglich elektrische Leitung, Verbindung wurde zur
         Abrechnung notiert

       - Vermittlungsstellen automatisierten, addierten nur Gebühren

         - Problem für Sicherheitsbehörden bei Verdächtigen

           - Fangschaltung

       - Computer: speicherten Daten wieder

       - bei Flatrates wiederumg nicht mehr nötig

    2. Vorratsdatenspeicherung in der Telekommunikation

       - Begriff heute zumeist bzgl. Telekommunikation verwendet

       - Grundrechte schützen

       - Speicherung erschwert außerdem

         - Informantenschutz von Journalisten, Eingriff in Pressefreiheit

         - Verschwiegenheitspflicht verletzt (Rechtsanwalt, Arzt)

           - Seelsorge- und Beichtgeheimnis ebenso

       - bisheriges Recht: unverzüglich löschen, falls nicht zu
         Abrechnungszwecken erforderlich

    3. Begründung

       - Terrorismus-, und Kriminalitätsbekämpfung

       - Erhöhung Aufklärungsquote um 0,006 Prozentpunkte

         - ebenso MPI: Wegfall kein Effekt auf Aufklärungsquote

    4. IP-Vorratsdatenspeicherung

       - Speicherung von IP-Adressen

         - erlauben, sehr vieles nachzuverfolgen

       - Lösungsvorschlag ohne IT-Vorratsdatenspeicherung : spezielle
         IT-Forensik

    5. Vorratsdatenspeicherungsentwurf des Bundesjustizministeriums

       - Anlassbezogenes "Einfrieren" ohne richterlichen Beschluss

       - zum "Auftauen", Zugriff auf die Daten: Richter

       - Benachrichtigung des Anschlussinhabers

    6. Europäische Richtlinie

       1. Entstehungsgeschichte

          - EU-Parlament 378 vs 197 (30 Enthaltungen)

       2. Binnenmarkt

          Begründung: unterschiedliche Vorschriften behinderten
          Binnenmarkt für elektronische Kommunikationsdienste

       3. Beschränkter Anwendungsbereich

          - kein Inhalt, keine Bewegungsprofile (erstellbar und
            metainformationen schon viel)

    7. Aufhebung der Vorratsdatenspeicherungs-Richtlinie durch den
       Europäischen Gerichtshof (EuGH)

       - 2014: Verstoß gegen Grundrechtecharta (GRC) Art.7 (Achtung
         Privat- und Familienleben),8 (Schutz der personenbezogenen
         Daten), und 52 (Verhältnismäßigkeit)

       - wohl in Zukunft anlasslose Speicherung nicht rechtens (siehe
         quotes)

    8. Umsetzung in Deutschland

       2015 Gesetz, gilt ab 18.12.2015, voriges von 2008 bis 2010

       1. Verabschiedung des Gesetzes 2007

          - nur CDU/CSU und SPD

       2. Inhalt des Gesetzes

          - Nummern, Dauer (Uhrzeiten), mobil: Funkzelle, voip: IP-Adresse

          - analog sms

          - IP: adresse, dauer (Uhrzeiten)

          - Email: alle Adressen beteiligter, Uhrzeit

       3. Bestandsdaten

          - Identifizierungspflicht: falls Anbieter Daten erheben,

            - Rufnummer/Email, Name, Anschrift, Datum, Geburtsdatum,
              Anschrift bei Festnetz

          - Stellen mit Zugriff: siehe quotes

       4. Verfassungsbeschwerden 2007 bis 2010

          - Klagen von AG VDS (34.939), FDP, Grüne, verdi

          - Verteidigung Bundesregierung: BVerG nicht zuständig, da EU

          - einstweilige Verfügung: nur bei schweren Straftaten, etc

          - Verbot in damaliger Form, aber grundsätzlich evtl möglich

       5. Diskussion im Anschluss an das Urteil des BVerfG

          - EU-Kommision: siehe quote

          - Norwegen: Forderungen, allerdings da schon VDS: siehe quote

       6. Klage der EU-Kommission

          - hohes Strafgeld

          - EUGh verwarf Gesetz, dann zurückgezogen

       7. Erneute Verabschiedung 2015

          - sehr ähnlich, jetzt mit Mobilfunk-Internetdaten

          - keine richterliche Kontrolle

    9. Umsetzung in Österreich

       - nach Androhung Millionenstrafe: Gesetz mit Zugriff bei
         Schweren Straftaten und für Staatsanwaltschaft mit
         Vier-Augen-Prinzip

       - Verfassungsgerichtshof (VfGH) Klage, fragte bei EUGH an, das
         kippte, Gesetz fristlos "gekippt"

    10. Umsetzung in anderen Ländern der EU
*** quotes
    4. [@4] IP-Vorratsdatenspeicherung
       - Der Bundesdatenschutzbeauftragte Peter Schaar warnte im
         September 2011, dass Internetanbieter wie Google anhand der
         IP-Adresse jeden Klick protokollierten und die Zuordnung von
         IP-Adressen deshalb „höchst sensibel“ sei.[33]
    6. [@6] Europäische Richtlinie
       1. Entstehungsgeschichte
          - bisher schnellste Gesetzgebungsverfahren in der EU-Geschichte
    7. Aufhebung der Vorratsdatenspeicherungs-Richtlinie durch den
       Europäischen Gerichtshof (EuGH)
       - Der EuGH kommt somit zum Schluss, dass der Unionsgesetzgeber
         beim Erlass der Vorratsdatenspeicherungs-RL (2006/24/EG) die
         Grenzen überschritten hat, die er zur Wahrung des Grundsatzes
         der Verhältnismäßigkeit in Bezug auf den Eingriff in das
         Grundrecht auf Achtung des Privat- und Familienlebens und des
         Grundrechts auf Schutz der personenbezogenen Daten einhalten
         musste.
       - Juristische Dienst des EU-Rates [...] mitgeteilt, dass [...]
         „nahe legen, dass eine allgemeine, voraussetzungslose
         Speicherung von Daten künftig nicht mehr möglich ist“.
    8. Umsetzung in Deutschland
       3. [@3] Bestandsdaten
          - Die Anbieter sind berechtigt, aber nicht verpflichtet, die
            Richtigkeit der Angaben des Kunden zu überprüfen, etwa
            anhand eines Personalausweises
          - [Zugriff] Gerichte, Strafverfolgungsbehörden,
            Polizeivollzugsbehörden des Bundes und der Länder für
            Zwecke der Gefahrenabwehr, Zollkriminalamt und
            Zollfahndungsämter für Zwecke eines Strafverfahrens,
            Zollkriminalamt zur Vorbereitung und Durchführung von
            Maßnahmen nach § 39 des Außenwirtschaftsgesetzes,
            Verfassungsschutzbehörden des Bundes und der Länder,
            Militärischer Abschirmdienst, Bundesnachrichtendienst,
            Notrufabfragestellen, Bundesanstalt für
            Finanzdienstleistungsaufsicht, Zollverwaltung zur
            Schwarzarbeitsbekämpfung.
       5. [@5] Diskussion im Anschluss an das Urteil des BVerfG
          - dass die Einführung einer Vorratsdatenspeicherung in
            keinem EU-Land zu einer signifikanten Änderung der
            Aufklärungsquote von Straftaten geführt habe.
          - Im April 2011 kündigte die EU-Kommission erhebliche
            Änderungen an der EU-Richtlinie zur
            Vorratsdatenspeicherung an, weil diese das Ziel einer
            Vereinheitlichung nicht erreicht habe.[79] Gleichzeitig
            forderte sie die Bundesrepublik Deutschland auf,
            „schnellstmöglich“ ein Gesetz zur Umsetzung der
            derzeitigen Richtlinie zu erlassen. Andernfalls drohe ein
            Verfahren wegen Verletzung des EU-Vertrags.[80]
          - habe die Vorratsdatenspeicherung in Norwegen die Anschläge
            nicht verhindern können
       6. Klage der EU-Kommission
          - Die Bundesrepublik Deutschland hätte bis zur Übertragung
            der Richtlinie täglich ein Zwangsgeld in Höhe von
            315.036,54 Euro zahlen müssen. [...] (Zwangsgeld-Spanne
            für Deutschland: 13.436 bis 807.786 Euro).[93]
** TODO [#B] Beck - extreme Programming explained
*** continue p. 115
*** summary
    1. Risk: The Basic Problem

       There are several risks in Software Development. XP addresses them.

    2. A Development Episode

       Two programmers complement each other in test case design,
       interaction, test refactoring, test writing, coding,
       refactoring, integration, often switching keyboard control.

    3. Economics of Software Development

       There are interest payments.

       - factors: cash flows in/out, interest rates, project mortality

       - earn by: (spending less), (earning more),

         - spending later/earning sooner

         - keeping the project alive

       - choices: abandon the project, switch direction, defer action or grow

       - example: could add feature, cost 10$, estimated return 15$ \pm 100%

         - strategy to wait: $7.87 (blackbox options theory calculator)

    4. Four Variables

       Cost, Time, Quality, Scope - pick three

    5. Cost of Change

       - exponential cost increase is no longer true, due to

         - object-orientation, message-passing

         - simple design

         - automated tests

         - practice in modifying the system

       - easily change system later, defer decisions

    6. Learning to Drive

       - small changes, adjust course

       - driver is customer

    7. Four Values

       communication, simplicity, feedback, courage

       1. Communication

          - problem arise when people don't communicate

          - some steps make you communicate: unit testing, pair
            programming, task estimation

          - coach to reintroduce non-communicating members

       2. Simplicity

          - "what is the simplest thing that could possibly work?"

          - simplicity is hard

          - makes communication easier

       3. Feedback

          - from the system

            - minutes/days: unit tests (+ programmers give feedback to
              customer stories?)

            - weeks/months: functional tests ("simplified use cases")
              (+customers review schedule)

          - deploy early

          - "when all tests run, you're done"

       4. Courage

          - just do it

          - change system if it is broken

            - or if you have an idea how to improve

          - supported by

            - simplicity (easy to change), and

            - communication: (agree before that bad solution, improvement)

            - feedback: feel safer if you have tests

          - supports simplicity: try to simplify asap

       5. The Values in Practice

          - next: how to implement xp

    8. Basic Priciples

       - Rapid feedback: helps in learning

       - Assume simplicity: 98% when it helps give you free resources
         for other 2%

       - Incremental change: small changes at a time

       - Embracing change

       - Quality work: makes work more fun

       - (less central principles)

       - teach learning: self-responsibility

       - small initial investment: start small, focuses, enough to
         solve one interesting problem

       - play to win

       - Concrete experiments: test every abstract decision's
         (code+requirements)

       - open, honest communication

       - work with people's instinct, not against them:

       - accepted responsibility: of your own choosing (still part of team...)

       - local adaptation: adapt this to local conditions

       - travel light: keep only tests and code

       - honest measurement

         - to measurable scale ("two weeks" instead of "14.176 hours")

         - with sensible metrics (not lines of code, f.ex.)

    9. Back to Basics

       1. Coding

          - code to test thoughts: computer does exactly what you tell
            it to do

          - code to communicate (intent, algorithms, expansion, tests)

       2. Testing

          - faster than just coding

          - write all tests that you think could break

          - balance this with "tolerable error rates" (1 complaint/month f.ex.)

          - unit tests *and* functional tests

       3. Listening

          - understand "customer", business people, etc

          - active listening, tell them what's hard and what's easy

          - structure communication

            - which things, when, which detail

            - not things that don't help, before understanding, too detailed

       4. Designing

          - just three above make coding harder in the long run

          - solution: design

            - good design

              - organize logic

              - separates parts of system

              - "logic near the data it operates on"

              - "allows extension with changes in only one place"

    10. A Quick Overview

        practices that complement each other

        1. The Planning Game

           - business people decide

             - scope: how much of a problem must be solved: (too
               much/not enough)

             - priority: A or B, what first

             - composition of releases

             - date of releases: important dates for business

           - technical people decide

             - estimates: how long to implement feature

             - consequences: f.ex. database

             - process: work and team organization

             - detailed scheduling: in release, f.ex. high-risk stuff
               first

        2. Small Releases

           small, self-contained releases

        3. Metaphor

           - f.ex. computer appears as desktop

           - "words used to identify technical entities should be
             consistently taken from the chosen metphor"

        4. Simple Design

           - Rules

             1. Runs all tests

             2. No duplicate logic

             3. "States every intention important to programmers"

             4. Fewest possible pieces (classes, methods,...)

           - Tufte book: design graph, then take away everything that does
             not add information

        5. Testing

           - functional and unit tests

           - increases confidence in program over time

           - test "production methods that could possibly break"

           - try out without tests, then "throw away your code and
             start over with tests"

        6. Refactoring

           - duplicate code: system asking for refactoring

           - both before or after programming feature

        7. Pair Programming

           - keyboarder: mainly implementation, other: strategy

           - pairing is dynamic

        8. Collective Ownership

           - should add value to any portion of the code, "if it makes
             their life easier"

        9. Continous Integration

           - integrate/test after a few hours, a day at most

           - dedicated integration machine: get tests to 100%

        10. 40-Hour Week

            - keep programmers fresh and "full of fire and ideas"

            - overtime: at most one week

            - vacation: (minimum) two-week vacation per year, another
              week or two for shorter breaks

        11. On-Site Customer

            - customer: real user when in production

            - have time to do normal work, (although physically
              separated)

            - real project: single customer grudgingly to three
              customers

        12. Coding Standards

            - need a code standard

              - least amount of work possible

                - Once and Only rule (no duplicate code)

              - voluntarily adopted by team

    11. How Could This Work?

        look at each practice's flaws, how canceled out by another's
        strengths

        1. The Planning Game

           - customers update plan, based on programmers estimates

           - rough idea plan at beginning (couple of years possible)

           - short releases

           - customer in team

        2. Short Releases

           - ci packages anyways

           - testing reduces defect rate

           - planning game sets most important stories

           - simple design, enough for this one release, not all time

        3. Metaphor

           - need concrete feedback from real code and tests

           - customer needs to be ok talking like this

           - refactor to refine understanding of metaphor in practice

        4. Simple Design

           - refactor to simplify design

           - clear metaphor, so that development coverges (?)

           - programming with partner: simple, not stupid design

        5. Testing

           - simple design, less tests

           - partner ensure that you write tests

           - you and customer: good feeling that (both sets of) tests run

        6. Refactoring

           - collective ownership and coding standards

           - pairs are more courageous to refactor

           - simple design makes it easier

           - testing and c.i. assure correctness

        7. Pair Programming

           - coding standards, well-rested, simple design, metaphor
             help understanding

           - write tests together before coding to "align their
             understanding"

        8. Collective Ownership

           - c.i. reduces chance of conflict

           - test and pair programming reduce breakage

           - coding standards

        9. Continuous Integration

           - runs fast, as do tests

           - pairs halve change streams

           - refactoring makes single changes smaller

        10. 40-Hour Week

            - planning game select "more valuable work"

              - better to estimate in combination with testing

            - all practices increase speed

        11. On-Site Customer

            produce value by writing test, and making priority decisions

        12. Coding Standards

            - xp makes you winning team, so you can bend your style

        13. Conclusion

            testing stands well on its own, the others support each other

    12. Management Strategy

        - application of basic principles (see ch. 8): manager

          - highlights what needs to be done

          - trusts programmers to deliver quality work

          - suggests incremental improvements all along the way

          - adapts XP locally

          - does not impose lengthy meetings, etc

          - measures honestly

        - metrics

          - main management tool, use "Big Visible Chart"

          - set metrics for points which mostly need improvement (at
            most 3-4 metrics)

        - coaching

          - coach is most experienced programmer

          - helps newbies in development, sets refactoring goals,
            explains to upper management, helps with individual
            technical skills

          - toys (f.ex. timer) and food

        - tracking: update metrics, need to know planning game by heart

        - intervention: unloved roles

          - personnel changes: sooner rather than later

          - change process: let team decide possibilities, measure effect

          - kill the project

    13. Facilities Strategy

        - try out whatevery works

        - xp rather has too much communal space than too little

    14. Splitting Business and Technical Responsibility

        - if Business dominates: too much risk for too little return

        - if Dev dominates: the same

        - split responsibility, inform other party

        - business chooses: scope/timing of releases, relative
          priorities and exact scope of features

        - dev contributes: time estimates, alternatives and
          consequences, dev process and set of practices (and how to
          review)

        - technology stack is business decision, dev shows
          consequences (including costs to keep system alive)

        - mostly easy, if hard: reduce risk as far as possible, then
          rough it

    15. Planning Strategy

        0. [@0]

           - plan to next horizons: increasing granularity as they get closer

           - aim: team, set scope, estimate cost, confidence, feedback

        1. The Planning Game

           - rules should help in developing mutual trust

           - goal: maximize value of software (-development, -risk incurred)

           - strategy: produce most valuable first (while reducing risk)

           - pieces: story cards (number, description, notes, tracking)

           - players: business and development (both collectives)

             - business f.ex. expert users

           - moves

             1. exploration

                - business writes a story,

                - developers estimate a story

                - (or, if impossible, ask for /split/ or /clarification/)

             2. commitment

                - sort by value (business): (1) essential, (2) significant
                  business value, (3) nice to have

                - sort by risk (development): (1) estimate precisely,
                  (2) estimate reasonably well, (3) can not estimate at all

                - set velocity (development): Ideal Engineering Time/month

                - choose scope (business): set date and choose cards,
                  or choose cards and calculate date

             3. steer

                - iteration: business selects story

                - recovery: development realizes velocity off, asks
                  business for correction: most important stories

                - new story: b writes, d estimates, b removes equivalent stories

                - reestimate: d can reestimate

        2. Iteration Planning

           1. Exploration: write task (part of story), or split/merge

           2. Commitment

              - programmer accepts task

              - estimate: programmer estimates ideal engineering days (=IEdays)

              - set load factors: programmer chooses load factor
                (=IEdays/iteration) based on previous ierations

              - balance: developers with too much (7-8 or 2-3 for new
                members) give up some (if team has too much, reestimate)

           3. Steering

              - implement take task card, find partner, write test,
                make work, integrate and if all tests release

              - record progress: one team member asks programmers
                "every two or three days" how much spent and much left

              - recovery: if overcommitted: (1) reduce task scope, (2)
                c reduces story scope, (3) shed nonessential tasks,
                (4) better help, (5) ask c to defer stories

              - verify: check functional tests

           4. notes on iteration planning:

              - here: take task first then estimate

              - unnecessary for two programmers, necessary for 10

        3. Planning in a Week (fixed-price tender)

           - previous experience

           - rough plan

    16. Development Strategy

        Normal programming plus iteration planning, CI, collective
        ownership, and pair programming.

        1. Continuous Integration

           - daily integration of all code

           - needs to be fast:

             - "reasonably complete" tests run in a matter of minutes

             - refactoring provides small classes and methods, easier
               to integrate

           - "dramatically reduces" project risk

           - one thing on your mind, until finished

        2. Collective Ownership

           - would fail without tests and ci

           - complex code gets simplified, if possible (otherwise
             simplification gets thrown away, but more people
             understand why needs to be complex)

             - complex code avoided in the first place

           - spreads knowledge of system around the team

        3. Pair Programming

           - simultaneous programming, dialog

           - not tutoring

           - spreads information throughout the team

           - more productive in his experience

           - suggestion: get good at it, try one iteration with, one
             without

           - higher code quality

           - ensures tests, refactoring, integrating

    17. Design Strategy

        1. The Simplest Thing That Could Possibly Work

           - four values

             - communication

               - easier to talk about simple design

               - design that communicates "important aspects of the
                 system"

             - simplicity: simple system and simple design strategy

             - feedback: be done quickly, code it and see how it appears

             - courage: stop when done, continue as needed (see quote)

           - aim: simple, quickly verifyable, with feed back, quick cycle

           - write test, write code, repeat, make simple if possible (see quote)

        2. How Does "Designing Through Refactoring" Work?

           - pick a test

           - implement it

           - pick the next test

           - maybe refactor to include design for next test

           - implement it

           - after a few steps, more team members are possible

           - from time to time refactoring days (see quote)

        3. What Is Simplest?

           1. code + tests communicate everything needed (once)

           2. no duplicate code (only once)

           3. "fewest possible classes"

           4. "fewest possible methods"

        4. How Could This Work?

           - reduces unnecessary design, which has

             - decision cost

             - upkeep cost

             - inertia added

        5. Role of Pictures in Design

           - pictures can simplify design, but do not give test feedback

           - few pictures, quickly find if on target or not

           - use them to win, not to stall

           - encourage pics from people who are good at it

           - toss pictures once code is written (travel light)

           - if best expressed as pictures, use a CASE tool f.ex.

        6. System Architecture

           - stories and metaphor create architecture

           - it evolves over time, bit by bit

    18. Testing Strategy

        0. [@0]

           - confidence manifest as tests

             - developer and customer

           - write tests that pay off

             - work when you don't expect them to

             - don't work when you expect them to

             - learn from both of these

        1. Who Writes Tests?
*** quotes
    00. [@00] unsorted
        - Use XP when requirements are vague or changing
        - Since feedback from system in production is vastly higher
          quality than any other kind of feedback, giving a project too
          much time will hurt it.
    0. [@0] Foreword by Erich Gamma
       - Delivering software is hard, and delivering quality software
         in time is even harder.
    0. [@0] Preface
       - In short, XP promises to reduce project risk, improve
         responsiveness to business changes, improve productivity
         throughout the life of a system, and add fun to building
         software in teams --- all at the same time. Really. Quit
         laughing. Now you'll have to read the rest of the book to see
         if I'm crazy.
    1. Risk: The Basic Problem
       - loneliness that is often at the heart of job dissatisfaction
    6. [@6] Learning to Drive
       - The driver of a software project is the customer.
       - Our job as programmers is to give the customer a steering
         wheel and give them feedback about exactly where we are on
         the road.
    7. Four Values
       - Short-term individual goals often conflict with long-term
         social goals.
       - Optimism is an occupational hazard of programming. Feedback
         is the treatment.
    8. Basic Priciples
       - Even the adoption of XP must be taken in little steps.
       - Most software development I see is played not to lose.
    9. Back to Basics
       1. Coding
          - For a system to live, it must retain its source code.
       2. Testing
          - The English Positivist philosophers Locke, Berkeley, and
            Hume said that anything that can't be measured doesn't
            exist. When it comes to code, I agree with them
            completely.
          - You are responsible for writing every test that you can
            imagine won't run immediately.
          - If you keep writing the tests, your confidence in the
            system increases over time.
          - Programming and testing together is also faster than just
            programming.
       3. Listening
          - programmers don't know anything that business people think
            is interesting
    10. A Quick Overview
        1. Refactoring
           - When the system requires that you duplicate code, it is
             asking for refactoring.
        2. 40-Hour Week
           - Overtime is a symptom of a serious problem on the
             porject.
           - If you come in Monday and say, "To meet our goals, we'll
             have to work late again," then you already have a problem
             that can't be solved by working more hours.
    12. [@12] Management Strategy
        - People inevitably get off on tangents.
        - If a metric gets close to 100%, replace it with another that
          starts comfortably down in the single digits.
        - Gathering real development data twice a week is plenty.
        - As soon as you can't think of any scenario in which the
          offender would be a help rather than a hindrance, you should
          make the move.
    14. [@14] Splitting Business and Technical Responsibility
        - Cutting edge implies risk.
    15. Planning Strategy
        1. The Planning Game
           - A simple method for estimating stories is to ask yourself
             "How long would this take me to implement if this story
             was all I had to implement, and I had no interruptions or
             meetings?" In XP we call this /Ideal Engineering Time/.
    16. Development Strategy
        1. Continuous Integration
           - At the end of every development episode, the code is
             integrated with the latest release and all the tests must
             run at 100%.
           - Constant refactoring has the effect of breaking the
             system in to lots of little objects and lots of little
             methods.
           - Learn/test/code/release. It's almost like breathing. You
             form an idea, you express it, you add it to the
             system. now your mind is clear, ready for the next idea.
        2. Collective Ownership
           - On an XP project you are never stuck with someone else's
             stupidity. You see something in the way, you get it out
             of the way.
        3. Pair Programming
           - It is a conversation at many levels, assisted by and
             focused on a computer.
    17. Design Strategy
        0. [@0]
           - We will continually refine the design of the system,
             starting from a very simple beginning. We will remove any
             flexibility that doesn't prove useful.
           - always have the simplest design that runs the current test
             suite.
        1. The Simplest Thing That Could Possibly Work
           - Courage --- What could be more courageous than stopping
             after a little bit of design, confident that when the
             time commes, you can add more, when and as needed?
           - design strategy.
             1. Start with a test, so we will know when we are
                done. We have to do a certain amount of design just to
                write the test: What are the objects and their visible
                methods?
             2. Design and impelment just enough to get that test
                running. You will have to design enough of the
                implementation to get this test and all previous tests
                running.
             3. Repeat.
             4. If you ever see the chance to make the design simpler,
                do it. See the subsection What Is Simplest? for a
                definition of the principles that drive this.
           - The first use only pays what it must. The second use
             pays for flexibility.
        2. How Does "Designing Through Refactoring" Work?
           - The team gets together for a day and restructures the
             system as a whole using a combination of CRC cards,
             sketches, and refactoring.
        3. What Is Simplest?
           - [...] eliminate all the duplicated logic in the
             system. This is the hardest part of design for me,
             because you first have to find the duplication, and then
             you have to find a way to eliminate it.
           - [view process] as erasure. You have a system that runs
             the test cases. You delete everything that doesn't have a
             purpose --- either a communication purpose or a
             computational purpose.
        4. How Could This Work?
           - In fact, that is what XP concludes. "Sufficient to the
             day are the troubles thereof."
    18. Testing Strategy
        0. [@0]
           - You should write the tests that help get programs working
             and keep programs working. Nothing more.
*** ref [[file:master.bib::xp][Beck 1999: Extreme Programming Explained]]
** TODO [#B] [[./Axelsson.pdf][Axelsson - The Base-Rate Fallacy and its Implications for the Difficulty of Intrusion Detection]]
*** TODO understand base rate fallacy intellectually
** TODO [#B] PART [[./hastie.pdf][Hastie - The Elements of Statistical Learning]]          :ML:
*** TODO Sections 5.8, 14.5.4, 18.5 and Chapter 12
*** TODO skim chapters 2.9, 3.3, 6.7, 7 (esp 7.10), 9, 12 and 13, 15, 16
    (leave out 11,14,17)
    - Radial basis functions are discussed in more detail in Section 6.7.
      - also recommended in 2.8.3: how to determine all parameters
*** TODO [#B] skim Section 13.3 (knn)
*** TODO [#C] sort unsorted quotes
*** summary
    1. Introduction

       examples

    2. Overview of Supervised Learning

       1. inputs

          == predictors (statistics)

          == independent variables (statistics)

          == features (pattern recognition)

          outputs

          == responses (statistics)

          == dependent variables (statistics)

       2. Variable Types and Terminology

          - qualitative aka discrete aka categorical variables aka factors

            example: handwritten digits, fisher's irises

          - regression for quantitative outputs Y, classification for
            qualitative outputs G

       3. Two Simple Approaches to Prediction: Least Squares and
          Nearest Neighbors

          0. [@0]

             - linear model

               - "stable but possibly inaccurate predictions"

             - k-nearest neighbors

               - "often accurate but can be unstable"

          1. Linear Models and Least Squares

             "solve" the quadratic equation in quotes by finding a
             minimum

          2. Nearest-Neighbor Methods

             - majority of the closest (by specific metric) k neigbors
               are of the class

             - k=1 - might lead to overfitting

          3. From Least Squares to Nearest Neighbors

             - defined later

               - variance: ca schwankung

               - bias: ca vorannahmen

             - ls: high bias, low variance

             - nn: low bias, high variance

             - knn - degrees of freedom: N / k

             - many models today are variations of knn and the linear
               model

       4. Statistical Decision Theory

          - minimize expected loss

            - knn: localized minimization

            - linear model: assume linearity

            - expected loss (for regression):

              - squared (L2) most often,

              - L1 has discontinuties in derivative (=nicht stetig diffbar)

            - expected loss for classification:

              - often zero-one loss function

              - results in Bayes classifiers

            - classification similar to regression

              - knn: localized conditional probability, estimated by
                training sample proportions

              - linear: map to dummy variable

       5. Local Methods in High Dimensions

          - unbiased methods like problematic supposedly more
            problematic, as dimension rises

       6. Statistical Models, Supervised Learning and Function
          Approximation

          0. [@0]

             Try to approximate the I/O relationship f(x) via f\circ(x)

             1. A Statistical Model for the Joint Distribution Pr(X, Y )

                - cont p. 47

       8. [@8] Classes of Restricted Estimators

          overlapping classes with some restriction

          1. Roughness Penalty and Bayesian Methods

             - penalize RSS (see [[(esl-2.3)]]) like
               #+BEGIN_SRC latex
                 PRSS(f; λ) = RSS(f) + λJ(f) (ref:esl-2.38)
               #+END_SRC
               with J(f) bigger the more f can vary over small spaces

             - see chapter 5

          2. Kernel Methods and Local Regression

             - local neighborhood optimized

             - f.ex. gaussian kernel, see [[(esl-2.40)]]

             - RSS in this case
               #+BEGIN_SRC latex
                 RSS(f_\theta, x_0) = \sum_{i=1}^N K_\lambda(x_0, x_i)(y_i -f_\theta(x_i))^2 (ref:esl-2.42)
               #+END_SRC

               - f_\theta(x) = \theta_0 yields Nadaraya-Watson estimate (esl-2.41)

               - f_\theta(x) = \theta_0 + \theta_1 x: local linear regression

          3. Basis Functions and Dictionary Methods

             - f(x) = \sum \theta_i h_i(x)

               - h_i could be spline parts, polynomials, etc

                 - radial basis functions: h_i = K_{\lambda_i}(\mu_i, x)

                   - f.ex. Gaussian = e^{-||x - \mu||^2}/ (2\lambda)

                 - neural network as composed of basis functions

       9. Model Selection and the Bias–Variance Tradeoff

    4. [@4] Linear Methods for Classification

       1. Introduction

          - split with linear methods

            - decision boundaries are linear

          - if model is f_k(x) = \beta_k_0 + \beta_k^T \cdot x, then

            - boundary between classes k and l is affine set (hyperplane)
              {x : ( \beta_k_0 − \beta_l_0 ) + ( \beta_k - \beta_l )^T \cdot x = 0}

          - generalization: add variables, example: adding squares and
            cross-products to input data gives you quadratic
            boundaries

       4. [@4] Logistic Regression

          model posterior probabilities linearly (?)

          1. Fitting Logistic Regression Models

             - multinomial distribution: different outcomes,
               "completely specif[ied]"

             - cont p. 120

       5. Separating Hyperplanes

          cont p. 148

    5. Basis Expansions and Regularization

       1. Introduction

          - beyond linearity: replace/augment inputs with additional
            variables/transformations of input

          - given replacement, models remain linear

          - increases variability, reduce by

            - limiting class of functions

            - selection methods: scan which functions contribute significantly

            - regularization methods: limit function coefficients

       2. Piecewise Polynomials and Splines

          cont. p. 160

       8. [@8] Regularization and Reproducing Kernel Hilbert Spaces

          general regularization problems, see [[(esl-5.42)]]

          1. Spaces of Functions Generated by Kernels

    6. Kernel Smoothing Methods

       7. [@7] Radial Basis Functions and Kernels

          - combination of (local) gaussian functions

            - normalized to be able to fit all values

          - solve:

            - either solve for all parameters at once, or

            - fix/metaestimate some parameters, solve RSS for the last (here: \beta)

    7. Model Assessment and Selection

       1. Introduction

          - how well a model learns the basic concept:
            /generalization/ performance/

       2. Bias, Variance and Model Complexity

          - loss function measures error

          - test error is error of prediction function on independent
            test sample T

          - expected prediction error is same over all variables

       10. [@10] Cross-Validation

           2. [@2] The Wrong and Right Way to Do Cross-validation

    9. [@9] Additive Models, Trees, and Related Methods

       continues ch.3-6

       1. Generalized Additive Models

          - related to logistic regression (4.4)

          - cont p. 315

    12. [@12] Flexible Discriminants

        1. Introduction

           - generalize linear decision boundaries

           - SVMs construct linear boundary in "large, transformed" space

           - linear discriminant analysis (LDA), 3 variants

             - flexible DA: similar to svm

             - penalized DA: singal/image classification, highly correlated

             - lmixture DA: irregularly shaped classes

        2. The Support Vector Classifier

           cont p.436

        3. Support Vector Machines and Kernels

           0. [@0]

              - linear classifiers seperate input by hyperplanes

              - enlarging feature space via =basis expansion= allow
                nonlinear decision boundaries in the original space

              - expand this idea via SVM: large/infinite dimension

           1. Computing the SVM for Classification
*** quotes
    0. [@0] Preface to the Second Edition
       - Our first edition was unfriendly to colorblind readers; in
         particular, we tended to favor red/green contrasts which are
         particularly troublesome. We have changed the color palette
         in this edition to a large extent, replacing the above with
         an orange/blue contrast.
       - Chapter 18 explores the “p ≫ N ” problem, which is learning in
         high-dimensional feature spaces. These problems arise in many
         areas, including genomic and proteomic studies, and document
         classification.
    0. [@0] unsorted
       - 15 Random Forests
       - 18 High-Dimensional Problems: p ≫ N
       - 18.3.3 The Support Vector Classifier
       - (ends Contents)
       - Average percentage of words or characters in an email message
         equal to the indicated word or character. We have chosen the
         words and characters showing the largest difference between spam
         and email.
         |       | george |  you | your |   hp | free |  hpl |    ! |  our |   re |  edu | remove |
         |-------+--------+------+------+------+------+------+------+------+------+------+--------|
         | spam  |   0.00 | 2.26 | 1.38 | 0.02 | 0.52 | 0.01 | 0.51 | 0.51 | 0.13 | 0.01 |   0.28 |
         | email |   1.27 | 1.27 | 0.44 | 0.90 | 0.07 | 0.43 | 0.11 | 0.18 | 0.42 | 0.29 |   0.01 |
       - This is a classification problem for which the error rate needs
         to be kept very low [...] In order to achieve this low error
         rate, some objects can be assignedto a “don’t know” category,
         and sorted instead by hand.
       - splines, wavelets and regularization/penalization methods for a
         single predictor, [...] kernel methods and local
         regression. Both of these sets of methods are important building
         blocks for high-dimensional learning techniques.
       - At the end of each chapter we discuss computational considerations
       - Chapters 1–4 be first read in sequence. Chapter 7 should also be
         considered mandatory, as it covers central concepts that pertain
         to all learning methods.
    2. [@2] Overview of Supervised Learning
       1. Introduction
          - In the pattern recognition literature the term /features/
            is preferred, which we use as well.
       3. [@3] Two Simple Approaches to Prediction: Least Squares and
          Nearest Neighbors
          1. Linear Models and Least Squares
             - predict the output Y via the model

               Ŷ = β̂_{0} + \sum_{j=1}^{p} X_{j} β̂_{j} . (2.1)
             - in vector form as an inner product

               Ŷ = X^{T} β̂, (2.2)
             - least squares. In this approach, we pick the coefficients β to
               minimize the *residual sum of squares* (RSS)
               #+BEGIN_SRC latex
                 RSS(β) = \sum_{i=1}^{N} (y_{i} − x^{T}_{i} β)^{2}. (ref:esl-2.3)
               #+END_SRC
             - in matrix notation. We can write
               #+BEGIN_SRC latex
                 RSS(β) = (y − Xβ)^{T} (y − Xβ), (ref:esl-2.4)
               #+END_SRC
             - the unique solution is given by
               #+BEGIN_SRC latex
                 β̂ = (X^{T} X)^{−1} X^{T} y, (ref:esl-2.6)
               #+END_SRC
             - In the case of one Gaussian per class, we will see in Chapter 4
               that a linear decision boundary is the best one can do, and that
               our estimate is almost optimal. The region of overlap is
               inevitable, and future data to be predicted will be plagued by
               this overlap as well.
          3. [@3] From Least Squares to Nearest Neighbors
             - linear decision boundary [...] has low variance and potentially
               high bias.
             - k-nearest-neighbor [...] high variance and low bias.
             - 1-nearest-neighbor, the simplest of all, captures a large
               percentage of the market for low-dimensional problems.
             - Kernel methods use weights that decrease smoothly to zero with
               distance from the target point, rather than the effective 0/1
               weights used by k-nearest neighbors.
             - Projection pursuit and neural network models consist of sums of
               nonlinearly transformed linear models.
             - (ends 2.3.3)
       4. Statistical Decision Theory
          - X ∈ R^{p} input
          - Y ∈ R output
          - function f(X) for predicting Y
          - /squared error loss/: L(Y, f(X)) = (Y − f (X))^{2} .
          - EPE(f) = E(Y − f (X))^{2} (2.9)

            = \int [y − f (x)]^{2} Pr(dx, dy) (2.10)

            the /expected (squared) prediction error/
          - The solution is

            f (x) = E(Y |X = x), (2.13)

            the conditional expectation, also known as the regression
            function. Thus the best prediction of Y at any point X = x is
            the conditional mean, when best is measured by average squared
            error.
       5. Local Methods in High Dimensions
          - In ten dimensions we need to cover 80% of the range of each
            coordinate to capture 10% of the data.
       6. Statistical Models, Supervised Learning and Function
          Approximation
          - Thus it is not surprising that the analysis of high-dimensional
            data requires either modification of procedures designed for the
            N > p scenario, or entirely new procedures
       8. [@8] Classes of Restricted Estimators
          2. [@2] Kernel Methods and Local Regression
             - the Gaussian kernel has a weight function based on the
               Gaussian density function
               #+BEGIN_SRC latex
                 K_\lambda(x_0, x) = 1/\lambda exp[-||x - x_0||^2 / 2\lambda] (ref:esl-2.40)
               #+END_SRC
               and assigns weights to points that die exponentially with
               their squared Euclidean distance from x_0.
          3. Basis Functions and Dictionary Methods
             - Radial basis functions have centroids μ_m and scales
               λ_m that have to be determined. The spline basis
               functions have knots. In general we would like the data
               to dictate them as well. Including these as parameters
               changes the regression problem from a straightforward
               linear problem to a combinatorially hard nonlinear
               problem. In practice, shortcuts such as greedy
               algorithms or two stage processes are used.
    4. [@4] Linear Methods for Classification
       1. Introduction
          - Strictly speaking, a hyperplane passes through the origin,
            while an affine set need not. We sometimes ignore the
            distinction and refer in general to hyperplanes.
          - the *logit* transformation: log[p/(1−p)]
       4. [@4] Logistic Regression
          - model the posterior probabilities of the K classes via
            linear functions in x, while at the same time ensuring
            that they sum to one and remain in [0, 1].
    5. Basis Expansions and Regularization
       8. [@8] Regularization and Reproducing Kernel Hilbert Spaces
          - A general class of regularization problems has the form
            #+BEGIN_SRC latex
              min_{f \in H} [ \sum_{i=1}^N L(y_i, f(x_i)) + \lambda J(f) ] (ref:esl-5.42)
            #+END_SRC
            where L(y, f (x)) is a loss function, J(f ) is a penalty
            functional, ...
    6. Kernel Smoothing Methods
       7. [@7] Radial Basis Functions and Kernels
          - Kernel methods achieve flexibility by fitting simple
            models in a region local to the target point x_0.
    7. Model Assessment and Selection
       2. [@2] Bias, Variance and Model Complexity
          - The *loss function* for measuring errors between Y and
            f^(X) is denoted by L(Y, fˆ(X)).
          - Typical choices are
            #+BEGIN_SRC latex
              L(Y, fˆ(X)) = (Y - f^(X))^2 (squared error) 
              L(Y, fˆ(X)) = |Y - f^(X)| (absolute error) (ref:esl-7.1)
            #+END_SRC
          - *Test error*, also referred to as *generalization error*,
            is the prediction error over an independent test sample
            #+BEGIN_SRC latex
              Err_{\mathcal T} = E[L(Y, fˆ(X))|\mathcal T] (ref:esl-7.2)
            #+END_SRC
          - expected prediction error (or expected test error)
            #+BEGIN_SRC latex
              Err = E[L(Y, fˆ(X))] = E[Err_{\mathcal T}] (ref:esl-7.3)
            #+END_SRC
    9. [@9] Additive Models, Trees, and Related Methods
       1. Generalized Additive Models
          - the *binary response* μ(X) = Pr(Y = 1|X)

    10. Boosting and Additive Trees
        6. [@6] Loss Functions and Robustness
           - One such criterion is the Huber loss criterion used for
             M-regression (Huber, 1964)

             L(y, f(x)) = \choose (ref:esl-10.23)
                 [y - f(x)]^2                       for |y - f(x) | \le \delta
                 2\delta |y-f(x)| -d \delta^2                 otherwise
        7. “Off-the-Shelf” Procedures for Data Mining
           - They [decision trees] are relatively *fast to construct*
             and they produce *interpretable models* (if the trees are
             small). As discussed in Section 9.2, they naturally
             incorporate mixtures of numeric and categorical predictor
             variables and missing values. They are invariant under
             (strictly monotone) transformations of the individual
             predictors. As a result, *scaling* and/or more general
             transformations are *not an issue*, and they are immune
             to the effects of predictor outliers. They perform
             internal feature selection as an integral part of the
             procedure. They are thereby *resistant*, if not
             completely immune, *to* the inclusion of many *irrelevant
             predictor variables*. These properties of decision trees
             are largely the reason that they have emerged as the most
             popular learning method for data mining.
           - Some characteristics of different learning methods. Key:
             +=good, o=fair, and -=poor
             #+BEGIN_EXAMPLE
|-----------------------------+--------+-----+-------+------+---------|
| Characteristic              | Neural | SVM | Trees | MARS | k-NN,   |
|                             | Nets   |     |       |      | Kernels |
|-----------------------------+--------+-----+-------+------+---------|
| Natural handling of data    |        |     |       |      |         |
| of “mixed” type             | -      | -   | +     | +    | -       |
| Handling of missing values  | -      | -   | +     | +    | +       |
| Robustness to outliers in   |        |     |       |      |         |
| input space                 | -      | -   | +     | -    | +       |
| Insensitive to monotone     |        |     |       |      |         |
| transformations of inputs   | -      | -   | +     | -    | -       |
| Computational scalability   |        |     |       |      |         |
| (large N )                  | -      | -   | +     | +    | -       |
| Ability to deal with irrel- |        |     |       |      |         |
| evant inputs                | -      | -   | +     | +    | -       |
| Ability to extract linear   |        |     |       |      |         |
| combinations of features    | +      | +   | -     | -    | o       |
| Interpretability            | -      | -   | o     | +    | -       |
| Predictive power            | +      | +   | -     | o    | +       |
|-----------------------------+--------+-----+-------+------+---------|
             #+END_EXAMPLE
    13. [@13] Prototype Methods and Nearest-Neighbors
        1. Introduction
           - simple and essentially model-free methods for
             classification and pattern recognition.
           - often among the best performers in real data
        2. Prototype Methods
           0. [@0]
              - Prototype methods represent the training data by a set of
                points in feature space. These prototypes are typically
                not examples from the training sample, except in the case
                of 1-nearest-neighbor classification discussed later.
           1. K-means Clustering
    15. [@15] Random Forests
        1. Introduction
           - a committee of trees each cast a vote for the predicted
             class
           - Boosting [...], although unlike bagging, the committee of
             weak learners evolves over time, and the members cast a
             weighted vote.
           - Random forests (Breiman, 2001) is a substantial
             modification of bagging that builds a large collection of
             de-correlated trees, and then averages them.
        2. Definition of Random Forests
           - Trees are ideal candidates for bagging, since they can
             capture complex interaction structures in the data, and
             if grown sufficiently deep, have relatively low bias.
           - trees are notoriously noisy, they benefit greatly from
             the averaging.
           - the bias of bagged trees is the same as that of the
             individual trees
           - the only hope of improvement is through variance
             reduction.
           - Not all estimators can be improved by shaking up the data
             like this. It seems that highly nonlinear estimators,
             such as trees, benefit the most.
           - random forests do remarkably well, with very little
             tuning required.
        3. Details of Random Forests
           0. [@0]
              - When used for classification, a random forest obtains a
                class vote from each tree, and then classifies using
                majority vote (see Section 8.7 on bagging for a similar
                discussion). When used for regression, the predictions
                from each tree at a target point x are simply averaged
           1. Out of Bag Samples
              - An important feature of random forests is its use of
                out-of-bag (oob) samples For each observation z_i =
                (x_i , y_i), construct its random forest predictor by
                averaging only those trees corresponding to bootstrap
                samples in which z_i did not appear.  An oob error
                estimate is almost identical to that obtained by
                N-fold cross-validation; [...] Hence unlike many other
                nonlinear estimators, random forests can be fit in one
                sequence, with cross-validation being performed along
                the way.
           2. Variable Importance
              - Variable importance plots can be constructed for
                random forests in exactly the same way as they were
                for gradient-boosted models (Section 10.13).
              - The left plot bases the importance on the Gini
                splitting index, as in gradient boosting. The rankings
                compare well with the rankings produced by gradient
                boosting (Figure 10.6 on page 354). The right plot
                uses oob randomization to compute variable
                importances, and tends to spread the importances more
                uniformly.
           3. Proximity Plots
              - The idea is that even though the data may
                behigh-dimensional, involving mixed variables, etc.,
                the proximity plot gives an indication of which
                observations are effectively close together in the
                eyes of the random forest classifier.
              - often look very similar, irrespective of the data,
                which casts doubt on their utility.
           4. Random Forests and Overfitting
              - When the number of variables is large, but the
                fraction of relevant variables small, random forests
                are likely to perform poorly with small m.
              - When the number of relevant variables increases, the
                performance of random forests is surprisingly robust
                to an increase in the number of noise variables.
        4. Analysis of Random Forests
           2. [@2] Variance and the De-Correlation Effect
              - the variance of the ensemble is dramatically lower
                than this tree variance.
              - the bias of a random forest is the same as the bias of
                any of the individual sampled trees T (x; Θ(Z)):
              - the improvements in prediction obtained by bagging or
                random forests are /solely a result of variance
                reduction/.
           3. Adaptive Nearest Neighbors
              - The random forest classifier has much in common with
                the k-nearest neighbor classifier (Section 13.3);
              - the name “random forests”, is exclusively licensed to
                Salford Systems for commercial release.
*** ref [[file:master.bib::esl][Hastie et al. 2008: Elements Statistical Learning]]
** TODO [#B] PART [[./2009-diss.pdf][Rieck - Machine Learning for Application-Layer Intrusion Detection]]
*** TODO [#B] skim for quotes
     [2016-11-23 Mi]
     [[file:~/da/git/diplomarbeit.org::*Acknowledgements][Acknowledgements]]
*** summary
    0. [@0] Acknowlegements

    0. [@0] Summary

       - IDS use attack signatures

       - contributions

         - network payloads in vector spaces

         - kernel functions to network data

         - normality characterized by hyperspheres and neighborhoods

         - significantly outperforms SNORT

    1. Introduction

       1. Machine Learning

          0. [@0]

             - handwritten digits example: different shapes

             - mitchell def: learning model \theta, error function
               E(f_{\theta}), prediction function f_{\theta}

             - sources: too broad, restricted to directly usable stuff

          1. Generalization and Regularization

             - can only minimize /empirical error/ E_{n}(f_{\theta})(error
               on training set),

               - but not sufficient to do so: f.ex. due to unkonwn attacks in
                 training data

             - relation empirical to expected error f.ex. modeled by
               "structural risk minimization by Vapnik (1995)"

               - E(f_{\theta}) \le E_{n}(f_{\theta}) + H(F, ...)

                 F is function class of f

             - prevent both underfitting and overfitting

          2. Discriminative and Generative Models

             - discriminative: learn from only data vectors, find
               boundaries etc (f.ex. SVM, boosting, etc)

             - generative: need to know how the data is created
               (f.ex. HMM), needs additional information

               - if poorly specified, less effective than discriminative

       2. Thesis Contributions
       3. Thesis Organization
    2. Feature Extraction at Application Layer
       1. Network Layers
          1. The TCP/IP Model
          2. The Application Layer
       2. Feature Maps
       3. Numerical Features for Payloads
          1. Normalization
       4. Sequential Features for Payloads
          1. Embedding Languages
          2. Feature Maps using Embedding Languages
       5. Syntactical Features for Payloads
          1. Protocol Grammars and Parse Trees
          2. Embedding Sets
          3. Feature Maps using Embedding Sets
       6. Related Work
          1. Vectorial features
          2. Sequential features
          3. Syntactical features
    3. From Network Features to Kernels
       1. Kernel Functions
          1. Geometry in Feature Space
          2. Designing Kernel Functions
       2. Kernels for Sequences
          1. Implementation using Sorted Arrays
          2. Implementation using Suffix Trees
          3. Run-time Performance of Sequence Kernels
       3. Kernels for Trees
          1. Convolution Kernels for Trees
          2. Implementation using Dynamic Programming
          3. Approximate Kernels for Trees
          4. Run-time Performance of Tree Kernels
       4. Normalization of Kernels
       5. Related Work
          1. Kernels for Sequences
          2. Kernels for Trees
    4. Learning for Intrusion Detection
       1. Machine Learning and Intrusion Detection
          1. Anomaly Detection
       2. Anomaly Detection using Hyperspheres
          1. Center of Mass
          2. One-Class Support Vector Machines
          3. Implementation
       3. Anomaly Detection using Neighborhoods
          1. Gamma Anomaly Score
          2. Zeta Anomaly Score
          3. Implementation
       4. Retraining and Calibration
          1. Manipulation Defense
          2. Calibration
       5. Visualization and Explainability
          1. Feature Differences
          2. Feature Coloring
       6. Related Work
          1. Global Anomaly Detection
          2. Local Anomaly Detection
          3. Visualization
    5. Empirical Evaluation and Applications
       1. Evaluation Data and Setup
          1. Evaluation Data
          2. Attack Description
          3. Evaluation Setup
       2. Detection Performance
       3. Comparison with State of the Art
       4. Robustness and Mimicry
          1. Attacks in Training Data
          2. Mimicry Attacks
       5. Run-time Performance
          1. Learning and Prediction Time
          2. Run-time Performance of Sandy
       6. An Application Scenario
    6. Conclusions
       1. Summary of Results
       2. Application Domains
       3. Future Work
    7. Appendix
       ...
*** quotes
    0. [@0]
       - Misuse detection as employed in current network security
         products relies on the timely generation and distribution of
         so called attack signatures.
    1. Introduction
       0. [@0]
          - the Internet protocol suite (Leiner et al., 1985)
          - While in 1998 the Computer Emergency Response Team (CERT) at
            Carnegie Mellon University reported 3,734 security incidents
            worldwide, there are no statistics for 2008, simply because
            the number of incidents has grown beyond limits.
          - Although signature-based detection provides effective defense
            against known attacks, it inherently lags behind attack
            development and fails to protect from unknown and novel
            threats.
       1. Intrusion Detection
       2. Machine Learning
          - learning aims at generalizing provided data to allow for
            accurate predictions on unseen instances.
          - minimizing the empirical error is not sufficient for
            learning accurate models
    2. Feature Extraction at Application Layer
       3. [@3] Numerical Features for Payloads
          1. Normalization
             - For each dimension i the original value φ_i(x) is
               shifted and scaled using the maximum max_i and minimum
               min_i value of i. Thus, we refer to this technique as
               min-max normalization:

               φ̄_{i} (x) = (\phi_i(x) - min_i) / (max_i - min_i)  (ref:rieck-2.3)
    4. [@4] Learning for Intrusion Detection
       1. Machine Learning and Intrusion Detection
          - Machine learning deals with automatically inferring and
            generalizing dependencies from data.
    5. Empirical Evaluation and Applications
       1. Evaluation Data and Setup
          3. [@3] Evaluation Setup
             - To simulate the presence of unknown attacks, the set of
               attacks is randomly split into known attacks for
               validation and unknown attacks for testing. That is, no
               attack tagged as “unknown” (including all variants) is
               available during training and validation. Learning
               methods are then applied to the training set for
               learning a model of normality, where model parameters
               such as the length of q-grams are adjusted on the
               validation set and the known attacks. Finally, the
               detection performance is measured on the testing set
               and the unknown attacks using the best model determined
               during validation.
             - The performance of intrusion detection method depends
               on two basic measures: the number of detected attacks
               (i.e., the true-positive rate) and the number of false
               alarms (i.e., the false-positive rate).
*** ref [[file:master.bib::rieckdiss][Rieck 2009: Machine Learning Application]]
** TODO [#B] [[./morphing09.pdf][Wright - Traffic Morphing: An Efficient Defense Against Statistical Traffic Analysis]]
*** summary
    0. [@0] Abstract

       morphing changes traffic to look like another site

    1. Introduction

       convex optimization to simulate (morph to) other site

       split etc packets to create the appearance of another site

       here: only packet size features obfuscated

    2. Related Work

       - Song: SSL timing analysis for login password length

       - Sun: SSL - use size of HTML objects to identify web pages

       - Liberatore: with persisten connections and SSH port forwarding

       - Wright: VoIP language spoken

       - Saponas: identify movies within encrypted connections

       - Wright: identify spoken phrases in encrypted VoIP

       - Wagner/Dean: mimicry attack vs IDS

       - Soto: against pH IDS

       - Tan: against stide IDS

       - Fogla: packet payloads, hide malicious payload

       - many: fake training data

    3. Traffic Morphing

       0. [@0]

          - alter "distribution of packet sizes"

       1. What is the Matrix?

          - morphing matrix: given input probability sizes X and
            output probability sizes Y, find matrix A such that Y=AX

          - receive packet of size s_j, sample target size s_i from j-th
            column of A via cumulative probability

          - pads with zeros if s_i > s_j, split otherwise

          - convex optimization to get minimal overhead

       2. Morphing via Convex Optimization

          - n^2 variables, 2n bounds \to infinite possibilities

          - cost function example: number of additional bytes

       3. Additional Morphing Constraints

          - if overspecified, use multilevel programming

          - step-by-step to solution

       4. Dealing with Large Sample Spaces

          - divide and conquer: divide matrix into submatrices, optimize these

       5. Practical Considerations

          1. Short Network Sessions

             f.ex. http: continue until some equality is reached

          2. Variations in Source Distribution

             website might give different distributions (hot/ cold)

             solution: meta-matrix (with clustering) which determines
             which matrix to use

          3. Reducing Packet Sizes

             better to also split packets. Easy with voip, HTTP: split
             packet, use rest verbatim (sample directly from target
             distributino)

    4. Evaluation

       0. [@0]

          - against wright and liberatore attacks

          - also against binary with/without morphing classifier

       1. Encrypted Voice over IP
*** quotes
    - morphing one class of traffic to look like another class.
    - show how to optimally modify packets in real-time to reduce the
      accuracy of a variety of traffic classifiers while incurring
      much less overhead than padding.
    - (ends 0)
    - For the remainder of this paper, we focus on the use of our
      morphing techniques in thwarting traffic classifiers that
      utilize features based on packet sizes.
    - (ends 1 + 2)
*** ref
    [[file:~/da/docs/master.bib::morphing09][Wright et al. 2009: Traffic Morphing]]
** TODO [#B] [[./Receiver operating characteristic - Wikipedia.html]]
*** quotes
    0. [@0]
       - The ROC curve was first developed by electrical engineers and
         radar engineers during World War II for detecting enemy
         objects in battlefields
** TODO [#B] [[./python-doc-tutorial.pdf][Python Tutorial]]
   :PROPERTIES:
   :ATTACH_DIR_INHERIT: t
   :END:
*** summary
    0. [@0]

       Python

       - easy

       - powerful

       - high-level data structures

       - dynamic typing

       - \to elegant

       - open source

       - extensible

       - tutorial incomprehensive

       - others: library, reference

    1. WHETTING YOUR APPETITE

       - automate

       - write, compile, test too slow

       - \to has interactive interpreter

       - compact, readable

       - \to very-high-level

       - modular

    2. USING THE PYTHON INTERPRETER

       1. Invoking the Interpreter

          =python=

          EOF (C-D) finishes, or =quit=

          readline adds history features

          =python -c command=

          =python -m module=

          =-i= enters interactive mode afterwards

          1. Argument Passing

             - =sys.argv=

             - no script/arguments: ''

             - -c: '-c'

             - -m: module name

          2. Interactive Mode

             >>>, resp ...

       2. The Interpreter and Its Environment

          1. Source Code Encoding

             "It is possible to use encodings different than ASCII in
             Python source files. The best way to do it is to put one
             more special comment line right after the #! line to
             define the source file encoding:
             #+BEGIN_SRC python
               # -*- coding: encoding -*-
             #+END_SRC

    3. AN INFORMAL INTRODUCTION TO PYTHON

       1. Using Python as a Calculator

          1. Numbers

             - floor division: // (default with ints)

             - ** powers
               #+BEGIN_SRC python
                 2 ** 7 # 2 to the power of 7
               #+END_SRC

             - "In interactive mode, the last printed expression is
               assigned to the variable _."

             - rounding
               #+BEGIN_SRC python
                 round(13.0563, 2) # yields 13.05
               #+END_SRC

          2. Strings

             - escape single quote
               #+BEGIN_SRC python
                 'doesn\'t' # use \' to escape the single quote...
               #+END_SRC

             - print erases newline
               #+BEGIN_SRC python
                 >>> s = 'First line.\nSecond line.' # \n means newline
                 >>> s # without print, \n is included in the output
                 'First line.\nSecond line.'
                 >>> print s # with print, \n produces a new line
                 First line.
                 Second line.
               #+END_SRC

             - raw strings
               #+BEGIN_SRC python
                 >>> print r'C:\some\name' # note the r before the quote
                 C:\some\name
               #+END_SRC

             - string combination Strings can be concatenated (glued
               together) with the + operator, and repeated with * :
               #+BEGIN_SRC python
                 >>> # 3 times 'un', followed by 'ium'
                 >>> 3 * 'un' + 'ium'
                 'unununium'
               #+END_SRC

             - combine strings via 'a' 'b':

               "This feature is particularly useful when you want to
               break long strings:"
               #+BEGIN_SRC python
                 >>> text = ('Put several strings within parentheses '
                 'to have them joined together.')
                 >>> text
                 'Put several strings within parentheses to have them joined together.'
               #+END_SRC

             - "Note that since -0 is the same as 0, negative indices
               start from -1."

             - Slice indices have useful defaults; an omitted first
               index defaults to zero, an omitted second index
               defaults to the size of the string being sliced.
               #+BEGIN_SRC python
                 >>> word[:2] # character from the beginning to position 2 (excluded)
                 'Py'
                 >>> word[4:] # characters from position 4 (included) to the end
                 'on'
                 >>> word[-2:] # characters from the second-last (included) to the end
                 'on'
               #+END_SRC

          3. Unicode Strings

             - The escape sequence \u0020 indicates to insert the
               Unicode character with the ordinal value 0x0020 (the
               space character) at the given position.
               #+BEGIN_SRC python
                 >>> u'Hello\u0020World !'
                 u'Hello World !'
               #+END_SRC

          4. Lists

             - All slice operations return a new list containing the
               requested elements. This means that the following slice
               returns a new (shallow) copy of the list:
               #+BEGIN_SRC python
                 >>> squares[:]
                 [1, 4, 9, 16, 25]
               #+END_SRC

             - replace some values
               #+BEGIN_SRC python
                 >>> letters[2:5] = ['C', 'D', 'E']
               #+END_SRC

       2. First Steps Towards Programming

    4. MORE CONTROL FLOW TOOLS

       1. if Statements

          - if, elif, else

          - replace switch-case with if ... elif ... elif ... else

       2. for Statements

          over iterator, =[:]= for a copy

       3. The range() Function

          does not include the argument,

          start, stop, step

       4. break and continue Statements, and else Clauses on Loops

          - "The break statement, like in C, breaks out of the
            smallest enclosing for or while loop."

            if loop not broken, =else:= is executed

       5. pass Statements

          does nothing

          use cases:

          - busy-wait

          - empty class

          - implement later

       6. Defining Functions

          - var lookup order:

            local \to surrounding locals \to global \to builtin

       7. More on Defining Functions

          different number of parameters

          1. Default Argument Values

             #+BEGIN_SRC python
               def func(retries = 4):
             #+END_SRC

             if set, takes that, otherwise default

          2. Keyword Arguments

             - "When a final formal parameter of the form **name is
               present, it receives a dictionary (see typesmapping)
               containing all keyword arguments except for those
               corresponding to a formal parameter. This may be
               combined with a formal parameter of the form *name
               (described in the next subsection) which receives a
               tuple containing the positional arguments beyond the
               formal parameter list."

          3. Arbitrary Argument Lists

             *var sums all others into a tuple

          4. Unpacking Argument Lists

             "when the arguments are already in a list or tuple but
             need to be unpacked for a function call requiring
             separate positional arguments. [...] If they are not
             available separately, write the function call with the *
             -operator to unpack the arguments out of a list or tuple:
             #+BEGIN_SRC python
               >>> args = [3, 6]
               >>> range(*args)
               [3, 4, 5]
             #+END_SRC

          5. Lambda Expressions

             - "Small anonymous functions"

             - "syntactically restricted to a single expression"

             - "pass a small function as an argument:"
               #+BEGIN_SRC python
                 >>> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]
                 >>> pairs.sort(key=lambda pair: pair[1])
                 >>> pairs
                 [(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]
               #+END_SRC

          6. Documentation Strings

             - "The first line should always be a short, concise
               summary of the object’s purpose. [...] This line should
               begin with a capital letter and end with a period."

       8. Intermezzo: Coding Style

          - 4 space indentation

          - 79 character lines

          - blank lines to separate functions, classes, larger code blocks

          - comments on line of their own if possible

          - docstrings

          - spaces right

          - CamelClass,  function_underscore,  method(self, ...)

          - ASCII

    5. DATA STRUCTURES

       1. More on Lists

          0. [@0] all methods

             - append,

             - extend

             - insert

             - remove

             - pop

             - index

             - count

             - sort

             - reverse

          1. Using Lists as Stacks

             via append() 
             and pop()

          2. Using Lists as Queues

             use collections.deque()

          3. Functional Programming Tools

             - filter(function, sequence) returns a sequence
               consisting of those items from the sequence for which
               function(item) is true.

               #+BEGIN_SRC python
                 >>> def f(x): return x % 3 == 0 or x % 5 == 0
                 ...
                 >>> filter(f, range(2, 25))
                 [3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24]
               #+END_SRC

          4. List Comprehensions

             - list of squares
               #+BEGIN_SRC python
                 squares = [x**2 for x in range(10)]
               #+END_SRC
               This is also equivalent to
               #+BEGIN_SRC python
                 squares = map(lambda x: x**2, range(10))
               #+END_SRC
               , but it’s more concise and readable.

             - flatten a list using a listcomp with two 'for'
               #+BEGIN_SRC python
                 >>> vec = [[1,2,3], [4,5,6], [7,8,9]]
                 >>> [num for elem in vec for num in elem]
                 [1, 2, 3, 4, 5, 6, 7, 8, 9]
               #+END_SRC

       2. The del statement

          - remove elements
            del a[3]

          - remove slices
            del a[3:]

          - remove whole list
            del a[:]

          - remove variable
            del a

       3. Tuples and Sequences

          - tuples, strings and lists are sequences

          - tuples for indexing, packing, ...

          - immutable

          - a = 'hello',
            is a tuple

          - /tuple packing/:
            #+BEGIN_SRC python
              t = 12345, 54321, ’hello!’
            #+END_SRC

          - /sequence unpacking/:
            #+BEGIN_SRC python
              x, y, z = t
            #+END_SRC

       4. Sets

          - "unordered collection with no duplicate elements"

          - fast "membership testing and eliminating duplicate entries"

          - union, intersection, difference, and symmetric difference
            #+BEGIN_SRC python
              >>> a = set('abracadabra')
              >>> b = set('alacazam')
              >>> a
              # unique letters in a
              set(['a', 'r', 'b', 'c', 'd'])
              >>> a - b
              # letters in a but not in b
              set(['r', 'd', 'b'])
              >>> a | b
              # letters in either a or b
              set(['a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'])
              >>> a & b
              # letters in both a and b
              set(['a', 'c'])
              >>> a ^ b
              # letters in a or b but not both
              set(['r', 'd', 'b', 'm', 'z', 'l'])
            #+END_SRC

          - set comprehension like list comprehension
            #+BEGIN_SRC python
              >>> a = {x for x in 'abracadabra' if x not in 'abc'}
              >>> a
              set(['r', 'd'])
            #+END_SRC

       5. Dictionaries

          - see quotes:

            - for key/value storage and retrieval

            - dict via array, =, dict comprehension

            - in

       6. Looping Techniques

          - enumerate: index, item

          - zip: combines

          - reversed: returns reversed

          - sorted: returns sorted

          - iteritems: key, value

          - modify: better create new list

       7. More on Conditions

          - in: containment

          - is: object equality

          - comparison chaining

            #+BEGIN_SRC python
              a < b == c
            #+END_SRC

          - not > and > or

          - assign the result of a comparison or other Boolean
            expression to a variable.

            #+BEGIN_SRC python
              >>> string1, string2, string3 = '', 'Trondheim', 'Hammer Dance'
              >>> non_null = string1 or string2 or string3
              >>> non_null
              'Trondheim'
            #+END_SRC

          - not inside expressions (avoids comparison typo a = b+c)

       8. Comparing Sequences and Other Types

          - same type: recursive

          - different types: ordered by name (list < string)

    6. MODULES

       persistent code

       1. More on Modules

          each module imported only once, use =reload()= to reload

          1. Executing modules as scripts

             - in script, __name__ is set to "__main__"

             - use f.ex.
               #+BEGIN_SRC python
                 if __name__ == "__main__":
                     import sys
                     fib(int(sys.argv[1]))
               #+END_SRC

             - not run when imported

          2. The Module Search Path

             searches =sys.path= for modules. This contains

             - script directory (or working directory)

             - PYTHONPATH (like PATH)

             - installation-default

          3. “Compiled” Python files

             - .pyc (no asserts) and .pyo (no docstrings)

             - automatically created

             - bytecode: faster to load, not to run

       2. Standard Modules

          - sys.path for modules

          - sys.ps1 and sys.ps2 for prompts

       3. The dir() Function

          - lists defined names, or module.dir() those defined in module

          - does not list __builtin__ modules, use
            #+BEGIN_SRC python
              import __builtin__; dir(__builtin__)
            #+END_SRC

       4. Packages

          0. [@0]

             - structure modules

             - file __init__.py signifies package

             - __all__ lists names to export

          1. Importing * From a Package

             - command-line only

             - safeguarded against with __all__

          2. Intra-package References

             - sys.path front is package

             - can import from same dir or absolute package (or .. f.ex.)

               - do not use with __main__ module

          3. Packages in Multiple Directories

             - __path__ contains name where package's __init__.py is found

               - modifyable

    7. INPUT AND OUTPUT

       1. Fancier Output Formatting

    8. ERRORS AND EXCEPTIONS

       1. Syntax Errors

          invalid syntax

       2. Exceptions

          - something went wrong

          - most often not caught

          - =bltin-exceptions= for exceptions

       3. Handling Exceptions

          - multiple with
            #+BEGIN_SRC python
              except (RuntimeError, TypeError, NameError):
                   ...
              pass
            #+END_SRC

          - use error:
            #+BEGIN_SRC python
              except IOError as e:
                  print "I/O error({0}): {1}".format(e.errno, e.strerror)
            #+END_SRC

          - unexpected exceptions
            #+BEGIN_SRC python
              except:
                  print "Unexpected error:", sys.exc_info()[0]
                  raise
            #+END_SRC

          - else clause if try did not throw ("better than adding
            additional code to the try clause")
            #+BEGIN_SRC python
              for arg in sys.argv[1:]:
                  try:
                      f = open(arg, 'r')
                  except IOError:
                      print 'cannot open', arg
                  else:
                      print arg, 'h
            #+END_SRC

       4. Raising Exceptions

          - raise sth based on =Exception=
            #+BEGIN_SRC python
              raise NameError('HiThere')
            #+END_SRC

          - re-raise via =raise=

       5. User-defined Exceptions

          - define: subclass =Error=, maybe override =__str__=
            #+BEGIN_SRC python
              class MyError(Exception):
                  def __init__(self, value):
                      self.value = value
                  def __str__(self):
                      return repr(self.value)
              try:
                  raise MyError(2*2)
              except MyError as e:
                  print 'My exception occurred, value:', e.value
              raise MyError('oops!')
            #+END_SRC

       6. Defining Clean-up Actions

          in =finally=, always called

       7. Predefined Clean-up Actions

          use =with ... as f:= block to invoke

    9. CLASSES

       0. [@0]

          - mixture of C++ and Modula-3

          - members public, functions virtual, use =self=

          - redefine built-in operators

       1. A Word About Names and Objects

          object passed by reference (=alias)

       2. Python Scopes and Namespaces

          - namespace: mapping of names to objects

            - different namespaces can use the same name for different objects

          - attribute: name following a dot

            - read-only (f.ex. =__dict__=) or writable

          - namespace lifetimes

            - global

              - built-in functions: =__builtin__= all the time

              - module: when read until end

              - interpreter: =__main__= module

            - local: function

          - scope: namespace is accessible:

            1. innermost: local names

            2. enclosing functions

            3. middle: current module's global names

            4. outermost: built-in names

          - if global: directly to middle scope

       3. A First Look at Classes

          new syntax, 3 object types, semantics

          1. Class Definition Syntax

             - =class ClassName:= creates new namespace

               - all assignments to local scope

             - (when leaving block): class object to handle namespace

          2. Class Objects

             - two operations:

               - attribute access, (=class.attribute=) and

               - instantiation (=a = MyClass()=)

                 - provide =__init__= method to set data members etc

          3. Instance Objects

             - methods and data attributes

               - instantiate data by assigning

               - methods defined by class

          4. Method Objects

             - can access via
               #+BEGIN_SRC python
                 xf = x.f
               #+END_SRC
               and later call =xf()=

          5. Class and Instance Variables
*** quotes
    4. [@4] MORE CONTROL FLOW TOOLS
       6. [@6] Defining Functions
          - it’s good practice to include docstrings in code that you write,
            so make a habit of it.
          - Thus, global variables cannot be directly assigned a value
            within a function (unless named in a global statement), although
            they may be referenced.
          - Actually, call by object reference would be a better
            description, since if a mutable object is passed, the caller
            will see any changes the callee makes to it (items inserted into
            a list).
          - Falling off the end of a function also returns None.
       7. More on Defining Functions
          1. Default Argument Values
             - (example in and raw_input
               #+BEGIN_SRC python
                 ok = raw_input(prompt)
                 if ok in ('y', 'ye', 'yes'):
                     return True
               #+END_SRC
          3. [@3] Arbitrary Argument Lists
             - Finally, the least frequently used option is to specify
               that a function can be called with an arbitrary number
               of arguments. These arguments will be wrapped up in a
               tuple (see Tuples and Sequences). Before the variable
               number of arguments, zero or more normal arguments may
               occur.
               #+BEGIN_SRC python
                 def write_multiple_items(file, separator, *args):
                   file.write(separator.join(args))
               #+END_SRC
    5. DATA STRUCTURES
       3. [@3] Tuples and Sequences
          - [tuples] may be input with or without surrounding parentheses,
            although often parentheses are necessary anyway
       5. [@5] Dictionaries
          - It is best to think of a dictionary as an unordered set of key:
            value pairs, with the requirement that the keys are unique
            (within one dictionary)
          - The main operations on a dictionary are storing a value with
            some key and extracting the value given the key.
          - >>> tel = {'jack': 4098, 'sape': 4139}
            >>> tel['guido'] = 4127
          - The dict() constructor builds dictionaries directly from
            sequences of key-value pairs:
            #+BEGIN_SRC python
              >>> dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])
              {'sape': 4139, 'jack': 4098, 'guido': 4127}
            #+END_SRC
          - In addition, dict comprehensions can be used to create
            dictionaries from arbitrary key and value expressions:
            #+BEGIN_SRC python
              >>> {x: x**2 for x in (2, 4, 6)}
              {2: 4, 4: 16, 6: 36}
            #+END_SRC
       6. Looping Techniques
          - To loop over two or more sequences at the same time, the entries
            can be paired with the zip() function.
            #+BEGIN_SRC python
              >>> questions = ['name', 'quest', 'favorite color']
              >>> answers = ['lancelot', 'the holy grail', 'blue']
              >>> for q, a in zip(questions, answers):
              ...
              print 'What is your {0}? It is {1}.'.format(q, a)
              ...
              What is your name? It is lancelot.
              What is your quest? It is the holy grail.
              What is your favorite color? It is blue.
            #+END_SRC
          - When *looping through dictionaries*, the key and corresponding
            value can be retrieved at the same time using the iteritems()
            method.
            #+BEGIN_SRC python
              >>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
              >>> for k, v in knights.iteritems():
              ...     print k, v
            #+END_SRC
          - It is sometimes tempting to change a list while you are looping
            over it; however, it is often simpler and safer to create a new
            list instead.
            #+BEGIN_SRC python
              >>> import math
              >>> raw_data = [56.2, float('NaN'), 51.7, 55.3, 52.5, float('NaN'),
                  47.8]
              >>> filtered_data = []
              >>> for value in raw_data:
              ...     if not math.isnan(value):
              ...     filtered_data.append(value)
              ...
              >>> filtered_data
              [56.2, 51.7, 55.3, 52.5, 47.8]
            #+END_SRC
    6. [@6] MODULES
       - Now enter the Python interpreter and import this module
         with the following command:
         #+BEGIN_SRC python
         >>> import fibo
         #+END_SRC
    7. INPUT AND OUTPUT
       1. Fancier Output Formatting
          - An optional ’:’ and format specifier can follow the field
            name. This allows greater control over how the value is
            formatted. The following example rounds Pi to three places after
            the decimal.
            #+BEGIN_SRC python
              import math
              print 'The value of PI is approximately {0:.3f}.'.format(math.pi)
            #+END_SRC
    9. [@9] CLASSES
       2. [@2] Python Scopes and Namespaces
          - Class definitions play some neat tricks with namespaces, and you
            need to know how scopes and namespaces work to fully understand
            what’s going on.
** TODO [#C] [[./wurster-nspw-08.pdf][The Developer is the Enemy]]
*** summary
    0. [@0] ABSTRACT

       - developers are sometimes sloppy

       - hard to teach them

       - solutions

         - focus on API developers

         - others: data tagging and unsuppressible warnings

    1. INTRODUCTION AND OVERVIEW

       - treat developers like users: limit their options

       - reduces risk that something can go wrong

       - stupid to assume all developers to be security-conscious

    2. REVIEWING SOME APPROACHES TO IMPROVING APPLICATION SECURITY
*** quotes
    1. INTRODUCTION AND OVERVIEW
       - rely on all application developers to be security experts. In
         recent years, it has been widely acknowledged that software
         developers do not by any means have sufficient security
         expertise to make this model work.
       - complexity is the enemy of security.
       - Apparently many developers requesting additional
         functionality are not fully aware of the negative security
         implications of their requests.
** TODO [#C] PART developer.mozilla.org
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Add-on_Manager/Addon.html][Addon]]
**** quotes
     3. [@3] Required properties
        - id Read only: string

          The ID of the add-on. No other installed add-on will have
          the same ID.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Add-on_Manager/AddonListener.html][AddonListener]]
**** quotes
     - onDisabling()

       Called when an add-on is about to be disabled.
       #+BEGIN_SRC js
         void onDisabling(
           in Addon addon,
           in boolean needsRestart
         )
       #+END_SRC
     - onUninstalling()

       Called when an add-on is about to be uninstalled.
       #+BEGIN_SRC js
         void onUninstalling(
           in Addon addon,
           in boolean needsRestart
         )
       #+END_SRC
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Add-on_Manager/AddonManager.html][AddonManager]]
**** quotes
     8. [@8] Methods
        17. [@17] addAddonListener()

            Adds a new AddonListener if the listener is not already
            registered.
            #+BEGIN_SRC js
              void addAddonListener(
                in AddonListener listener
              )
            #+END_SRC

        18. removeAddonListener()

            Removes an AddonListener if the listener is registered.
            #+BEGIN_SRC js
              void removeAddonListener(
                in AddonListener listener
              )
            #+END_SRC
*** [[../../../chive/developer.mozilla.org/en-US/docs/JavaScript/Reference/Global_Objects/Array/splice.html][Array.prototype.splice()]]
**** quotes
     - The splice() method changes the content of an array by removing
       existing elements and/or adding new elements.
     - array.splice(start, deleteCount[, item1[, item2[, ...]]])
*** [[developer.mozilla.org/en-US/Add-ons/SDK/Tools/console.html][console]] ([[../../../chive/IT-gg/js/Mozilla_Addon_SDK_doc/developer.mozilla.org/en-US/Add-ons/SDK/Tools/console.html][local link]])
    - When you run your add-on using jpm run or jpm test, the global
      extensions.sdk.console.logLevel preference is automatically set
      to "info". This means that calls to console.log() will appear
      in the console output.
    - When you install an add-on into Firefox, the logging level will
      be "error" by default (that is, unless you have set one of the
      two preferences). This means that messages written using
      debug(), log(), info(), trace(), and warn() will not appear in
      the console.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Creating_Sandboxed_HTTP_Connections.html][Creating Sandboxed HTTP Connections]]
**** summary
**** quotes
     4. [@4] Handling cookies
        #+BEGIN_SRC js
        observerService.addObserver(listener, "http-on-examine-response", false);
        #+END_SRC
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/Tools/jpm.html][jpm]]
**** quotes
     2. [@2] Command reference
        7. [@7] jpm sign
           - This feature is only supported from jpm 1.0.4 onwards.
           - You will need to create API credentials on addons.mozilla.org
             before using this command.

             `jpm sign --api-key ${AMO_API_KEY} --api-secret ${AMO_API_SECRET}`
**** ref [[file:master.bib::moz-sdk-jpm][Caspy7 et al. 2016]]
*** [[../../../chive/developer.mozilla.org/en-US/docs/XPCOM_Interface_Reference/nsIChannel.html][nsIChannel]]
*** [[../../../chive/developer.mozilla.org/en-US/docs/XPCOM_Interface_Reference/nsIHttpChannel.html#getResponseHeader()][nsIHttpChannel]]
**** quotes
     - Attributes
       - responseStatus | unsigned long

         Get the HTTP response code (For example 200). Read only.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/High-Level_APIs/page-mod.html][page-mod]]
**** summary
     2. [@2] Globals

        1. Constructors

           1. PageMod(options)

              - Parameters

                - attachTo: where to attach the script:
                  #+BEGIN_EXAMPLE
                    ["existing", "top"]
                  #+END_EXAMPLE
**** quotes
     3. [@3] PageMod
        1. Methods
           1. destroy()

              Stops the page-mod from making any more
              modifications. Once destroyed the page-mod can no longer
              be used.
*** [[./platform_xpcom - Mozilla | MDN.html][platform/xpcom]]
**** summary
     1. Usage

        XPCOM interfaces, factories, services

        1. Implementing XPCOM Interfaces

           - modules exports class =Unknown= which implements nsISupports

             - subclass Unknown to implement XPCOM interfaces
               #+BEGIN_SRC js
                 var { Class } = require('sdk/core/heritage');
                 var { Unknown } = require('sdk/platform/xpcom');

                 var StarObserver = Class({
                     extends:  Unknown,
                     interfaces: [ 'nsIObserver' ],

                     // implement methods of nsIObserver
                 });
               #+END_SRC

     3. [@3] Unknown

        0. [@0]

           - base class for all XPCOM objects

           - subclass to create your own

             - list interfaces in =interfaces= property

               - nsISupports always included, no need to list

        1. Methods

           1. QueryInterface(interface)

**** quotes
**** ref [[file:master.bib::moz-sdk-xpcom][wbamberg 2016]]
*** [[../../../chive/developer.mozilla.org/en-US/docs/Glossary/CSS_Property.html][Property (CSS)]]
**** quotes
     - A CSS property is a characteristic (like color) whose
       associated value defines one aspect of how the browser should
       display the element.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/High-Level_APIs/self.html][self]]
**** quotes
     1. Globals
        1. Properties
           2. [@2] id

              This property is a printable string that is unique for
              each add-on. It comes from the id property set in the
              package.json file in the main package (i.e. the package
              in which you run jpm xpi).
*** [[../../../chive/developer.mozilla.org/en-US/docs/Setting_HTTP_request_headers.html][setting http request headers]]
**** summary
**** quotes
     - Note that the number of parameter that the observe method takes
       is important. It takes 3 parameters (as we've shown in the
       example code above). For the "http-on-modify-request" topic, the
       first parameter (named subject in the code above) will be the
       nsIHttpChannel. However, it is passed to us as an nsISupports. So
       we need to change the nsISupports into a nsIHttpChannel which is
       what the QueryInterface call does.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/High-Level_APIs/simple-prefs.html][simple-prefs]]
**** quotes
     2. [@2] Globals
        1. Functions
           1. on(prefName, listener)
              - Preference change events are triggered for every
                character typed by the user. They are also triggered
                once during add-on initialization.
*** system/events
    - Usage The system/events module provides core (low level) API
      for working with the application observer service, also known
      as nsIObserverService. You can find a list of events dispatched
      by firefox codebase here.
      #+BEGIN_SRC javascript
        var events = require("sdk/system/events");
        var { Ci } = require("chrome");

        function listener(event) {
          var channel = event.subject.QueryInterface(Ci.nsIHttpChannel);
          channel.setRequestHeader("User-Agent", "MyBrowser/1.0", false);
        }

        events.on("http-on-modify-request", listener);
      #+END_SRC
*** [[../../../chive/developer.mozilla.org/en-US/docs//Mozilla/Add-ons/SDK/Low-Level_APIs/system_unload.html][system/unload]]
**** quotes
     1. Globals
        1. [@2] when(callback)
           - callback : function

             A function that will be called when the module is
             unloaded. It is called with a single argument, one of the
             following strings describing the reason for unload:
             "uninstall", "disable", "shutdown", "upgrade", or
             "downgrade". If a reason could not be determined,
             undefined will be passed instead.
*** [[../../../chive/developer.mozilla.org/en-US/docs/Mozilla/Add-ons/SDK/High-Level_APIs/timers.html][timers]]
**** summary
     - setTimeout as js default, needs to be imported via
       #+BEGIN_SRC js
         const { setTimeout } = require("sdk/timers");
       #+END_SRC
*** [[../../../chive/developer.mozilla.org/en-US/docs/CSS/Media_queries.html][Using media queries]]
**** summary
     1. Syntax
        - when condition is met, element is applied
          - stylesheets are downloaded regardless of whether needed
**** quotes
     1. Syntax
        #+BEGIN_SRC css
          <!-- CSS media query on a link element -->
          <link rel="stylesheet" media="(max-width: 800px)" href="example.css" />

          <!-- CSS media query within a stylesheet -->
          <style>
          @media (max-width: 600px) {
            .facet_sidebar {
              display: none;
            }
          }
          </style>
        #+END_SRC

When a media query is true, the corresponding style sheet or style rules are applied, following the normal cascading rules.
*** window/utils
    - getWindowLoadingContext(window)
      Returns the nsILoadContext.
      - Parameters
        window : nsIDOMWindow
      - Returns
        nsILoadContext
*** TODO (part) [#C] [[../../../chive/developer.mozilla.org/en-US/docs/MDN/Contribute/Guidelines/Style_guide.html][Writing style guide]]
**** summary
     1. Basics
        1. Page titles
           1. Title and heading capitalization
              - sentence-style instead of headline-style capitalization
           2. Choosing titles and slugs
              slug: short, title: as long as necessary
           3. Creating new subtrees
              - specific to mozilla: top-level slows site down and
                makes "site navigation less effective"
        2. Sections, paragraphs, and newlines
**** quotes
     0. [@0]
        - don't feel obligated to learn the style guide before
          contributing
     1. Basics
        2. [@2] Sections, paragraphs, and newlines
           - Don't have bumping heads, which are headings followed
             immediately by headings. Aside from looking horrible, it's
             helpful to readers if every heading has at least a brief intro
             after it to introduce the subsections beneath.
*** XUL Migration Guide
    - Using XPCOM This example uses the action button API, which is
      only available from Firefox 29 onwards.

      Finally, if none of the above techniques work for you, you can
      use the require("chrome") statement to get direct access to the
      Components object, which you can then use to load and access
      any XPCOM object.

      The following complete add-on uses nsIPromptService to display
      an alert dialog:
      #+BEGIN_SRC javascript
        var {Cc, Ci} = require("chrome");

        var promptSvc = Cc["@mozilla.org/embedcomp/prompt-service;1"].
                        getService(Ci.nsIPromptService);

        require("sdk/ui/button/action").ActionButton({
          id: "xpcom-example",
          label: "Hello from XPCOM",
          icon: "./icon-16.png",
          onClick: function() {
            promptSvc.alert(null, "My Add-on", "Hello from XPCOM");
          }
        });
      #+END_SRC

      It's good practice to encapsulate code which uses XPCOM by
      packaging it in its own module. For example, we could package
      the alert feature implemented above using a script like:

      #+BEGIN_SRC javascript
        var {Cc, Ci} = require("chrome");

        var promptSvc = Cc["@mozilla.org/embedcomp/prompt-service;1"].
                    getService(Ci.nsIPromptService);

        exports.alert = function(title, text) {
            promptSvc.alert(null, title, text);
        };
      #+END_SRC

      If we save this as "alert.js" in our add-on's lib directory, we
      can rewrite main.js to use it as follows:

      #+BEGIN_SRC javascript
        require("sdk/ui/button/action").ActionButton({
          id: "xpcom-example",
          label: "Hello from XPCOM",
          icon: "./icon-16.png",
          onClick: function() {
            require("./alert").alert("My Add-on", "Hello from XPCOM");
          }
        });
      #+END_SRC
** TODO [#C] [[./csbuflo.pdf][Cai - CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense]]
*** summary
    0. [@0] Abstract:

       - fingerprint infers, even if tor,

       - previous defenses are ineffective,

       - spec of cs-buflo,

       - implementation

    1. Introduction

       - several website fingerprinting attacks, several defenses

       - 80%, which of 128 pages

       - BuFLO: over 400% bandwidth overhead

       - DLSVM fingerprinting attack greater than 75% success rate
         against numerous defenses, including application-level
         defenses, such as HTTPOS and randomized pipelining

       - CS-BuFLO, congestion avoidance, TCP-friendly,

       - here: adapt its transmission rate dynamically, and improve
         its stream padding: less bandwidth, hiding more

       - adapting too quickly can reveal info, solve: limit adaptation

       - alexa 200: 91% of web use

       - CS-BuFLO: 2.8 times as much bandwidth as SSH, only a 20% success rate

       - CS-BuFLO ratio 2.8. BuFLO ratio of 2.2.

    2. RELATED WORK

       - dyer: lists stuff like padM, padE, ...

       - wright: morphing

       - dyer defeats

       - Lu extends morphing

       - Dyer BuFLO

       - Fu: CPU load changes, recommend randomized intervals

       - HTTPOS

       - Tor randomized

       - Cai defeated

       - many attacks agains https, ipsec, vpn, etc

       - herrmann: tunnels attack, fails on tor

       - panchenko

       - dyer vng++

       - cai string edit distance

       - wang improved

       - danezis, yu, cai hmm to extend to web site fp

    3. WEBSITE FINGERPRINTING ATTACKS

       - (Eve) monitors communication (Alice) to proxy (Walter)

         - traffic encrypted, Eve sees timing, direction and size of packets

       - f.ex. government which would not block, but monitor
         international communication

       - closed-world: 100-800 pages, metric: average success rate

       - open-world: victims visit websites, Eve tries to guess of
         "website of interest" is visited: metric: FPR/FNR

       - cai(4): bootstrap cw to ow, compare attacks and defenses

    4. Congestion-Sensitive BuFLO

       0. [@0]

          - better security and efficiency than BuFLO

          - white-list: hide all features, selective reveal stuff to
            reduce overhead

       1. Review of BuFLO

          - buflo "shortcomings"

            - completely hides or completely reveals

            - large overhead for small websites

            - not TCP-friendly

            - must be tuned to user's connection

            - lu: seems to yield load information

          - can be mitigated/improved upon

       2. Overview of Congestion-Sensitive BuFLO

          - interacts with TCP

          - target average inter-packet rate \rho*, updated infrequently

          - extra data after website has finished loading

       3. Rate Adaptation

          - adapts receiving to sending

          - limit information leaked by

            - adjusting only at (?all?) 2^k bytes

            - use median of bandwidth, stable metric

            - (only collect during uninterrupted bursts)

            - round up to power of two

       4. Congestion-Sensitivity
*** quotes
    - Our experiments find that Congestion-Sensitive BuFLO has high
      overhead (around 2.3-2.8x)
    - it is not currently known whether there exists any efficient and
      secure defense against website fingerprinting attacks.
    - all previously-proposed defenses provide little security.
*** ref [[file:master.bib::wpes14-csbuflo][Cai et al. 2014: CS BuFLO]]
** TODO [#C] [[./webtraffic-imc11.pdf][Towards Understanding Modern Web Traffic]]
*** summary
    0. [@0] ABSTRACT

       - traffic has changed

       - new analysis algorithm

       - redundancy

    1. INTRODUCTION

       - dataset: five years, 70000 daily user from 187 countries
*** quotes
    1. INTRODUCTION
      - generate a synthetic workload for benchmarking or simulation.
*** ref [[file:master.bib::DBLP:conf/imc/IhmP11][Ihm & Pai 2011: Towards]]
** TODO [#C] [[./10.1.1.20.98.bloom-nw.pdf][Broder - Network Applications of Bloom Filters: A Survey]]
*** summary
    0. [@0] Abstract

       - "space efficient"

       - "small probability of false positives"

    1. Introduction

       - various uses: peer-to-peer, packet routing, locating
         resources, measurement

       - can replace lists and sets

       - design: weigh false positives

    2. Bloom filters: Mathematical preliminaries

       1. Standard Bloom filters

          - represent set of size n by m bits

          - k hash functions h_1, ..., h_k

            - to add element x to filter, set bits at h_1(x), ..., h_k(x) to 1

            - element y maybe in set if all bits h_1(y), ..., h_k(y) are 1

          - /false positive rate/ with n bits in filter is (assuming
            perfectly random hashes)

            (1 - e^{-kn/m})^{k}

            - asymptotically same behavior if bits are split per hash
              function

              - but non-asymptotically slightly better to have one bit field

          - minimal fp-rate [(1/2)^k \approx (0.6185)^{m/n} at

            k = ln 2 \cdot (m/n)

            (in reality, choose smaller of integers bounding k to
            reduce computation)

       2. Hashing vs. Bloom filters

          - hashing has guaranteed results, bloom filters do not

          - they offer a trade-off: space reduction for false positive
            probability

       3. Standard Bloom filter tricks

          - union of two sets: OR bit vectors

          - halve size: OR lower and upper set, mask hash of high order bit

       4. Counting Bloom filters

          - use counter (4 bits) instead of single bit

          - allows deletion

       5. Compressed Bloom filters

          you can optimize for compressed size (sent f.ex. over the
          network), larger bloom filter gives improved false positive
          rate

    3. Historical Applications

       - spell-checkers: store bloom-filter of dictionary:
         "significant performance advantages"

       - password checking, also those with edit-distance 1

       - databases: semijoin: send bloom-filter, receives pairs to
         filter false positives

    4. A Sample Network Application: Distributed Caching

       - caches perodically send bloom filters of their cache contents

       - might lead to additional requests and delays for false positives, but

       - greatly reduces network traffic

    5. Applications: P2P/Overlay Networks

       browsing assistant vistabar: bloom filter for all annotated URLs

       1. Moderate-sized P2P networks

          - save offered objects in bloom filters

            - reduce 64-bit identifier to 8 or 16 bits per object

       2. Approximate Set Reconciliation for Content Delivery

          - network: set of differences client A to client B

            - false positive: do not send some files

          - part of more complex data structures

          - group download: each peer downloads part

       3. Set Intersection for Keyword Searches
*** quotes
    1. Introduction
       - A Bloom filter is a simple space-efficient randomized data
         structure for representing a set in order to support
         membership queries.
       - If false positives do not cause significant problems, the
         Bloom filter may provide improved performance.
       - The Bloom filter principle: Wherever a list or set is used,
         and space is a consideration, a Bloom filter should be
         considered. When using a Bloom filter, consider the potential
         effects of false positives.
    2. Bloom filters: Mathematical preliminaries
       1. Standard Bloom filters
          - A *Bloom filter* for representing a set S = {x_1, x_2,...,
            x_n} of n elements is described by an array of m bits,
            initially all set to 0.
*** ref [[file:master.bib::Broder02networkapplications][Broder & Mitzenmacher 2002: Network Applications Bloom Filters]]
** TODO [#C] [[./geurtz.pdf][Geurts - Extremely randomized trees]]
*** summary
    0. [@0] Abstract

       - randomized ensemble method based on decision trees

       - "randomizing strongly both attribute and cut-point choice
         while splitting a tree node"

       - evaluation: default parameters, strength (efficiency),
         bias/variance, geometrical + kernel characterization

    1. Introduction

       - increase randomization even further over forests of
         randomized trees

    2. Extra-Trees algorithm

       0. [@0]

          - classification, many input, one output var

          - algo description and rationale

          - systematic evaluation (accuracy + efficiency)

       1. Algorithm description and rationale

          - select K non-constant (in S) attributes

          - split S on each attribute

          - return split with best score

          - do with M different trees, majority vote

          - (see quotes for full algorithm)

       2. Empirical evaluation

          0. [@0]

             - datasets

             - (tree-based) comparison algorithms

             - evaluate (accuracy, computational resources)

          1. Datasets

             - 24 datasets: half classification (2-26 classes), half
               regression

             - different conditions:

               - attributes: 2-617

               - sample size: 300-10000

               - redundancy: 10-1000 ("observations per attribute")

               - different attribute importances: un, always, depending

               - noisy/noise-free

               - a priori, none later excluded

          2. Compared algorithms

             - single CART tree

             - tree bagging

             - random subspace

             - random forests

             - parameters:

               - M = 100

               - n_min = 2 for classification, 5 for regression

               - K = \sqrt{n} for classification, n for regression

             - score: normalized shannon information gain [[(et-a1)]]

             - aggregation scheme

               - majority vote for classification

               - arithmetic average for regression

          3. Protocols

             - each set 10 times (50 for small sets)

             - use best values of K for RS and RF

             - also compare to kNN, other variants, least squares
               linear regression (table 8 appendix d)

             - statistical test: "corrected paired two-sided t-test"

          4. Discussion of results

             - many problems: et slightly worse only in two

             - significance tests: et > rs > rf >> cart

             - setting K by cross-validation not recommended: "very
               small gain" and "very high cost in terms of
               compatational burden" (see section 3)

       3. Computational requirements

          - ET is faster than Random Forests and Tree Bagging, and way
            faster than Random Subspace (was taken out)

          - takes about 3 times as long as ST for classification

            - longer for regression

    3. On the effect of parameters

       effect of parameters K, n_{min} and M on Extra-Trees

       1. Attribute selection strength K

          - depends strongly on attribute correlation

          - if correlated: K=1 is best

          - if superfluous variables, rising K gives better result

          - else (variable importance): default \sqrt{K} does a good job

       2. Smoothing strength n_{min}

          - bigger n_{min}: smaller trees, higher bias, smaller variance

          - default of 2 (classification)

          - should be higher for noisier data

          - overall ok

       3. Averaging strength M

          - the higher M, the lower error (Breiman, 2001)

          - depends on K

       4. Bias/variance analysis

          0. [@0]

             - randomization increases both,

             - but randomization variance decreased by averaging

             - high levels of bias still ok for classification (class
               probability estimates)

          1. Experiments and protocols
*** quotes
    1. Introduction
       - selects its cut-point fully at random, i.e., independently of
         the target variable.
       - Extra-Trees (for extremely randomized trees)
    2. Extra-Trees algorithm
       1. Extra-Trees splitting algorithm (for numerical attributes)
          - Split a node(S)
            - Input: the local learning subset S corresponding to the
              node we want to split
            - Output: a split [a < a_c] or nothing
            - If Stop_split(S) is TRUE then return nothing.
            - Otherwise select K attributes {a_1 , ..., a_K} among all
              non constant (in S) candidate attributes;
            - Draw K splits {s_1 , ..., s_K }, where

              s_i = Pick_a_random_split(S, a_i), ∀i = 1,...,K;
            - Return a split s_∗ such that

              Score(s_∗ , S) = max_{i=1,...,K} Score(s_i , S).
          - Pick_a_random_split(S,a)
            - Inputs: a subset S and an attribute a
            - Output: a split
            - Let a_max^S and a_min^S denote the maximal and minimal
              value of a in S;
            - Draw a random cut-point a_c uniformly in [a_min, a_max];
            - Return the split [a < a_c].
          - Stop_split(S)
            - Input: a subset S
            - Output: a boolean
            - If |S| < n_min , then return TRUE;
            - If all attributes are constant in S, then return TRUE;
            - If the output is constant in S, then return TRUE;
            - Otherwise, return FALSE.
    100) [@100] Appendix
         1. A. Pseudo-code of the complete Extra-Trees algorithm and
            score measures
            - Our score measure in classification is a particular
              normalization of the information gain. For a sample S
              and a split s, this measure is given by:
              #+BEGIN_EXAMPLE
                Score_C(s, S) = 2I^S_C(S) / (H_S(S)+H_C(S)) (ref:et-a1)
              #+END_EXAMPLE
              where H_C(S) is the (log) entropy of the classification
              in S, H_S(S) is the split entropy (also called split
              information by Quinlan (1986)), and I_C^S(S) is the
              mutual information of the split outcome and the
              classification.
*** ref [[file:master.bib::DBLP:journals/ml/GeurtsEW06][Geurts et al. 2006: Extremely]]
** TODO [#C] PART [[./Applied_Cryptography.pdf][Schneier - Applied Cryptography]]
*** TODO parts: maybe 10 (see quote t.a.)
*** summary
    1. Foundations

       1. Terminology

          - sender writes *plaintext*, *encrypts* to *ciphertext*, sends
            (secure) message to receiver, *decrypts* to *plaintext*

          - *cryptology* is *cryptography* for writing
            and *cryptanalysis* for checking if secure

          - *Authentication*, *Integrity*, and *Nonrepudiation*:
            additional to *confidentiality*: as in face-to-face
            communication

          - Algorithms and Keys:

            - *cryptographic algo* == *cipher*: function(message[, key])
              \to ciphertext

            - *restricted* algorithm must be kept secure, hard

            - *key* from *keyspace* modifies cipher

            - *cryptosystem*: algo, {plaintexts}, {ciphertexts}, {keys}

          - *Symmetric Algorithms*: encryption key iff decryption key

            - *stream* (one bit/byte at a time) vs *block* ciphers

          - *Public-Key Algorithms*:

            - *public key* to encrypt, *private key* to decrypt

          - *Cryptanalysis*: find message without key

            - loss of key == *compromise* vs cryptanalysis *attack*

              1. *Ciphertext-only attack*: given only ciphertexts

              2. *Known-plaintext attack*: also access to some plaintexts

              3. *Chose-plaintext attack*: select plaintext, get ciphertext

              4. *Adaptive-chosen-plantext attack*: retries of (3.)

              5. *Chosen-ciphertext attack*: need to find key

              6. *Chosen-key attack*: know sth about keys

              7. *Rubber-hose cryptanalysis*: threaten, torture, bribe, etc

            - reverse-engineering etc most often gives algorithm

            - known plaintext: code, executable, greetings, etc

          - Security of Algorithms

            - how long to break the algorithm vs how long data needs
              to stay secure

            - types of break:

              1. *total break*: find key

              2. *global deduction*: algorithms with the same output

              3. *instance deduction*: find plaintext of ciphertext

              4. *information deduction*: find some info about message

            - algo *unconditionally secure*: no matter info, unbreakable: otp

            - algo *computationally secure*: not enough
              time/resources/data to break

       2. ...

    2. Protocol Building Blocks

       1. Introduction to Protocols

          0. [@0]

             - "cryptography solves problems [of...] secrecy,
               authentication, integrity, and dishonest people."

             - protocol: "series of steps, involving two or more parties,
               designed to accomplish a task"

               - must be

                 - well-known by all parties,

                 - agreed to

                 - unambiguous === well-defined

                 - complete: specific action for every possible situation

             - cryptographic protocol

               - prevent or detect eavesdropping or cheating via cryptography

               - should not be possible to learn or do more than
                 specified in the protocol

          1. The Purpose of Protocols

             - how computers talk to each other (people do it
               informally all the time)

          2. The Players

             - Alice, Bob, Carol, Dave

             - Eve: Eavesdropper

             - Mallory: Malicious active attaker

             - Trent: Trusted arbitrator

             - Walter: Warden; guards Alice, Bob, ...

          3. Arbitrated Protocols

             - truthful entity with no vested interest

               - like lawyer as escrow agent, or stock exchange, or banker

             - trusted party old institution,

             - problems with computers:

               - cannot see,

               - network must bear the cost,

               - delay,

               - arbitrator is bottleneck

               - vulnerable point for subverter

          4. Adjudicated Protocols

             - (only) detect cheating, do not prevent

               - this should discourage cheating, though

             - adjudicator: external authority, called on only in conflict

             - like judge in real life

          5. Self-Enforcing Protocols: guards itself against cheating

          6. Attacks against Protocols

             - *passive* (listen) vs *active* (do sth)

             - *cheaters* are involved (p follow/a disrupt)

       2. ...

    10. [@10] Using Algorithms

        Security is like a chain. The weakest link breaks it. (RNG, algo, code, ...)

        1. ...

        3. [@3] Encrypting Communications Channels

           *link-by-link* vs *end-to-end encryption*

           1. Link-by-Link Encryption

              - on each link separately

              - synchronous: always, else send random data:
                +: Eve has no clue if data is being sent or not

              - asynchronous: sometimes send. May send dummy traffic
                otherwise

              - disadvantages

                - needs to encrypt every link (cost)

                - protect every node

           2. End-to-End Encryption

              - disadvantages

                - key management more difficult

                - metadata: who is talking to whom

              - traffic analysis possible

           3. Combining the Two

              - best approach, yet most expensive

              - able to separate keys for low-level (admins) with high-level (users)

           4. Encrypting Data for Storage

              ...
*** quotes
    2. [@2] Protocol Building Blocks
       1. Introduction to Protocols
          1. The Purpose of Protocols
             - It is naïve to assume that [people on, the managers of,
               the designers of] computer networks are honest. Most
               are, but the dishonest few can do a lot of damage.
    10. [@10] Using Algorithms
        3. [@3] Encrypting Communications Channels
           - *traffic-flow security*: the enemy is not only denied
             access to the information, but also access to the
             knowledge of where and how much information is flowing.
           - *Traffic analysis* is the analysis of encrypted messages:
             where they come from, where they go to, how long they
             are, when they are sent, how frequent or infrequent they
             are, whether they coincide with outside events like
             meetings, and more. A lot of good information is buried
             in that data, and a cryptanalyst will want to get his
             hands on it.
*** ref [[file:master.bib::applied96][Schneier 1996: Applied Cryptography]]
** TODO [#C] [[./weis2006.pdf][Anonymity Loves Company: Usability and the Network Effect]]
*** summary
    1. Usability for others impacts your security
       - the more usable a product is, the more security it provides
    2. Usability is even more important for privacy
       - the more usable, the more users, the bigger the anonymity set
       - conversely, the less users, the easier it is to infer identity
    3. Case study: usability means users, users mean security
       - high-security, high latency vs lower-security, low latency
         - against global adversary, need high latency
         - for web browsing/ssh, need low latency
         - if you can choose: lower-security might be more secure
           - bigger anonymity set, at least protects against other threats

    4. Case study: against options

       - options delegate security decision to less-knowable end-user

       - and make it harder to audit the code

       - most users only use default config, alter only to make system
         work

       - choose between insecure and inconvenient: sign of bad design

       - better to have user describe their situation (f.ex. home vs
         enterprise, shared vs single-user host), than solution

       - sometime better even for security-conscious users to use
         standard options: using these guarantees more privacy (unless
         broken)

    5. Case study: Mixminion and MIME

       - MIME leaks much info

       - but you want to attract many users which use it

       - solution: "as much normalization as" possible, warn user
         about identifying doc types (MS Word)

    6. Case study: Tor Installation

       - first: tech-savvy users

       - later: enthusiasts who needed more help

       - f.ex. dns leaks: message: "you are insecure, see webpage"
         (first: email, but that proved lengthy)

    7. Case study: JAP and its anonym-o-meter
*** quotes
    4. [@4] Case study: against options
       - The real issue here is that designers often end up with a
         situation where they need to choose between "insecure" and
         "inconvenient" as the default configuration—meaning they’ve
         already made a mistake in designing their application.
    6. [@6] Case study: Tor Installation
       - Usability and marketing have also proved important in the
         development of Tor,
*** ref [[file:master.bib::usability:weis2006][Dingledine & Mathewson 2006: Anonymity Loves Company]]
** TODO [#C] [[./torspec/pt-spec.txt][Pluggable Transport Specification (Version 1)]]
*** summary (stopped at [[file:torspec/pt-spec.txt::"TOR_PT_SERVER_TRANSPORT_OPTIONS"][3.2.3]])
    0. [@0] Abstract

       censorship circumvention using modular, traffic-transforming
       sub-processes

    1. Introduction

       0. [@0]

          - decouple obfuscation from client-server code

       1. Requirements Notation: MUST etc as usual (RFC2119)

    2. Architecture Overview

       - client app to[lo] pt-client to[inet] pt-server to[lo] server app

       - pt client as SOCKS proxy to client app

       - configured via environment variables, write output to stdout

    3. Specification

       0. [@0] workflow

          1. env values, start pt as sub-process

          2. pt determines spec version

          3. pt parses env

          4. pt client validates upstream proxy

          5. pt initializes transports

          6. pt does job: forward/transform

          7. signal of termination, shut down ("gracefully")

       1. Pluggable Transport Naming

          unique, C identifier

       2. Pluggable Transport Configuration Environment Variables

          prefixed with "TOR_PT_"

          1. Common Environment Variables

             - TOR_PT_MANAGED_TRANSPORT_VER: comma-separated =1,1a,2b,valid_ver

             - TOR_PT_STATE_LOCATION: dir to store persistent state

             - TOR_PT_EXIT_ON_STDIN_CLOSE: 1\to close if stdin == EOF, 0\to NOT

          2. Pluggable Transport Client Environment Variables

             - TOR_PT_CLIENT_TRANSPORTS: list of transports =obfs2,wfpad

             - TOR_PT_PROXY: upstream proxy (optional)

          3. Pluggable Transport Server Environment Variables

             - TOR_PT_SERVER_TRANSPORTS: server list =obfs3,wfpad

             - TOR_PT_SERVER_TRANSPORT_OPTIONS:

               - list of transport:arg=value pairs, f.ex.
                 #+BEGIN_EXAMPLE
                   scramblesuit:key=banana;automata:rule=110;automata:depth=3
                 #+END_EXAMPLE

             - TOR_PT_SERVER_BINDADDR: transport-host:port,t2-h2:p2,...

             - TOR_PT_ORPORT: forward to after transformation: =host:port=

             - TOR_PT_EXTENDED_SERVER_PORT: use instead of ORPORT, also
               metadata, set to empty string if not supported, =host:port=

             - TOR_PT_AUTH_COOKIE_FILE: Extended ORPort autho cookie =/path/to=

       3. Pluggable Transport To Parent Process Communication
*** quotes
*** ref [[file:master.bib::tor-spec-pt][Yawning 2015: Pluggable Transport Specification Version]]
** TODO [#C] [[./xxx-multihop-padding-primitives.txt][Multihop Padding Primitives]]
*** summary
    0. [@0] Overview and Direction

       - reviewed tamaraw, adaptive padding, cs-buflo and supersequence

       - created padding message primitives

    1. Padding Message Primitives

       high-level bidirectional or one-directional (important to avoid attacks)

       1. Generic Messages (common to all defenses)

          - RELAY_DROP: CELL_SIZE padding cell (already existsn in Tor)

          - RELAY_SEND_PADDING(N, t): send N RELAY_DROP packets in t microseconds

          - RELAY_APP_HINT: app starts or stops (f.ex. hash of url bar domain)

       2. Adaptive Padding Messages
*** quotes
    0. [@0] Overview and Direction
       - In a world of very cheap and excessive middle and Guard node
         bandwidth, we would run this padding to the middle node
    1. Padding Message Primitives
       - exploiting padding to create a side channel
*** TODO ref
** TODO [#C] [[./ethical.html][Ethical Tor Research: Guidelines]]
*** summary
    1. Goals of this document.

       how to do responsible research on Tor etc: guidelines,
       unacceptable-list, process for "potentially dangerous" activity

    2. General principles
*** quotes
*** ref [[file:master.bib::ethical][Ethical Tor Research]]
** TODO [#C] [[./crov1297.pdf][Long-Lasting Transient Conditions in Simulations with Heavy-Tailed Workloads]]
*** summary
    0. [@0] ABSTRACT

       - heavy tailed distributions = tail declines like a power law

       - occur often

       - slow convergence to steady state and high variability

    1. INTRODUCTION
*** quotes
** TODO [#C] [[./PEP 3333 -- Python Web Server Gateway Interface v1.0.1 | Python.org.html]]
*** summary
*** quotes
   3. [@3] Original Rationale and Goals (from PEP 333)
      - WSGI must be easy to implement, so that an author's initial
        investment in the interface can be reasonably low.
*** ref [[file:master.bib::pep3333][Eby 2010: Python Web Server]]
** TODO [#C] [[./torspec/tor-spec.txt][Tor Protocol Specification]]
*** TODO refactor
*** [#B] 2.2-renegotiation to 3 (see also spacing), see that correct
*** summary
    0. [@0] Preliminaries:

       MUST, etc. keywords like RFC

       1. Notation:

          0. [@0]

             - PK public key, SK private key, K symmetric key,

             - a|b concatenation, [a0 b1 c2] three-byte sequence, H(m) hash

          1. all multibyte values are big-endian

       2. Security parameters:

          - KEY_LEN length of stream cipher key,

          - PK_ENC_LEN public key-encrypted message length,

            - PK_PAD_LEN padding added to PK_ENC_LEN,

            - DH_LEN number of bytes for dh-group-member,

              - DH_SEC_LEN dh private key length,

          - HASH_LEN length of hash function output,

          - PAYLOAD_LEN longest allowable cell payload (=509 bytes)

          - CELL_LEN(v) length of tor cell for protocol version v

       3. Ciphers

          - stream: 128bit counter-mode AES with IV=0x0...0

          - pk: 1024 bit AES with exponent of 65537 with 0aef-mgf1
            padding, sha1 digest, label unset

          - ntor: curve25519

          - dh generator 2, modulus from rfc2409
            optimization: SHOULD private keys of 320 bits, MUST NOT reuse

          - hash: SHA-1

          - hybrid encryption:

            - if small (< PK_ENC_LEN-PK_PAD_LEN), encrypt with PK

            - else: generate KEY_LEN key, split M1 of len PK_ENC_LEN -
              PK_PAD_LEN - KEY_LEN

              - first message: K|M1 with PK

              - rest M2 with stream cipher using K

    1. System overview

       TOR low-latency tcp distributed overlay

       1. Keys and names

          - long-term *identity* keys (RSA 1024 + Ed25519), signing only

          - medium-term *onion* RSA and *signing* Ed25519-keys, keep
            at least one week after advertisement

          - (short-term TLS *connection* key, rotate at least once a
            day)

    2. Connections

       0. [@0]

          - link layer: 3 ways to TLS-handshake

            1. "certificates up-front"

               both send two-certificate chain

               - init: short-term X.509 cert + self-signed identity X.509

               - resp: similar

               - MAY ONLY INCLUDE: TLS_DHE_RSA_WITH_AES_256_CBC_SHA,
                 TLS_DHE_RSA_WITH_AES_128_CBC_SHA,
                 SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA

                 - jeder zwei x.509 Zertifikate: eines fuer connection,
                   eines fuer identity

            2. "renegotiation" (tor >= 0.2.0.21)

               - init: no cert

               - responder single certificate,

               - initiator renegotiates

                 - MUST HAVE ONE NOT IN ABOVE

            3. "in-protocol" (tor >= 0.2.3.6-alpha)

               distinguish vs v2 via one of:

               - self-signed cert

               - commonName ends other than ".net"

               - public key modulus >= 1024 bits

               then: send VERSIONS cell, ...

          - several security /accessibility features:
            - fixed protocol list
            - fixed choice of response
            - no server without valid certificates
    3. Cell Packet Format
       ...
    5. [@5]
       5. [@5] Routing relay cells
          checks circID, crypts payload, inspect payload (6.1), work it
*** quotes
    0. [@0] older version
       - Tor relays are also identified by "nicknames"; these are specified in
         dir-spec.txt.
    2. [@2] Connections
       - <<O1>> All implementations MUST support the SSLv3 ciphersuite
         "SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA", and SHOULD support the TLS
         ciphersuite "TLS_DHE_RSA_WITH_AES_128_CBC_SHA" if it is
         available.
       - In all of the above handshake variants, certificates sent in the
         clear SHOULD NOT include any strings to identify the host as a
         Tor relay. In the "renegotiation" and "backwards-compatible
         renegotiation" steps, the initiator SHOULD choose a list of
         ciphersuites and TLS extensions to mimic one used by a popular
         web browser.
       - TLS connections are not permanent.
    7. [@7] Flow control
       1. Link throttling
          - Each client or relay should do appropriate bandwidth throttling to
            keep its user happy. [...]
            The mainline Tor implementation uses token buckets (one for
            reads, one for writes) for the rate limiting.
*** ref [[file:master.bib::tor-spec][Dingledine & Mathewson 2016: Tor Protocol Specification]]
** TODO [#C] [[./randomforest2001.pdf][Breiman - RANDOM FORESTS]]
*** summary
*** quotes
*** ref [[file:master.bib::DBLP:journals/ml/Breiman01][Breiman 2001: Random Forests]]
** TODO [#C] [[./Oya.pdf][Do dummies pay off-Limits of dummy traffic protection in anonymous communications]]
*** summary
    0. [@0] Abstract

       - Communication patterns recognizable

       - Dummy evaluated

    1. Introduction

       - least squares approach

       - model adversary error

       - design guidelines

    2. System and Adversary Model

       - mix-based

       - 4 stages

         1. discard dummies

         2. Pool: select the ones to send

         3. Mix: change from input

         4. add new dummies

       - Global passive adversary

         - goal: determine communication channel probabilities

    3. A Least Square Profile Estimator for Dummy-based Anonymization
       Systems
*** quotes
    - In practice, user behavior and latency constrain the composition
      of anonymity sets
    - Another shortcoming of previous works [5, 9, 10] is that the
      proposed evaluation strategies cannot be used to guide the
      design of effective dummy generation strategies, which is
      recognized to be a hard problem [11]. This has lead the deployed
      high latency anonymous communication systems to either implement
      arbitrary dummy strategies [12] or no dummy traffic at all [11].
    - (ends 1)
    - capital letters to denote random variables andlower-case letters
      to denote their realizations.
    - high-latency mix-based anonymous communication channel
    - (ends 2)
*** ref
    [[file:~/da/docs/master.bib::pets14-dummy-traffic][Simon & Perez-Gonzalez 2014: Do]]
** TODO [#C] [[./browser.html][The Design and Implementation of the Tor Browser]]
*** summary
    1. Introduction

       adversary mode, design requirements and implementation of 4.5

       1. Browser Component Overview

          based on firefox esr, patches, torbutton, pref changes

          tor laucher addon: splash screen & progress bar

          https-everywhere, noscript, extension prefs changed:  https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/Bundle-Data/linux/Data/Browser/profile.default/preferences/extension-overrides.js

          pluggable transports

    2. Design Requirements and Philosophy

       0. [@0]

          - security (MUST) and privacy (SHOULD) requirements

          - browsers meeting security are endorsed, privacy leads to tbb

       1. Security Requirements: minimal requirements

          - proxy obedience: MUST NOT bypass

          - State Separation: no shared state with non-Tor browsing
            (including plugins, SSL, etc)

          - Disk Avoidance: must not write browsing-revealing info to
            disk, except for opt-in

          - Application Data Isolation: able to completely uninstall
            evidence that it was used

       2. Privacy Requirements

          optional features, makes tbb prefer one browser instead of another

          1. cross-origin identifier unlinkability: identifiers
             (cookies etc) not linkable

          2. cross-origin fingerprinting unlinkability: fingerprinting
             from one domain to another should not happen

          3. long-term unlinkability: give user possibility to clear state

       3. Philosophy

          - stay a browser, act as user expects it

          - break as few sites as possible

          - restrict plugins (proxy-mode not heeded, help fingerprint, ...)

          - minimize options (these help fingerprint, etc), do not
            load addons by default, decide privacy per URL bar origin

          - do not use domain-based filter addons

          - update

    3. Adversary Model

       1. Adversary Goals

          1. Bypassing proxy settings: make browser not use Tor

          2. Correlation of Tor vs Non-Tor Activity: f.ex. cookies, js, etc

          3. History disclosure: query user's history

          4. Correlate activity across multiple sites: f.ex. server
             targeted ads ("without the user's explicit consent")

          5. Fingerprinting/anonymity set reduction: browser build or web site

          6. History records and other on-disk information: seize computer

       2. Adversary Capabilities - Positioning

          can be at

          - exit node

          - ad servers

          - local ISP/network/upstream

          - physical access

       3. Adversary Capabilities - Attacks

          1. Read and insert identifiers

             - many storage ways: DOM, cookie, plugin, etc

               - also per MITM possible (unless fixed)

          2. Fingerprint users based on browser attributes

             - request-parameters

             - many attributes just by JS

             - additional by CSS media queries

             - more by plugins

          3. Website traffic fingerprinting

             - deduce website from encrypted traffic pattern

             - hard to do as internet is huuuge

             - light-weight defenses "worthwhile and effective"

          4. Remotely or locally exploit browser and/or OS

             - after-the fact attack: avoid disk writes

             - otherwise hard to mitigate, maybe via TAILS with
               frequent reboots

    4. Implementation

       subsections, correspond to design goals, sometimes
       Implementation Status when not complete or problematic

       1. Proxy Obedience

          - set prefs, disable webrtc, patch browser, audit code, (see
            quotes)

          - disable plugins by default, allow only flash and gnash,
            load these only on-demand

          - popup for helper-apps, filter drag-and-drop

          - ask for every addon install, disable system-wide addons

       2. State Separation

          separate from other firefox state, flash cookies disabled
          via plugin-disabling

       3. Disk Avoidance
*** quotes
    4. [@4] Implementation
       1. Proxy Obedience
          -  We have verified that these settings and patches properly
            proxy HTTPS, OCSP, HTTP, FTP, gopher (now defunct), DNS,
            SafeBrowsing Queries, all JavaScript activity, including
            HTML5 audio and video objects, addon updates, WiFi
            geolocation queries, searchbox queries, XPCOM addon
            HTTPS/HTTP activity, WebSockets, and live bookmark
            updates. We have also verified that IPv6 connections are
            not attempted, through the proxy or otherwise (Tor does
            not yet support IPv6). We have also verified that external
            protocol helpers, such as SMB URLs and other custom
            protocol handlers are all blocked.
       8. [@8] Other Security Measures
          2. [@2] Website Traffic Fingerprinting Defenses
             - These shortcomings and fallback behaviors are the
               primary reason that Google developed SPDY as opposed to
               simply extending HTTP to improve pipelining.
             - Unfortunately, the bias in favor of compelling attack
               papers has caused academia to ignore this request thus
               far, instead publishing only cursory (yet
               "devastating") evaluations that fail to provide even
               simple statistics such as the rates of actual pipeline
               utilization during their evaluations, in addition to
               the other shortcomings and shortcuts mentioned earlier.
*** ref [[file:master.bib::tor-browser-design-impl][Perry et al. 2015: Design Implementation Tor Browser]]
** TODO [#C] [[./LZCLCP_NDSS11.pdf][Luo - HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows]]
*** summary
    0. [@0] Abstract

       - defense against HTTPS information leakage

    1. Introduction

       HTTPOS (http[s] obfuscation) modifies traffic.

       was very successful in their tries against single-hop attacks

    2. Threat models

       - web sites and pages (at page level, IP visible)

       - ajax-interaction regarded as new pages

       - HTTP proxy

         - can use TCP features

    3. Defending against traffic-analysis attacks

       analyze classification and propose defenses

       1. The state-of-the-art attacks

          - Sun's SSWRPQ: "number and size"

            - amount from server between two consecutive requests

            - then jaccard coefficient

          - Bissias: time and size

            - cross-correlation(packet size,IAT sequence)

          - Liberatore: flow direction, packet size; jaccard + bayesian

            - LLJC: D = {d_1, d_2 ...} set of tuples

              S(D_{new}, D_i) = |D_{new} \cap D_i| / |D_{new} \cup D_i|

              for D_i tuples for i-th website and D_{new} new trace
              observed sizes

            - LLNBC: "The relationship between V and D is that for a
              given feature in V, its value is equal to zero if the
              corresponding tuple is not in D and KDE is not used."

          - Chen: also,

            - flow vector, not individual packets

       2. Two defense strategies

          0. [@0]

             - "introducing features that have not been involved in
               training the algorithm."

             - confuse classifier to make it misclassify

          1. The diffusion strategy

             - (Lemma 1) if based on packet size, and a flow has only
              packets of size not know to classifier, then random guess
              is best alternative for LLJC, LLNBC and SSWRPQ

               - Proof:

                 - LLJC: S(D_new, D_i) = 0 for all,

                 - similar for SSWRPQ as also JC

                 - LLNBC: V by definition 0, if kernel function adds up
                   to \neq 0, choose dissimilar sizes, so that sum = 0

               - [34]: small number of values in packet sizes, thus
                 lemma1-defense feasible

             - solution: only choose values not occuring in size set

          2. The confusion strategy
*** quotes
   0. [@0] Abstract
      - Extensive evaluation of HTTPOS on live web traffic shows that it
        can successfully prevent the state-of-the-art attacks from
        inferring private information from encrypted HTTP flows.
   1. Introduction
      - the efficacy of these methods has not been validated thoroughly
        based on actual implementations and live HTTP traffic.
      - TCP (e.g., Maximal Segment Size (MSS) negotiation and
        advertising window) and HTTP (e.g., HTTP Range and HTTP
        Pipelining).
   3. [@3] Defending against traffic-analysis attacks
      0. [@0]
         - four basic features that can affect the information used by
           those traffic-analysis attacks: packet size, timing of packets,
           web object size, and flow size.
      1. [@3] The diffusion strategy
         - we extract HTTP flows that have at least five packets
*** ref [[file:master.bib::httpos][X. & Perdisci 2011: HTTPOS]]
** TODO [#C] [[./ton96.pdf][Crovella - Self-Similarity in World Wide Web Traffic: Evidence and Possible Causes]]
*** summary
*** quotes
*** ref M. E. Crovella and A. Bestavros. Self-similarity in World Wide
      Web traffic: evidence and possible causes. Networking, IEEE/ACM
      Transactions on, 5(6):835–846, 1997.
** TODO [#C] [[./python-doc-howto-doanddont.pdf][Idioms and Anti-Idioms in Python]]
*** summary
    1. Language Constructs You Should Not Use

       1. from module import *

          - Inside Function Definitions

            invalid

          -
*** quotes
    - [import * inside functions] made the function execution slower,
      because the compiler could not be certain which names were local
      and which were global.
    - You should try to use as few except clauses in your code as you
      can
** TODO [#C] PART [[./Curose-Ross - Computer_Networking_-_A_Top-down_Approach_Featuring_the_Internet__Third_Edition.pdf][Kurose - Computer Networking: A Top-Down Approach Featuring the Internet]]
*** TODO http
*** summary (up to p.103)
    1. Computer Networks and the Internet

       7. [@7] Protocol Layers and Their Service Models

          0. [@0]

             - example: flying

               - complex system

               - as user: built in layers:

                 - ticket,

                 - baggage/claim

                 - gate: load people+bags/unload

                 - takeoff/landing

                 - flight routing

               - at arrival: back up the stack

               - could change layer's implementation if service it
                 provides remains the same

          1. The Internet Protocol Stack

             - layer names: frame, datagram, segment, message

             - layers

               - application, such as HTTP

               - transport: TCP or UDP

               - link: *IP*

               - physical: ethernet etc

          2. Network Entities and Layers

             - hosts: all layers

             - bridge: 1+2

             - router: 1-3

       8. Internet Backbones, NAPs and ISPs

    2. [@2] Application Layer

       2. [@2] The World Wide Web: HTTP

          0. [@0]

             - "killer application" of the internet

             - major societal impacts:

               - telephone 1870

               - broadcast radio/television 1920-30

               - web

          1. Overview of HTTP

             - client/server

             - web page with objects: HTML file, pictures, (stylesheets), etc

               - most: base HTML file and "referenced objects"

             - HTTP: client/server language

               - on top of TCP

               - stateless protocol

               - 1.0 to 1.1 (to 2.0)

          2. Non-Persistent and Persistent Connections

             - 1.0 is non-persistent by default, 1.1 the opposite

             - Non-Persistent Connections

               - example: TCP connection for each object

               - parallel or serial: depends on browser (standard)

             - Persistent Connections

               - advantages

                 - less overhead at client and *server*

                 - each object suffers two RTTs

                 - each object suffers from slow-start (but parallelizable)

               - without pipelining: one RTT, connection hangs

               - with:

                 - one RTT for all referenced objects (not one for each)

                 - hangs less

          3. HTTP Message Format

             request and response messages

             1. HTTP Request Message

                - request
                  method url version
                  =GET /somedir/page.html HTTP/1.1=

                - header[s]
                  name: value
                  =Accept: text/html, image/gif, image/jpeg=

                - CRLF (empty line)

                - body if method requires it (POST) f.ex.

             2. HTTP Response Message
*** quotes
*** ref [[file:master.bib::DBLP:books/daglib/0001977][Kurose & Ross 2001: Computer]]
** TODO [#C] [[~/da/git/docs/timing-fc2004.pdf][Timing Attacks in Low-Latency Mix Systems]]
** TODO [#C] [[./cacr2015-09.pdf][Wang - On Realistically Attacking Tor with Website Fingerprinting]]
*** summary
    0. [@0] Abstract

       - laboratory vs real

         - training data similar to testing data

         - single page each

         - noise (not studied)

       - to real

         - maintain fresh training set

         - split traces

         - users cannot generate noise (?)

    1. INTRODUCTION

       - 6 laboratory assumptions

         1. template websites

            but only used by cai

         2. closed-world

            but: expanded to open-world f.ex. by w&g

         3. replicability

            recording conditions and data freshness

         4. behavior: sequential one-at-a-time browsing

         5. parsing/splitting data: possible to split

         6. no background traffic: adversary can filter

       - 3 addressed by

         - needs only small amount of data

         - easy to keep fresh

         - update method

       - 4+5 addressed by splitting (time- and classification-based)

       - 6 hard problem, but hard to generate bg noise

       - no new classifier

    2. RELATED WORK

       0. [@0]

          - open world is practical

          - XKEYSCORE

       1. Closed world on encrypted channels

          - first each resource on separate connection (before
            persistent http)

          - later just packet sizes

          - worked: which out of 100

       2. From encrypted channels to Tor

          - Dyer: dummies

          - Panchenko: first (57% vs 0.2%)

          - WG (97% vs 0.2%) single page open-world

       3. From closed world to open world

          - WG only (83% vs 6%) for 100 pages open-world

          - WG-kNN: (85% vs 0.6%), faster

    3. BACKGROUND

       1. Tor and Tor Browser

          - entry, middle, exit, padding, no global protection

          - here: much weaker: local passive adversary

       2. Website Fingerprinting

          - tor: only (timing,direction)-tuples

          - wrong classified page counts as false positive

          - "As the base incidence rate [...] is expected to be low,
            the false positive rate must be low as well to avoid the
            base rate fallacy."

       3. Experimental setup and evaluation

          - custom profile to enable automatic page loading

          - modify Tor to distinguish side-loaded pages

          - disabled entry guards

          - sets: 120 censored pages

            - 5000 of alexa top 10000 for open world background

    4. TRAINING SET MAINTENANCE

       0. [@0]

          - keeping training data updated is easy

            - you need little training data

            - it can be updated on a single PC by age, consistency, or relevance

       1. Training set size

          - number_sites * number_instances + number_non_monitored_sites

          - fixed number_sites using wang-kNN

            - number_instances = 31

            - number_non_monitored_sites = 3700

            - for number_sites = 100: 6800 pages

            - FPR \uparrow with TPR \uparrow for fewer non-monitored data points

          - doable

          - versioning: auto-update

          - must be at most 10 days apart

       2. Training set update

          - some pages change fast, others slowly

            - better to update smartly

          - score for consistency and relevancy

          - update based on this score gave better results than
            updating all

    5. SPLITTING ALGORITHMS

       0. [@0]

          - in lab, easy to split while capturing,

          - in reality, harder

          - missplits lead to misclassification

       1. Terminology

          - full sequence split into cell segments,

            - aim: each is single-page segment, not two-page or multi-page

       2. Splitting process

          1. time-based splitting may already yield single-page

          2. classification-based to split multi-page segments via ML

          multi-page unlikely (probability and explicit minimization
          strategy)

       3. Time-based splitting

          - gap parameter

            - too big yields multi-page segments

            - too small splits single-page segments

       4. Classification-based splitting

          - Classes of problems

            1. two pages, positive-time: split by less than time parameter

            2. two pages, zero-time: user clicks link on loading page

            3. two pages, negative-time: multi-tab browsing

            4. single page

          - Split decision: split or not, alternative classifiers

            1. kNN: original by Wang et al

            2. Time-kNN: with added time parameters

            3. SVM: panchenko with fixed parameters

          - Split finding: if decision == "yes", select page after
            which to split (TODO)

            1. kNN: 23 new features, highest scoring selected as split

            2. LF-kNN: cells before and after splits to classify

            3. naive bayes, due to probabilistic scoring, same feature
               set as kNN

    6. SPLITTING RESULTS

       optimal time-based, classification works, too

       1. Experimental setup

          - collect via tor

          - log cell types

          - start new at =STREAM BEGIN= cell

          - multi-tab and single-tab with 5--10 secs in between

            - re-sort already finished first tab

       2. Time-based splitting

          - optimal at t=1.5s

            - both less and more lead to decreased tpr

          - re suggestion to decrease further: studies show mean dwell
            rate at 1minute

       3. Classification-based splitting

          - metrics

            - accuracy: consider correct if within 25 cells of real,
              yields tpr above 50--60%

            - deviance: how far from correct split

          - split decision: time-knn and knn same quality, svm lacked
            at distinguishing 4

          - split finding: NB like random guess, kNN more tolerant,
            LF-kNN slightly worse than kNN

    7. REMOVING NOISE

       hard to classify if there, but hard to create with Tor Browser Bundle

       1. Characterizing noise
*** quotes
    1. INTRODUCTION
       - it is very hard to generate sufficient background noise on
         Tor to disrupt WF due to the design of the Tor Browser.
    2. RELATED WORK
       - Tor is bandwidth starved [21]
    5. [@5] SPLITTING ALGORITHMS
       4. [@4] Classification-based splitting
          - This is the hardest class to split as there is no noticeable gap
            nor a clear pattern of cells indicating the gap.
          - We selected parameters for the SVM as it is highly
            sensitive to incorrect parameters.
    6. SPLITTING RESULTS
       3. Classification-based splitting
          - under 5 cells the decrease in TPR is negligible
          - a cell segment which lost its first few cells is
            significantly harder to classify (due to the fact that the
            kNN uses the first few cells as an important feature)
    10. [@10] REFERENCES
        - [21] Tor. Tor Metrics Portal.
          https://metrics.torproject.org/. Accessed Feb. 2015.
*** ref [[file:master.bib::realistic][Wang & Goldberg 2015: Realistically Attacking Tor Website Fingerprinting]]
** TODO [#C] [[./webtiming.pdf][Felten - Timing Attacks on Web Privacy]]
*** ref [[file:master.bib::DBLP:conf/ccs/FeltenS00][Felten & Schneider 2000: Timing Web]]
** TODO [#C] [[./p5-extended.pdf][Sherwood - P5 : A Protocol for Scalable Anonymous Communication]]
** TODO [#C] [[./oakland05torta.pdf][Murdoch - Low-Cost Traffic Analysis of Tor]]
*** summary
*** quotes
*** ref [[file:master.bib::torta05][Murdoch & Danezis 2005: Low Cost Traffic Analysis Tor]]
** TODO [#C] [[./p434.pdf][Casalicchio - A client-aware dispatching algorithm for web clusters providing multiple services]]
*** ref
E. Casalicchio and M. Colajanni. A client-aware dispatching
      algorithm for web clusters providing multiple services. In
      Proceedings of the 10th international conference on World Wide
      Web, pages 535–544, 2001.
** TODO [#C] [[./rfc3986.html][Berners-Lee - Uniform Resource Identifier (URI): Generic Syntax]]
*** summary
    0. [@0] Abstract

       - URI: compact identifier ("abstract or physical")

    1. Introduction

       supersedes URL, URN docs

       1. Overview of URIs

          0. [@0]

             - uniform: same form for different things, extensible

             - resource: electronic doc, image, information source,
               service, book, corporation, "human beings",
               mathematical operator, mathematical relation

             - identifier: distinguish from set, not necessarily access

             - does not limit nature, reasons, or systems for resource
               access

             - does not require persistence

             - global scope, but context may change
               (f.ex. =http://localhost=)

          1. Generic Syntax

             - scheme: starts, specifies how identifiers are assigned

          2. Examples: several schemes, see quotes

          3. URI, URL, and URN

             - URL provides way to access/locate

             - URN: globally unique and persistent

             - maybe both at the same time

       2. Design Considerations

          1. Transcription

             - URI sequence of characters, not always octets

               - might be transcribed, needs to be enterable on keyboards

               - needs to be remembered

             - non-ascii possible by percent-encoded octets, if
               allowed by scheme

          2. Separating Identification from Interaction

             - URIs are not necessarily accessible

             - /resolution/ of an URI determines access mechanism

             - /dereference/ URI via access mechanism: do sth with URI

               - /retrieval/: use URI to retrieve "representation of
                 its associated resource"

               - /representation/: octets plus state metadata

          3. Hierarchical Identifiers

             - documents often organized on trees

             - relative referencing of URIs allows for different
               scheme-access (f.ex. file, ftp and http for tree of
               HTML docs)

       3. Syntax Notation: ABNF as of RFC2234, see appendix a

    2. Characters

       based on US-ASCII (but codepoints == numbers)

       1. Percent-Encoding

          - if outside the allowed set or used as a delimiter

          - =%= followed by two hexadecimal digits

       2. Reserved Characters

          - encode unless specifically allowed (see quotes)

       3. Unreserved Characters

          - all others (alpha, digit, [-._~])

       4. When to Encode or Decode
*** quotes
    0. [@0] Abstract
       - A Uniform Resource Identifier (URI) is a compact sequence of
         characters that identifies an abstract or physical resource.
    1. Introduction
       1. Overview of URIs
          0. [@0]
             - These terms should not be mistaken as an assumption
               that an identifier defines or embodies the identity of
               what is referenced
             - URIs that identify in relation to the end-user's local
               context should only be used when the context itself is
               a defining aspect of the resource, such as when an
               on-line help manual refers to a file on the end- user's
               file system (e.g., "file:///etc/hosts").
          2. [@2] Examples
             - news:comp.infosystems.www.servers.unix
             - urn:oasis:names:specification:docbook:dtd:xml:4.1.2
    2. Characters
       2. [@2] Reserved Characters
          - reserved    = gen-delims / sub-delims
            gen-delims  = ":" / "/" / "?" / "#" / "[" / "]" / "@"
            sub-delims  = "!" / "$" / "&" / "'" / "(" / ")"
                        / "*" / "+" / "," / ";" / "="
** TODO [#C] [[./wright-passive.pdf][Wright - Defending Anonymous Communications Against Passive Logging Attacks]]
*** summary
    0. [@0] Abstract

       - passive logging via anonymous communication networks

       - better: non-uniform placement, practice harder than theory

       - worse: dynamic membership makes more vulnerable

       - frequent communication with the same site happens

    1. Introduction

       - previous work: predecessor attack

         - find out often-visited communication path initiator

         - has some assumptions

         - shown that possible to find out

       - assumptions relaxed

    2. Background

       1. Our Prior Work

          - restrict to: anonymity protocol with active set chosen at
            random

          - bounds to deanonymize given c of n hostile nodes

       2. Related Work

          - only few anonymity protocols provide security analysis
            /over time/

          - very little on user's web surfing: repeated sites over time

       3. The Predecessor Attack on Recent Protocols

          - p5 protocol assues users does not leave, protected

          - tarzan more secure than original onion routing

          - two others

    3. Paths Based on Non-Uniform Peer Selection

       0. [@0]

          - expand previous model

            - non-uniform path selection

            - nodes join and leave anonymity set

          - onion routing: intermediate anonymity level

          - predecessor: two attacker nodes. one last. if other is one
            hop before /more often/, this makes it more likely that it
            is the originator

       1. Model

          - n nodes, of which c < n attackers

            - fixed membership in static model

          - each node has one fixed responder, creates proxies, sends
            at least one message each round

          - only passive eavesdropping (no protocol circumvention, or
            active attacks)

          - attacker should be able to trigger rounds (otherwise
            inconvenient for others people who join and then need to
            wait)

       2. Static Membership Model

          0. [@0]

             - initiator does not choose at random, but fixes some positions

          1. Fixed First Node Defense

             - always choose first node H fixed

               - if innocent, predecessor found is H

               - else, can detect more easily

                 - (but how about frequent connections, don't they
                   betray the initiator clearly and fast?)

          2. Fixed Last Position

             similar to fixed first node (need two, grows to fixed limit)

          3. Fixed First and Last Positions

             best: either broken at once, or never

          4. Middle Node

             weak, as helper can be found, and distinguished from initiator

       3. Dynamic Membership Model

          0. [@0]

             - from static (all nodes at the same time) to dynamic: nodes
               join and leave with iid

          1. Intersection Attack

             - observe all online nodes in time frame, choose frames
               where responder is active

             - intersect these lists until only member is initiator

             - caveat: two parties communicate with responder

             - depending on model: (pareto/exponential) either easy to
               limit to small number, or bit harder but still easy to
               find node

          2. Fixed Nodes with Instability

             - assumption: whole system resets after \rho rounds

             - detection rate bounded by 1 again

               - the more resets, the closer the detection rate goes to 1

       4. Simulating the Predecessor Attack
*** quotes
    2. [@2] Background
       2. [@2] Related Work
          - Reiter and Rubin's seminal work (crowds: anonymity for web
            transactions)
*** ref [[file:master.bib::wright03][Wright et al. 2003: Defending Anonymous Communication]]
** TODO [#C] [[./torspec/proposals/ideas/xxx-using-spdy.txt][Murdoch - Using the SPDY protocol to improve Tor performance]]
*** ref [[file:master.bib::tor-spec-spdy][Murdoch 2010: Using SPDY Tor]]
** TODO [#C] PART [[../../../chive/GG/it/emacs/org.pdf][Dominik - The Org Manual]]
*** TODO [#D] org-mode how to filter by tags
    [2016-07-29 Fr]
    [[file:~/Desktop/main.org::*Tasks][Tasks]]

*** summary
    1. Introduction

       1. Summary

          editor, planner, notes taker, agenda, technical doc system

       2. Installation

          pre-installed with emacs

       3. Activation

          - enabled by default

          - enable four global keys with
            #+BEGIN_SRC elisp
              (global-set-key "\C-cl" 'org-store-link)
              (global-set-key "\C-ca" 'org-agenda)
              (global-set-key "\C-cc" 'org-capture)
              (global-set-key "\C-cb" 'org-iswitchb)
            #+END_SRC

          - start non =.org= file with line like
            #+BEGIN_EXAMPLE
              MY PROJECTS -*- mode: org; -*-
            #+END_EXAMPLE

            - auto by setting =org-insert-mode-line-in-empty-file=

       4. Feedback

          how to report a bug/troubleshoot: complete example to
          mailing list with =M-x emacs-version= and =M-x org-version=

       5. Typesetting conventions used in this manual

          1. TODO keywords, tags, properties, etc.

             - TODO, WAITING: all caps for user-defined and built-in TODOs

             - boss, ARCHIVE: all caps for built-in, small caps for user tags

             - Release, PRIORITY: user-defined properties are
               capitalized, system are in all-caps

          2. Keybindings and commands

    2. Document structure

       4. [@4] Motion

          - C-c C-[n|p] [Next|Previous] heading.

          - C-c C-[f|b] [Next|Previous] heading same level.

       11. [@11] The Orgstruct minor mode

           - org-editing in non-org documents

           - when in sth like list (line starts with =1.=, f.ex)

           - start with =orgstruct-mode=

           - or =(add-hook 'message-mode-hook 'turn-on-orgstruct)= to .emacs

           - something about folding headline if defined right ...

             - but did not work

    3. Tables

       yes, with spreadsheet functionality via emacs' calc-package

       1. The built-in table editor

       5. [@5] The spreadsheet

          1. References

             - find out table reference via =C-c ?=

             - address field as @row$column

             - @0/$0 refers to current row/column (omitting does, too)

             - ranges: $1..$3: "first three fields in the current row"

             - $>>>: "third column to the right

             - Field coordinates in formulas

               - other table: =@3 = 2 * remote(FOO, @1$$#)=
                 (copies 2* row 1 of FOO-table to row 3)

               - example same file
                 #+BEGIN_SRC org
                   ,#+TBLFM: $1 = '(identity remote(asdf, @@#$1)):: $2 = '(identity remote(asdf, @@#$2))::$3 = '(identity remote(asdf, @@#$6)):: $4 = '(identity remote(asdf, @@#$7))
                 #+END_SRC
                 when table has #+NAME: asdf

          2. Formula syntax for Calc

             - different from RPN

             - function(function, data), fex

               - f.ex.: =$8=100*reduce(max, $2..$6)=

       6. Org-Plot

    4. Hyperlinks

       2. [@2] Internal links

          0. [@0]

             - use either Property =CUSTOM_ID= and reference with
               #CUSTOM_ID,

             - or <<target>>

               - refer to it via [ [target][link name] ]

               - or [ [target] ] which is turned into the object's number
                 (item, section, whatever)

          1. Radio targets

             use =<<<target>>>= to automatically highlight & link target
             anywhere in the document

    5. TODO items

       in-place: mark as TODO

       1. Basic TODO functionality

          ...

       6. [@6] Checkboxes

          - start list item with =[ ]=

          - toggle with =C-c C-c=

          - count with =[/]= or =[%]=

            - use property COOKIE_DATA := 'checkbox’ to toggle checkbox counting

    6. Tags

       labels, use :tag_name: "at the end of the headline"

       1. Tag inheritance

          - higher level tags inherited to all lower levels

          - many ways to customize this and its display

       2. Setting tags

          - =C-c C-q= anywhere, or =C-c C-c= on headline

            - adds tag right-aligned in line

          - auto-completes with all tags in a file

          - set default tags by

            - =org-tag-alist= hard global tags (replaces dynamic)

            - header: =#+TAGS: @work @home @tennisclub= overrides above

              - override to use dynamic with =#+TAGS:=

            - additional global tags: =org-tag-persistent-alist' (add #+TAGS)

              - disable per-file: =#+STARTUP: noptag=

            - quick-select:
              #+BEGIN_EXAMPLE
                #+TAGS: { @work(w) @home(h) @tennisclub(t) } laptop(l) pc(p)
              #+END_EXAMPLE
              (with mutual exclusion of work, home and tennisclub)

              - presents special interface:

                - key selects

                - RETURN finishes

                - C-g abort, TAB enter other tag, ...

       3. Tag hierarchy

    7. Properties and columns

       like tag, but key-value

       1. Property syntax

          see quotes

    8. Dates and times

       called "timestamp"

       1. Timestamps, deadlines, and scheduling
          types:

          - Plain timestamp; Event; Appointment
            #+BEGIN_SRC org
            <2006-11-01 Wed 19:15>
            #+END_SRC

          - Timestamp with repeater interval
            #+BEGIN_SRC org
               <2007-06-06 Mit 12:30 +1w>
            #+END_SRC

          - Diary-style sexp entries
            <%%(diary-float t 4 2)>

          - Time/Date range
            <2004-08-23 Mon>--<2004-08-26 Thu>

          - Inactive timestamp: triggers no agenda entry
            [2006-11-01 Wed]

       2. Creating timestamps

          0. [@0]

             - C-c . calls 'org-time-stamp

             - ...

             - S-[left|right] calls 'org-timestamp-[down|up]-day,
               change day at cursor by one day

          1. The date/time prompt

             - absolute:

               - *default date* filled in: date under cursor OR current

               - many ways to specify: 14, 2/5, Fri, sep 15,  12:45, w4

             - relative:

               - +/- is relative to today

               - ++/-- is relative to default date

               - add [hdwmy]

               - "." for today

             - range: start-[-]end OR start+duration

             - calendar: S-[right/left] day S-[down/up] week M-S[right/left]mon

          2. Custom time format

             - possible

       3. Deadlines and scheduling

          0. [@0]

             - start with special words =DEADLINE= and =SCHEDULED=

             - DEADLINE: warning that it needs to appear

               - different warning period with -3d, f.ex. =DEADLINE:
                 <2004-02-29 Sun -5d>=

             - SCHEDULED: similar, but scheduling removes deadline

               - delays information with -3d, f.ex. =<2004-12-25 Sat -2d>=

             - use -- to reschedule only first of repeating d/s

          1. Inserting deadlines or schedules

             - schedule via C-c C-s, (deadline via C-c C-d)

             - check with C-c / [d b a] (deadlines, both before, both after)

          2. Repeated tasks

             - every month
               #+BEGIN_SRC org
                 DEADLINE: <2006-01-01 Son +1m>
               #+END_SRC

             - combine w/warning|delay:
               #+BEGIN_SRC org
                 DEADLINE: <2006-01-01 Son +1m -3d>
               #+END_SRC

             - this updates by exactly one month:

               - next instance after today:++
                 #+BEGIN_SRC org
                   DEADLINE: <2008-08-17 Son ++1w>
                 #+END_SRC

               - exactly from today:.+
                 #+BEGIN_SRC org
                   DEADLINE: <2005-11-01 Tue .+1m>
                 #+END_SRC

       4. Clocking work time

    9. Capture - Refile - Archive

       special process to capture notes

       1. Capture

          quickly

          1. Setting up capture
             in [[file:~/.emacs]]
             #+BEGIN_SRC elisp
               (setq org-default-notes-file (concat org-directory "/notes.org"))
               (define-key global-map "\C-cc" 'org-capture)
             #+END_SRC
             or with another files instead of (concat...)

          2. Using capture

             - C-c c to capture

             - C-c C-c to end capture

             - C-c C-w to other place

             - C-c C-k to abort

             - others

          3. Capture templates

    10. Agenda views

        orga-stuff is scattered through file, views solve this

        1. Agenda files

    11. Markup for rich export

        f.ex. LaTeX can do more than org, how to prepare for richer export

        1. Structural markup elements

           - Document title: use a =#TITLE: ...= line

           - Headings and sections: set depth of level-to-headings via
             =#+OPTIONS: H:4=

           - Table of contents: by default before first headline

             - else: insert =#+TOC: headlines= at desired location

             - table of: listings: =#+TOC: listings=, tables: =#+TOC: tables=

           - Paragraphs, line breaks, and quoting:

             - paragraph by (\ge one) empty line, =\\= enforces line breaks

             - use =#+BEGIN_VERSE= for normal formatting,

             - =#+BEGIN_QUOTE= for indented and =#+BEGIN_CENTER= for centering

        2. Images and Tables

           - precede with
             #+BEGIN_EXAMPLE
               ,#+CAPTION: This is the caption for the next table or image (or link)
               ,#+NAME tab:sth OR fig:sth
             #+END_EXAMPLE

           - Images exported if no description in link (f.ex. =[./img.png]=)

        3. Literal examples

           see quotes

        4. Include files

           #+BEGIN_EXAMPLE
             ,#+INCLUDE: "~/.emacs" src emacs-lisp
           #+END_EXAMPLE

           - instead of src, f.ex. quote, example, etc

           - can also have =:minlevel n= to adjust levels to that (with
             org-include)

           - =:lines "5-10"= to only include certain lines

           - visit the file: =C-c ’=

        5. Index entries

           =#INDEX: keyword[!subkeyword]= on a line of its own below heading

        7. [@7] Embedded LATEX

           3. [@3] LATEX fragments

              - single =$= mode only when directly followed by letter,
                etc, ended by whitespace or punctuation

              - environments

           5. [@5] Using CDLATEX to enter math

              - simplifies, f.ex. inserts {} for ^, fr-TAB to \frac{}{} etc

              - install cdlatex, then =M-x org-cdlatex-mode RET=

    12. Exporting

        7. [@7] LATEX and PDF export

           3. [@3] Header and sectioning structure

              - by default, article class

              - set by =#+LATEX_CLASS: scrreprt=, f.ex.

              - class needs to be defined in =org-latex-classes=
*** quotes
    2. [@2] Document structure
       4. [@4] Motion
          - C-c C-n: Next heading.
       5. Structure editing
          - C-c *: Turn a normal line or plain list item into a
            headline (so that it becomes a subheading at its
            location).
       7. [@7] Plain lists
          - S-up, S-down: Jump to the previous/next item in the current list
       10. [@10] Footnotes
           - The footnote reference is simply the marker in square brackets,
             inside text. For example:
             #+BEGIN_EXAMPLE
               The Org homepage[fn:1] now looks a lot better than it used to.
               ...
               [fn:1] The link is: http://orgmode.org
             #+END_EXAMPLE
           - [fn:: This is the inline definition of this footnote]

             A LATEX-like anonymous footnote where the definition is
             given directly at the reference point.
    3. Tables
       1. The built-in table editor
          - C-c | org-table-create-or-convert-from-region

            Convert the active region to a table. If every line
            contains at least one TAB character, the function assumes
            that the material is tab separated. If every line contains
            a comma, comma-separated values (CSV) are assumed. If not,
            lines are split at whitespace into fields
          - M-[LEFT/RIGHT] org-table-move-column-[left/right]

            Move the current column left/right.
          - M-S-LEFT org-table-delete-column

            Kill the current column.
          - M-x org-table-export RET

            Export the table, by default as a TAB-separated file.
       2. Column width and alignment
          - To set the width of a column, one field anywhere in the column
            may contain just the string ‘<N>’ where ‘N’ is an integer
            specifying the width of the column in characters. [...]  Note
            that the full text is still in the buffer but is hidden. To see
            the full text, hold the mouse over the field—a tool-tip window
            will show the full content. To edit such a field, use the command
            C-c ‘ (that is C-c followed by the backquote). This will open a
            new window with the full field. Edit it and finish with C-c C-c.
    4. Hyperlinks
       4. [@4] Handling links
          - C-c C-x C-v: org-toggle-inline-images

            Toggle the inline display of linked images. Normally this
            will only inline images that have no description part in
            the link, i.e., images that will also be inlined during
            export. When called with a prefix argument, also display
            images that do have a link description. You can ask for
            inline images to be displayed at startup by configuring
            the variable `org-startup-with-inline-images`.
    5. TODO Items
       1. Basic TODO functionality
          - S-right / S-left

            Select the following/preceding TODO state, similar to
            cycling. Useful mostly if more than two TODO states are possible
       2. Extended use of TODO keywords
          2. [@2] TODO keywords as types
             - (setq org-todo-keywords '((type "Fred" "Sara" "Lucy"
               "|" "DONE")))

               In this case, different keywords do not indicate a
               sequence, but rather different types. So the normal
               work flow would be to assign a task to a person, and
               later to mark it DONE.
          5. [@5] Setting up keywords for individual files
             - For example, to set one of the two examples discussed
               above, you need one of the following lines, starting in
               column zero anywhere in the file:
               #+BEGIN_EXAMPLE
                 #+TODO: TODO FEEDBACK VERIFY | DONE CANCELED
               #+END_EXAMPLE
             - A setup for using several sets in parallel would be:
               #+BEGIN_EXAMPLE
                 ,#+TODO: TODO | DONE
                 ,#+TODO: REPORT BUG KNOWNCAUSE | FIXED
                 ,#+TODO: | CANCELED
               #+END_EXAMPLE
    6. Tags
       1. Setting tags
          - C-c C-q org-set-tags-command

            Enter new tags for the current headline. Org mode will
            either offer completion or a special single-key interface
            for setting tags, see below. [...] When called with a =C-u=
            prefix, all tags in the current buffer will be aligned to
            that column, just to make things look nice. TAGS are
            automatically realigned after promotion, demotion, and TODO
            state changes.
    7. Properties and columns
       1. Property syntax
          - need to be inserted into a special drawer (see Section 2.8
            [Drawers], page 15) with the name PROPERTIES. Each property
            is specified on a single line, with the key (surrounded by
            colons) first, and the value after it. Here is an example:
            #+BEGIN_SRC org
              ,* CD collection
              ,** Classic
              ,*** Goldberg Variations
                  :PROPERTIES:
                  :Title:     Goldberg Variations
                  :Composer:  J.S. Bach
                  :Artist:    Glen Gould
                  :Publisher: Deutsche Grammophon
                  :NDisks:    1
                  :END:
            #+END_SRC
          - C-c C-x p org-set-property

            Set a property. This prompts for a property name and a
            value. If necessary, the property drawer is created as well.
          - C-c C-c org-property-action

            With the cursor in a property drawer, this executes
            property commands.
    8. Dates and times
       2. [@2] Creating timestamps
          - C-c . org-time-stamp

            Prompt for a date and insert a corresponding timestamp.

          - C-u C-c .
            C-u C-c !

            Like C-c . and C-c !, but use the alternative format which
            contains date and time.
       4. [@4] Clocking work time
          1. Clocking commands
             - C-c C-x C-i org-clock-in

               Start the clock on the current item (clock-in).
             - C-c C-x C-o org-clock-out

               Stop the clock (clock-out). This inserts another
               timestamp at the same location where the clock was last
               started. It also directly computes the resulting time
               and inserts it after the time range as ‘=> HH:MM’.
    9. Capture - Refile - Archive
       6. [@6] Archiving
          1. Moving a tree to the archive file
             - =C-c C-x C-s= or short =C-c $=        =org-archive-subtree=

               Archive the subtree starting at the cursor position to
               the location given by org-archive-location.
          2. [@2] Internal archiving
             - Archived trees are not exported (see Chapter 12
               [Exporting], page 136), only the headline is. Configure
               the details using the variable
               org-export-with-archived-trees.
    11. [@11] Markup for rich export
        1. Structural markup elements
           1. Document title
              - The title of the exported document is taken from the special line
                #+BEGIN_EXAMPLE
                  #+TITLE: This is the title of the document
                #+END_EXAMPLE
                If this line does not exist, the title will be the name of the
                file associated to buffer, without extension, or the buffer
                name.
           5. [@5] Paragraphs, line breaks, and quoting
              - When quoting a passage from another document, it is customary to
                format this as a paragraph that is indented on both the left and
                the right margin. You can include quotations in Org mode
                documents like this:
                #+BEGIN_EXAMPLE
                  ,#+BEGIN_QUOTE
                  Everything should be made as simple as possible,
                  but not any simpler -- Albert Einstein
                  ,#+END_QUOTE
                #+END_EXAMPLE
           7. [@7] Emphasis and monospace
              - You can make words *bold*, /italic/, _underlined_,
                =code= and ~verbatim~
        3. [@3] Literal examples
           - You can include literal examples that should not be
             subjected to markup. Such examples will be typeset in
             monospace, so this is well suited for source code and
             similar examples.
             #+BEGIN_EXAMPLE
             Some example from a text file.
             #+END_EXAMPLE
           - For simplicity when using small examples, you can also
             start the example lines with a colon followed by a
             space. There may also be additional whitespace before the
             colon:

             Here is an example
               : Some example from a text file.
           - This is done with the ‘src’ block, where you also need to
             specify the name of the major mode that should be used to
             fontify the example
           - *In literal examples*, Org will interpret strings like
             (ref:name) as labels, and use them as targets for
             special hyperlinks like [[(name)]]
        4. Include files
           - During export, you can include the content of another file.
           - If the =:only-contents= property is non-nil, only the
             contents of the requested element will be included,
             omitting properties drawer and planning-line if present.
           - examples:
             #+BEGIN_EXAMPLE
               ,#+INCLUDE: "./paper.org::#theory" :only-contents t
               Include the body of the heading with the custom id theory
               ,#+INCLUDE: "./paper.org::mytable" Include named element.
               ,#+INCLUDE: "./paper.org::*conclusion" :lines 1-20
               Include the first 20 lines of the headline named conclusion.
             #+END_EXAMPLE
        7. [@7] Embedded LATEX
           1. Special symbols
              - =\ nbsp= will become &nbsp; in HTML and ~ in LATEX
           5. [@5] Using CDLATEX to enter math
              - =(add-hook 'org-mode-hook 'turn-on-org-cdlatex)=
    12. Exporting
        7. [@7] LATEX and PDF export
           1. LATEX export commands
              - C-c C-e l l	org-latex-export-to-latex

                Export as a LATEX file
           3. [@3] Header and sectioning structure
           4. Quoting \LATEX code
              - special code that should only be present in LATEX
                export with the following constructs: [...]
                #+BEGIN_LATEX
                All lines between these markers are exported literally
                #+END_LATEX
           5. LATEX specific attributes
              - Tables in LATEX export
                - :align :font :width

                  Set, respectively, the alignment string of the
                  table, its font size and its width. They only apply
                  on regular tables.
                - :float :placement

                  The :float specifies the float environment for the
                  table. Possible values are =sideways=,
                  =multicolumn=, =t= and =nil=. When unspecified, a
                  table with a caption will have a table
                  environment. Moreover, the =:placement= attribute
                  can specify the positioning of the float. Note:
                  =:placement= is ignored for =:float= sideways
                  tables.
              - Images in LATEX export
                #+BEGIN_SRC org
                  ,#+ATTR_LATEX: :width 5cm :options angle=90
                #+END_SRC
                - To modify the placement option of any floating
                  environment, set the placement attribute.
                  #+BEGIN_SRC org
                    ,#+ATTR_LATEX: :float wrap :width 0.38\textwidth :placement {r}{0.4\textwidth}
                    [[./img/hst.png]]
                  #+END_SRC
              - Example blocks in LATEX export
                - By default, when exporting to LATEX, example
                  blocks contents are wrapped in a ‘verbatim’
                  environment.
    15. [@15] Miscellaneous
        10. [@10] Interaction with other packages
            1. Packages that Org cooperates with
               - table.el by Takaaki Ota
                 - C-c ' org-edit-special

                   Edit a table.el table. Works when the cursor is in
                   a table.el table.
                 - C-c ~ org-table-create-with-table.el

                   Insert a table.el table. If there is already a
                   table at point, this command converts it between
                   the table.el format and the Org mode format. [...]
    16. Hacking
        6. [@6] Tables and lists in arbitrary syntax
           3. [@3] Translator functions
              - In particular, properties passed into the function
                (i.e., the ones set by the ‘ORGTBL SEND’ line) take
                precedence over translations defined in the
                function. So if you would like to use the LATEX
                translator, but wanted the line endings to be
                ‘\\[2mm]’ instead of the default ‘\\’, you could just
                overrule the default with
                #+BEGIN_EXAMPLE
                  ,#+ORGTBL: SEND test orgtbl-to-latex :lend " \\\\[2mm]"
                #+END_EXAMPLE
              - Please check the documentation string of the function
                [[file:/usr/share/emacs/site-lisp/org-mode/org-table.el::(defun%20orgtbl-to-generic%20(table%20params)][orgtbl-to-generic]] for a full list of parameters
                understood by that function, and remember that you can
                pass each of them into orgtbl-to-latex,
                orgtbl-to-texinfo, and any other function using the
                generic function.
** TODO [#C] PART [[./numprint.pdf][The numprint package]]
*** TODO [#C] reduce numprint printout
*** summary
    3. [@3] Customization

       4. [@4] Rounding numbers

          - use \nprounddigits{<digits>} to round each number (also in
            tables) to this many digits after the dot

    6. [@6] Print aligned numbers in tabulars

       1. The new column types

          - supposedly with =>{{\nprounddigits{4}}}n{3}{4}=

    8. [@8] Advanced customization

       1. Changing the output

          1. Without the autolanguage option

             - use =\npdecimalsign{.}= to switch to english decimal
               sign
*** quotes
    b) [@b] Lists of options and commands
       a) [@2] Commands
          - \npthousandthpartsep Change the thousand separator (only
            after the decimal sign).
** TODO [#C] [[./1410.2087v3.pdf][Feghhi - A Web Traffic Analysis Attack Using Only Timing Information]]
*** summary
    0. [@0] Abstract

       - timing-only attack against anonymizing proxies

       - also possible to cut streams into single traces

    1. INTRODUCTION

       - timing-only attack

         - eases splitting

         - successful

    2. Related Work

       - bissias

    3. ANATOMY OF A WEB PAGE FETCH

       - encrypted tunnel (vpn) hides source, deestination, ports,
         payload

         - assumption: equals size, start and end might also be
           concealed

         - observable: direction and timing of packets
*** quotes
    3. [@3] ANATOMY OF A WEB PAGE FETCH
       - Our experiments on use of uplink, downlink and
         uplink+downlink traffic suggest that downlink traffic
         provides no additional information regarding timing patterns
         over uplink traffic. The reason is that the timing of ACKs in
         uplink traffic is correlated to that of downlink packetswhich
         means that using only uplink traffic provides sufficient
         information.
*** ref [[file:master.bib::DBLP:journals/tifs/FeghhiL16][Feghhi & Leith 2016: Web Traffic Analysis]]
** TODO [#E] demultiply git/docs/ and chive/keepimport [|/home/web/keep]
** TODO [[./DOM3-Core.pdf][Document Object Model (DOM) Level 3 Core Specification]]
*** part: read some to quote for [[file:~/da/git/diplomarbeit.org::*Estimate%20Parameter:%20Number%20of%20Embedded%20Objects][Estimate Parameter: Number of Embedded Objects]]
*** summary
    0. [@0] What is the Document Object Model?

       1. Introduction

          - DOM = API for HTML and XML

          - language-independent: spec in CORBA OMG IDL,

          - bindings (examples) in EcmaScript and Java

       2. What the Document Object Model is

          - tree of DOM objects (actually forest)

          - objects: provide means to manipulate, ... data, encapsulate

            - not data model (which only encapsulates data)

       3. What the Document Object Model is not

          - binary spec

          - way of persisting objects

          - set of data structures, but *object model*

          - importance of data (semantics)

          - no competitor to Component Object Model, just HTML/XML access

       4. Where the Document Object Model came from

          - JavaScript way of accessing,

          - Dynamic HTML

          - W3C group joined by SGML/XML/HTML editor vendors (API for those)

       5. Entities and the DOM Core

          all entities (f.ex. =&amp;=) assumed to converted beforehand

       6. DOM Architecture

          - Core is main model

            - XML, HTML, XPath, Traversal, Range, Validation, Events,
              LS, Views, StyleSheets depend on Core

            - HTMLEvents, MutationEvents and UIEvents depend on Events

            - UIEvents and CSS depend on Views

            - KeyboardEvents, TextEvents and MouseEvents depend on UIEvents

            - CSS2 depends on CSS, MutationNameEvents depends on MutationEvents

       7. Conformance

          - can conform to some, or all, test with
            =DOMImplementation.hasFeature=

       8. DOM Interfaces and DOM Implementations

          - attributes do not imply specific data members - use get() and set()

          - DOM implementations can provide additional interfaces and objects

          - unclear which constructor to call. use createX()

    1. Document Object Model Core
*** quotes
*** TODO ref [[file:master.bib::dom3][W3C 2004: Document Object Model]]
** TODO [[./usersrouted-ccs13.pdf][Johnson - Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries]]
*** summary
    0. [@0] ABSTRACT

       Tor is more vulnerable than previously thought.

       - security evaluation based on

         - different user models

         - ASes etc

         - paths

    1. INTRODUCTION

       - controlling ASes or Internet exchange points (IXP)s lets you
         deanonymize easily after some time

         - six months for 80% of all types of users

         - single AS: 100% for users "in some common locations" in three months

           - IXP: 95% in three months

         - two AS: 1 day instead

         - bittorrent worse (and they degrade)

    2. BACKGROUND

       - GUARD flag: uptime median of familiar relays
         AND bandwidth \ge min(250kB, median_bw)

       - STABLE flag: weighted MTBF \ge median(weighted MTBF for active relays)

    3. RELATED WORK

       - Metrics and Methods for Evaluating Anonymity.

         - entropy

         - bandwidth-limited adversary

         - controls some relays

       - Traffic Correlation Attacks.

         - Øverlier,Syverson: discover Tor Hidden Servers

         - Murdoch,Danezis: traffic correlation attacks against Tor

         - here more in depth: how long to deanonymize

       - Network Adversaries

         - Feamster,Dingledine: diverse paths might be weaker

         - Edman,Syverson: AS-aware path selection

         - Akhoondi &al:geographic-based relay selection

         - Wacek &al: 18% of circuits may have same AS

         - Murdoch,Zielinski: IXPs vs ASs

         - Juen: AS and IXP diversity

         - here: fixed set of ASes and IXPs, correlation attacks

    4. SECURITY MODEL AND METRICS

       "types and amounts of adversary resources, as well as how he
       may use them."

       1. Adversary Model

          passive observer who can additionally add and "corrupt"
          existing resources

          1. Adversary Resources.

             - bandwidth view vs onion router view

             - Autonomous Systems route IP-traffic

             - Internet eXchange Points connect multiple

          2. Resource Endowment.

             - Institutions: ISP controls ASes

               - companies control one or multiple IXPs

             - might use number of compromised relays as metric, but
               Tor weighs by bandwidth

               - bandwidth more accurate as measure [6]

               - better yet "guard and/org exit bandwidth"

          3. Adversary Goals.

             - broad: deanonymize

             - more specific

               - certain user [class]

               - certain destinations

               - ever connect at all [during specific time]

       2. Security Metrics

          user-relevant metrics:

          1. "probability distribution for number of path compromises"
             (for user)

          2. probability distribution "until first path compromise"

    5. METHODOLOGY

       "Monte Carlo method"

       1. Path Simulator

          0. [@0]

             historical data, models user, network and client

          1. Tor Network Model

             tor metrics provide: relay status(time) [flags, exit policies,
             hibernation, ...]

          2. User Model

             - five scripts:

               - HTTP(S) 4 traces: gcalendar, docs; gmail, chat; gsearch; fb

               - IRC: one trace, 8am to 5pm, mo-fr, 27/day

               - bittorrent: download of single file 18/day, 12 to 6 a.m.

               - worst: HTTP via 6523

               - best: HTTP via 443

          3. Tor Client Model
*** quotes
    0. [@0] Abstract
       - Specific contributions of the paper include (1) a model of
         various typical kinds of users, (2) an adversary model that
         includes Tor network relays, autonomous systems (ASes), Internet
         exchange points (IXPs), and groups of IXPs drawn from empirical
         study, (3) metrics that indicate how secure users are over a
         period of time, (4) the most accurate topological model to date
         of ASes and IXPs as they relate to Tor usage and network
         configuration, (5) a novel realistic Tor path simulator (TorPS),
         and (6) analyses of security making use of all the above.
    1. [@10] References
       - [40] Tor Project, Inc. Tor Metrics Portal.
         https://metrics.torproject.org/, 2013.
*** ref: [[file:master.bib::ccs2013-usersrouted][Johnson et al. 2013: Users Get Routed]]
** TODO [[~/da/git/docs/onion-discex00.pdf][Onion Routing Access Configurations]]
** TODO [[./802163c-01_30r1.pdf][Traffic Model for 802.16 TG3 MAC/PHY Simulations]]
*** summary
    1. Introduction

       - model (wireless WAN)

       - one direction only

       - different types available

       - user: different type and different volume

    2. Description of the HTTP/TCP and FTP Model

       0. [@0]

          - 4 interrupted Poisson Processes (IPP)

       1. Application of the 4IPP model to fixed, wireless,
          point-to-multipoint WANs

          - 4 IPPs

          - self-similar traffic

          - ON vs OFF

          - transition probability rates c1 (to off), c2 (to on)

          - mean dwell time 1/c1 or 1/c2

          - short-to-long times c11, c14

          - one direction

       2. Basic assumptions of the model

          - 40% of max capacity used

          - 10% (of lan traffic) exiting

          - asymmetric

    3. Description of the HTTP/TCP and FTP traffic simulation model

       1. Basic 4IPP parametric model.

          four ipps, for each, _i, c1i, c2i per *unit of time*

       2. Scaling of the model: internal vs. external traffic

          - scale by computing the packets per timeframe, several
            tables given

    4. Description of Individual Subscriber Internet model

       - IPP, one per user per direction (up/down)

       - approximation

       - needs to yield to f.ex. voip

    5. Description of Individual Subscriber Internet traffic
       simulation model

       1. Basic IPP model (IPP#2)

          |    _i |       c1i |       c2i | packets/unit of time |
          | 1.698 | 1.445E-02 | 1.084E-02 |                .7278 |

       2. Scaling of the model

          to 15kbps

          |    _i |   c1i |   c2i | packets/unit of time |
          | 22.79 | .1940 | .1455 |                 9.77 |

    6. Description of voice model
*** quotes
*** ref
    [[file:master.bib::80216model][Baugh et al.: Traffic Model]]
** TODO [[./ipccc12-tor-performance.pdf][Panchenko - Improving Performance and Anonymity in the Tor Network]]
*** ref [[file:master.bib::ipccc12-performance][Panchenko et al. 2012: Improving Performance Anonymity Tor Network]]
** TODO [[https://trac.torproject.org/projects/tor/ticket/8470#comment:7][#8470 Request randomization a lot less random in FF17]]
   local: [[./lessrand.html]]
*** quotes
    - [...] the browser is often too slow to be able to keep the
      pipeline full of requests, and requests aren't packed together
      in Tor cells. This might be important, because it may also be
      the case that a browser that is driven around by selenium in a
      VM might be similarly too slow for pipelining's request
      combining to happen.
** TODO [[./tor-design.pdf][Dingledine - Tor: The Second-Generation Onion Router]]
*** summary
    2. [@2] Related work

       The first design ever was called Mix-Net. From there, two
       directions diverged. The one was highly anonymized,
       high-latency. The other tried to minimize latency. Among these,
       there are IP, TCP and application-level (e.g. HTTP)
       filters. TOR does a TCP design that is mostly distributed but
       has some fixed directory structure.

    3. Design goals and assumptions

       TOR's design aims to be easy to use by providing deployability,
       usability, flexibility and a simple design. It does not try to
       hide its traffic, normalize protocols, be 100% p2p or prevent
       end-to-end analysis. It is assumed that an adversary can control
       some onion routers and thus analyze or disturb the network.

       1. Threat Model

          adversaries can
          - observe fraction of traffic
          - can generate, modify, delete traffic
          - operate own ORs
          - compromise fraction of other ORs

          they are not global.

    4. The TOR design incorporates encapsulation over SSL (link-layer)
       links.  There are two cells types, for control and
       content. Checksumming, QOS-type rate limiting and congestion
       control are implemented.
       + Every OR has a TLS connection to every other OR. They send
         each other fixed-size cells each encrypted via
         quickly-changing TLS keys. The connection is signed via
         mid-term onion keys. The identity is provided via [[permanent
         identity key]].
       + TORs messages are either command/control/direct (2b circID, 1b
         cmd, 509b data) messages to the next OR or relay (2b cirdID,
         1b relay, 2b streamID, 6b digest, 2b len, 1b cmd, 498b data)
         messages to some other host. The circuit is created
         recursively using several hops, each of which creates its own
         symmetric key pair via DH. Each circuit OR can be asked to
         open a connection out. circuitID changes from hop to hop
       + when an OR receives a relay message, it attemps to decrypt the
         relay header and read the digest. If the digest is correct, it
         is accepted. Else, it is forwarded (or the stream is killed if
         at the end)
       + for QoS and fairness, a token-bucket mechanism is used and
         interactive streams get preferential treatment
       + congestion control is handled via two window mechanisms: one
         for circuit-level, the other for stream-level-throttling
    5. hidden services in TOR are implemented via introduction points:
       - bob advertises these, alice creates a rendezvous point, leaves
         a message at those intro points, bob connects to this
         rendezvous point
       - both the server and the client can be used unmodified: the
         server just behind tor, the client just using its onion proxy
       - there was some previous work, also on anonymizing cellphone
         usage
    6. miscellany
       - dos might be possible but has not been observed yet
       - exit policies:
         - thoughts of adding headers
         - mixed policies
       - directory servers
         - cached statements at ORs,
         - more efficient that just propagating messages
    7. attack & defense
       - passive attacks
         - traffic confirmation attacks outside of design goal, but
         - fingerprinting might be effective
         - user content, option distinguishability, timing
           correlations, size correlations, fingerprinting, latency
       - active attacks
         - five different keys can be compromised:
           tls session, tls cert, circuit session, circuit cert, identity
         - compromise of the OR itself has to be done in tight timeframe
         - run recipient, eases end-to-end attacks
           - approached by privoxy
         - run onion proxy
           - no solution
         - DoS non-observed nodes
           - best defense robustness
         - Run hostile OR, timing attacks, tagging cells, replace
           content, replay attacks, smear tor's name, hostile code
           (altered TOR software)
       - directory attacks
         - destroy directory server, subvert directory server, subvert
           majority, encourage dissent, insert hostile OR, do as if
           working correctly
       - rendezvous attacks
         - make many request, attack intro, compromise intro, compromise rend
    8. In da wild
       - works for variety of uses
         - download fast, latency varies greatly
    9. open questions
       - which path length (static/dynamic), which renewal frequency?
       - cascade / hydra /own OR
       - what if some central directory server do not suffice,
         non-clique aka restricted-route
       - w/o central authority, how to avoid bad nodes
       - anonymity gains from running own OR to lead to recommendation?
    10. future directions
        - scalability,
        - bandwidth classes [DSL, T1, T3] for ORs
        - incentives, cover traffic, cachin gat exit nodes, better
          directory distribution, further spec review, multisystem
          interoperability, wider-scale deployment
    11. references
*** quotes
  - Clients choose a path through the network and build a circuit, in
    which each node (or “onion router” or “OR”) in the path knows its
    predecessor and successor, but no other nodes in the circuit.
  - Rather than using a single multiply encrypted data structure (an
    onion) to lay each circuit, Tor now uses an incremental or
    telescoping path-building design, where the initiator negotiates
    session keys with each successive hop in the circuit.
  - Tor multiplexes multiple TCP streams along each circuit to improve
    efficiency and anonymity.
  - Tor’s decentralized congestion control uses end-to-end acks to
    maintain anonymity while allowing nodes at the edges of the
    network to detect congestion or flooding and send less data until
    the congestion subsides.
  - Certain more trusted nodes act as directory servers: they provide
    signed directories describing known routers and their current
    state.
  - Variable exit policies: Tor provides a consistent mechanism for
    each node to advertise a policy describing the hosts and ports to
    which it will connect. These exit policies are critical in a
    volunteer-based distributed infrastructure, because each operator
    is comfortable with allowing different types of traffic to exit
    from his node.
  - End-to-end integrity checking
  - Modern anonymity systems date to Chaum’s Mix-Net design [10].
  - Tor as described herein, Tarzan, MorphMix, Cebolla [9], and
    Rennhard’s Anonymity Network [44] build circuits in stages,
    extending them one hop at a time. Section 4.2 describes how this
    approach enables <<perfect forward secrecy>>.
  - by treating application connections as data streams rather than
    raw TCP packets, they avoid the inefficiencies of tunneling TCP
    over TCP.
  - Distributed-trust anonymizing systems need to prevent attackers
    from adding too many servers and thus compromising user paths. Tor
    relies on a small set of well-known directory servers, run by
    independent parties, to decide which nodes can join.
  - Each onion router maintains a TLS [17] connection to every other
    onion router.
  - Actually, the negotiated key is used to derive two symmetric keys:
    one for each direction.
  - Preliminary analysis with the NRL protocol analyzer [35] shows
    this protocol to be secure (including perfect forward secrecy)
    under the traditional <<Dolev-Yao>> model.
  - Once Alice has established the circuit (so she shares keys with
    each OR on the circuit), she can send relay cells.
  - (as an optimization, the first two bytes of the integrity check
    are zero, so in most cases we can avoid computing the hash).
  - Thus the “<<break a node and see which circuits go down>>” attack [4]
    is weakened.
  - (usually the last node, but maybe others due to exit policy
    conflicts; see Section 6.2.)
  - Because Tor uses TLS on its links, external adversaries cannot
    modify data. Addressing the insider malleability attack, however,
    is more complex.
  - given that the OP or OR tear down the circuit if they receive a
    bad hash.
  - a circuit’s edges can heuristically distinguish interactive
    streams from bulk streams by comparing the frequency with which
    they supply cells.
  - If enough users choose the same OR-to-OR connection for their cir-
    cuits, that connection can become saturated.
  - These arbitrarily chosen parameters seem to give tolerable
    throughput and delay; see Section 8.
  - This type of anonymity protects against distributed DoS attacks:
    attackers are forced to attack the onion routing network because
    they do not know Bob’s IP address.
  - thus hostnames take the form x.y.onion where x is the
    authorization cookie and y encodes the hash of the public key.
  - Rather than searching exit connections for timing and volume
    correlations, the adversary may build up a database of
    “fingerprints” contain- ing file sizes and access patterns for
    targeted websites. He can later confirm a user’s connection to a
    given site simply by consulting the database. This attack has been
    shown to be effective against SafeWeb [29].
  - On the other hand, an attacker who learns a node’s identity key
    can replace that node indefinitely by sending new forged descrip-
    tors to the directory servers.<<<permanent identity key>>>
  - The best defense here is robustness.
  - Our threat model explicitly assumes directory server operators
    will be able to ﬁlter out most hostile ORs.
  - once we have more experience, and assuming we can resolve the
    anonymity issues, we may partition traffic into two relay cell
    sizes: one to handle bulk traffic and one for interactive traffic.
  - Second, our end-to-end congestion control algorithm focuses on
    protecting volunteer servers from accidental DoS rather than on
    optimizing performance.
  - On the other hand, as our users remain satisﬁed with this
    increased latency, we can address our performance incrementally as
    we proceed with development.
  - [...] we still expect the network to support a few hundred nodes
    and maybe 10,000 users before we’re forced to become more
    distributed.
  - Throughout this paper, we have assumed that end-to-end trafﬁc
    conﬁrmation will immediately and automatically defeat a
    low-latency anonymity system.
  - (...somewhere...)
  - "Our threat model explicitly assumes directory server operators
    will be able to ﬁlter out most hostile ORs."
*** ref [[file:master.bib::tor-design][Dingledine et al. 2004: Tor]]
*** TODO indicate which sections are common to the 2014 design paper
** TODO [[./marionette_client.pdf][Marionette Python Client Documentation]]
*** summary
    0. [@0]

       - remotely control firefox (gecko-based browser)

    1. Getting the Client

       - install via

         #+BEGIN_SRC sh
           pip install marionette_client
         #+END_SRC

       - use virtualenv

    2. Using the Client for Testing

       see Marionette tests section

    3. Session Management

       initialize via

       #+BEGIN_SRC python
         client = Marionette('localhost', port=2828)
         client.start_session()
       #+END_SRC

       session_capabilities holds what the session can do
       (f.ex. rotate window on FF OS)

    4. Context Management
*** quotes
    - class marionette_driver.marionette.Marionette(host=’localhost’,
      port=2828, app=None, app_args=None, bin=None, profile=None,
      emulator=None, sd-card=None, emulator_img=None,
      emulator_binary=None, emulator_res=None,
      connect_to_running_emulator=False, gecko_log=None, homedir=None,
      baseurl=None, no_window=False, logdir=None, busybox=None,
      symbols_path=None, timeout=None, socket_timeout=360,
      device_serial=None, adb_path=None, process_args=None,
      adb_host=None, adb_port=None, prefs=None)
    - navigate(url)

      Navigate to given url. Navigates the current top-level browsing
      context’s content frame to the given URL and waits for the
      document to load or the session’s page timeout duration to
      elapse before returning. The command will return with a failure
      if there is an error loading the document or the URL is
      blocked. This can occur if it fails to reach the host, the URL
      is malformed, the page is restricted (about:* pages), or if
      there is a certificate issue to name some examples. The document
      is considered successfully loaded when the DOMContentLoaded
      event on the frame element associated with the window triggers
      and document.readState is “complete”. In chrome context it will
      change the current window‘s location to the supplied URL and
      wait until document.readState equals “complete” or the page
      timeout duration has elapsed. Parameters url – The URL to
      navigate to.
** TODO [[./Tor Project: Overview.html][Tor: Overview]]
** TODO [[shell:ristretto ./malamud][Malamud - Privacy and Private States]]
*** summary
*** quotes
    - Primo Levi observed that "solitude in a Camp is more precious
      and rare than bread."
** TODO [[./www.ted.com/talks/glenn_greenwald_why_privacy_matters/transcript.html][Greenwald:privacy]]
*** summary
    - youtube videos of people who think they are unobserved, do sth,
      horror on being seen (singing, dancing, nose picking, ...
    - Snowden revealed: US spying on every internet user
    - common thinking: only bad people have a reason for privacy
    - this is self-deprecation
    - about this mentality: people do not believe that. they safeguard
      their privacy (passwords, locks on bedroom and bathroom doors
    - cnet found out stuff about eric schmidt via google, he
      subsequently forbid his employees from talking to cnet
    - zuckerberg bought own house plus all four adjacent houses to
      have a zone of privacy
    - tell people who say they do not care: give me all your
      email/social media-passwords, let me publish whatever I find
      interesting
*** quotes
    - mass, indiscriminate surveillance
    - People can very easily in words claim that they don't value
      their privacy, but their actions negate the authenticity of that
      belief.
    - There are dozens of psychological studies that prove that when
      somebody knows that they might be watched, the behavior they
      engage in is vastly more conformist and compliant.
    - crucial to this design was that the inmates could not actually
      see into the panopticon, into the tower, and so they never knew
      if they were being watched or even when.
*** ref
    [[file:master.bib::privacy-g][Greenwald: Why Privacy Matters]]
** TODO [[./Levenshtein.html][Levenshtein-Distanz]]
*** quotes
    - ist die minimale Anzahl von Einfüge-, Lösch- und
      Ersetz-Operationen, um die erste Zeichenkette in die zweite
      umzuwandeln.
*** statt ref improved... zitieren. [[file:master.bib::wiki:levenshtein][Wikipedia 2016: Levenshtein]]
** TODO [[./EdmanS09.pdf][AS-awareness in Tor Path Selection]]
*** ref: [[file:master.bib::DBLP:conf/ccs/EdmanS09][Edman & Syverson 2009: AS Tor]]
** TODO [[./2012-jmlr.pdf][Sally: A Tool for Embedding Strings in Vector Spaces]]
** TODO [[./supersequence.pdf][On the Approximation of Shortest Common Supersequences and Longest Common Subsequences]]
** TODO [[./projects.html][Software & Services]]
*** summary
    - Tor Browser

      - "everything you need to safely browse the Internet"

      - requires no installation

    - ...
*** ref: [[file:master.bib::tor-ecosystem][2015: Software Services]]
** PART [[./python-doc-howto-logging.pdf][Logging HOWTO]]
*** summary
    1. Basic Logging Tutorial

       log(level[, data])

       1. When to use logging

          normal output for the user: use =print=, else use logging.level()

          WARNING is the default level

       2. A simple example

          only warning and above are shown:

          import logging
          logging.warning('Watch out!') # will print a message to the console
          logging.info('I told you so') # will not print anything

          WARNING:root:Watch out!

       3. Logging to a file

          logging.basicConfig(filename='example.log',level=logging.DEBUG)

       4. Logging from multiple modules

          import in every module

          call basicConfig in main entry point

       5. Logging variable data

          %s style, other options work maybe, too

       6. Changing the format of displayed messages

          #+BEGIN_SRC python
            logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
          #+END_SRC

       7. Displaying the date/time in messages
*** quotes
    1. Basic Logging Tutorial
       1. When to use logging
         |          | <56>                                                     |
         | DEBUG    | Detailed information, typically of interest only when diagnosing problems. |
         | INFO     | Confirmation that things are working as expected.        |
         | WARNING  | An indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected. |
         | ERROR    | Due to a more serious problem, the software has not been able to perform some function. |
         | CRITICAL | A serious error, indicating that the program itself may be unable to continue running. |
** TODO Sarle - Neural Network FAQ Part 1
*** summary
    1. Introduction

       - What is a neural network (NN)?

         *Artificial* Neural Network:

         parallelistic learning, experiential learning, asynchronous

       - Where can I find a simple introduction to NNs?

         see chapter 4

       - Are there any online books about NNs?

*** quotes
    - Some care should be invested into a summary:
      - simple concatenation of all the answers is not enough: instead,
       redundancies, irrelevancies, verbosities, and errors should be
       filtered out (as well as possible)
      - the answers should be separated clearly
      - the contributors of the individual answers should be
        identifiable (unless they requested to remain anonymous [yes,
        that happens])
      - the summary should start with the "quintessence" of the
        answers, as seen by the original poster
      - A summary should, when posted, clearly be indicated to be one
        by giving it a Subject line starting with "SUMMARY:"
      Note that a good summary is pure gold for the rest of the newsgroup
      community
    - If trained carefully, NNs may exhibit some capability for
      generalization beyond the training data, that is, to produce
      approximately correct results for new cases that were not used
      for training.
*** ref
    Sarle, W.S., ed. (1997), Neural Network FAQ, part 1 of 7:
    Introduction, periodic posting to the Usenet newsgroup
    comp.ai.neural-nets, URL: ftp://ftp.sas.com/pub/neural/FAQ.html
** TODO [[file:Regularization%20(mathematics)%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Regularization (mathematics) - Wikipedia, the free encyclopedia.html]]
** TODO [[~/da/git/docs/10.1.1.96.7225.pdf][A Guided Tour to Approximate String Matching]]
*** quotes
    - Despite the fact that most existing algorithms concentrate on
      the simple edit distance, many of them can be easily adapted to
      the generalized edit distance, and we pay attention to this
      issue throughout this work.
    - by allowing only insertions and deletions at cost 1, we can
      compute the *longest common subsequence* (LCS) between two
      strings.
    - simplification that has received a lot of attention is the
      variant that allows only substitutions (*Hamming distance*).
    - error correction. The physical transmission of signals is
      error-prone. To ensure correct transmission over a physical
      channel, it is necessary to be able to recover the correct
      message after a possible modification (error) introduced during
      the transmission. The probability of such errors is obtained
      from the signal processing theory and used to assign a cost to
      them.
    - Although this area has not developed much with respect to
      approximate searching, it has generated the most important
      measure of similarity, known as the Levenshtein distance
      [Levenshtein 1965; 1966] (also called “edit distance”).
    - (ends 2.2)
    - This paper is most concerned with the simple edit distance,
      which we denote ed(·). Although transpositions are of interest
      (especially in case of typing errors), there are few algorithms
      to deal with them. However, we will consider them at some point
      in this work (note that a transposition can be simulated with an
      insertion plus a deletion, but the cost is different). We also
      point out when the algorithms can be extended to have different
      costs of the operations (which is of special interest in
      computational biology), including the extreme case of not
      allowing some operations. This includes the other distances
      mentioned.
    - (ends 3.1)
** TODO [[~/da/git/docs/mlj12.pdf][Good edit similarity learning by loss minimization]]
** TODO Hypertext Transfer Protocol -- HTTP/1.0
*** summary
*** quotes
*** ref: [[file:master.bib::rfc1945][Berners-Lee et al. 1996: Hypertext Transfer Protocol]]
** TODO [[./Getting Started - Ajax.html]]
** TODO [[./XMLHttpRequest - Web API Referenz | MDN.6limit.html]]
** TODO [[./Vererbung und die Prototypkette - JavaScript | MDN.html]]
*** summary
    0. [@0]

       - chain of prototypes, last is =null=

       - more powerful than classic model

    1. Vererbung mit der Prototyp-Kette
** TODO [[./js/javascript%20-%20Firefox%20Addon%20observer%20http-on-modify-request%20not%20working%20properly%20-%20Stack%20Overflow.html][Firefox Addon observer http-on-modify-request not working properly]]
*** quotes
    - set up
      #+BEGIN_SRC javascript
        httpRequestObserver =
        {
          observe: function(subject, topic, data)
          {
            if (topic == "http-on-modify-request") {
                // [...] do sth here, from answer:
                var httpChannel = subject.QueryInterface(Ci.nsIHttpChannel);
                var uri = httpChannel.URI;
                var domainloc = uri.host;
            }
          },

          register: function()
          {
            var observerService = Cc["@mozilla.org/observer-service;1"]
                    .getService(Ci.nsIObserverService);
            observerService.addObserver(this, "http-on-modify-request", false);
          },

          unregister: function()
          {
            var observerService = Cc["@mozilla.org/observer-service;1"]
                    .getService(Ci.nsIObserverService);
            observerService.removeObserver(this, "http-on-modify-request");
          }
        };
      #+END_SRC
    - register observer
      #+BEGIN_SRC javascript
        httpRequestObserver.register();
      #+END_SRC
    - unload register
      #+BEGIN_SRC javascript
        exports.onUnload = function(reason) {
            httpRequestObserver.unregister();
        };
      #+END_SRC
*** TODO ref
** TODO [[./top_ml_algo.pdf][Top 10 algorithms in data mining]]
** TODO [[./esa2006b.pdf][An Improved Construction for Counting Bloom Filters]]
** TODO [[./torspec/proposals/001-process.txt][Mathewson - The Tor Proposal Process]]
*** TODO refactor from 4)
*** summary
   1. Overview: how to change spec, how proposals work, relationship to spec

   2. Motivation: sync spec to code, make participation easier, make
      spec readable

   3. How to change the specs now:

      - write detailed, with implementation

      - changes over time until consensus, until either rejected or
        accepted

   4. How new proposals get added: (new section)

      - devel list, format draft, rough developer consensus to investigate

   5. What should go in a proposal:

      - header: filename, title, author, created, status

        - optional: target, implemented-in

      - overview

      - free form
        - motivation
        - design
        - security
        - spec
        - compatibility
        - implementation
        - performance and scalability
   6. status: open, accepted, finished, closed, rejected, draft,
      needs-revision, dead, needs-research, meta
   7. numbers from 100 for actual proposals, meta and special 0 to 99
*** quotes
   3. [@3] How to change the specs now:
      - no proposal is ever the canonical documentation for an
        implemented feature
      - {It's still okay to make small changes directly to the spec if
        the code can be written more or less immediately, or cosmetic
        changes if no code change is required.  This document reflects
        the current developers' _intent_, not a permanent promise to
        always use this process in the future: we reserve the right to
        get really excited and run off and implement something in a
        caffeine-or-m&m-fueled all-night hacking session.}
   4. How new proposals get added: (new section)
      - To get your proposal in, send it to the tor-dev mailing list.

** TODO chaum-mix.pdf: Untraceable Electronic Mail, Return Addresses and Digital Pseudonyms
*** TODO refactor
*** summary
    1. Abstract: untraceable electronic mail over insecure
       connections, ballots, untraceable return addresses
    2. Introduction: "key distribution problem" addressed by public
       key cryptography, "traffic analysis problem" addressable based
       on this (without common authority)
    3. Notation: public K, private $K^{-1}$, encryption K(X),
       seal=encrypt, sign, random R, constant C (all zeros)
    4. Assumptions: network watchable, cryptography secure
    5. Mail System:
       - each mix i decrypts K_i(R_i,K_a(R_0,m),a) \to K_a(R_0,m), a
       - receipts allow for clients to prove that an item was left out
       - cascade of "mixes", with lexicographic ordered batches of items
    6. Return Addresses:
       - intermediary 1 decrypts and reencrypts K_1(R_1, A_x), K_x(R_0, M)
         \to A_x, R_1(K_x(R_0, M))
       - R_1 is part key, must not be repeated
       - allows for certified mail
       - cascade possible
    7. Digital Pseudonyms:
       ...
*** quotes
    - The order of arrival is hidden by outputting the uniformly sized
      items in lexicographically ordered batches.
    - The following two sections introduce the notation and
      assumptions.  Then the basic concepts are introduced for some
      special cases involving a series of one or more authorities.
      The final section covers general purpose mail networks.
** TODO control-spec.txt
*** TODO refactor
*** summary
    0. communication with locally running Tor process
*** quotes
** TODO dir-spec.txt
*** TODO refactor
*** summary
    0. Scope and preliminaries
       0. "describes TOR directory service v3"
       1. History:
          - before version 1, there was a fixed list of hosts
          - 0.0.2 introduced "directory authorities" that signed router lists,
            0.0.8 introduced "directory caches" that cached that list,
            there were also "router status" documents: just up/down-status
          - Version 2 switched to voting and partitioning the router list
            format: (hash(id_key), hash(descriptor), status summary)
       2. Goals of the version 3 protocol:
          less info->less bandwidth,
          avoid partitioning,
          store identity keys of authorities offline
       3. Some remaining questions:
          in V3: sha-1 is old, hardcoded IPs, longer router identity keys,
          later: all clients (/ all caches) knowing all routers won't scale
     1. ...
*** quotes
** TODO 2009-03-11-performance.pdf: Performance Improvements on Tor or, Why Tor is slow and what we’re going to do about it
*** TODO refactor
*** summary
    - so far mostly usability and blocking-resistance
    - six reasons why slow
      1. congestion control broken / web browsing
         - tcp/tor interaction
         - windows sizes
      2. some users clog network
         - squeeze circuit
         - throttle some at exit
         - throttle some at client
         - throttle all at client
         - default exit policy 80,443
         - user education
      3. overall capacity
         - advocacy
           - talk
           - support
           - fb app
           - look more
         - fund directly
         - fast tor on windows
         - find overloaded or broken relays
         - dynamic IPs
         - incentives
         - automatic relay in tor sw
      4. path selection not distributed
         - better balancing of traffic via ORs
         - inaccurate bandwidth measurements
         - bandwidth might not be the best metric
         - exit policy in relay selecton
      5. clients dumb
         - every-second-bump
         - how long to establish circuit, when to dump
         - extend to other places when next hop build fails?
         - bundle data and relay cell
      6. low-bandwidth users: much directory information
         - directory optimization
         - tls bundling (negligeable)
    - conclusion
      - better network -> more users ->slower network
      - this means less usability or other work
*** quotes
    - Over the past few years, our funding (and thus our development
      effort) has focused on usability and blocking-resistance.
    - Tor clients choose paths imperfectly
      - We don’t balance traffic over our bandwidth numbers correctly
      - The bandwidth estimates we have aren’t very accurate
      - Bandwidth might not even be the right metric to weight by
    - But over the past year, research has shown that it’s [one tcp
      connection] a bad idea in terms of performance, since TCP’s
      backoff mechanism only has one option when that connection is
      sending too many bytes: slow it down, and thus slow down all the
      circuits going across it.
    - Experiments show that moving congestion management to be fully
      end-to-end offers a significant im- provement in performance.
    - We could optimistically have this [UDP-TLS] testbed network
      deployed in late 2009.
    - Currently when we’re picking cells to write onto the network, we
      choose round-robin from each circuit that wants to write. We
      could instead remember which circuits have written many cells
      recently, and give priority to the ones that haven’t.
    - Plus, we could make the defaults higher if you sign up as a
      relay and pass your reachability test.
    - Our experience is that visiting them in person produces much
      better results, long-term, than Slashdot articles.
    - Several users have posted that they get much better Tor
      performance if they hard-code their paths to only use the
      fastest ten relays (and ignore the huge anonymity implications,
      of course).





    - ...
    - If web-browsing users are more sensitive to latency than
      bandwidth, then we could optimize the network for latency rather
      than throughput.
*** TODO ref plus file
** TODO blocking.pdf: Design of a blocking-resistant anonymity system
*** TODO refactor
*** quotes
*** summary
    1. Introduction
       Previously: TOR anonymizes, threat: first only passive, then active
       Now: TOR provides censorship avoidance, threat: also interrupt
    2. Adversary Assumptions
       ....

    3. tor anti-blocking procedures
       - different solutions
         - paid for proxy
         - volunteer work
         - tor
       - arms race
         - tor weaknesses
           - 2 certs
           - special traffic
         - strengths
           - many users: help freedom
           - discoverability
*** up to
    - Identity keys as part of addressing information
** TODO shadow-ndss2012.pdf: Shadow: Running Tor in a Box for Accurate and Efﬁcient Experimentation
*** TODO refactor
*** summary
    1. Introduction:
      - Many research papers on tor are loth to interefere with the
        actual network, yet would profit from a realistic simulation
        environment
      - Shadow uses unmodified TOR source code in an discrete-event
        simulation
      - This is verified by comparing to live tor and PlanetLAB
      - The EWMA-scheduling is tested and found wanting
    2. Requirements:
      - Accuracy: original source code, realistic system simulation
      - Usability: easy setup, easy config, obtainable
    3. Design:
      library calls substituted by shadow
      1. the simulation is set up by a simulation script that describes
        nodes (upstream, downstream, latency, cpu [, name) and events
        (commands)
      2. a plug-in is a wrapper from the application to shadow. the app
        must be non-blocking, single-threaded and must register all
        state variables to shadow
      3. virtual nodes are the hosts of shadow's simulation
        1. network is a token bucket algorithm
        2. libraries are preloaded/overridden with shadow-specific
           code: time, hostname, crypto, CPU-byte-passing
        3. instead of the whole plug-in being swapped in and out of
           memory, the registered state variables are replaced when
           switching virtual nodes
    4. The Scallion Plug-In: Running Tor in Shadow
       - State registration: Scallion registers variables including
         static (via objcopy, readelf and nm) and globals
       - Bandwidth Measurements: Scallion intercepts bandwidth
         messages to the directory servers
       - Preloaded Functions: sockets, CPU worker, bw measurement
       - Configurability: network generation via scripts
    5. evaluate accuracy
       1. HTTP client, (delay) server (multiple/buffering), and socks
          proxy plug-in and code
       2. trying to model planet-lab
          - 2 setups: one bulk 5MB download, one 320KB with delays
          - on planetlab and TOR in planetlab
       3. live tor network
    6. evaluate EWMA
       - ewma prioritises those circuits with the shortest queue
       - testing: two bulk, then 10 minutes later 1 bulk, 1 web
       - result: mixed resultst, sometimes better, sometimes worse
    7. related work:
       - others (defunct)
       - ExperimenTOR: shadow is
         - more usable: stand-alone user, little configuration
         - more efficient and scalable, can run faster (or slower)
         - minimizes memory overhead
         - more customizable, f.ex. adversarial nodes
    8. conclusion
       - uses approximations of network that may or not be correct
       - is single-threaded
*** quotes
    - Tor is a large and popular overlay network providing both
      anonymity to its users and a platform for anonymous
      communication research.
    - Shadow, an architecture for efﬁciently running accurate Tor
      experiments on a single machine.
    - Although Shadow simulates the network layer, it links to and
      runs real Tor software, allowing us to experiment with new
      designs by implementing them directly into the Tor source
      code.
    - Shadow was designed especially for running simulations using the
      Tor application.
    - the plug-in implementation itself is minimal (roughly 1500 lines
      of code).
    - Each Tor relay reports its recent bandwidth history to the
      directory authorities to help balance bandwidth across all
      available relays. However, relays’ reports are based on the
      amount of data it has recently transferred, and the reported
      value is updated every twenty minutes only if it has not changed
      signiﬁcantly from the last reported value.  This causes relays
      to be underutilized when ﬁrst joining the network, and causes
      bootstrapping problems in new networks since every node’s
      bandwidth will be zero for the ﬁrst twenty minutes of the
      simulation.
    - For example, several relays with smaller bandwidth capacities
      will not result in the same network throughput as fewer relays
      with larger bandwidth capacities, even if the total capacities
      are equal. Further, correctly distributing this bandwidth among
      entry, middle, and exit nodes can be tricky. Although live Tor
      consensus documents may be used to assist in network scaling,
      two randomly generated consensus topologies can have dras-
      tically different network throughput measurements.
    - The most significant improvement will enhance Shadow’s ability
      to run in parallel environments, leading to faster experiments
      and better utilization of hardware resources.
** TODO [[./traffic.pdf][Traffic Analysis Attacks and Trade-Offs in Anonymity Providing Systems]]
*** summary
*** quotes
*** ref [[file:master.bib::back01][Back et al. 2001: Traffic Analysis Attacks]]
** TODO [[./9501101.pdf][Dietterich - Solving Multiclass Learning Problems via Error-Correcting Output Codes]]
*** summary
*** quotes
*** ref [[file:master.bib::DBLP:journals/jair/DietterichB95][Dietterich & Bakiri 1995: Solving Multiclass Learning]]
** TODO [[./SS03.pdf][Passive Attack Analysis for Connection-Based Anonymity Systems]]
*** summary
*** quotes
*** ref [[file:master.bib::SS03][Serjantov & Sewell 2003: Passive Attack Analysis]]
** TODO [[/home/uni/chive/GG/it/misc/rfc7159.json-storage-and_xfer.txt][Bray - The JavaScript Object Notation (JSON) Data Interchange Format]]
*** summary
    0. [@0] Abstract

       "JavaScript Object Notation (JSON) is a lightweight, text-based,
       language-independent data interchange format"

    1. Introduction
*** quotes
*** ref: [[file:master.bib::rfc7159][Bray 2014: JavaScript Object Notation]]
** TODO [[./python-doc-howto-functional.pdf][Rossum - Functional Programming HOWTO]]
*** quotes
    6. [@6] Small functions and the lambda expression
       - =lowercase = lambda x: x.lower()=
       - An alternative is to just use the def statement and define a
         function in the usual way.
       - rules for refactoring uses of lambda:
         1. Write a lambda function.
         2. Write a comment explaining what the heck that lambda does.
         3. Study the comment for a while, and think of a name that
            captures the essence of the comment.
         4. Convert the lambda to a def statement, using that name.
         5. Remove the comment.
** TODO [[./move-ndss.pdf][M O V E: An End-to-End Solution To Network Denial of Service]]
** TODO [[./pets2011-defenestrator.pdf][DefenestraTor: Throwing out Windows in Tor]]
*** TODO refactor
*** quotes
    0. [@0] unsorted
       - We also argue that our proposals do not enable any new attacks
         on Tor users’ privacy.
       - The Tor network is a decentralized circuit-switching overlay
         consisting of volunteer-run Tor routers hosted around the world
       - Fig. 1. A Tor router’s queuing architecture
         (defenestrator.router_queues.png)
    0. [@0] Abstract
       - the most significant obstacle to Tor adoption continues to be
         its slow performance
*** summary
    2. Introduction: tor: distributed circuit-switching overlay
       - Context: big hurdle in TOR adoption lies in delays
       - Improving Congestion and Flow Control: 2 approaches
         - end-to-end window sizing like tcp congestion control
         - n23
       - Evaluation:
         - end2end poor downloads
         - n23 seems to work
    3. TOR Background: chain of onion routers, selected by bw, onion
       encryption, special entry guard, mostly http, mostly
       downstream, 512-byte cells (newer: different)
    4. Tor’s Approach to Congestion and Flow Control
       1. Congestion and Flow Control Mechanisms
          - Pairwise TCP: tcp flow control, BUT circuit multiplexing
          - Tiered Output Buffers: conn fifo->decrypt->circ fifo->out fifo(32k)
          - ....
*** [[file:master.bib::pets2011-defenestrator][AlSabah et al. 2011: DefenestraTor]]
** TODO Entwurfsmuster
*** summary (weiter bei Besucher, S. 317)
    0. [@0] Preface

       1. Vorwort

          - muster die entstanden sind, gut funktionieren

       2. Geleitwort

          Je mehr design vorweg, desto besser.

       3. Leitfaden

          1. Typen

             - erzeugungs-

             - struktur-

             - verhaltensmuster

          2. wie lesen

             - problemorientiert (abschnitt 1.6)

             - einfach: 107, 131, 171, 239, 287, 366, 199, 373

          3. nicht wahllos anwenden

             - effizient vorher abwaegen

    1. Einfuehrung

       0. [@0] wie die Experten:  Muster, um Problem zu loesen

       1. Was ist ein Entwurfsmuster?

          vier Teile

          1. Name

          2. Problemabschnitt: wann, welches Problem wird adressiert, Kontext

          3. Lösungsabschnitt: eigentliches Muster, Elemente, Beziehungen

          4. Konsequenzenabschnitt: z.B. Laufzeit, Vor- und Nachteile

       2. Entwurfsmuster in Smalltalk MVC

          - benutzerschnittellen per MVC konstruiert

            - model: anwendungsobjekt

            - view: bildschirmrepraesentation

            - controller: reaktion auf benutzungseingaben

          - model sagt view: hat sich geaendert

            - views holen neue werte von model

            - beobachtermuster (287)

          - view aus anderen zusammengesetzt

            - kompositionsmuster (239)

          - controller beschreibt reaktion auf bildschirmeingaben

            - aenderbar durch andere controller

            - strategiemuster (373)

          - weitere muster

            - fabrikmethode(131) voreingestellte controllerklasse

            - dekorierer (199) scrollbar hinzufuegen

       3. Beschreibung von Entwurfsmustern

          - Mustername und Klasse, Zweck, Auch bekannt als,
            Motivation, Anwendbarkeit, Struktur, Teilnehmer,
            Interaktionen, Implementierung, Beispielcode, Bekannte
            Verwendungen, Verwandte Muster

          - Konsequenzen: was auch varriert werden kann

       4. Der Katalog von Entwurfsmustern

          1. Abstrakte Fabrik (Abstract Factory): ?

          2. Adapter: merge two apis

          3. Befehl (Command), command as object

          4. Beobachter (Observer): 1-n Objekte, notify on change

          5. Besucher (Visitor): Operation on elements of f.ex. array

          6. Brücke (Bridge): function and implementation de-coupling

          7. Dekorierer (Decorator): dynamic alternative to subclassing

          8. Erbauer (Builder): create object not in object itself,
             create many

          9. Fabrikmethode (Factory Method): subclasses of creation

          10. Fassade (Facade): einheitliche Schnittstelle fuer mehrere

          11. Fliegengewicht (Flyweight): ?

          12. Interpreter (Interpreter): DSL

          13. Iterator: sequentieller zugriff ohne details zu kennen

          14. Kompositum (Composite): baumstrukturen als teil-ganzes (XML?)

          15. Memento: z.B. serialisierung

          16. Protoyp: neue Objekte durch Kopieren des Protoypen

          17. Proxy: zugriffskontrolle

          18. Schablonenmethode (Template Method): delegierender Algorithmus

          19. Singleton: genau ein Exemplar

          20. Strategie (Strategy): austauschbare Familie von Algorithmen

          21. Vermittler (Mediator): koppelt und kapselt unabhaengige Objekte

          22. Zustand (State): Objekte wechselt verhalten bei wechselndem zustand

          23. Zustaendigkeitskette (Chain of Responsibility): aka pipes

       5. Organisation des Katalogs

          - klassen- vs objekt-basiert

          - erzeugungs-, vs struktur-, vs verhaltensmuster

       6. Wie Entwurfsmuster Entwurfsprobleme lösen

          1. Finden passender Objekte

             - reale welt,

             - beschreibung \to verben+nomen

             - etc, am ende weiterentwickelt

             - entwurfsmuster geben wiederverwendbarkeit

          2. Bestimmen von Objektgranularität

             - fassade: subsystem als objekt

             - fliegengewicht: viele objekte kleinster granularitaet

             - unterteilen hilfe: abstrakte fabrik, erbauer, besucher, befehl

          3. Spezifizieren von Objektschnittstellen

             - *Signatur*: rueckgabewert functionsname(parameter, ...)

             - *Schnittstelle*: alle signaturen

             - *typ* name einer schnittstelle, *sub-* und *supertyp*,
               mehrere moeglich pro objekt

             - *polymorphie*: ersetzbarkeit objekte gleichen typs

          4. Spezifizieren von Objektimplementierungen

             0. [@0]

                - *Klasse* erzeugt *Exemplar*

                - */Abstrakte Klasse/* hat *KonkreteUnterklasse*

                - *Mixin-Klasse* bei Mehrfachvererbung

             1. Klassen- versus Schnittstellenvererbung

                - Klasse vererbt Implementation, Schnittstelle nicht

                - wichtig fuer Entwurfsmuster

                  - Typ: Zustaendigkeitskette, Kompositionsmuster,
                    Befehl, Beobachter, Zustand, Strategie

                  - Klasse: Kompositum

             2. Programmieren auf eine Schnittstelle hin, nicht auf
                eine Implementierung

                - erlaubt unwissenheit ueber klassen der objekte

                - und ueber klassen, die die objekte implementieren

          5. Wiederverwendungsmechanismen anwenden

             1. Vererbung versus Komposition

                - vererbung

                  - +: einfach, von programmiersprache unterstuetzt,
                    modifizieren

                  - -: nicht zur laufzeit aendern, aenderung der
                    oberklasse aendert auch die unterklasse

                    - loesung: nur von abstrakten klassen erben

                - komposition

                  - +: kapselung beibehalten, kann ersetzt werden,
                    klassen bleiben klein

                  - -: mehr objekte

             2. Delegation

                - hat-ein statt ist-ein, rueckreferenz

                - aufrufe an interne objekte weitergeben

                - z.b. zustand (an zustandsobjekt delegieren),
                  strategie (spezifische strategie), besucher (jedes
                  objekt der hierarchie wird von besuchsobjekt bearbeitet)

                - manchmal: vermittler, zustaendigkeitskette (falls
                  referenz auf erstes bearbeitendes objekt),

             3. Vererbung versus parametrisierbare Typen

                - C++/Java Templates

                - z.b. sortieren liste objekte

                  1. unterklassen sortieren je nach typ (vererbung)

                     standard vorsehen/ ueberschreiben, statisch

                  2. sortierobjekt mitgeben (komposition)

                     kann weniger effizient sein

                  3. templates: fest benannte funktion wird aufgerufen

                     typen aendern, statisch

                - hier nicht explizit verwendet, fuer c++ beispiele,
                  unnoetig in smalltalk

          6. Strukturen der Laufzeit- und Übersetungszeit aufeinander
             beziehen

             - aggregation: teil

             - bekanntschaft: aufrufbar, aber auch unterschiedliche laufzeiten

             - gleiche programmierpraktiken

          7. Veränderungen in Entwürfen vorhersehen

             - Ursachen von Entwurfsrevisionen

               1. Erzeugen eines Objekts unter expliziter Nennung seiner Klasse

                  besser: abstrakte fabrik, fabrikmethode, prototyp

               2. Abhaengigkeit von spezifischen Operationen fuer Aufgabe

                  besser: zustaendigkeitskette, befehl

               3. Abhaengigkeit von Hardware- und Softwareplattformen

                  besser: abstrakte fabrik, bruecke

               4. Abhaengigkeit von Objektrepraesentationen oder
                  Objektimplementierungen, z.b. wie gespeichert

                  besser: abstrakte fabrik, bruecke, memento, proxy

               5. aogorithmische abhaengigkeiten: objekte bieten
                  spezielle funktionen hierfuer an

                  besser: erbauer, iterator, strategie,
                  schablonenmethode, besucher

               6. Enge Kopplung: besser lose Kopplung (parameter bei erzeugung)

                  besser: abstrakte fabrik, bruecke,
                  zustaendigkeitskette, befehl, fassade, vermittler,
                  beobachter

               7. mehr funktionalitaet durch unterklassen

                  besser: komposition und delegation

                  muster: bruecke, zustaendigkeitskette, kompositum,
                  dekorierer, beobachter, strategie

               8. umoeglichkeit, klassen bequem zu aendern (z.b. closed source)

                  muster: adapter, dekorierer, besucher

             - Anwendungsprogramme

               interne wiederverwendung, wartbarkeit und
               erweiterbarkeit durch lose kopplung

             - Klassenbibliotheken

               - bieten funktionalitaet ohne design vorzugeben

               - schwieriger als programme: in mehreren kontexten

             - Frameworks

               - design vorgegeben

               - man erweitert um spezielle teile

               - schwieriger als klassenbibliotheken

               - dokumentation noch besser mit entwurfsmustern (struktur)

       7. Wie man ein Entwurfsmuster auswaehlt

          - bedenken welche wie loesen

          - zweckabschnitte (1.4) querlesen

          - beziehungen bedenken

          - muster mit gleicher aufgabe untersuchen

          - gruende fuer entwurfsrevisionen (1.6) pruefen, ob dabei

          - was sollte variabel sein

       8. Wie man ein Entwurfsmuster verwendet

          1. einmal lesen

          2. struktur- teilnehmer und interaktionsabschnitt verstehen

          3. beispielcode

          4. klassenname waehlen (lokal + muster)

          5. klassen definieren (schnittstellen, vererbung, exemplar

          6. funktionsnamen waehlen, konsistent (z.b. "erzeuge" fuer
             fabrikmethode)

          7. implementieren operationen

    3. [@3] Erzeugungsmuster

       0. [@0] Einleitung

          - von vererbung zu objektkomposition immer wichtiger

          - kapseln wissen um klassen in typen

            - und wie diese typen kombiniert werden

          - beispiel

            - raum mit angrenzenden raum, tuer, wand

          - macht flexibler, nicht notwendigerweise kleiner

          - erweiterbarer durch

            - virtuelle funktionen helfen bei aenderung

            - erzeuge erhaelt objekt um varianten zu erzeugen

            - prototypische raum, tuer und wandobjekte

       1. Abstrakte Fabrik

          1. Zweck: schnittstelle zum erzeugen von objekten, ohne
             konkrete klasse zu benennen

          2. Auch bekannt als: Kit

          3. Motivation

             abstrakte fabrik erzeugt typen, implementiert durch
             konkretere klassen je nach umgebung

          4. Struktur

             - Klient verwendet AbstrakteFabrik und AbstraktesProduktA,...

             - AbstrakteFabrik ist Typ fuer KonkreteFabrik1 und KonkreteFabrik2

             - AbstraktesProduktA ist typ von ProduktA1 und ProduktA2

             - KonkreteFabrik1(etc) erzeugt ProduktA(etc)1

          5. Anwendbarkeit

             - familien von jeweils verwandten objekten

          6. Teilnehmer

             - AbstrakteFabrik deklariert abstrakte schnittstelle fuer
               konkrete konstruktoren

             - konkretefabrik erzeugt bestimmten typ von objekten

             - abstraktesprodukt deklariert schnittstelle

             - konkretesprodukt implementiert schnittstelle, von
               konkretefabrik gebaut

          7. Interaktionen

             - eine konkretefabrik zur laufzeit, von abstraktefabrik aufgerufen

          8. Konsequenzen

             - isolation konkreter klassen

             - einfacher austausch von produktfamilien

             - konsistenzsicherung unter objekten

             - (-) schwierige unterstuetzung neuer Produkte

          9. Implementierung

             1. Fabriken als Singletons

             2. Erzeugen von Produkten: eine fabrikmethode pro
                produkt, oder bei vielen produkten per prototyp, oder
                klassenbasiert (bei klassen als objekten erster
                ordnung)

             3. definieren von erweiterbaren fabriken durch
                parametrisierte =erzeuge=-funktion. (-): ein rueckgabetyp

          10. Beispielcode

              erweiterbares labyrinth um bomben und zauber

          11. Bekannte Verwendungen

              ET++, Interview ;-)

          12. Verwandte Muster

              fabrikmethoden oder prototyp, fabrik ist oft singleton

       2. Erbauer

          1. Zweck:

             trenne Konstruktion von Repraesentation, erzeuge
             verschiedene Repraesentationen

          2. Motivation

             RTF-Dokumentkonvertierer mit unterschiedlichen
             Anzeigeklassen: ASCII, \TeX, TextWidget

          3. Anwendbarkeit

             - erzeugen unabhaengig von teilen, aus denen es besteht,
               speicherarts

             - erlauben verschiedener repraesentationen

          4. Struktur

             - Direktor { Konstruiere() { for all objekts in structur
               {erbauer.baueTeil()} }, erzeugt erbauer

             - Erbauer { abstract BaueTeil() }:

             - KonkreterErbauer { BaueTeil(); GibErgebnis } implementiert Erbauer

          5. Teilnehmer

             1. Erbauer: abstrakte Schnittstelle

             2. KonkreterErbauer: konstruiert, fuegt zusammen, gibt produkt

             3. Direktor: konstuiert objekt unter verwendung Erbauer

             4. Produkt: komplexes Objekt, beinhaltet Klassen fuer Teile

          6. Interaktionen

             - Klient erzeugt Direktor, konfiguriert mit Erbauer, triggert

             - Erbauer bearbeitet Anfragen des Direktors

             - Klient erhaelt Produkt vom Erbauer

          7. Konsequenzen

             - Interne Definition aenderbar, lediglich neuen erbauer definieren

             - Isolierter Code fuer Konstruktion und repraesentation

             - genauere Steuerung der Konstruktion:
               schritt-fuer-schritt unter Kontrolle des Direktors
               statt in einem durchgang

          8. Implementierung

             1. Konstruktionsschnitstelle

                granularitaet klein genug, um verschiedene Objekte zuzulassen

             2. Keine abstrakte Produktklasse: macht wenig sinn
                (z.b. ascii vs widget vs tex vs rtf)

             3. leere methoden statt abstrakt (also vererbung statt
                schnittstelle): manchmal muss nichts getan werden,
                z.b. formatierung zu ASCII

          9. Beispielcode

             - Erbauer ohne baueWand, da dies in =BaueRaum=
               geschieht. Bei Tueren wird die jeweilige Seite auf Tuer
               gesetzt

             - z.b. zaehlenderlabyrintherbauer: zaehlt nur zahlen der
               tueren/waende

          10. Bekannte Verwendungen

              z.b. compiler

              - classbuilder auch direktor und produkt in einer klasse

              - bytecodestream erbauer, erzeugt programmcode, nicht klasse

          11. Verwandte Muster

              - abbstrakte fabrik: hier schrittweise, abstrakt: familie

              - erbauer bauen oftmals komposita

       3. Fabrikmethode

          - Anwendbarkeit: wenn zu erzeugende klassen nicht im voraus klar

          - implementierung abstrakte fabrik

          - weitergabe der implementierung an konkrete unterklassen

       4. Prototyp

          - kopiere Prototyp. um neues Objekt zu erzeugen

          - Anwendbarkeit: weniger Generatorklassen, wenige
            festgelegte Parametersaetze

          - Konsequenzen:

            - versteckt Produktklassen wie 3.1 und 3.2

            - variation definiert, default gegeben

          - implementierung:

            - dict-prototype,

            - deep vs shallow copy

       5. Singleton

          - exactly one instance

          - examples: fs, window manager, print spooler, a/d coverter, ...

          - advantages: variable but fixed number of instances, point to get

          - implement: private instance, public accessor, protected constructor

            - inheritance: constructor complicated
              (f.ex. environmental variable sets flavor)

       6. Diskussion der Erzeugungsmuster

          - Objektkomposition more versatile than inheritance

          - prototypes for many small classes (which would otherwise
            each need a factory class), =clone= also useful f.ex. to
            duplicate drawing parts

          - often starts with factory pattern, move to other patterns
            when more flexibility is needed

    4. Strukturmuster

       0. [@0]

          0. [@0]

             - objektbasiert vs klassenbasiert

               - klassenbasiert z.b. adapter

               - objektbasiert z.b. Kompositionsmuster

             - teils verwandt

          1. Adapter

             - anpassung von schnittstellen aneinander

             - klassenadapter: mehrfachvererbung

               - objektadapter: komposition

             - konsequenzen

               - klassenadapter:

                 - +: anpassung, ueberschreiben da unterklasse, einzelnes objekt

               - objektadapter

                 - +: mehrere anzupassende klassen

                 - -: ueberschreiben schwieriger

               - aufwand abhaengig vom grade der verschiedenheit

               - weniger annahmen machen implementierung einfacher

               - zweiweg-adapter um beide richtungen abzudecken

             - implementierung: eines oeffentlich erben, anderes intern/privat

               - kleinste gemeinsame Menge von Operationen fuer Adaptierung

          2. Bruecke (Bridge)

             - objektbasiert

             - Zweck: entkopple interface und implementierung

             - Motivation: implementierung zur Laufzeit auswaehlen

               - FensterImp grundklasse, kindklassen verwenden ihre funktionen

             - Anwendbarkeit:

               - dauerhafte verbindung vermeiden: zur laufzeit auswaehlen

               - unterklassen fuer abstraktion (benutze elternklasse)
                 und implementierung (benutze imp-elternklasse)

             - Struktur + Teilnehmer:

               - abstraktion hat operation, definiert schnittstelle,
                 verwaltet referenz auf implementierer

               - spezialisierte abstraktion: erweitert abstraktion

               - implementierer hat operationsImp

               - implementierer-kindklassen operationsImp konkret um bzw. verwendent sie

             - Interaktion: abstraktion leitet anfragen (von klienten) weiter

             - Konsequenzen

               - entkopplung Schnittstelle/Implementierung

               - verbesserte Erweiterbarkeit

             - Implementierung

               - degeneriert: ein Implementor-typ, falls nicht neu uebersetzt werden soll

               - welches Implementorobjekt erzeugen?

                 - entweder default, dann anpassen

                 - oder Fabrikmuster fuer erzeugung

             - Verwandte Muster:

               - Abstrakte Fabrik kann Bruecke erzeugen,

               - Adapter wird in bereits bestehendem System verwendet,
                 bruecke im Design

          3. Dekorierer

             - objektbasiert

             - Zweck: objekt dynamisch erweitern

             - Motivation

               - vererbung ist unflexibel

               - besser in anderem Objekt einschliessen, dass
                 z.b. scrolling hinzufuegt: /Dekorierer/

                 - hat dieselbe schnittstelle wie anderes object,
                   evtl. erweitert, z.b. um /scrolleBis()/

             - Anwendbarkeit:

               - zusaetzliche funktionalitaet

                 - wieder entfernbar

               - unterklassen vermeiden (versteckt oder exponentielle vermehrung)

             - Struktur / Teilnehmer:

               - abstrakteKomponente hat

                 - konkreteKomponente (erweiterbar), und

                 - Dekorierer, der enthaelt abstrakteKomponente und Unterklassen

             - Interaktionen:

               - leitet anfragen weiter

               - fuehrt zusaetliche aktionen davor oder danach aus

             - Konsequenzen:

               - flexibilitaet

                 - weniger klassen als bei mehrfachvererbung

                 - mehrfach dasselbe (zwei rahmen)

               - erweitern unten im klassenstammbaum, nicht eine komplexe
                 elternklasse

               - keine tests auf objektidentitaet

               - viele kleine gleiche Objekte, schwerer zu verstehen

             - Implementierung:

               - schnittstellen: alle funktionen uebernehmen

               - leichgewichtige komponentenklassen

               - Strategiemuster ändert das Verhalten, Dekorierer die "Hülle"

                 - Strategie ("Delegation eines Teils der Verhalten an
                   separates Strategieobjekt") bei schwergewichtigen
                   Komponenten

                 - z.B. separates Rahmenobjekt

                 - z.B. in MacView, da seine Views sehr schwergewichtig,
                   also extra /Adorner/-Klasse

                 - für Strategie muss die Klasse verändert werden

             - Beispielcode: VisuelleKomponente als Metaklasse

               - wird von Dekorierern und wirklicher Komponente abgeleitet

             - Bekannte Verwendungen:

               - Android Views

               - I/O-Streams (java)

             - Verwandte Muster:

               - Adapter verändert Schnittstelle

               - Strategie verändert inneres Verhalten, Dekorierer äußeres

               - Kompositum: Dekorierer ist degeneriertes Kompositum,
                 allerdings auch mit zusätlicher Funktionalität

          4. Fassade

             - objektbasiert

             - Zweck: einheitliche Schnittstelle (zu Menge von Schnittstellen)

             - Motivation: zusätzlich zu eigentlicher Funktionalität noch
               Fassadenklasse, die die Benutzung vereinfacht

             - Anwendbarkeit:

               - einfache Schnittstelle zu komplexen System: häufige Verwendung

               - Abhängigkeiten zwischen Klienten und
                 Implementierungsklassen entfernen

                 - bessere Unbaehängigkeit und Portabilität des Subsystems

               - Subsysteme in Schichten aufteilen, dann Abhängigkeiten
                 vereinfachen

             - Struktur/Teilnehmer:

               - Fassade: delegiert Anfragen an das zuständige Subsystemmodul

               - Subsystemklassen:

                 - implementieren Funktionalität

                 - wissen nichts von der Fassade, führen nur aus

             - Interaktionen: Klient an Fassade, Fassade an Subsystem

             - Konsequenzen:

               - schirmt Klienten ab,

               - macht Subsystem einfacher zu benutzen

               - lose Kopplung zwischen Subsystem und Klienten

               - Subsystemklassen weiterhin verwendbar wenn nötig

             - Implementierung:

               - abstrakte Klassen machen das Subsystem austauschbar

                 - Komponenten ebenso

               - oeffentliche versus private Subklassen

             - Beispielcode: Compilersubsystem mit Uebersetzer(file eingabe,
               Bytecode ausgabe)-Fassade

             - Bekannte Verwendungen: Uebersetzer inspiriert von Smalltalk

             - Verwandte Muster:

               - Abstrakte Fabrik zusammen, um Unabhaengigkeit vom
                 konkreten Subsystem zu erreichen

               - Vermittlermuster: dem Subsystem bekannt (hilft bei
                 Kommunikation), dagegen Fassade unbekannt und keine neue
                 Funktionalität

               - Fassaden als Singletons

          5. Fliegengewicht (Flyweight)

             - objektbasert

             - Zweck: viele Objekte kleiner Granularität

             - Motivation: Texteditor:

               - Zeichen sind Objekte

                 - es gibt z.B. ein Objekt für "a"

               - Glyph hat Fkt Zeichne(Kontext) und Unterklassen Zeile,
                 Spalte, Zeichen

               - speichert nur intrinsischen Zustand (welches Zeichen)

               - extrinsischer Zustand ist vom Kontext vorgegeben

             - Anwendbarkeit: nur wenn (alles gemeinsam)

               - viele Objekte,

               - Speicherkosten deswegen hoch,

               - Großteil des Zustands in Kontext verlagerbar,

               - dann Gruppen zusammenfassbar zu wenigen gemeinsam
                 genutzten Objekten, UND

               - Identität der Objekte unwichtig.

             - Struktur: Fabrik erzeugt und cached Fliegengewichte

             - Teilnehmer:

               - Fliegengewicht (Glyph): Scnittstelle um externen Zustand
                 zu erhalten und verarbeiten

               - Konkretes Fliegengewicht (Zeichen): evtl +intrinsischer Zustand

               - Getrennt Genutztes Fliegengewicht: z.B. Zeile

               - Fabrik: erzeugt, verwaltet == stellt korrekte gemeinsame
                 Nutzung sicher

               - Klient: etc

             - Interaktionen:

               - klare Trennung Zustand extrinsisch/intrinsisch

               - nie direkt erzeugt, immer per Fabrik

             - Konsequenzen:

               - Laufzeitkosten,

               - Speicherplatzgewinne, durch

                 - weniger objekte

                 - evtl. weniger intrinsischer Zustand

                 - evtl. extrinsisch berechnet

               - häufig mit Kompositionsmuster

             - Implementierung:

               - extrinsischer Zustand lohnt nur wenn auch zusammengefasst

               - Map für Objekte, evtl. mit Referenzzähler

                 - außer bei wenigen fixen Objekten, wie z.B. ASCII

             - Beispielcode:

               - B-Baum für Zeichensatz der Glyphen: geordnet nach
                 Glyphenposition (siehe Buch)

                 500 \to {1, 300, 199}
                 1 \to Times 24

                 300 \to {100, 6, 194}
                 100 \to Times 12
                 6 \to TimesItalic 12
                 194 \to (auch) Times 12

                 199 \to {8, 1, 3, 187}
                 8 \to TimesBold 12
                 1 \to (auch) Times 12
                 3 \to Courier 24
                 187 \to (auch) Times 12

               - bei z.b. entfernen des italic wird der obere Teil zu
                 300 \to Times12 (300mal Times12 für Glyph)

             - Bekannte Verwendungen:

               - Doc-Editor (480 Zeichen, Layout auch als Fliegengewicht)

             - Verwandte Muster:

               - häufig zusammen mit Kompositionsmuster

               - Zustandsobjekte und Strategieobjekte häufig als Fliegengewicht

          6. Kompositum (Composite)

             - objekbasiert

             - Zweck: Objekte zu Baumstrukturen kombinieren

             - Motivation:

               - Schemaeditor hat Linien, können kombiniert werden, dies
                 gibt neues Objekt (z.B. Klasse), kann wieder kombiniert
                 werden, etc

               - Implementierung ist schwieriger, wenn zwischen Behälter
                 und Basisobjekt (Linie, Rechteckt, Schrift)
                 unterschieden wird

               - Lösung: abstrakte Klasse für beides: Graphik

                 - mit Funktionen Zeichne, FuegeHinzu(g), Entferne(g),
                   GibKindObjekt(id)

             - Anwendbarkeit:

               - Teil/Ganzes Hierarchien,

               - Unterschiede ignorieren

             - Struktur/Teilnehmer:

               - Klient benutzt Komponente (abstrakte Klasse=Schnittstele)

                 - kann beides sein: Blatt oder Kompositum

               - Kompositum ruft Operation der Kindobjekte auf

             - Interaktionen:

               - Blatt führt direkt aus,

               - Kompositum leitet meistens weiter, führt evtl. vorher
                 oder nachher zusätzliche Aktionen aus

               - Klienten verwenden die Klassenschnittestelle von Komponente

             - Konsequenzen

               - Klienten soll der Typ gleich sein: einfacherer Code

               - vereinfacht Hinzufügen neuer Klassen

               - kann Entwurf zu allgemein werden lassen, verhindert
                 compiler type checking

             - Implementierung

               - definiere besser Elternobjekt (in Komponentenklasse)

                 - erleichtert z.B. Zuständigkeitskettenmuster und Löschen

                 - erschwert gemeinsame Nutzung

                   - externer Zustand (siehe Fliegengewichtsmuster) als Lösung

               - maximale Komponentenschnittstelle

                 - Defaultimplementierung z.B. die für Blätter, Koposita
                   überschreiben diese

               - deklarieren der Verwaltungsoperationen

                 - entweder transparent: in der Komponentenklasse,

                   - allerdings könnten Klienten dann versuchen, Blättern
                     z.B. Kinder hinzuzufügen

                 - oder sicherer: in der Kompositaklasse.

                   - allerdings haben dann Komposita und Blätter
                     unterschiedliche Schnittstellen

                   - evtl. Exception als Defaultimplementation, in
                     Komposita überschrieben

               - Speichern der enthaltenen Komponenten in der
                 Kompositionsklasse: in Elternklasse unnötiger
                 Blatt-Platzverbrauch

               - Ordnung der Kinder: bei Zugriff und Verwaltung
                 beachten. Siehe auch das Iteratormuster.

               - Caching bei häufigem Zugriff

                 - z.B. speichert Graphikkompositum die
                   Koordinaten. Falls nicht sichtbar, werden keine
                   Handlungen ausgeführt

                   - Zwischenspeicher invalidieren bei Änderung

               - Löschen: ohne GC: Kompositum löscht

                 - außer bei gemeinsam genutzten Blattobjekten

               - Listen, Bäume, Arrays, Hahs-Tabellen, hängt von
                 Ziel-Effizienz ab

             - Beispielcode: Geraet > ZusammengesetztesGeraet > Gehaeuse

               - GesamtPreis als Iterator aller Kinder

               - ZusammengesetztesGeraet hat =Liste<GErate*> _teile=

             - Bekannte Verwendungen: Smalltalk View (geaendert spaeter)

             - Verwandte Muster:

               - Kompositum verwendet für Zuständigkeitskettenmuster

               - Dekorierer und Kompositum häufig gemeinsam

               - Fliegengewichtsmuster ermöglicht gemeinsame Nutzung von
                 Komponenten (allerdings Problem mit Verweis auf Eltern)

               - Iteratormuster "um Komposita zu taversieren"

               - Besuchermuster: "lokalisiert Operationen"

          7. Proxy

             - objektbasiert (auch "Surrogat")

             - Zweck: kontrolliere Zugriff

             - Motivation: lazy loading: eingebettete große Bilder, die
               nur geladen werden sollen, wenn das Dokument sie auch
               anzeigen muss

             - Anwendbarkeit: u.a.

               - Remote-Proxy: anderer Adreßraum, z.B. Netzwerkzugriff

               - virtuelles Proxy: teure Objekte wenn benötigt, z.B. Bild s.o.

               - Schutzproxy: bei unterschiedlichen Zugriffsrechten

               - Smart-Reference: ersetzt Referenz, evtl. mit

                 - Zählen der Referenzen (bei 0 wieder freigeben)

                 - Test, ob gelockt vor Zugriff

                 - Laden/Speichern bei Benutzung/Schließen

             - Struktur und Teilnehmer:

               - Proxy und Echtes Subjekt leiten beide von Subjekt ab

               - Proxy ruft echtes Subjekt auf, bietet identische
                 Schnittstelle

               - Proxy erzeugt und löscht echtes Subjekt

               - Subjekt definiert gemeinsame Schnittstelle

             - Interaktionen: wenn angebracht, Anfragen weiterleiten

             - Konsequenzen:

               - Indirektion (Objekt woanders, Optimierungen, Schutz)

               - Z.B. copy-on-write: bei schwerem Schreiben, erst bei
                 Änderung kopieren, sonst viele Klienten selbes Objekt.

             - Implementierung: C++: ->-Operator überladen:
               #+BEGIN_SRC c++
                 Bild* BildZeige::LadeBild() {
                   if (_bild == 0) {
                     _bild = LadeBildDatei(_bildDatei);
                   }
                   return _bild;
                 }
                 Bild* BildZeiger::operator->() {
                   return LadeBild();
                 }
                 // Beispiel
                 BildZeiger bild = BildZeiger("DateiName");
                 bild->Zeichne(Punkt(50, 100));
                 // wird zu (bild.operator->())->Zeichne(Punkt(50, 100));
               #+END_SRC

               Allerdings unterschiedlich behandeln. Außerdem nicht immer
               eine Lösung (wissen, wann benutzt; nur bei Zeichnen, nicht
               bei ref, ...)

             - Beispielcode: Proxy, der die Ausmaße beim ersten Mal aus
               der Datei extrahiert, danach aber noch gespeichert und
               geladen werden kann

             - Bekannte Verwendungen: ET++-Textbaustein, NXProxy in NEXTSTEP

             - Verwandte Muster:

               - Adapter bietet andere Schnittstelle als Objekt (also
                 auch als Proxy)

               - Dekorierer hat ähnliche Implementierung, aber der Zweck
                 ist die Erweiterung

                 - Schutzproxy genau wie Dekorierer

                 - Remoteproxy hat keine direkte Referenz

                 - virtueller Proxy erzeugt direkte Referenz bei Zugriff

       1. Diskussion der Strukturmuster

          Ähnlichkeiten der uster, da alle auf Einfach- und
          Mehrfachvererbung (Klassenmuster) und Objektkomposition beruhen.

          1. Adapter versus Brücke

             - Adapter macht inkompatible Klasse kompatibel, Brücke
               ist von vornherein auf mehrere Klassen ausgelegt

             - Adapter zu Beginn, Brücke um Codeduplizierung zu vermeiden

             - Fassade definiert neue Schnittstelle, Adapter passt an

          2. Kompositum versus Dekorierer versus Proxy

             - Kompositum fügt zusammen, Dekorierer ergänzt Funktionalität

               - oft zusammen verwendet: "Zusammenstecken" von Klassen

             - Proxy kontrolliert oder speichert Zugriff zwischen,
               Dekorierer erweitert Funktionen

    5. Verhaltensmuster

       0. [@0]

          Interaktion von Objekten

          - klassenbasiert via Vererbung

          - objektbasiert via Objektkomposition

       1. Befehl (Command)

          - Zweck: Befehl als Objekt kapseln

          - Motivation

            - abstrakte Klasse Befehl, welche Ausfürhung ermöglicht

            - z.B. Menüs

              - auch Aneinanderreihung (Macro) von Befehlen

          - Anwendbarkeit

            - z.B. callback-Funktion

            - unterschiedliche Zeitpunkte der Spezifikation,
              Aufreihung und Ausführung

              - auch evtl. für Parallelisierung

            - Undo: Speichern des relevanten Zustands im Objekt,
              Befehl hat Rückgängig-Operation

            - Protokollieren der änderungen (z.B. Logbuch oder
              restart)

            - [SQL-]Transaktionen (mehrere Befehle aus primitiven)

          - Struktur und Teilnehmer:

            - Befehl ist abstrakte Oberklasse von KonkreterBefehl

            - KonkreterBefehl wird von Anwendung erzeugt, speichert Empfänger

            - Aufrufer (z.B. MenueEintrag) ruft Befehl auf

            - Empfänger (Z.B. Dokument) setzt die Aktion um

          - Interaktionen: Anwendung bestimmt Empfänger

          - Konsequenzen

            - entkoppelt Aufrufer von umsetzenden Objekt

            - Erweiterung von Befehlen möglich, auch Zusammensetzung

            - einfach, neue hinzuzufügen

          - Implementierung

            - Intelligenz: von weiterleiten bis alles selbst tun alles
              ok, auch Empfänger dynamisch bestimmen

            - Undo/Redo:

              - speichere Empfänger, Argumente, auch geänderte Werte

              - Liste für mehrere undo/redos

                - evtl. Befehl hierfür kopieren, dann Befehl als Prototyp

            - Vermeiden von Fehlerlawinen bei Undo/Redo: genug
              Informationen z.B. per Memento speichern

          - Beispielcode: Editor

          - Bekannte Verwendungen: (Android-Intents?)

          - Verwandte Muster:

            - MakroBefehle per Kompositum

            - Memento für Zustand bei Undo/Redo

            - Prototyp für Kopie in Befehlsgeschichte

       2. Beobachter (Observer)

          - Zweck: bei Änderung eines Objekts werden alle
            registrierten Objekte benachrichtigt.

          - Motivation:

            - Konsistenz zwischen Klassen aufrechterhalten ohne enge Kopplung

            - Z.B. Tabelle verändern ändert das Säulendiagramm
              (zugrundeliegendes Datensubjekt)

            - beliebig viele Beobachter des Subjekts

          - Anwendbarkeit:

            - entkoppelte, wiederverwendet Aspekte

            - vorher unklar, wie viele Objekte geändert werden müssen

            - ein Objekt soll andere benachrichtigen können, ohne sie
              zu kennen

          - Struktur / Teilnehmer:

            - Abstraktes Subjekt zum An-/Abmelden, Konkretes für Werte

            - Abstrakter Beobachter für aktualisieren, Konkreter
              speichert Subjekt

          - Interaktionen:

            - Subjekt informiert Beobachter über Änderungen, diese
              können dann seine Werte abfragen

          - Konsequenzen:

            - minimale Kopplung: Subjekt weiss nur von Liste von
              Beobachtern (oder nicht einmal das)

            - Broadcast-Kommunikation möglich

            - eventuell unsinnige Aktualisierungen (?z.b. bei jedem
              neuen Zeichen in Feld?)

          - Implementierung:

            - assoziative Subjekt \to Beobachter

            - bei mehreren Beobachtern Schnittstelle erweitern:
              welches Subjekt hat Benachrichtigung ausgelöst

            - Auslöser:

              - Subjekt (nicht zu vergessen, allerdings evtl. zu viele), oder

              - Klient (evtl keine unnötigen, aber Fehler wahrscheinlicher)

            - gelöschte Subjekte: Beobachter benachrichtigen (auf
              jeden Fall berücksichtigen)

            - bei Vererbung: wer löst Benachrichtigung aus? wann?
              (evtl. mit Schablonen lösbar)

            - Push vs Pull: nur benachrichtigen (pull) oder gleich viele
              Informationen mitschicken (push)

            - welche Änderungen interessieren beobachter: Aspekte
              (?oder messages?)

            - Aktualisierungsmanager bei verschachtelten Systemen:
              z.b. einfach (aktualisiert alle) oder DAG (aktualisiert
              in richtiger Reihenfolge)

          - Beispielcode: ZeitGeber als Subjekt, =Tick()= wird extern
            aufgerufen, benachrichtigt Beobachter, diese fragen dann
            Werte ab

          - Bekannte Verwendungen: MVC (Smalltalk), (?Android onClick?)

          - Verwandte Muster:

            - Vermittler: Änderungsmanager kapselt "komplexe
              Aktualisierungssemantik"

            - Singleton: Änderungsmanager als Singleton um einmalig
              und global zugreifbar zu sein

       3. Besucher (Visitor)

          - Zweck: Kapsele Operation auf Objektstruktur in Objekt,
            mache es austauschbar (?wie map?)

          - Motivation:

            - z.B. Compiler

              - Syntaxbaum, jedes Element bietet unterschiedliche Operationen

              - aber eigentlich ist die Operation mehr oder weniger
                unabhängig von den Elementen, sondern vom Elementtyp

              - also besser eine =NimmEntgegen(Besucher)=-Operation,
                die den Besucher auf sich ausführt

          - Anwendbarkeit:

            - viele Klassen mit unterschiedlichen Schnittstellen

              - Objektoperationen darauf abhängig vom konkreten Typ

            - Klassen der Objektstruktur nicht "verschmutzen"

            - Klassen der Objektstruktur sind konstant, aber neue
              Operationen sind häufiger

          - Struktur/Teilnehmer:

            - KonkreterBesucher implementiert Besucher, hat Operation
              =besuche...= für jede nötige Klassenart

            - Klasse implementiert =nimmEntgegen(Besucher b)=

            - ObjektStruktur: listet Elemente

          - Interaktionen: Klient erzeugt KonkreterBesucher-Objekt,
            traviersiert Objektstruktur, gibt jedem Objekt das
            KonkreterBesucher-Objekt

          - Konsequenzen

            1. Hinzufügen neuer Operationen leichter

            2. Zentralisierung zusammenhängendes Verhaltens im Besucher

            3. Hinzufügen neuer KonkreteElement-Klassen schwerer:

               - neue Operation in jeder KonkreterBesucher-Klasse

               - vorher fragen, ob eher Objektstruktur (dann dieses)
                 oder eher Algorithmen (dann nicht dieses Muster)
                 statisch

            4. KonkreteElement-Klassen müssen keine gemeinsame
               Oberklasse haben (wie z.B. bei Iterator)

            5. Besucher können bei der Traversierung
               Zustandsinformationen sammeln (sonst globales Objekt
               oder Parameter)

            6. Kapselung verletzt, da der Besucher auf viele Dinge
               zugreifen können muss

          - Implementierung

            - Funktionsoverloading =besuche()= oder
              =besucheElementA|B|...()=: Glaubensfrage

            - DoubleDispatch: Operation hängt vom Typ des Besuchers
              *und* der besuchten Klasse ab

            - Traversierung: in Objektstruktur, Iterator, oder Besucher

              - Besucher bei komplexer Traversierung

          - Beispielcode: C++, GeraetBesucher, berechnet Preis,
            anderer berechnet Stueckzahl

          - Verwandte Muster:

            - Besucher wirkt auf Kompositionsmuster-Baum

            - Interpreter als Besucher umgesetzt

       4. Interpreter

          - Zweck: Sprache gegeben, definiere in Grammatik und Inerpreter
            um zu interpretieren

          - Motivation:

            - häufiges Problem, dann Sprache zur Beschreibung

              - z.B. regex

              - einzelne Klassen für einzelne Teile der Grammatik

          - Struktur/Teilnehmer:

            - *Klient* ruft *AbstrakterAusdruck.interpretiere(Kontext)*

            - *AbstrakterAusdruck* hat Kindklassen *TerminalAusdruck*
              und *NichtTerminalAusdruck* und Methode *interpretiere*

            - *Kontext* enthält globale Informationen.

          - ENDE erst einmal

       6. [@6] Memento

          1. Zweck: ohne Kapselung zu verletzen, speichere, so dass
             wieder zurückversetzt werden kann

          2. Auch bekannt als: Token

          3. Motivation: Aktionen korrekt rueckgaengig machen ohne
             Objektkapselung zu verletzen

          4. Anwendbarkeit

             - Momentaufnahme zur Wiederherstellung

             - direkte Schnittstelle schlecht (legt details offen)

          5. Struktur

             - Urheber erzeugt Memento,

             - Memento wird aggregiert in Aufbewahrer

          6. Teilnehmer

             - Urheber erzeugt memento mit =ErzeugeMemento()=

               - stellt wieder her mit =SetzeMemento(Memento m)=

             - Memento hat =GibZustand()= und =SetzeZustand()=

               - gibt Funktionalitaet nur dem Urheber frei

             - Aufbewahrer bewahrt auf

          7. Interaktionen

             - Aufbewahrer tiggert =ErzeugeMemento()=, wodurch Urheber
               =new Memento()= erzeugt und =SetzeZustand()= aufruft.

             - (evtl) Aufbewahrer =SetzeMemento(...)=, wodurch Urheber
               vom Memento =GibZustand()= bei sich setzt

          8. Konsequenzen

             1. Wahrung der Kapselungsgrenzen

             2. vereinfacht Urheber

             3. kann teuer sein, wenn viele Informationen zu kopieren

             4. schmale vs breite Schnittstellen kann schwierig sein

             5. Versteckte Kosten bei Aufbewahrung

          9. Implementierung

             - c++ z.b. friend-klasse

             - kann ausreichen, inkrementelle aenderungen zu speichern

          10. Beispielcode

              - c++:

                #+BEGIN_SRC c++
                  private:
                  friend class ConstraintLoeser;
                  ConstraintLoeserMemento();
                #+END_SRC

          11. Bekannte Verwendungen

              - behaelter ist friend von iterationszustand, Im
                gegensatz zu Iterator: iterationszustand is friend von
                baehlter

          12. Verwandte Muster

              - befehl: rueckgaengigmachen von Operationen

              - iterator: s.o. (11)

              - tmp: hier nicht weiter, sondern oben (entwurfsmuster)
*** quotes
    - [Muster:] Beschreibungen zusammenarbeitender Objekte und
      Klassen, die maßgeschneidert sind, um ein allgemeines
      Entwurfsproblem in einem bestimmten Kontext zu lösen
    - (ends 1.2?-1.5)
    - Beispielsweise erscheinen Objekte, die einen Prozeß oder einen
      Algorithmus repräsentieren, nicht in der Natur, sind aber
      trozdem ein essentieller Teil flexibler Entwürfe.
    - (ends ?-1.6.3)
    - Die im Deutschen haeufig anzutreffende Uebersetzung on
      "instance" mit Instanz trifft den Originalbegriff
      nicht. Instanzen sind im Deutschen zu Beispiel Behoerden oder
      gerichtliche Instanzen, nicht aber konkrete Auspraegungen einer
      Klasse. Hier wird deswegen der Begriff Exemplar verwendet. Die
      Verbform "instantiieren" ist zwar verwendbar, wird aber, um
      Verwirrung zu vermeiden, in dieser Uebersetzung durch "erzeugen"
      ersetzt.
    - *Programmiere auf eine Schnittstelle hin, nicht auf eine
      Implementierung*.
    - *Ziehe Objektkomposition der KLassenvererbung vor*
    - (ends ...3.3)
    - *In C++* sind statische Member-Funktionen zudem niemals
      virtuell, so dass Unterklassen sie nicht polymorph
      überschreiben können.
*** ref [[file:master.bib::gof][Erich et al. 1994: Design Patterns]]
** TODO [[./10.1.1.60.7300.measureanon.pdf][Towards measuring anonymity]]
*** summary
*** quotes
*** ref [[file:master.bib::Diaz02towardsmeasuring][Diaz et al. 2002: Towards Measuring Anonymity]]
** TODO [[./ssh-use01.pdf][Timing Analysis of Keystrokes and Timing Attacks on SSH]]
*** summary
*** quotes
*** ref [[file:master.bib::DBLP:conf/uss/SongWT01][Song et al. 2001: Timing Analysis Keystrokes]]
** PART [[info:gnuplot#Top][Broeker - gnuplot]]
*** summary
    1. [[info:gnuplot#Top][gnuplot]]

       2. [@2] [[info:gnuplot#Introduction][Introduction]]

          - many output forms: files, printer, plotter, terminals, ...

          - case sensitive language

          - separate commands by semicolons

          - document syntax:

            - {} optional arguments

            - | mutually exclusive choices

            - <> replaceable tokens (have default value if omitted)

          - read first about plotting

          - command line:
            #+BEGIN_SRC sh
              gnuplot   file1.in   -e "reset"   file2.in
            #+END_SRC
            with -e giving commands as well as the files containing them

       14. [@14] [[info:gnuplot#Expressions]]

           mathematical expressions as in C (or FORTRAN, Pascal, BASIC)

           1. [[info:gnuplot#Functions]]

              23. [@23] [[info:gnuplot#exp][exp]]

                  - =exp(x)= for e^x, beware e^-x for very large x (undefined)

       15. Fonts

           2. [@2] [[info:gnuplot#gd_(png][gd (png, gif, jpeg terminals)]]

              - for png etc output

              - five fonts possible: tiny, small, medium, large, giant

              - otherwise select font via =set term png font "/path/to/font"=

       17. [@17] [[info:gnuplot#linetypes][linetypes, colors, and styles]]

           0. [@0]

              - linetype/lt: predefined type of "lines" (color etc)
                #+BEGIN_EXAMPLE
                  plot "cumul.dat" with lines lc rgb "orange" title "msn.com", "soso.dat" with lines lc rgb "blue" title "soso.com"'
                #+END_EXAMPLE

           1. [[info:gnuplot#colorspec][colorspec]]

              - use  (other available)
                #+BEGIN_EXAMPLE
                  rgbcolor "colorname"
                  rgbcolor "#RRGGBB"
                  palette frac <val>      # <val> runs from 0 to 1
                #+END_EXAMPLE

       19. [@19] [[info:gnuplot#Plotting][Plotting]]

           - =plot= for 2d, =splot= for 3d, =replot= to draw additional stuff

           - different coordinates possible (=set polar=, mapping)

           - different borders as independent axis: x, x2 (top), y, y2 (right)

           - splot can do contours and surfaces, see isosamples

       20. [[info:gnuplot#Start-up_(initialization)][Start-up (initialization)]]

           system-wide =gnuplotrc=, then =.gnuplot= in home dir

    3. [@3] [[info:gnuplot#Commands][Commands]]

       7. [@7] [[info:gnuplot#fit][fit]]

          - fit function to data using least-squares

          - example:
            #+BEGIN_SRC gnuplot
              f(x) = a*x**2 + b*x + c
              fit f(x) './data/trash.dat' via a, b, c
            #+END_SRC

          - by default first column independent variable, second dependent

       15. [@15] [[info:gnuplot#plot][plot]]

           primary method of plotting, example:
           #+BEGIN_SRC gnuplot
             plot "datafile.1" with lines, "datafile.2" with points
             plot [t=1:10] [-pi:pi*2] tan(t), \
                  "data.1" using (tan($2)):($3/$4) smooth csplines \
                  axes x1y2 notitle with lines 5
             plot f(x) = sin(x*a), a = .2, f(x), a = .4, f(x)
           #+END_SRC

           3. [@3] [[info:gnuplot#data][data]]

              plot from data file

              4. [@4] [[info:gnuplot#smooth][smooth]]

                 - unique, frequency, cumulative and cnormal make "monotonic"

                 - kdensity, csplines, acsplines, bezier, sbezier draw curve

                 - bezier: smooth via bezier curve of degree n

                 - csplines: natural cubic splines for points (after unique)

                 - unique: points with the same x value get average y-value

           6. [@6] [[info:gnuplot#functions][functions]]

              - function, sampled on the x axis to plot
                #+BEGIN_SRC gnuplot
                  plot sin(x) title "sin(x)", approx(x) title "approximation"
                #+END_SRC

           8. [@8] [[info:gnuplot#ranges][ranges]]

              - set ranges for plot
                #+BEGIN_SRC gnuplot
                  plot [-10:30] sin(pi*x)/(pi*x)
                #+END_SRC

           11. [@11] [[info:gnuplot#with][with]]

               - set style

       25. [@25] [[info:gnuplot#set-show][set-show]]

           =set= options, show via =show [all]=

           36. [@36] [[info:gnuplot#logscale][logscale]]

               - set axes ... to logscale

                - example: x axis
                  #+BEGIN_SRC gnuplot
                   set logscale x
                  #+END_SRC

           50. [@50] [[info:gnuplot#output][output]]

               - display screen to "filename"

               - set terminal first

               - if unset, to STDOUT

               - pipe to program possible
                 #+BEGIN_SRC gnuplot
                   set output "|lpr -Plaser filename"
                 #+END_SRC

           69. [@69] [[info:gnuplot#terminal][term]]

               - with output: set terminal first

               - see [[info:gnuplot#Terminal_Index]] for list of all

               - set term <type> size <xx>, <yy> sets size of picture

           76. [@76] title

               - syntax:
                 #+BEGIN_EXAMPLE
                   set title {"<title-text>"} {offset <offset>}
                             {font "<font>{,<size>}"}
                             {{textcolor | tc} {<colorspec> | default}}
                             {{no}enhanced}
                 #+END_EXAMPLE

               - no parameters clears the title

           93. [@93] [[info:gnuplot#xlabel][xlabel]]

               - syntax
                 #+BEGIN_EXAMPLE
                   set xlabel {"<label>"} {offset <offset>} {font "<font>{,<size>}"}
                                         {textcolor <colorspec>} {{no}enhanced}
                                         {rotate by <degrees> | rotate parallel | norotate}
                 #+END_EXAMPLE

           95. [@95] [[info:gnuplot#xrange][xrange]]

               - syntax: set xrange { [{{<min>}:{<max>}}] {{no}reverse} {{no}writeback} }

               - if min/max is asterisk, auto-scale

               - writeback saves auto-scale to xrange

               - can also give during plot

               - range for autoscale =set xrange [*<10:50<*]= f.ex.
*** quotes
** PART [[../../../chive/GG/it/LaTeX/Symbols/symbols.pdf][Carlisle - The Great, Big List of \LaTeX Symbols]]
*** quotes
    14. [@14] Delimiters
        - \langle =\ langle=
        - \rangle =\ rangle=
** PART [[./icml-tutorial.pdf][Christiani - Support Vector and Kernel Machines]]
*** quotes
    - A Little History
      - SVMs introduced in COLT-92 by Boser, Guyon, Vapnik. Greatly
        developed ever since.
    - Very Informal Reasoning
      - The class of kernel methods _implicitly_ defines the class of
        possible patterns by introducing a notion of similarity
        between data
    - Modularity
      - Any K-B [kernel-based] algorithm can be fitted with any kernel
    - Dual Representation
      - The decision function can be re-written as follows:

        f ( x ) = \langle w , x \rangle + b = ∑ α_i y_i \langle x_i , x \rangle + b
        w = ∑ α_i y_i x_i
      - And also the update rule can be rewritten as follows:

        if y_i (∑ α_j y_j \langle x_j , x_i\rangle + b) ≤ 0
        then α_i ← α_i + η
      - data appears only inside dot products
    - Non-Linear Classifiers
      - One solution: creating a net of simple linear classifiers
        (neurons): a Neural Network (problems: local minima; many
        parameters; heuristics needed to train; etc)

      - Other solution: map data into a richer feature space including
        non-linear features, then use a linear classifier
    - Kernel-Induced Feature Spaces
      - In the dual representation, the data points only appear inside
        dot products:

        f ( x ) = ∑ α_i y_i \langle \phi(x_i) , \phi(x) \rangle + b
      - The dimensionality of space F not necessarily important. May
        not even know the map φ
    - at page 27
** PART [[./javascript_the_good_parts.pdf][Crockford - JavaScript: The Good Parts]]
*** summary
    1. Good parts

       - some: functional programming, prototypal inheritance

       - very different from other languages

    2. Grammar

       1. Railroad diagrams

          - left, to right

          - literal: oval, rules: rect

          - legal iff following tracks

          - one bar: whitespace allowed, two: not

       2. Whitespace

          - comments as usual

          - better use // instead of /* */ (regex f.ex. /a*/.match(s);)

       3. Names

          - letter, then also digit, underscore

          - reserved words

          - not undefined, NaN, infinity

       4. Numbers

          - only float (double)

          - (- is operator)

       5. Strings

          - sequence

          - ' or "

          - escape characters: '"\/bfnrt or u1212 (f.ex.)

          - .length

          - exactly same characters \to equal

       6. Statements

          - var: function's private variables

          - loads of parts, mostly like other languages

       7. Literals

          - number, string, object, array, function, regexp

          - object { name | "string": values, ...}

       8. Functions

          - function [name] parameters function_body

          - body: { var_statements, statements }

    3. Objects

       0. [@0] preface

          - prototype linkage

       1. Object Literals

          - can nest

          - name: unquoted possible if could be identifier

       2. Retrieval

          - use =.= instead of =[ ]=: easier to read

          - undefined in not existing

          - fill in default values if undefined:

            #+BEGIN_SRC js
              var middle = stooge["middle-name"] || "(none)";
            #+END_SRC

          - guard against typeerror (value from undefined) via &&

            #+BEGIN_SRC js
              flight.equipment && flight.equipment.model
            #+END_SRC

       3. Update

          - create if not exist, else change

       4. Reference

          - pass by reference: same object

       5. Prototype

          - like class in other languages

          - snippet to copy class: [[(object.create)]]

       6. Reflection

          - introspection

          - also gives functions, and stuff up the hierarchy

            - solution: use =hasOwnProperty=

       7. Enumeration

          - avoid functions with =typeof= and prototype's stuff by
            =hasOwnProperty=

          - possible: use explicit array of properties

       8. Delete

          - allows prototype to "shine through"

       9. Global Abatement

          - single global variable =MYAPP=

            - becomes container

          - also closure possible

    4. Functions

       0. [@0]

          - "fundamental modular unit of JavaScript."

       1. Function Objects

          - each function has a =Function.prototype=

            - context and code implementing

       2. Function Literal

          1. =function= reserved word

          2. name, or anonymous

             - used to call recursively, etc

          3. parameters

          4. body

          has access to outer scope (closure)

       3. Invocation

          - caller suspends execution

          - function receives parameters + =this= and =arguments=

            - this determined by how it is called

       4. The Method Invocation Pattern

          - method: refinement, aka =.= or =[subscript]=

          - this bound at invocation time to object

       5. The Function Invocation Pattern

          - function: bound to global scope

          - *also inner functions*

          - workaround: use =that=:
            #+BEGIN_SRC js
              myObject.double = function ( ) {
                  var that = this;
                  // Workaround.
                  var helper = function ( ) {
                      that.value = add(that.value, that.value);
                  };
                  helper();
              };

              myObject.double();
              document.writeln(myObject.getValue( )); // 6
            #+END_SRC

       6. The Constructor Invocation Pattern

          - js is prototypal

          - extend prototypes like
            #+BEGIN_SRC js
              var Quo = function (string) {
                  this.status = string;
              };
              Quo.prototype.get_status = function ( ) {
                  return this.status;
              };
              var myQuo = new Quo("confused");
            #+END_SRC

            not recommended

       7. The Apply Invocation Pattern

          - apply(new_this, [args])

            - can also be called from other class member

              #+BEGIN_SRC js
                var status = Quo.prototype.get_status.apply(statusObject);
              #+END_SRC

       8. Arguments

          - extra function parameter, array-like (=length=)

       9. Return

          - if not specified, =undefined= is returned (except for constructor)

       10. Exceptions

           - raise
             #+BEGIN_SRC js
               throw {
               name: 'TypeError',
               message: 'add needs numbers'
               };
             #+END_SRC

           - except
             #+BEGIN_SRC js
               try {
                   add("seven");
               } catch (e) {
                   document.writeln(e.name + ': ' + e.message);
               }
             #+END_SRC

           - one =catch= block

       11. Augmenting Types

           - add f.ex. =trim= to string

           - add only if not yet existing ;-)

       12. Recursion

           - not tail-recursive

           - great for dom-walking f.ex.

       13. Scope

           - function scope:
             #+BEGIN_SRC js
               var foo = function ( ) {
                   var a = 3, b = 5;
                   var bar = function ( ) {
                       var b = 7, c = 11;
                       // At this point, a is 3, b is 7, and c is 11
                       a += b + c;
                       // At this point, a is 21, b is 7, and c is 11
                   };
                   // At this point, a is 3, b is 5, and c is not defined
                   bar( );
                   // At this point, a is 21, b is 5
               };
             #+END_SRC

           - recommendation: declare variables at top of function body

       14. Closure

           - allow for private variables
             #+BEGIN_SRC js
               var myObject = function ( ) {
                   var value = 0;
                   return {
                       increment: function (inc) {
                           value += typeof inc === 'number' ? inc : 1;
                       },
                       getValue: function ( ) {
                           return value;
                       }
                   };
               }( );
             #+END_SRC

             - not capitalized as =this=-functionality not used

           - has access to actual values, not copies

       15. Callbacks

           - non-blocking paused execution, f.ex
             #+BEGIN_SRC js
               request = prepare_the_request( );
               send_request_asynchronously(request, function (response) {
                   display(response);
               });
             #+END_SRC

       16. Module

           - use closure to make functions with private parts

       17. Cascade

           - if would return nothing: return this to allow multiple calls
             #+BEGIN_SRC js
               getElement('myBoxDiv').
                   move(350, 150).
                   width(100).
                   height(100).
                   color('red').
                   border('10px outset').
                   padding('4px').
                   appendText("Please stand by").
                   on('mousedown', function (m) {
                       this.startDrag(m, this.getNinth(m));
                   }).
                   on('mousemove', 'drag').
                   on('mouseup', 'stopDrag').
                   later(2000, function ( ) {
                       this.
                           color('yellow').
                           setHTML("What hath God wraught?").
                           slide(400, 40, 200, 200);
                   }).
                   tip('This box is resizeable');
             #+END_SRC

       18. Curry
           
           set fixed parameters to functions

           #+BEGIN_SRC js
             Function.method('curry', function ( ) {
                 var slice = Array.prototype.slice,
                     args = slice.apply(arguments),
                     that = this;
                 return function ( ) {
                     return that.apply(null, args.concat(slice.apply(arguments)));
                 };
             });
           #+END_SRC

       19. Memoization

           store already computed values in function via

           #+BEGIN_SRC js
             var memoizer = function (memo, fundamental) {
                 var shell = function (n) {
                     var result = memo[n];
                     if (typeof result !== 'number') {
                         result = fundamental(shell, n);
                         memo[n] = result;
                     }
                     return result;
                 };
                 return shell;
             };
           #+END_SRC
           use as
           #+BEGIN_SRC js
             var fibonacci = memoizer([0, 1], function (shell, n) {
                 return shell(n - 1) + shell(n - 2);
             });
           #+END_SRC

    5. Inheritance

       code reuse, save development time. js facilitates many types of
       inheritance

       1. Pseudoclassical

          complicated, not really necessary

       2. Object Specifiers

          - use one object with members instead of parameters

          - params can be any order, easier to read, bonus: JSON creation

       3. Prototypal

          differential inheritance: specify what is new

          - create object

          - [[(object.create)]]

          - specify changes

          - (-) no private variables

       4. Functional

          hide members by using closure and only giving out an object
          which accesses these

       5. Parts

          extend by taking the =that= object, defining a new variable
          and a new function which accesses the variable internally,
          returning =that=

    6. Arrays

       not a real array of ocnsecutive memory locations

       1. Array Literals

          - ['zero', 'one'] just like {'0': 'zero', '1': 'one'}

            except for

            - length property

            - array methods

       2. Length

          biggest index +1

          setting to smaller causes larger objects to be deleted

       3. Delete

          leaves a hole (=undefined=)

          better use =myarray.splice(2, 1);=

       4. Enumeration

          for in can enumerate parent properties,
          better use for ( i = 0; i < myarray.length; i += 1 )

       5. Confusion

          detect whether array:
          #+BEGIN_SRC js
            var is_array = function (value) {
                return value &&
                    typeof value === 'object' &&
                    typeof value.length === 'number' &&
                    typeof value.splice === 'function' &&
                    !(value.propertyIsEnumerable('length'));
            };
          #+END_SRC

       6. Methods

          extend arrays, f.ex. via
          #+BEGIN_SRC js
            Array.method('reduce', function (f, value) {
                var i;
                for (i = 0; i < this.length; i += 1) {
                    value = f(this[i], value);
                }
                return value;
            });
          #+END_SRC

       7. Dimensions

          can add method to initialize array to known length, not built-in

    7. Regular Expressions

       best keep simple

       more efficient than other methods

       "regexp.exec, regexp.test, string.match, string.replace,
       string.search, and string.split"

       1. An Example
          #+BEGIN_SRC js
            var parse_url = /^(?:([A-Za-z]+):)?(\/{0,3})([0-9.\-A-Za-z]+)(?::(\d+))?(?:\/([^?#]*))?(?:\?([^#]*))?(?:#(.*))?$/;
          #+END_SRC

       2. Construction

          - =g= flag: global

          - =i= case insensitive

          - =m= multiline

          construct via regex
          #+BEGIN_SRC js
          var my_regexp = /"(?:\\.|[^\\\"])*"/g;
          #+END_SRC
          or constructor (double slashes, escape quotes)
          #+BEGIN_SRC js
          var my_regexp = new RegExp("\"(?:\\.|[^\\\\\\\"])*\"", 'g');
          #+END_SRC

       3. Elements

          1. Regexp Choice

             sequence =|= sequence ...

          2. Regexp Sequence

             factor (quantifier factor)*

          3. Regexp Factor

             escape | class | group | any unicode except /,\,[,],{,},?,+,*,|,C^..

          4. Regexp Escape

             \ starts

             - \f formfeed, \n newline, \r carriage return, \t tab, \u unicode,

             - \d 0-9 \D negates

             - \s [\f\n\r\t\u000B\u0020\u00A0\u2028\u2029] \ S negates

             - \w supposed to be words, but only ascii, so useless, \W negates

               - \b similarly useless

             - \1 refers to group one:
               #+BEGIN_SRC js
                 var doubled_words = /[A-Za-z\u00C0-\u1FFF\u2800-\uFFFD'\-]+\s+\1/gi;
               #+END_SRC

          5. Regexp Group

             - capturing =(=

             - non-capturing =(?:=, slighly faster

             - positive lookahead =(?=, bad part

             - negative lookahead =(!=, bad part

          6. Regexp Class

             - enclosed by =[]=, negatable,

               - escape /,\,[,],^,-

          7. Regexp Class Escape

             - [\b] for backspace

          8. Regexp Quantifier

             - ?,*,+,{a,b}

             - ? at end: lazy matching, "as few repetitions as possible"

    8. Methods

       1. Array

          1. concat(items):

             appends each item in items, elements of array individually

          2. array.join(separator): makes a string, with separator
             between elements, faster than =+= for individual elements

          3. array.pop( ): stack-like

          4. array.push(item...): unlike =concat=, appends arrays wholesale

          5. array.reverse() reverse array in-place

          6. array.shift(): like =pop=, first element, much slower than =pop=

          7. array.slice(start, end=array.length): shallow copy

          8. array.sort(comparefn): sort in place, default comparator
             is string-based

             for numbers:
             #+BEGIN_SRC js
               n.sort(function (a, b) {
                   return a - b;
               });
             #+END_SRC

          9. array.splice(start, deleteCount, item...): remove
             elements in-place (see book for implementation)

          10. array.unshift(item...): like push, but at the front

       2. Function

          1. function.apply(thisArg, argArray): calls function with
             argArray and this = thisArg
*** quotes
    0. [@0] unsorted
       - We appreciate, but do not require, attribution. An attribution
         usually includes the title, author, publisher, and ISBN. For
         example: “JavaScript: The Good Parts by Douglas
         Crockford. Copyright 2008 Yahoo! Inc., 978-0-596-51774-8.”
       - (ends 0+1)
       - It is usually necessary to test object.hasOwnProperty(variable)
         to determine whetherthe property name is truly a member of the
         object or was found instead on the prototype chain.
         #+BEGIN_SRC js
           for (myvar in obj) {
               if (obj.hasOwnProperty(myvar)) {
                       ...
               }
           }
         #+END_SRC
       - prefix operator [...] + to number
       - The && operator produces the value of its first operand if the
         first operand is falsy. Otherwise, it produces the value of the
         second operand.
       - (ends 2)
       - The create method creates a new object that uses an old object
         as its prototype. [...]
         #+BEGIN_SRC js
           if (typeof Object.create !== 'function') {
               Object.create = function (o) {   (ref:object.create)
                   var F = function () {};
                   F.prototype = o;
                   return new F();
               };
           }
           var another_stooge = Object.create(stooge);
         #+END_SRC
       - (ends 3?)
       - Regular expressions usually have a significant performance
         advantage over equivalent string operations in JavaScript.
       - ...
       - (bad parts)
       - I have never seen a piece of code that was not improved by
         refactoring it to remove the continue statement.
       - (ends bp)
    1. JSLint
       1. Options
          - Semicolon

            JSLint expects that every statement be followed by ;
            except for for , function , if , switch , try , and while
            . JSLint does not expect to see unnecessary semicolons or
            the empty statement.
*** ref [[file:master.bib::javascript][Crockford 2008: JavaScript]]
** PART Ferguson - Practical Cryptography
*** summary
    0. [@0] Preface

       - how to engineer secure crypto

       - errata at macfergus.com/pc

         - submit if you find a new one

    1. Our Design Philosophy

       - no single system they inspected was secure

       - IT needs "ruthless" security focus

       - no use to lock the front door securely if the back door is left open
*** quotes
    5. [@5] Block cipher modes
       7. [@7] Which Mode Should I Use?
          - Always keep in mind that an encryption mode only provides
            confidentiality. That is, the attacker cannot find any
            information about the data you are communicating, other
            than the fact /that/ you are communicating, /when/ you are
            communicating, /how much/ you are communicating, and
            /whom/ you are communicating with.[fn::This kind of
            analysis is called /traffic analysis/. It can provide very
            useful information to an attacker. Preventing traffic
            analysis is possible, but generally too expensive in terms
            of bandwidth for anyone but the military.]
    8. [@8] The Secure Channel
       1. Problem Statement
          4. [@4] Security Properties
             - It is extremely hard to hide information such as the
               size of the timing of the messages. The known solutions
               require Alice to send a continuous stream of messages
               at the maximum bandwidth that she will ever use. If she
               doesn't have any messages to send, she should invent
               some trivial ones and send those. This might be
               acceptable for military applications, but it is not
               acceptable for most civilian applications. Given
               that Eve can see the size and timing of messages on a
               communication channel, she can find out who is
               communicating with thom, how much, and when. This is
               called /traffic analysis/. It yields a host of
               information, and is extremely hard to prevent.
*** ref [[file:master.bib::practical][Ferguson & Schneier 2003: Practical Cryptography]]
** PART [[../../../chive/GG/it/js/Eloquent%20JavaScript/contents.html][Haverbeke - Eloquent Javascript]]
*** summary
    8. [@8] Object-oriented Programming

       - 90s trend, persistent, "solid and useful"

       - create objects, attach methods and properties

       - easy: just attach function values to object

       - =apply(object, params)=: first parameter (mapped to =this=) is object

         - also =call(object, params*)= separate arguments instead of array

       - constructor

         - Capitalized

         - use with =new=

         - always sets =this= to object, returns it

         - also sets =constructor= property and some other stuff

       - .prototype

         - like =class=, but dynamically augmentable

       - always changes object, object never changes .prototype

         - access goes up the prototype hierarchy until name is found

       - use =hasOwnProperty= to find out if really the object has the
         attribute, or it's somewhere else

         - workaround for when hasOwnProperty is an attribute

         - code to apply function to each property of object
           #+BEGIN_SRC js
             function forEachIn(object, action) {
                 for (var property in object) {
                     if (object.hasOwnProperty(property))
                         action(property, object[property]);
                 }
             };
           #+END_SRC

       - above fails in Firefox, which assigns a __proto__ to all objects

         - objects are "values with methods" and "sets of properties"

       - =Dictionary= class for "sets of properties"

       - external api vs internal implementation

       - could add methods to f.ex. =Object=: be careful, but possible

       - Exercise project: Terrarium

         - 2d, discrete time and place, # wall, ' ' free, o

         - functions: =toString=, =step=, =start=, =stop= for terrarium

       - first point, addpoints, samepoint
         #+BEGIN_SRC js
           function Point(x, y) {
               this.x = x;
               this.y = y;
           };

           Point.prototype.add(other) {
               return new Point(this.x + other.x, this.y += other.y);
           };

           Point.prototype.isEqualTo(other) {
               return this.x === other.x && this.y === other.y;
           };
         #+END_SRC

       - need to decide what are methods, what are functions, what are
         new objects

         - keep object as small as possible

       - 2-dim array as 1-dim, accessed via
         #+BEGIN_SRC js
           var grid = ["0,0", "1,0", "2,0",
                       "0,1", "1,1", "2,1"];
           show(grid[2 + 1 * 3]);
         #+END_SRC

       - ex. 8.2 =each=
         #+BEGIN_SRC js
           Grid.prototype.each(func(point, stuff)) {
               for ( var j = 0; j < width; j += 1; ) {
                   for ( var i = 0; i < height; i+= 1; ) {
                       var p = new Point(i, j);
                       func(p, valueAt(p));
                   }
               }
           };
         #+END_SRC

       - direction object (dictionary, {"n", 'ne', 'nw', ...})

         - action object ({"move", direction})

         - Bug.act function() = .. return action

       - Terrarium: initialized with ascii plan

         - #=wall, o=bug, ' ' = nothing

         - elementFromCharacter maps above

    12. [@12] The Document-Object Model

        - =<HTML>= via =document.documentElement=

          - =<BODY>= via =document.body=

        - f.ex. =document.body.parentNode= to get the parent

          - =.childNodes=, =.firstChild= and =.lastChild=

          - =.previousSibling= and =.nextSibling=

        - if text node, then =node.nodeType= = 3, (most often) else 1

        - =.nodeName= for tag name (CAPITALIZED)

          - =.firstChild.nodeValue= for text value

        - exercise: get text of html tags

          #+BEGIN_SRC js
            function getText(node) {
              if ( isTextNode(node) ) {
                return node.nodeValue;
              } else {
                var out = "";
                for ( var i = 0; i < node.childNodes.length; i++ ) {
                  out += getText(node.childNodes[i]);
                }
                return out;
              }
            };
            print(getText(document.body));
          #+END_SRC

        - =node.innerHTML= gives HTML, also changeable which effects website

        - =document.getElementById("picture")= gives element

          - also =document.getElementsByTagName[0]=

        - =var h = document.createElement("H1");= creates new header

          - =var text = document.createTextNode("hi there");= creates text node

          - =h.appendChild(text)= and =document.appendChild(h)= appends both

          - also =newImage.setAttribute("src", "img/Hiva Oa.png");=

        - =setAttribute= workaround for IE, then

          - build dom elements
            #+BEGIN_SRC js
              function dom(name, attributes) {
                  var node = document.createElement(name);
                  if (attributes) {
                      forEachIn(attributes, function(name, value) {
                          node.setAttribute(name, value);
                      });
                  }
                  for (var i = 2; i < arguments.length; i++) {
                      var child = arguments[i];
                      if (typeof child == "string")
                          child = document.createTextNode(child);
                      node.appendChild(child);
                  }
                  return node;
              };
            #+END_SRC

        - =.insertBefore(someNode, beforeThis)=

          - =.replaceChild(newNode, toBeReplaced)=

          - =.removeChild(childNode)=

        - "Write the convenient function removeElement which removes
          the DOM node it is given as an argument from its parent
          node."

          #+BEGIN_SRC js
            function easyRemove(node) {
                node.parentNode && node.parentNode.removeChild(node);
            }
          #+END_SRC

        - only moveable within single document, not from tab to tab, f.ex.

        - html tables: ie needs tbody tag

        - create table function

          her solution
          #+BEGIN_SRC js
            function makeTable(data, columns) {
              var headRow = dom("TR");
              forEach(columns, function(name) {
                headRow.appendChild(dom("TH", null, name));
              });

              var body = dom("TBODY", null, headRow);
              forEach(data, function(object) {
                var row = dom("TR");
                forEach(columns, function(name) {
                  row.appendChild(dom("TD", null, String(object[name])));
                });
                body.appendChild(row);
              });

              return dom("TABLE", null, body);
            };
          #+END_SRC
          my part
          #+BEGIN_SRC js
            function makeTable(objects, columnNames) {
              var tableEls = [],
                  i;

              tableEls.push(dom("tr", null, dom("th", null, columnNames[0]), dom("th", null, columnNames[1])));
              for (i = 0; i < objects.length; i++ ) {
                tableEls.push(dom(tr, null, dom(td, null, objects[columnNames[0]]), dom(td, null, objects[columnNames[1]])));
              };
              // somehow append
            };
          #+END_SRC

        - CSS

          style attribute to node

        - other style

          display = ""; makes it reappear, code
          #+BEGIN_SRC js
            $("picture").style.display = "";
          #+END_SRC

        - change position via js

          #+BEGIN_SRC js
            node.style.position = "absolute";
            node.style.left = 100px;
            node.style.top = 10%; //?
          #+END_SRC

        - also =.width=, =.height= to set

          - and =.offsetWidth= etc and =.clientWidth= to differentiate
            outer/inner

    13. Browser Events
*** quotes
    4. [@4] Data structures: Objects and Arrays
       - The keyword `delete` cuts off properties. Trying to read a
         non-existent property gives the value undefined.
       - var now = new Date();
    8. [@8] Object-oriented Programming
       - Object-orientation's longevity can largely be explained by the
         fact that the ideas at its core are very solid and useful. In
         this chapter, we will discuss these ideas, along with
         JavaScript's (rather eccentric) take on them.
       - typing ={}= is equivalent to typing =new Object()=
       - example object creation
         #+BEGIN_SRC js
           function Rabbit(adjective) {
             this.adjective = adjective;
           }
           Rabbit.prototype.speak = function(line) {
             print("The ", this.adjective, " rabbit says '", line, "'");
           };

           var hazelRabbit = new Rabbit("hazel");
           hazelRabbit.speak("Good Frith!");
         #+END_SRC
       - Personally, I treat writing documentation as a 'finishing touch'
         to add to a system. When it feels ready, it is time to write
         something about it, and to see if it sounds as good in English
         (or whatever language) as it does in JavaScript (or whatever
         programming language).
    10. [@10] Regular Expressions
        - If we have our word list as an array, we can build the
          regular expression like this:
          #+BEGIN_SRC js
            var badWords = ["ape", "monkey", "simian", "gorilla", "evolution"];
            var pattern = new RegExp(badWords.join("|"), "i");
          #+END_SRC
        - The first argument to the RegExp constructor is a string
          containing the pattern, the second argument can be used to
          add case-insensitivity or globalness.
*** ref [[file:master.bib::eloquent-javascript][Haverbeke 2007: Eloquent JavaScript]]
** PART [[./Matplotlib.pdf][Hunter - Matplotlib]]
*** summary
    5. [@5] BEGINNER’S GUIDE

       1. Pyplot tutorial

          0. [@0]

             - each command changes figure in some way
               #+BEGIN_SRC python
                 import matplotlib.pyplot as plt
                 plt.plot([1,2,3,4])
                 plt.ylabel('some numbers')
                 plt.show()
               #+END_SRC

             - plot(x, y, format_string='b-')

          2. [@2] Working with multiple figures and axes

             - plt.figure() new figure, seldomly used

             - plt.subplot(row, col, id)
               creates subplot, row and col are maximum, id traverses

          3. [@3] Working with text

             - =text()= at arbitrary location, else =xlabel=, =ylabel=, =title=

             - Using mathematical expressions in text

               - can use any TeX-command, need to use a raw string for that
                 #+BEGIN_SRC python
                   plt.title(r'$\sigma_i=15$')
                 #+END_SRC

       4. [@4] Logarithmic and other nonlinear axis

          5. [@5] Typesetting With XeLaTeX/LuaLaTeX

             - pgf supported, use =plt.savefig('figure.pgf')=

             - use default LaTeX fonts by clearing =rcParams=:
               #+BEGIN_SRC python
                 import matplotlib as mpl
                 mpl.use("pgf")
                 pgf_with_rc_fonts = {
                     "font.serif": [],
                     "pgf.texsystem": "pdflatex"
                 }
                 mpl.rcParams.update(pgf_with_rc_fonts)
               #+END_SRC
               also possible to clean =font.sans-serif= or
               =font.monospace=

               - the above also sets the texsystem to pdflatex (use if
                 there is only pdflatex installed)

               - use("pgf") is not necessary, except if you want to
                 generate PDF graphics with pgf backend

                 - set this live via
                   #+BEGIN_SRC python
                     from matplotlib.backends.backend_pgf import FigureCanvasPgf
                     matplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)
                   #+END_SRC

             - other parameters:

               - ="pgf.rcfonts": False=

               - ="text.usetex": True=

       6. [@6] Legend guide

    68. [@68] matplotlib.pyplot

        - add label (per line) via
          #+BEGIN_SRC python
            plot([1,2,3], [1,2,3], 'go-', label='line 1', linewidth=2)
            legend() # call just once after all plot() commands
          #+END_SRC
          afterwards, just do =show()=, etc

        - reconfigure subplots so that lower title does not overlap
          upper xlabel: =plt.tight_layout()=
*** quotes
    4. [@4] CONFIGURATION GUIDE
       3. [@3] Using matplotlib in a python shell
          3. [@3] Controlling interactive updating
             - ion() turns interactive mode on
             - ioff() turns interactive mode off
    5. Beginner's Guide
       1. Pyplot tutorial
          - For example, to plot the above with red circles, you would issue
            #+BEGIN_SRC python
              import matplotlib.pyplot as plt
              plt.plot([1,2,3,4], [1,4,9,16], 'ro')
              plt.axis([0, 6, 0, 20])
              plt.show()
            #+END_SRC
    44. [@44] axes
        - set_xbound(lower=None, upper=None)

          Set the lower and upper numerical bounds of the x-axis. This
          method will honor axes inversion regardless of parameter
          order. It will not change the _autoscaleXon attribute.
        - set_xticklabels(labels, fontdict=None, minor=False, **kwargs)

          Set the xtick labels with list of strings labels. Return a
          list of axis text instances.
          - rotation

            [ angle in degrees | ‘vertical’ | ‘horizontal’ ]
    68. [@68] matplotlib.pyplot
        - matplotlib.pyplot.tight_layout(pad=1.08, h_pad=None,
          w_pad=None, rect=None)

          Automatically adjust subplot parameters to give specified padding.
        - matplotlib.pyplot.savefig(*args, **kwargs)

          Save the current figure.
        - matplotlib.pyplot.xticks(*args, **kwargs)

          Get or set the x-limits of the current tick locations and labels.
          #+BEGIN_SRC python
            locs, labels = xticks()
            # set the locations and labels of the xticks
            xticks( arange(5), ('Tom', 'Dick', 'Harry', 'Sally', 'Sue') )
          #+END_SRC
    72. [@72] text
        1. matplotlib.text
           - set_text(s)

             Set the text string s. It may contain newlines (\n) or
             math in LaTeX syntax. ACCEPTS: string or anything
             printable with ‘%s’ conversion.
    89. [@89] PYLAB_EXAMPLES EXAMPLES
        20. [@20] pylab_examples example code: bar_stacked.py [...]
            #+BEGIN_SRC python
              # a stacked bar plot with errorbars
              N = 5
              menMeans = (20, 35, 30, 35, 27)
              womenMeans = (25, 32, 34, 20, 25)
              menStd = (2, 3, 4, 1, 2)
              womenStd = (3, 5, 2, 3, 3)
              ind = np.arange(N)
              # the x locations for the groups
              width = 0.35
              # the width of the bars: can also be len(x) sequence
              p1 = plt.bar(ind, menMeans, width, color='r', yerr=menStd)
              p2 = plt.bar(ind, womenMeans, width, color='y', bottom=menMeans,
                           yerr=womenStd)
              plt.xticks(ind + width/2., ('G1', 'G2', 'G3', 'G4', 'G5'))
              plt.yticks(np.arange(0, 81, 10))
            #+END_SRC
** PART [[../../../chive/GG/it/LaTeX/a026.pdf][Jürgens - LATEX — eine Einführung und ein bisschen mehr ...]]
*** summary
    4. [@4] Ein erstes Beispiel

       - text bold durch =\textbf{fetter text}= (math-mode bold durch
         ${\bf x}$)

    14. [@14] Erstellen von Verzeichnissen

        - Inhalt: =\tableofcontents=

        - Tables: =\listoftables=

        - Figures: =\listoffigures=

    18. [@18] Aufteilung eines Dokumentes in Teildokumente

        Teildokumente (verbatim) mit =\input= einfügen

    23. [@23] NUMMERIERTE ABBILDUNGEN UND TABELLEN

        - nummerierte, gelabelte abbildung mit
          #+BEGIN_LaTeX
          \begin{figure}[position]
          Abbildung
          \caption{Bildunterschrift}
          \label{markenname}
          \end{figure}
          #+END_LaTeX

        - nummerierte tabelle mit
          #+BEGIN_LaTeX
          \begin{table}
            \begin{tabular}{spec}
              :
            \end{tabular}
            \caption{...}
          \end{table}
          #+END_LaTeX

        - einfügen an dem Ort via =\begin{table}[H]= mit header
          #+BEGIN_LaTeX
          \usepackage{float}
          \restylefloat{table}
          #+END_LaTeX

    25. [@25] Verdrehen von Objekten

        - mit =\usepackage{rotating}= z.b. per \begin{turn}{gradzahl}...

        - (Bilder haben eigene =angle= Optionen, siehe Kap. 24)

    27. [@27] ERSTELLEN VON MEHRSEITIGEN TABELLEN

        - example
          #+BEGIN_SRC latex
            \usepackage{longtable}
            \setlongtables
            \begin{longtable}{c c c}
              Page: google.com & Page: tumblr.com & Page: netflix.com \\
              \endfirsthead
              Page: google.com & Page: tumblr.com & Page: netflix.com \\
              \endhead
            \end{longtable}
          #+END_SRC
*** quotes
    14. [@14] Erstellen von Verzeichnissen
        - Die Erstellung der Verzeichnisse für floating charts erfolgt
          über die Befehle
          #+BEGIN_SRC latex
            \listoftables
          #+END_SRC
          für ein Tabellenverzeichnis und
          #+BEGIN_SRC latex
            \listoffigures
          #+END_SRC
          für ein Abbildungsverzeichnis.
    15. [@15] Erstellen von Tabellen
        2. [@2] Arbeiten mit Tabellen
           - Im Zusammenhang mit Tabellen sei aber noch auf das
             Erstellen mehrspaltiger Überschriften hingewiesen, die
             mit dem Befehl
             #+BEGIN_LaTeX
             \multicolumn{AnzahlSpalten}{Ausrichtung}{Text}
             #+END_LaTeX
             erzeugt werden können.
    23. [@23] NUMMERIERTE ABBILDUNGEN UND TABELLEN
        - Abbildungen Ein „wanderndes Objekt“, das eine Abbildung
          beinhaltet erstellen Sie in der Umgebung:
          #+BEGIN_SRC latex
            \begin{figure}[position]
            Abbildung
            \caption{Bildunterschrift}
            \label{markenname}
            \end{figure}
          #+END_SRC
        - Falls Sie keine eigene Option zur Platzierung angegeben
          haben, benutzt LATEX die Voreinstellung
          =\begin{figure}[tbp]=. Sinnvoller ist in vielen Fällen die
          Angabe der Option =[htbp]=;
        - h	hier, an der Stelle, an der die Abbildung definiert wird
** PART [[../../../chive/GG/it/LaTeX/a027.pdf][Jürgens - LATEX — Fortgeschrittene Anwendungen]]
*** summary
    16. [@16] Verzeichnisse — alte und neue

        2. [@2] Erstellen von Sachregistern

           - Präambel: =\makeindex=

           - im text =\index{eintrag}=
*** quotes
    9. [@9] Tabellen — weitere Möglichkeiten
       - p{breite}

         erstellt eine Spalte mit der angegebenen Breite. Der in die
         Spalte eingetragene Text wird dann auf der festgelegten
         Spaltenbreite umbrochen.
    16. [@16] Verzeichnisse — alte und neue
        2. [@2] Erstellen von Sachregistern
           - Wenn Sie einen Index benötigen, so müssen Sie LATEX
             dies bereits in der Präambel ankündigen mit dem Befehl
             =\makeindex=
           - Die einzelnen Einträge für das Sachregister können Sie
             markieren mit =\index{eintrag}=
    26. [@26] Schriften — weitere Zeichensätze und Familien
        - Zunächst wollen wir uns aber sechs PostScript-Schriften näher
          ansehen: LATEX stellt Ihnen die Pakete times, avant, bookman,
          newcent, palatino, helvet zur Verfügung. Binden Sie eines der
          Pakete mit =\usepackage= in Ihr Dokument ein, so wird für das
          gesamte Dokument die ausgewählte Schrift verwendet.
** PART [[./irbookonlinereading.pdf][Manning - An Introduction to Information Retrieval]]
*** summary
    0. [@0] Preface

       - from specialized academic study to www

       - Book organization and course development

         - chapter: 75 to 90 minutes

         - 1-8 foundation topics

       - Prerequisites

       - Book layout

         - pencil: worked examples

         - scissors: advanced, can leave out

         - ? exercise, with difficulty * to ***

       - Acknowledgments

    1. Boolean retrieval

    8. [@8] Evaluation in information retrieval

       3. [@3] Evaluation of unranked retrieval sets

          - Precision: "the fraction of retrieved documents that  are
            relevant"

          - Recall: "the fraction of relevant documents that are retrieved"

          - Problem with accuracy: f.ex. if many (f.ex. 99.9%) of
            documents do not match, then always saying no yields high
            (f.ex. 99.9%) accuracy.

            - "data is extremely skewed"

          - web surfers want high precision (first page fits)

          - legals / analysts want high recall

          - F-measure: harmonic mean of P and R (see quotes)

    14. [@14] Vector space classification

        6. [@6] The bias-variance tradeoff

           - why use "weaker" (f.ex. linear) classifiers?

           - "there is no universally optimal learning method"

           - here: linear/nonlinear as proxies for weak/strong
*** quotes
  0. [@0] Preface
     - Chapter 10 considers information retrieval from documents that
       are structured with markup languages like XML and HTML.
  3. [@3] Dictionaries and tolerant retrieval
     3. [@3] Spelling correction
        - Most commonly, the edit operations allowed for this purpose are:
          (i) insert a character into a string; (ii) delete a character
          from a string and (iii) replace a character of a string by
          another character; for these operations, edit distance
          is sometimes known as *Levenshtein distance*.
        - EDIT DISTANCE ( s_{1} , s_{2} )
          1. int m [ i, j ] = 0
          2. for i ← 1 to | s_{1} |
          3. do m [ i, 0 ] = i
          4. for j ← 1 to | s_{2} |
          5. do m [ 0, j ] = j
          6. for i ← 1 to | s_{1} |
          7. do for j ← 1 to | s_{2} |
          8. do m [ i, j ] = min { m [i−1, j−1]
             + if ( s1[i] = s2[j]) then 0 else 1 fi,
          9. m [ i − 1, j ] + 1,
          10. m [ i, j − 1 ] + 1 }
          11. return m [| s_{1} | , | s_{2} |]
  8. [@8] Evaluation in information retrieval
     3. [@3] Evaluation of unranked retrieval sets
        - Precision (P) is the fraction of retrieved documents that
          are relevant

          Precision = # ( relevant items retrieved ) / # ( retrieved items )
          = P ( relevant | retrieved )
        - Recall (R) is the fraction of relevant documents that are retrieved

          Recall = # ( relevant items retrieved ) / # ( relevant items )
          = P ( retrieved | relevant )
        - A single measure that trades off precision versus recall
          is the F measure, which is the weighted harmonic mean of
          precision and recall:

          F = 1 / ((\alpha 1/P) + (1-\alpha) 1/R) = ( β 2 + 1 ) PR / (\beta^2P + R)

          where \beta^2 = (1-\alpha)/\alpha
        - Values of β < 1 emphasize precision, while values of β > 1
          emphasize recall.
  14. [@14] Vector space classification
      - *Contiguity hypothesis*. Documents in the same class form a
          contiguous region and regions of different classes do not
          overlap.
      - Nonlinear models have more parameters to fit on a limited amount
          of training data and are more likely to make mistakes for small
          and noisy data sets.
*** ref [[file:master.bib::intro2ir][Manning et al. 2009: Introduction Information Retrieval]]
** PART [[./10_3-ndss2016-slides.pdf][Panchenko - Website Fingerprinting at Internet Scale Presentation]] pic
*** ref  https://www.internetsociety.org/sites/default/files/10_3-ndss2016-slides.pdf
** PART [[./btxdoc.pdf][Patashnik - BIBTEXing]]
*** quotes
    2. [@2] Changes
       1. New BibTEX features
          - the special *crossref* field tells BibTEX that the
            no-gnats entry should inherit any fields it’s missing from
            the entry it cross references, gg-proceedings.
    3. [@3] The Entries
       1. Entry Types
          1. *article* An article from a journal or magazine. Required
             fields: author, title, journal, year. Optional fields:
             volume, number, pages, month, note.
          2. *book* A book with an explicit publisher. Required
             fields: author or editor, title, publisher,
             year. Optional fields: volume or number, series, address,
             edition, month, note.
          3. *booklet* A work that is printed and bound, but without a
             named publisher or sponsoring institution. Required
             field: title. Optional fields: author, howpublished,
             address, month, year, note.
          4. *inbook* A part of a book, which may be a chapter (or
             section or whatever) and/or a range of pages. Required
             fields: author or editor, title, chapter and/or pages,
             publisher, year. Optional fields: volume or number,
             series, type, address, edition, month, note.
          5. *manual* Technical documentation. Required field:
             title. Optional fields: author, organization, address,
             edition, month, year, note.
          6. *mastersthesis* A Master’s thesis. Required fields:
             author, title, school, year. Optional fields: type,
             address, month, note.
          7. *techreport* A report published by a school or other
             institution, usually numbered within a series. Required
             fields: author, title, institution, year. Optional
             fields: type, number, address, month, note.
          8. *unpublished* A document having an author and title, but
             not formally published. Required fields: author, title,
             note. Optional fields: month, year.
       2. Fields
          1. *address* Usually the address of the publisher or other
             type of institution. For major publishing houses, van
             Leunen recommends omitting the information entirely. For
             small publishers, on the other hand, you can help the
             reader by giving the complete address.
          2. *edition* The edition of a book—for example, “Second”. This
             should be an ordinal, and should have the first letter
             capitalized, as shown here; the standard styles convert
             to lower case when necessary.
          3. *year* The year of publication or, for an unpublished work,
             the year it was written.
    4. Helpful Hints
       2. [@2] Thus, you should feel free to be creative in how you use
          these entry types (but if you have to be too creative,
          there’s a good chance you’re using the wrong entry type).
       7. [@7] If you want to comment out an entry, simply remove the
          ‘@’ character preceding the entry type.
       9. [@9] It’s best to use the three-letter abbreviations for the
          month, rather than spelling out the month yourself. This
          lets the bibliography style be consistent. And if you want
          to include information for the day of the month, the month
          field is usually the best place. For example
          #+BEGIN_EXAMPLE
            month = jul # "~4,"
          #+END_EXAMPLE
** PART [[./harry.pdf][Rieck - Harry Version 0.4.2]]
*** quotes
    3. [@3] DESCRIPTION
       - The focus of harry lies on implicit similarity measures, that
         is, comparison functions that do not give rise to an explicit
         vector space.
*** ref [[file:master.bib::harry-manual][2016: Harry User Manual]]
** PART [[./The web is Doom - mobiForge.html][ronan - The web is Doom]]
*** summary
    1. The web is Doom

       - size of average web site == doom install image

    2. Overall trends in page size

       size growth has slowed

       1. Top performers vs. the rest

          - top websites are smaller than average

          - and not getting bigger

          - not normal-distributed

       2. Outlook for the web
*** quotes
*** ref [[file:master.bib::web-is-doom][Cremin 2016: Doom]]
** PART [[./random.pdf][Saucier - Computer Generation of Statistical Distributions]]
*** summary
    1. SUMMARY

       many different distributions, C++, easy to use

    2. INTRODUCTION

       - simulations useful to

         - supplement theory (may be understood and too hard to
           analyse or just not well understood)

         - supplement experiment (too costly)

       - getting faster exponentially

    3. METHODS FOR GENERATING RANDOM NUMBER DISTRIBUTIONS

    5. [@5] PROBABILITY DISTRIBUTION FUNCTIONS

       1. Continuous Distributions

          27. [@27] Weibull

              - a is just additive, can be left out, leaving
                two-parameter form  X = b \cdot (−ln(U ))^{1/a}

              - b is just multiplicative, can be multiplied-on later,
                leaving one-parameter form X = (−ln(U ))^{1/a}
*** quotes
    5. [@5] PROBABILITY DISTRIBUTION FUNCTIONS
       1. Continuous Distributions
          8. [@8] Exponential

             - (1) Generate U ~ U(0, 1)
               (2) Return X = a − b ln U

             - Source Code:
               #+BEGIN_SRC C++
                 double exponential( double a, double b )
                 {
                   assert( b > 0. );
                   return a - b * log( uniform( 0., 1. ) );
                 }
               #+END_SRC

          11. [@11] Gamma

              - Density Function:
                f(x) = 1/Gamma(c) *b^{-c} *(x-a)^{c-1} *e^{-(x-a)/b}
                if x>a, else 0

              - Input: scale parameter b > 0; shape parameter c > 0

                c == \kappa, b == \theta

              - Algorithm:

                Case 1: c < 1
                Let β = 1 + c/e.
                1. Generate U_{1} ~ U(0, 1) and set P = β U_{1} .
                   If P > 1, go to step 3; otherwise, go to step 2.
                2. Set Y = P^{1/c} and generate U_{2} ~ U(0, 1).
                   If U_{2} ≤ e^{−Y} , return X = Y ; otherwise, go back to step 1.
                3. Set Y = − ln [( β − P)/c] and generate U_{2} ~ U(0, 1).
                   If U_{2} ≤ Y^{c−1}, return X = Y ; otherwise, go back to step 1.

              - Source Code:
                #+BEGIN_SRC C++
                  #include <math.h>
                  double gamma( double a, double b, double c )
                  {
                    assert( b > 0. && c > 0. );
                    const double A = 1. / sqrt( 2. * c - 1. );
                    const double B = c - log( 4. );
                    const double Q = c + 1. / A;
                    const double T = 4.5;
                    const double D = 1. + log( T );
                    const double C = 1. + c / M_E;
                    if ( c < 1. ) {
                      while ( true ) {
                        double p = C * uniform( 0., 1. );
                        if ( p > 1. ) {
                          double y = -log( ( C - p ) / c );
                          if ( uniform( 0., 1. ) <= pow( y, c - 1. ) ) return a + b * y;
                        }
                        else {
                          double y = pow( p, 1. / c );
                          if ( uniform( 0., 1. ) <= exp( -y ) ) return a + b * y;
                        }
                      }
                    }
                    else if ( c == 1.0 ) return exponential( a, b );
                    else {
                      while ( true ) {
                        double p1 = uniform( 0., 1. );
                        double p2 = uniform( 0., 1. );
                        double v = A * log( p1 / ( 1. - p1 ) );
                        double y = c * exp( v );
                        double z = p1 * p1 * p2;
                        double w = B + Q * v - y;
                        if ( w + D - T * z >= 0. || w >= log( z ) ) return a + b * y;
                      }
                    }
                  }
                #+END_SRC

          15. [@15] Lognormal
              - Distribution Function:

                F(x) = 1/2 [1+erf[(ln(x-a)-\mu)/(\sqrt{2} \cdot \sigma)]]	for x > a
                       0 					else
              - Input:

                Location parameter a, any real number, merely shifts
                the origin; shape parameter σ > 0; scale parameter μ
                is any real number

              - Mean:
                a + e^(\mu + \sigma^2/2)

              - Variance:
                e^(2*mu+sig^2) * (e^(sig^2)-1)

              - (1) Generate V ~ N ( μ , σ^2 )
                (2) Return X = a + e^{V}

              - Source Code:
                #+BEGIN_SRC C++
                  double lognormal( double a, double mu, double sigma )
                  {
                    return a + exp( normal( mu, sigma ) );
                  }
                #+END_SRC

          16. Normal (Gaussian)

              - Algorithm:

                1. Independently generate U 1 ~ U(−1, 1) and U 2 ~ U(−1, 1)

                2. Set U = U 1 2 + U 2 2 (note that the square root is
                   not necessary here)

                3. If U < 1, return X = μ + σ U_{1} √(−2 ln U/U);
                   otherwise, go back to step 1

              - Source Code:

                #+BEGIN_SRC c++
                  double normal( double mu, double sigma )
                  {
                    assert( sigma > 0. );
                    double p, p1, p2;
                    do {
                      p1 = uniform( -1., 1. );
                      p2 = uniform( -1., 1. );
                      p = p1 * p1 + p2 * p2;
                    } while ( p >= 1. );
                    return mu + sigma * p1 * sqrt( -2. * log( p ) / p );
                  }
                #+END_SRC

          27. [@27] Weibull

              - Algorithm:

                1. Generate U ~ U(0, 1)
                2. Return X = a + b(− ln U) 1/c

              - Source Code:

                #+BEGIN_SRC c++
                  double weibull( double a, double b, double c )
                  {
                    assert( b > 0. && c > 0. );
                    return a + b * pow( -log( uniform( 0., 1. ) ), 1. / c );
                  }
                #+END_SRC

              - Notes:
                1. When c = 1, this becomes the exponential
                   distribution with scale b.
                2. When c = 2 for general b, it becomes the Rayleigh
                   distribution.
       2. Discrete Distributions
          2. [@2] Binomial
             - Density Function:
               - (^{n}_{k})p^{k} (1-p)^{n-k} [for k \in {0, 1, ..., n}]
               - 0 [otherwise]
             - where the binomial coefficient  (^{n}_{i}) = n! / (i!(n-1)!)
             - Notes:
               1. For large n, the binomial can be approximated by
                  N(np,np), provided np > 5 and 0. 1 ≤ p ≤ 0. 9 — and
                  for all values of p when np > 25.
          3. Geometric
             - "The geometric distribution represents the probability
               of obtaining k failures before the first success in
               independent Bernoulli trials, where the probability of
               success in a single trial is p."
             - Algorithm:
               1. Generate U ~ U(0, 1)
               2. Return X = int ( ln U/ ln (1 − p) )
             - Source Code:
               #+BEGIN_SRC c++
                 int geometric( double p )
                 {
                   assert( 0. < p && p < 1. );
                   return int( log( uniform( 0., 1. ) ) / log( 1. - p ) );
                 }
               #+END_SRC
          5. [@5] Multinomial
             - The multinomial distribution is a generalization of the
               binomial so that instead of two possible outcomes,
               success or failure, there are now m disjoint events
               that can occur, with corresponding probability p_{i},
               where i ∈ 1, 2, ..., m, and where p_{1} + p_{2} + ... + p_{m} = 1.
*** ref [[file:master.bib::compgen][Saucier 2000: Computer Generation Statistical Distributions]]
** PART [[./comp.ai.neural-nets FAQ, Part 2 of 7: Learning.html][Sarle - comp.ai.neural-nets FAQ, Part 2 of 7: Learning]]
*** summary
    - Should I normalize/standardize/rescale the data?

      - *rescale*: add/subtract, then divide/multiply, example: Celsius
        to Fahrenheit

      - *normalize*: divide by norm of vector: make length 1

      - *standardize*: subtract location, divide by scale,
        f.ex. normal distribution subtract mean, divide by std \to
        standard normal

      - but used interchangeably

      - following: input instance as row, input variables as columns

      - Should I standardize the input variables (column vectors)?

        - how are they used?

          - if combined via distance function, then "standardizing
            inputs can be crucial"

          - if combined linearly, can speed up training, "and reduce
            the chances of getting stuck in local optima"

          - if random values as starters (NN-specific), should be
            scale with input scale
*** quotes
    - Should I normalize/standardize/rescale the data?
      - Should I standardize the input variables (column vectors)?
        - In particular, scaling the inputs to [-1,1] will work better
          than [0,1], although any scaling that sets to zero the mean
          or median or other measure of central tendency is likely to
          be as good, and robust estimators of location and scale
          (Iglewicz, 1983) will be even better for input variables
          with extreme outliers.
*** ref [[file:master.bib::sarle2][Sarle 2002]]
** PART [[./python-doc-howto-sorting.pdf][van Rossum - Sorting HOW TO]]
*** summary
    1. Sorting Basics

       - =sorted(an_iterable)= returns sorted list, =list.sort()=
         sorts in-place, returns =None=

    2. Key Functions

       - accept any lambda, f.ex.
         #+BEGIN_SRC python
           >>> student_tuples = [('john', 'A', 15),('jane', 'B', 12),('dave', 'B', 10),]
           >>> sorted(student_tuples, key=lambda student: student[2])
           [('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)]
         #+END_SRC

    3. Operator Module Functions
*** quotes
** PART [[~/da/git/docs/python-doc-reference.pdf][van Rossum - The Python Language Reference]]
*** summary
    1. INTRODUCTION

       not a tutorial, some mention of standard Python implementation,
       see library reference for classes

       1. Alternate Implementations

          - cpython as default and most used

          - jython (java), ironpython (.net), python for .net language specific

          - pypy written in python for experimenting

       2. Notation

    3. [@3] DATA MODEL

       1. Objects, values and types

          - everything is an object

            - has a unique =id()=, compared via =is=

            - has a =type= (the class)

          - mutable and immutable

          - close your files (via =with=) or =try...finally=

       2. The standard type hierarchy

          - None, NotImplemented, Ellipsis

            - false, true, true

          - Numbers: Integral, Real, Complex

          - Sequences:

            - Immutable: String, Unicode, Tuple

            - Mutable: List, bytearray

          - Set types: =set=, =frozenset= (latter is hashable)

          - Mappings: Dictionaries

          - Callable types: can be =call=-ed

            - User-defined functions, created by function definition

              - function attributes on User-defined functions, set/get via
                dot-notation

            - User-defined methods
              <<user-defined methods>>

              - combines class, class instance and callable object
                (often function)

              - also extra (read-only) attributes

              - somehow transformed with that

            - Generator functions: use =yield=, =return= creates
              =StopIteration= exception

            - Built-in functions: wrapper for C

            - Built-in methods: also wrapper with first param =self= object

            - Class Types: "new-style" act as factories for further instances

            - Classic Classes: calls =__init__=

            - Class instances: callable if class has a =__call__= method

          - Modules: =import=-ed, have __name__, __doc__ and __file__
            (path) and __dict__ dictionary for module members

          - Classes:

            - created by class definitions

            - members most often accessed via =__dict__=

            - name resolution by c3 method (for multi inheritance
              paths) or depth-first search

            - also has =__name__=, =__module__=, =__bases__= (and =__doc__=)

          - Class instances:

            - "A class instance is created by calling a class object"

            - if =__setattr__= exists, it is called instead of
              updating directly

              - same for =__delattr__=

            - can pretend to be built-in classes by implementing stuff (see 2.4)

            - =__class__= gives access to class object

          - Files: open file, =sys.stdin= f.ex., created by =open()= etc.

          - Internal types:

            - Code objects: bytecode, params etc are in the function object

              - has a bunch of internal read-only attributes:
                =co_filename=, =co_name=, =co_code=, ...

            - Frame objects: like stack frames,

              - link to code objects =f_code=

              - and previous stack frames =f_back=

              - =f_trace= is called at the start of each line

            - Traceback objects

              - =sys.exc_traceback=,

                - also =sys.last_traceback= interactive

            - Slice objects: extended slice syntax =[start:stop:step]=

            - Static method objects

              - created by =staticmethod= constructor

              - belong to class, but less transformation

            - Class method objects

              - similar to static method, see above ([[user-defined methods]])

       3. New-style and classic classes

          - old vs new style

          - =type()= vs =__class__=

          - motivation: unified object model for classes

       4. Special method names

          extra functionality

          1. Basic customization

             - =__new__= create a new instance

               - often, super(currentclass, cls)..__new__ and then modify

             - =__init__=: between creation and returning the class instance

             - =__del__=: before instance is destroyed

               - =del= x does not call =__del__=, but only decrements
                 reference count. calling is done upon GC (refcount == 0)

               - ignores exceptions thrown during execution

             - =__repr__=: '"official" string representation"'

             - =__str__=: user-readable for print statement

             - =__lt__= le, gt, ge, eq, ne: comparison

             - =__cmp__= if above not defined, return number <0 if self<other

             - =__hash__= return integer if eq then hash equal

               - rules for use:

                 - if neither cmp nor eq, then not hash

                 - if cmp or eq, but not hash, then unusable in hashed
                   collections

                 - if mutable and cmp or eq, should not implement hash

             - =__nonzero__= truth value testing, if not implemented,
               __len__ is called

          2. Customizing attribute access

             - =__getattr__= called if member access failed

             - =__setattr__= set attribute, use self.__dict__[name] = value

               - new style, call base class
                 #+BEGIN_SRC python
                   object.__setattr__(self, name, value)
                 #+END_SRC

             - =__delattr__= for deletion, only if =del obj.name= makes sense

             - =__getattribute__= new-style: always called, call
               through to super
*** quotes
   5. [@5] EXPRESSIONS
      2. [@2] Atoms
       3. [@5] Displays for sets and dictionaries
         - For constructing a set or a dictionary Python provides
           special syntax called “displays”, each of them in two
           flavors:

           + either the container contents are listed explicitly, or
           + they are computed via a set of looping and filtering
             instructions, called a comprehension.

           Common syntax elements for comprehensions are:

           #+BEGIN_SRC python
           comprehension ::= expression comp_for
           comp_for      ::= “for” target_list “in” or_test [comp_iter]
           comp_iter     ::= comp_for | comp_if
           comp_if       ::= “if” expression_nocond [comp_iter]
           #+END_SRC

           [...] evaluating the expression to produce an element each
           time the innermost block is reached.

           Note that the comprehension is executed in a separate
           scope, so names assigned to in the target list don’t “leak”
           in the enclosing scope.
*** ref [[file:master.bib::python-lang-ref][Rossum & Python 2015: Python Language Reference]]
** PART [[./python-doc-library.pdf][van Rossum - The Python Library Reference]]
*** summary
    1. INTRODUCTION

       - also semantics of builtin data types and functions

       - then ordered from least to most important

    5. [@5] BUILT-IN TYPES

       6. [@6] Sequence Types — str, unicode, list, [...] buffer, xrange

          - use =string.strip()= to trim off f.ex. whitespace at the
            ends (see quotes)

    7. [@7] String Services

       1. Common string operations

          3. [@3] Format String Syntax

             - '{' [name|id|auto_increment] ['!' r|s (convert)] [':' format]'}'
               (see also quotes)

             - format:

               - set width: ={:8}= \to sets field width of 8

               - set type: ={:x}= \to sets type to hexadecimal
                 #+BEGIN_SRC python
                   "{:x}{:02x}".format(23, 10)
                 #+END_SRC
    8. DATA TYPES

       3. [@3] collections — High-performance container datatypes

          1. Counter objects

             - dict how often each element occurs in a iterable or
               mapping (f.ex. list)
    13. [@13] File Formats

        1. csv — CSV File Reading and Writing

           - read csv file via
             #+BEGIN_SRC python
               read = []
               with open('/tmp/names.csv') as csvfile:
                   reader = csv.reader(csvfile)
                   for row in reader:
                       read.append(row)
             #+END_SRC

           - write csv file via
             #+BEGIN_SRC python
               with open('/tmp/names.csv', 'w') as csvfile:
                   writer = csv.writer(csvfile)
                   for row in b[:8]:
                       writer.writerow(row)
             #+END_SRC

           - see quotes for writing of =dict=

    15. [@15] GENERIC OPERATING SYSTEM SERVICES

        1. os — Miscellaneous operating system interfaces

           1. Process Parameters

              current process and user info

              - os.environ

                changes this process's environment variables,
                f.ex. =environ[’HOME’]=

              - os.uname

                - get tuple (sysname, nodename, release, version, machine)

                - use =socket.gethostname()= for hostname to get a
                  more reliable version
    18. [@18] INTERNET DATA HANDLING
        12. [@12] base64 — RFC 3548: Base16, Base32, Base64 Data
            Encodings

            - encode and decode as in RFC 3548
              #+BEGIN_SRC python
                import base64, string
                base64.b32encode(string.printable)
              #+END_SRC
    20. [@20] Internet Protocols and Support
        4. [@4] wsgiref — WSGI Utilities and Reference Implementation
           - sample based on simplehttpserver

           - use as
             #+BEGIN_SRC python
               from wsgiref.simple_server import make_server
               httpd = make_server('', 8000, my_app) # (environ, start_response)
               httpd.serve_forever()
             #+END_SRC
    21. Multimedia Services
        7. [@7] colorsys — Conversions between color systems

           converts from one to the other, example
           #+BEGIN_SRC python
             >>> colorsys.hls_to_rgb(1,0.5,1)
             (0.0, 0.03999999999999959, 1.0)
             >>> colorsys.hls_to_rgb(0.33,0.5,1)
             (1.0, 0.0, 0.0)
             >>> colorsys.hls_to_rgb(0.66,0.5,1)
             (0.019999999999999796, 1.0, 0.0)
             >>> [hex(int(x * 255)) for x in colorsys.hls_to_rgb(0.66,0.5,1)]
             ['0x0', '0xa', '0xff']
           #+END_SRC
    25. [@25] DEVELOPMENT TOOLS
        1. pydoc — Documentation generator and online help system

           - generates on calls to =help=

           - recursively list docstrings

             - or close comment if no docstring

           - command-line: -w writes to HTML

             - -k does apropos-search

             - -p start doc web server on 1234

             - -g also starts tkinter web interface

        2. doctest — Test interactive Python examples

           0. [@0]

              - any interpreter command in documentation executed, compared via
                #+BEGIN_SRC python
                  if __name__ == "__main__":
                      import doctest
                      doctest.testmod()
                #+END_SRC

           1. Simple Usage: Checking Examples in Docstrings

              ... (TODO)

           2. Options

              - ellipsis (...) to match =.*=, call like
                #+BEGIN_SRC python
                  doctest.testmod(optionflags=doctest.ELLIPSIS)
                #+END_SRC

        3. unittest — Unit testing framework

           0. [@0]

              - test vocabulary

                - fixture: setup/teardown

                - TestCase: smallest unit

                - TestSuite: collection of cases and suites

                - TestRunner: runs tests

           1. Basic example

              - necessary parts
                #+BEGIN_SRC python
                  import unittest
                  class TestMyStuff(unittest.TestCase):
                      def test_sth(self):
                          self.assertEqual(someExpression, 'equalToThis')
                      def test_sth_else(self):
                          self.assertTrue(truthStatement)
                  if __name__ == '__main__':
                      unittest.main()
                #+END_SRC

              - more verbose output (and control which tests) via
                #+BEGIN_SRC python
                  suite = unittest.TestLoader().loadTestsFromTestCase(TestStringMethods)
                  unittest.TextTestRunner(verbosity=2).run(suite)
                #+END_SRC

           2. Command-Line Interface

              - example (module, class, single test)
                #+BEGIN_SRC sh
                  python -m unittest test_module1 test_module2
                  python -m unittest test_module.TestClass
                  python -m unittest test_module.TestClass.test_method
                #+END_SRC

              - also: -c catches C-c terminates after current test

           3. Test Discovery

              - via python -m unittest discover

           4. Organizing test code

              - simplest class: override =runTest=

              - =setUp= loaded for each instantiation

                - similar, =tearDown=

              - =test_*=-methods get executed

           5. Re-using old test code

              - use =FunctionTestCase=

                - AssertionErrors signal test failures

                  - but not guaranteed to

              - also =DocTestSuite= available

           6. Skipping tests and expected failures

    26. Debugging and Profiling

        2. [@2] pdb — The Python Debugger

           - built on top of bdb and cmd, extensible, see source code

           - most often used after exception with =pdb.pm()= (see quotes)

           - also =import pdb; pdb.set_trace()= (see quotes)
*** quotes
    2. [@2] BUILT-IN FUNCTIONS
       - staticmethod(function)

         Return a static method for function. A static method does not
         receive an implicit first argument. To declare a static
         method, use this idiom:
         #+BEGIN_SRC python
          class C(object):
              @staticmethod
              def f(arg1, arg2, ...):
                  ...
         #+END_SRC
    5. [@5] BUILT-IN TYPES
       4. [@4] Numeric Types — int, float, long, complex
          - =sys.maxint= is always set to the maximum plain integer
            value for the current platform, the minimum value is
            =-sys.maxint - 1=
       6. [@6] Sequence Types — str, unicode, list, [...] buffer, xrange
          1. String Methods
             - str.lstrip( [ chars ] )

               Return a copy of the string with leading characters
               removed. The chars argument is a string specifying the
               set of characters to be removed.
             - str.partition(sep)

               Split the string at the first occurrence of sep, and
               return a 3-tuple containing the part before the
               separator, the separator itself, and the part after the
               separator. If the separator is not found, return a
               3-tuple containing the string itself, followed by two
               empty strings.
             - str.strip( [ chars ] )

               Return a copy of the string with the leading and
               trailing characters removed. The chars argument is a
               string specifying the set of characters to be
               removed. If omitted or =None=, the chars argument
               defaults to removing whitespace.
       7. Set Types — set, frozenset
          - the non-operator versions of =union()=, =intersection()=,
            =difference()=, and =symmetric_difference()=,
            =issubset()=, and =issuperset()= methods will accept any
            iterable as an argument. In contrast, their operator based
            counterparts require their arguments to be sets.
          - update(other, ...)
            set |= other | ...

            Update the set, adding elements from all others.
       9. [@9] File Objects
          - file.name

            If the file object was created using =open()=, the name of
            the file. Otherwise, some string that indicates the source
            of the file object, of the form =<...>=.
    6. BUILT-IN EXCEPTIONS
       - exception IOError

         Raised when an I/O operation (such as a print statement, the
         built-in open() function or a method of a file object) fails
         for an I/O-related reason, e.g., “file not found” or “disk
         full”.
       - exception OSError

         This exception is derived from =EnvironmentError=. It is
         raised when a function returns a system-related error (not
         for illegal argument types or other incidental errors). The
         =errno= attribute is a numeric error code from =errno=, and
         the =strerror= attribute is the corresponding string, as
         would be printed by the C function =perror()=. See the module
         =errno=, which contains names for the error codes defined by
         the underlying operating system.
    7. String Services
       1. Common string operations
          3. [@3] Format String Syntax
             - “{” [field_name] [”!” conversion] [”:” format_spec] “}”
             - [[fill]align][sign][#][0][width][,][.precision][type]
    8. Data Types
       3. [@3] collections — High-performance container datatypes
          4. [@4] namedtuple() Factory Function for Tuples with Named Fields
             - Named tuples are especially useful for assigning field
               names to result tuples returned by the =csv= or
               =sqlite3= modules:
               #+BEGIN_SRC python
                 EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')
                 import csv
                 for emp in map(EmployeeRecord._make, csv.reader(open("employees.csv", "rb"))):
                     print emp.name, emp.title
               #+END_SRC
    9. [@9] Numeric and Mathematical Modules
       7. [@7] itertools — Functions creating iterators for efficient looping
          - itertools.chain(*iterables)

            Make an iterator that returns elements from the first
            iterable until it is exhausted, then proceeds to the next
            iterable, until all of the iterables are exhausted. Used
            for treating consecutive sequences as a single sequence.
          - itertools.islice(iterable, stop)
            itertools.islice(iterable, start, stop [ , step ] )

            Make an iterator that returns selected elements from the
            iterable.
       8. functools — Higher-order functions and operations on callable objects
          - [example how to use total_ordering, also of =__eq__= and =__lt__= ]
            #+BEGIN_SRC python
              @total_ordering
              class Student:
                  def __eq__(self, other):
                      return ((self.lastname.lower(), self.firstname.lower()) ==
                              (other.lastname.lower(), other.firstname.lower()))
                  def __lt__(self, other):
                      return ((self.lastname.lower(), self.firstname.lower()) <
                              (other.lastname.lower(), other.firstname.lower()))
            #+END_SRC
    10. FILE AND DIRECTORY ACCESS
        1. os.path — Common pathname manipulations
           - os.path.basename(path)

             Return the base name of pathname path. 2nd part of
             os.path.split().
        6. [@6] tempfile — Generate temporary files and directories
           - tempfile.mkdtemp( [ suffix=’‘ [ , prefix=’tmp’ [ , dir=None ]]] )

             Creates a temporary directory in the most secure manner possible.
           - tempfile.tempdir

             When set to a value other than None, this variable
             defines the default value for the dir argument to all the
             functions defined in this module.
        7. [@7] glob — Unix style pathname pattern expansion
           - glob.glob(pathname)

             Return a possibly-empty list of path names that match
             pathname, which must be a string containing a path
             specification.
           - glob.iglob(pathname)

             Return an iterator which yields the same values as glob()
             without actually storing them all simultaneously.
        10. [@10] shutil — High-level file operations
            - shutil.rmtree(path [ , ignore_errors [ , onerror ]] )

              Delete an entire directory tree; path must point to a
              directory (but not a symbolic link to a directory).
    13. [@13] File Formats
        1. csv — CSV File Reading and Writing
           - class csv.DictWriter(csvfile, fieldnames, restval=’‘,
             extrasaction=’raise’, dialect=’excel’, *args, **kwds)

             Create an object which operates like a regular writer but
             maps dictionaries onto output rows. The fieldnames
             parameter is a sequence of keys that identify the order
             in which values in the dictionary passed to the
             =writerow()= method are written to the csvfile. [...]
             #+BEGIN_SRC python
               with open('names.csv', 'w') as csvfile:
                   fieldnames = ['first_name', 'last_name']
                   writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                   writer.writeheader()
                   writer.writerow({'first_name': 'Baked', 'last_name':'Beans'})
                   writer.writerow({'first_name': 'Lovely', 'last_name':'Spam'})
             #+END_SRC
    15. [@15] GENERIC OPERATING SYSTEM SERVICES
        1. os — Miscellaneous operating system interfaces
           1. Process Parameters
              - os.environ

                A mapping object representing the string
                environment. For example, environ[’HOME’] is the
                pathname of your home directory (on some platforms),
                and is equivalent to getenv("HOME") in C.

                This mapping is captured the first time the os module
                is imported, typically during Python startup as part
                of processing site.py.
              - os.geteuid()

                Return the current process’s effective user id.
           4. [@4] Files and Directories
              - os.mkdir(path [ , mode ] )

                Create a directory named path with numeric mode
                mode. The default mode is 0777 (octal). On some
                systems, mode is ignored. Where it is used, the
                current umask value is first masked out. If the
                directory already exists, =OSError= is raised.
              - os.remove(path)

                Remove (delete) the file path.
              - os.walk(top, topdown=True, onerror=None, followlinks=False)

                Generate the file names in a directory tree by walking
                the tree either top-down or bottom-up. For each
                directory in the tree rooted at directory top
                (including top itself), it yields a 3-tuple (dirpath,
                dirnames, filenames).  dirpath is a string, the path
                to the directory. dirnames is a list of the names of
                the subdirectories in dirpath (excluding ’.’ and
                ’..’). filenames is a list of the names of the
                non-directory files in dirpath. Note that the names in
                the listscontain no path components. To get a full
                path (which begins with top) to a file or directory in
                dirpath, do os.path.join(dirpath, name).
           6. [@6] Miscellaneous System Information
              - os.sep

                qThe character used by the operating system to
                separate pathname components. This is ’/’ for POSIX
                and ’\\’ for Windows.
              - os.pathsep

                The character conventionally used by the operating
                system to separate search path components (as in
                =PATH=), such as ’:’ for POSIX or ’;’ for Windows.
        3. [@3] time — Time access and conversions
           - time.strftime(format [ , t ] )

             Convert a tuple or struct_time representing a time as
             returned by gmtime() or localtime() to a string as
             specified by the format argument.

             [...] The following directives can be embedded in the
             format string.

             [...] Additional directives may be supported on certain
             platforms, but only the ones listed here have a meaning
             standardized by ANSI C. To see the full set of format
             codes supported on your platform, consult the strftime(3)
             documentation.
        7. [@7] logging — Logging facility for Python
           7. [@7] LogRecord attributes
              |          |              | <30>                           |
              | filename | %(filename)s | Filename portion of pathname.  |
              | funcname | %(funcName)s | Name of function containing the logging call. |
              | lineno   | %(lineno)d   | Source line number where the logging call was issued (if available). |
    18. [@18] Internet Data Handling
        2. [@2] json — JSON encoder and decoder
           - Pretty printing:
             #+BEGIN_SRC python
               import json
               print json.dumps({'4': 5, '6': 7}, sort_keys=True,
                                indent=4, separators=(',', ': '))
             #+END_SRC
           - Using json.tool from the shell to validate and pretty-print:
             #+BEGIN_SRC sh
               $ echo '{"json":"obj"}' | python -mjson.tool
               {
               "json": "obj"
               }
             #+END_SRC
    20. [@20] Internet Protocols and Support
        5. [@5] urllib — Open arbitrary resources by URL
           2. [@2] Utility functions
              - urllib.unquote(string)

                Replace %xx escapes by their single-character
                equivalent. Example: unquote(’/%7Econnolly/’) yields
                ’/~connolly/’.
        7. [@7] httplib — HTTP protocol client
           0. [@0] ...
              - class httplib.HTTPConnection(host [ , port [ , strict [ ,
                timeout [ , source_address ]]]] )

                An HTTPConnection instance represents one transaction
                with an HTTP server.
           1. HTTPConnection Objects
              - HTTPConnection.request(method, url [ , body [ ,
                headers]])

                This will send a request to the server using the HTTP
                request method /method/ and the selector url.

           2. HTTPResponse Objects
              - HTTPResponse.getheaders()

                Return a list of (header, value) tuples.
              - HTTPResponse.status

                Status code returned by server.
        16. [@16] urlparse — Parse URLs into components
            - Parse a URL into six components, returning a
              6-tuple. This corresponds to the general structure of a
              URL:
              scheme://netloc/path;parameters?query#fragment. Each
              tuple item is a string, possibly empty.
        17. SimpleHTTPServer — Simple HTTP request handler
            - The SimpleHTTPServer module can also be invoked directly
              using the -m switch of the interpreter with a port
              number argument. Similar to the previous example, this
              serves the files relative to the current directory.
              #+BEGIN_SRC sh
                python -m SimpleHTTPServer 8000
              #+END_SRC
    21. Multimedia Services
        7. [@7] colorsys — Conversions between color systems

           The colorsys module defines bidirectional conversions of
           color values between colors expressed in the RGB (Red Green
           Blue) color space used in computer monitors and three other
           coordinate systems: YIQ, HLS (Hue Lightness Saturation) and
           HSV (Hue Saturation Value).
    25. [@25] Development Tools
        2. [@2] doctest — Test interactive Python examples
           - doctest.ELLIPSIS

             When specified, an ellipsis marker (...) in the expected
             output can match any substring in the actual output. This
             includes substrings that span line boundaries, and empty
             substrings
        3. unittest — Unit testing framework
           - assertEqual(first, second, msg=None)

             Test that first and second are equal. If the values do
             not compare equal, the test will fail. In addition, if
             first and second are the exact same type and one of list,
             tuple, dict, set, frozenset or unicode or any type that a
             subclass registers with addTypeEqualityFunc() the
             type-specific equality function will be called in order
             to generate a more useful default error message
           - assertAlmostEqual(first, second, places=7, msg=None,
             delta=None)

             assertNotAlmostEqual(first, second, places=7,
             msg=None, delta=None)

             Test that first and second are approximately (or not
             approximately) equal by computing the difference,
             rounding to the given number of decimal places (default
             7., and comparing to zero.
    26. Debugging and Profiling
        2. [@2] pdb — The Python Debugger
           - pdb.run('mymodule.test()')
           - pdb.py can also be invoked as a script to debug other
             scripts. For example:
             #+BEGIN_SRC python
               python -m pdb myscript.py
             #+END_SRC
           - The typical usage to break into the debugger from a
             running program is to insert
             #+BEGIN_SRC python
               import pdb; pdb.set_trace()
             #+END_SRC
             at the location you want to break into the debugger.
           - pdb.post_mortem( [ traceback ] )

             Enter post-mortem debugging of the given traceback
             object. If no traceback is given, it uses the one of the
             exception that is currently being handled (an exception
             must be being handled if the default is to be used).
           - pdb.pm()

             Enter post-mortem debugging of the traceback found in
             sys.last_traceback.
    27. virtual environment

        A cooperatively isolated runtime environment that allows Python
        users and applications to install and upgrade Python
        distribution packages without interfering with the behaviour of
        other Python applications running on the same system.
    28. PYTHON RUNTIME SERVICES
        1. sys — System-specific parameters and functions
           - sys.exc_info()

             This function returns a tuple of three values that give
             information about the exception that is currently being
             handled.
             [...]
             the values returned are (=type=, =value=,
             =traceback=). Their meaning is: /type/ gets the exception
             type of the exception being handled (a class object);
             /value/ gets the exception parameter (its associated value
             or the second argument to raise, which is always a class
             instance if the exception type is a class object)
           - sys.stdin sys.stdout sys.stderr

             File objects corresponding to the interpreter’s standard
             input, output and error streams.
        12. [@12] gc — Garbage Collector interface
            - gc.collect([generation ])

              With no arguments, run a full collection.
*** ref [[file:master.bib::python-lib-ref][Rossum & Python 2015: Python Library Reference]]
** PART [[./scipy-ref-0.17.0.pdf][SciPy Reference Guide]]
*** summary
1. Tutorial

   13. [@13] Statistics (scipy.stats)

       1. Introduction: many, but not all features, see ref

       2. Random Variables

          0. [@0]

             - 89 continuous, 13 discrete

             - please add if you create one

          1. Getting Help

             - docstring =stats.norm.__doc__=

             - bounds:

               - lower: =norm.a=,

               - upper: =norm.b=

             - list all properties with =dir(norm)=, but some of
               those are private

               - better: list frozen distribution
                 #+BEGIN_SRC python
                   >>> rv = norm()
                   >>> dir(rv) # reformatted
                   [’__class__’, ’__delattr__’, ’__dict__’, ’__doc__’,
                    ’__getattribute__’, ’__hash__’, ’__init__’, ’__module__’, ’__new__’,
                    ’__reduce__’, ’__reduce_ex__’, ’__repr__’, ’__setattr__’, ’__str__’,
                    ’__weakref__’, ’args’, ’cdf’, ’dist’, ’entropy’, ’isf’, ’kwds’,
                    ’moment’, ’pdf’, ’pmf’, ’ppf’, ’rvs’, ’sf’, ’stats’]
                 #+END_SRC

               - obtain list of available distributions
                 #+BEGIN_SRC python
                   import warnings
                   warnings.simplefilter(’ignore’, DeprecationWarning)
                   dist_continu = [d for d in dir(stats) if
                                   isinstance(getattr(stats,d), stats.rv_continuous)]
                   dist_discrete = [d for d in dir(stats) if
                                    isinstance(getattr(stats,d), stats.rv_discrete)]
                   print ’number of continuous distributions:’, len(dist_continu)
                   print ’number of discrete distributions: ’, len(dist_discrete)
                 #+END_SRC

          2. Common Methods

             - methods: rvs, pdf, cdf,

               - sf: survival (1-cdf)

               - ppf: percent point (CDF^{-1}) = quantile

               - isf: inverse survival (sf^{-1})

               - stats: mean, variance, skew, kurtosis

               - moment

             - pass array of values:
               #+BEGIN_SRC python
                 norm.cdf([-1., 0, 1])
                 import numpy as np
                 norm.cdf(np.array([-1., 0, 1]))
               #+END_SRC

             - median via
               #+BEGIN_SRC python
                 norm.ppf(0.5)
               #+END_SRC

             - generate several via =size= parameter

             - set via =np.random.seed()= for reproducible results

               - or use =random_state= parameter per function

          3. Shifting and Scaling

             - set =loc= and =scale=

             - recommended to use as (explicit) keywords, not as arguments

             - freeze as seen below

          4. Shape Parameters

             - one more parameter

               - f.ex. for gamma distribution (a)

               - evaluate by
                 #+BEGIN_SRC python
                   from scipy.stats import gamma
                   gamma.numargs
                   gamma.shapes
                 #+END_SRC

               - use as
                 #+BEGIN_SRC python
                   gamma(1, scale=2.).stats(moments="mv")
                   gamma(a=1, scale=2.).stats(moments="mv")
                 #+END_SRC

          5. Freezing a Distribution

             fix parameters to not repeat them
             #+BEGIN_SRC python
               rv = gamma(1, scale=2.)
               rv.mean(), rv.std()
             #+END_SRC

          6. Broadcasting

             combine values (numpy possibility) works

          7. Specific Points for Discrete Distributions

             - pmf instead of pdf

             - no scale, but loc

             - ppf has other definition

          8. Fitting Distributions

             - rest of main methods

               - fit: estimates all

               - fit_loc_scale: shape given

               - nnlf

               - expect: "Calculate the expectation of a function
                 against the pdf or pmf"

          9. Performance Issues and Cautionary Remarks

             - some functions very slow (generic method)

          10. Remaining Issues

              - some bugs

              - fit uses maximum likelihood and needs good starting points

       3. Building Specific Distributions

          - Making a Continuous Distribution, i.e., Subclassing rv_continuous

5. [@5] REFERENCE

   27. [@27] Statistical functions (scipy.stats)

       - scipy.stats.rv_continuous

         - base class, cannot be used directly

         - override pdf or cdf to create new rv

         - freezing possible

       - scipy.stats.lognorm

         - parametrization via normal(mu, sigma) is implemented by
           setting =s=sigma= and =scale=math.exp(mu)=, s is first
           parameter anyways

       - scipy.stats.gamma

         - parameters

           - shape parameter: computer ... c, here shape=a

             - kappa == a

           - loc parameter: computer... a, here none (or loc)

           - computer... b, here ==1
*** quotes
    1. SciPy Tutorial
       13. [@13] Statistics (scipy.stats)
           - Freezing a Distribution

             Passing the loc and scale keywords time and again can
             become quite bothersome. The concept of freezing a RV is
             used to solve such problems.
             #+BEGIN_SRC python
              rv = gamma(1, scale=2.)
             #+END_SRC
             By using rv we no longer have to include the scale or the
             shape parameters anymore. Thus, distributions can be used
             in one of two ways, either by passing all distribution
             parameters to each method call (such as we did earlier) or
             by freezing the parameters for the instance of the
             distribution.
    5. [@5] Reference
       24. [@24] Spatial algorithms and data structures (scipy.spatial)
           - scipy.spatial.distance.euclidean(u, v)

             Computes the Euclidean distance between two 1-D arrays.

           - Y = pdist(X, ’braycurtis’)

             Computes the Bray-Curtis distance between the points. The
             Bray-Curtis distance between two points u and v is

             d(u, v) = ∑_{i} u_{i} − v_{i} / ∑_{i} u_{i} + v_{i}

       26. [@26] Special functions (scipy.special)
           - scipy.special.btdtr(a, b, x) = <ufunc ‘btdtr’>

             Cumulative beta distribution. Returns the area from zero
             to x under the beta density function:

             gamma(a+b)/(gamma(a)*gamma(b)))*integral(t**(a-1)
             (1-t)**(b-1), t=0..x)

       27. Statistical functions (scipy.stats)
           - scipy.stats.gamma
             - The probability density function for gamma is:
               gamma.pdf(x, a) = x**(a-1) * exp(-x) / gamma(a)

             - gamma has a shape parameter _a_ which needs to be set
               explicitly.

             - To shift and/or scale the distribution use the loc and
               scale parameters. Specifically, gamma.pdf(x, a, loc,
               scale) is identically equivalent to gamma.pdf(y, a) /
               scale with y = (x - loc) / scale
           - Poisson distribution
             #+BEGIN_SRC python
                     from scipy.stats import poisson
               import matplotlib.pyplot as plt
               fig, ax = plt.subplots(1, 1)
               # Calculate a few first moments:
               mu = 0.6
               mean, var, skew, kurt = poisson.stats(mu, moments=’mvsk’)
               # Display the probability mass function (pmf):
               x = np.arange(poisson.ppf(0.01, mu),
                             poisson.ppf(0.99, mu))
               ax.plot(x, poisson.pmf(x, mu), ’bo’, ms=8, label=’poisson pmf’)
               ax.vlines(x, 0, poisson.pmf(x, mu), colors=’b’, lw=5, alpha=0.5)

               plt.show()
             #+END_SRC
           - scipy.stats.mstats.gmean(a, axis=0, dtype=None)

             Compute the geometric mean along the specified
             axis. Returns the geometric average of the array
             elements. That is: n-th root of (x1 * x2 * ... * xn)
    6. whereever
       - help(scipy.stats.lognorm)
** PART [[file:Curse%20of%20dimensionality%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Curse of dimensionality - Wikipedia, the free encyclopedia.html]]
** PART [[./rijsbergen79_infor_retriev.pdf][van Rijsbergen - INFORMATION RETRIEVAL]]
*** quotes
    |X ∩ Y| / |X ∪ Y| Jaccard's coefficient
** PART [[./Truncated distribution - Wikipedia, the free encyclopedia.html]]
*** quotes
    1. Definition
       - CDF: \frac{\int_a^x g(t) dt}{F(b)-F(a)}
       - Mean: \frac{\int_a^b x g(x) dx}{F(b)-F(a)}
       - g(x) = f(x) for all a <x \leq b and g(x) = 0 everywhere else
       - we wish to know how X is distributed given a < X \leq b.

         f(x|a < X \leq b) = \frac{g(x)}{F(b)-F(a)} = Tr(x)
       - A truncated distribution where the top of the distribution has
         been removed is as follows:

         f(x|X \leq y) = \frac{g(x)}{F(y)}
*** ref [[file:master.bib::wiki:truncated][Wikipedia 2015: Truncated]]
** PART [[./Split or filter your PCAP files with SplitCap - NETRESEC Blog.htm]]
*** summary
    - fast splitter for files
    - split by
      - host
      - ip
      - ...
    - can also just filter out stuff (ports)
    - compare splitcap
      #+BEGIN_SRC sh
        SplitCap.exe -r dump.eth0.1059726000.pcap -ip 12.129.71.102 -s nosplit
      #+END_SRC
      to tshark:
      #+BEGIN_SRC sh
        tshark.exe -r dump.eth0.1059726000.pcap -R "ip.addr eq 12.129.71.102" -w defcon11_tshark_filtered.pcap
      #+END_SRC
*** ref: [[file:master.bib::splitcap][2011: Split PCAP SplitCap NETRESEC Blog]]
** PART [[./sally/Example1.html]]
*** summary
    - input files example1.cfg reuters.zip
    - sally -c example1.cfg reuters.zip reuters.libsvm
** PART [[./wireshark-debian.html][wireshark/trunk/debian/README.Debian]]
*** summary
    capture as root, analyze as user

    or add users to the wireshark-group dpkg-reconfigure ... (see below)
*** quotes
    - The installation method can be changed any time by running:
      dpkg-reconfigure wireshark-common
** PART [[./numpy-ref-1.10.1.pdf][NumPy Reference]]
*** summary
    1. Array objects

       4. [@4] Indexing

          field access, basic slicing, advanced indexing

          1. Basic Slicing and Indexing

             - python slicing to N dimensions

             - index is tuple, f.ex. [:,0]

             - ellipsis =...= expands to right number of =:= to fill
               dimension

             - ...
*** quotes
    1. Array objects
       4. [@4] Indexing
          1. Basic Slicing and Indexing
             - A common use case for this is filtering for desired element
               values. For example one may wish to select all entries from an
               array which are not NaN:
               #+BEGIN_SRC python
                 >>> x = np.array([[1., 2.], [np.nan, 3.], [np.nan, np.nan]])
                 >>> x[~np.isnan(x)]
                 array([ 1., 2., 3.])
               #+END_SRC
    3. [@3] Routines
       1. Array creation routines
          1. Ones and zeros
             - numpy.zeros(shape, dtype=float, order=’C’)

               Return a new array of given shape and type, filled with zeros.
          2. From existing data
             - numpy.array(object, dtype=None, copy=True, order=None,
               subok=False, ndmin=0)

               Create an array.
               - Parameters
                 - [...]
                 - copy : bool, optional

                   If true (default), then the object is
                   copied. Otherwise, a copy will only be made if
                   __array__ returns a copy, if obj is a nested
                   sequence, or if a copy is needed to satisfy any of
                   the other requirements (dtype, order, etc.).
          5. [@5] Numerical ranges
             - numpy.ma.arange( [ start ] , stop [ , step ] , dtype=None)

               Return evenly spaced values within a given interval.  [...]
               When using a non-integer step, such as 0.1, the results will
               often not be consistent. It is better to use linspace for these
               cases.
             - numpy.linspace(start, stop, num=50, endpoint=True,
               retstep=False, dtype=None)

               Return evenly spaced numbers over a specified interval.
       2. Array manipulation routines
          - numpy.ravel(a, order=’C’)

            Return a contiguous flattened array.
       10. [@10] Floating point error handling
           - class numpy.errstate(**kwargs)

             Context manager for floating-point error handling.
             #+BEGIN_SRC python
               >>> with np.errstate(divide=’warn’):
               ... np.arange(3) / 0.
             #+END_SRC
       16. [@16] asdf
           6. [@6] Text formatting options
              - numpy.set_printoptions(precision=None, threshold=None,
                edgeitems=None, linewidth=None, suppress=None,
                nanstr=None, infstr=None, formatter=None)

                Set printing options. These options determine the way
                floating point numbers, arrays and other NumPy objects
                are displayed.

                Parameters
                - threshold : int, optional

                  Total number of array elements which trigger
                  summarization rather than full repr (default 1000).
       18. [@18] Logic functions
           5. [@5] Comparison
              - numpy.allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)

                Returns True if two arrays are element-wise equal
                within a tolerance.
       20. [@20] Mathematical functions
           - numpy.interp(x, xp, fp, left=None, right=None, period=None)

             One-dimensional linear interpolation. Returns the
             one-dimensional piecewise linear interpolant to a function
             with given values at discrete data-points.

             Parameters
             - x : array_like

               The x-coordinates of the interpolated values.
             - xp : 1-D sequence of floats

               The x-coordinates of the data points, must be increasing
               if argument period is not specified. Otherwise, xp is
               internally sorted after normalizing the periodic
               boundaries with xp = xp % period.
             - fp : 1-D sequence of floats

               The y-coordinates of the data points, same length as xp.
       25. [@25] Random sampling (numpy.random)
           3. [@3] Distributions
              - numpy.random.exponential(scale=1.0, size=None)

                Draw samples from an exponential distribution. Its
                probability density function is

                f (x; 1/\beta) = 1/\beta exp(- x/\beta)

                for x > 0 and 0 elsewhere. β is the scale parameter,
                which is the inverse of the rate parameter λ = 1/\beta.
              - numpy.random.gamma(shape, scale=1.0, size=None)

                Draw samples from a Gamma distribution. Samples are
                drawn from a Gamma distribution with specified
                parameters, shape (sometimes designated “k”) and scale
                (sometimes designated “theta”), where both parameters
                are > 0.

                Parameters

                shape : scalar > 0 The shape of the gamma distribution.

                scale : scalar > 0, optional The scale of the gamma
                distribution. Default is equal to 1.

                [...]

                Draw samples from the distribution:
                #+BEGIN_SRC python
                  shape, scale = 2., 2. # mean and dispersion
                  s = np.random.gamma(shape, scale, 1000)
                #+END_SRC
                Display the histogram of the samples, along with the probability
                density function:
                #+BEGIN_SRC python
                  import matplotlib.pyplot as plt
                  import scipy.special as sps
                  count, bins, ignored = plt.hist(s, 50, normed=True)
                  y = bins**(shape-1)*(np.exp(-bins/scale) /
                  (sps.gamma(shape)*scale**shape))
                  plt.plot(bins, y, linewidth=2, color=’r’)
                  plt.show()
                #+END_SRC
              - numpy.random.weibull(a, size=None) [...]

                Draw samples from a 1-parameter Weibull distribution
                with the given shape parameter a.

                X = (−ln(U ))^{1/a}

                Here, U is drawn from the uniform distribution over
                (0,1]. The more common 2-parameter Weibull, including
                a scale parameter λ is just X = λ(−ln(U ))^{1/a} .

                [...]

                The probability density for the Weibull distribution
                is

                p(x) = a/\lambda (x/\lambda)^{a−1} e^{−(x/λ)^a}

                where a is the shape and λ the scale.
** PART [[./Google JavaScript Style Guide.html][Google JavaScript Style Guide]]
*** summary
    1. JavaScript Language Rules

       1. var

          always use, else gets global

       2. Constants

          - NAMES_LIKE_THIS

          - const not supported in IE

       3. Semicolons

          Always use

       4. Nested functions

          feel free

       5. Function Declarations Within Blocks

          don't, instead use a variable initialized with a Function Expression

          #+BEGIN_SRC js
            if (x) {
              var foo = function() {};
            }
          #+END_SRC

       6. Exceptions

          Yes

       7. Custom exceptions

          Yes, f.ex. for exceptional return values

       8. Standards features

          Always prefer

       9. Wrapper objects for primitive types

          Do not do

          #+BEGIN_SRC js
            var x = new Boolean(false);
          #+END_SRC

          but for type casting it is fine

          #+BEGIN_SRC js
            var x = new Boolean(0);
          #+END_SRC

       10. Multi-level prototype hierarchies

           Avoid, instead use sth like =goog.inherits()=

       11. Method and property definitions

           #+BEGIN_SRC js
             /** @constructor */
             function Foo() {
               this.bar = value;
             }
             Foo.prototype.bar = function() {
               /* ... */
             };
           #+END_SRC

           optimized access

       12. delete

           use instead
           #+BEGIN_SRC js
             this.property_ = null;
           #+END_SRC

       13. Closures

           yes, but be aware of memory leaks

       14. eval

           No, use JSON.parse() instead

       15. with() {}

           no: unclear code

       16. this

           in object constructors, methods (and closures)

       17. for-in loop

           only for objects, not for arrays

       18. Associative Arrays

           never use Array ([]) for that. It is a property of Object

       19. Multiline string literals

           no, not in standard, use ' asdf ' + 'asdffe' instead

       20. Array and Object literals

           yes, [] clearer than Array()

       21. Modifying prototypes of builtin objects

           forbidden

       22. Internet Explorer's Conditional Comments

           no

    2. JavaScript Style Rules

       1. Naming

          0. [@0] "functionNamesLikeThis, variableNamesLikeThis,
             ClassNamesLikeThis, EnumNamesLikeThis, methodNamesLikeThis,
             CONSTANT_VALUES_LIKE_THIS, foo.namespaceNamesLikeThis.bar,
             and filenameslikethis.js"

          1. Properties and methods

             - private: suffix =_=

          2. Method and function parameter

             - optional methods: =opt_=-prefix

             - variable arguments indicated by parameter =var_args= do
               not use in body

          3. Getters and Setters

             discouraged

          4. Accessor functions

             - getFoo, setFoo, isFoo

          5. Namespaces

             can lead to problems. solution:

             1. Use namespaces for global code

                one global object

             2. Respect namespace ownership

                do not change someone else's, or tell them

             3. Use different namespaces for external code and internal code

                export explicitly into other project if you do that

                #+BEGIN_SRC js
                  foo.provide('googleyhats.BowlerHat');

                  foo.require('foo.hats');

                  /**
                   ,* @constructor
                   ,* @extends {foo.hats.RoundHat}
                   ,*/
                  googleyhats.BowlerHat = function() {
                    ...
                  };

                  goog.exportSymbol('foo.hats.BowlerHat', googleyhats.BowlerHat);
                #+END_SRC

             4. Alias long type names to improve readability

                aliases for long names, f.ex.

                #+BEGIN_SRC js
                  myapp.main = function() {
                    var MyClass = some.long.namespace.MyClass;
                    var staticHelper = some.long.namespace.MyClass.staticHelper;
                    staticHelper(new MyClass());
                  };
                #+END_SRC

                but not =MyClass.someProperty= unless Enum

             5. Filenames

                all lowercase (for some platforms ...), only =-= and =_=

       2. Custom toString() methods

          yes, but must always succeed without side effects

       3. Deferred initialization: fine, as direct init not always possible

       4. Explicit scope: always (??use parameters instead of global objects??)

       5. Code formatting: follow C++ guide in spirit, additionally:

          1. Curly Braces: same line as opening:
             #+BEGIN_SRC js
               if (something) {
                 // ...
               } else {
                 // ...
               }
             #+END_SRC

          2. Array and Object Initializers

             - single line ok if fits single line

             - multi-line with block intending (2 spaces), just like 1)

             - non-aligned for different lengths:
               #+BEGIN_SRC js
                 CORRECT_Object.prototype = {
                   a: 0,
                   b: 1,
                   lengthyName: 2
                 };
               #+END_SRC

          3. Function Arguments

             - best single line

             - else: reindent at parenthesis or 4 spaces (block or global)

          4. Passing Anonymous Functions

             indented 2 spaces from left edge of (statement or function keyword)

          5. Aliasing with goog.scope

       8. [@8] Visibility (private and protected fields)

          use @private in jsdoc

          ...

       10. [@10] Comments

           1. 

              use JSDoc-style comments

              - like javadoc

              - indent breaks by four spaces

              - CONTINUE ...

           4. [@4] Top/File-Level Comments: @fileoverview
              (description, uses, dependencies)

           5. Class Comments: =@constructor=, like 6), maybe =@extends=

           6. [@6] Method and Function Comments: parameters and return type

              - omit description if clear from params/return type

              - description: third person declarative:
                #+BEGIN_SRC js
                  /**
                   ,* Operates on an instance of MyClass and returns something.
                   ,* @param {project.MyClass} obj Instance of MyClass which leads to a long
                   ,*     comment that needs to be wrapped to two lines.
                   ,* @return {boolean} Whether something occurred.
                   ,*/
                  function PR_someMethod(obj) {
                    // ...
                  }
                #+END_SRC

           8. [@8] JSDoc Tag Reference

              1. @author of file or owner of test, included in @fileoverview

                 #+BEGIN_SRC js
                   @author username@google.com (first last)
                 #+END_SRC

              2. [@2] @code: {@code sth formatted as code, maybe a function}

              3. @fileoverview: "Makes the comment block provide file
                 level information."
** PART [[./Google%20Python%20Style%20Guide.html][Google Python Style Guide]]
*** summary
    1. Python Language Rules
       1. Lint

          "Run pylint over your code."

          disable warnings by-case using symbolic names

       2. Imports

          "Use imports for packages and modules only."

          - "Use import x for importing packages and modules."

          - "Use from x import y where x is the package prefix and y
            is the module name with no prefix."

          - "Use from x import y as z if two modules named y are to be
            imported or if y is an inconveniently long name."

       5. [@7] List Comprehensions

          use for simple cases: each part (x, for, if) fits on max one line
*** quotes
    1. Avoid using the + and += operators to accumulate a string within
      a loop. Since strings are immutable, this creates unnecessary
      temporary objects and results in quadratic rather than linear
      running time. Instead, add each substring to a list and ''.join
      the list after the loop terminates (or, write each substring to
      a =io.BytesIO= buffer).
    2. Python Style Rules
       1. [@16] Naming
          - module_name, package_name, ClassName, method_name,
            ExceptionName, function_name, GLOBAL_CONSTANT_NAME,
            global_var_name, instance_var_name,
            function_parameter_name, local_var_name
** PART [[./Quantile%20function%20-%20Wikipedia,%20the%20free%20encyclopedia.html][Quantile function]]
*** summary
    1. Definition

       - strictly monotonic continuos function f: quantile Q_f is the
         "threshold value x below which random draws from [f] would
         fall p percent of the time."

         - Q(q) = inf{x \in R: p \le F(x)}

         - if F continuous: Q = F^{-1}

    2. Simple example
*** quotes
    0. [@0]
       - It is also called the percent point function or inverse
         cumulative distribution function.
*** ref: [[file:master.bib::wiki:quantile][Wikipedia 2016: Quantile]]
** PART [[./diveintopython.pdf][Dive Into Python]]
*** summary
    8. [@8] HTML Processing

       1. Diving in

    9. XML Processing

       5. [@5] Searching for elements

          accessing elements by tag name
          #+BEGIN_SRC python
            from xml.dom import minidom
            xmldoc = minidom.parse('binary.xml')
            reflist = xmldoc.getElementsByTagName('ref')
          #+END_SRC

          searches recursively through the whole document,

          returns list of /Element/ s
** PART [[./doc_meek – Tor Bug Tracker & Wiki.choose_pluggable_transport.html][doc/meek – Tor Bug Tracker & Wiki]]
*** summary
    0. [@0]

       - pluggable transport: hides traffic via http-domain fronting

    7. [@7] How to run a meek-server (bridge)

       - compile

       - update torrc (see quotes)

       - can test on client via (quotes)
*** quotes
    7. [@7] How to run a meek-server (bridge)
       - sample on
https://gitweb.torproject.org/pluggable-transports/meek.git/tree/meek-server/torrc
       - [multiple ports] use something like this:
         #+BEGIN_EXAMPLE
           ServerTransportPlugin meek exec /usr/local/bin/meek-server --port 7002 --disable-tls --log /var/log/tor/meek-server.log
         #+END_EXAMPLE
       - [test client]
         #+BEGIN_SRC sh
           Bridge meek 0.0.2.0:3 url=http://my-bridge.example.com:7002/
         #+END_SRC
*** ref [[file:master.bib::tor-meek][Tor 2016: Tor Bug Tracker Wiki]]
** PART [[shell:man tshark]]
*** quotes
    - -e  <field>

      Add a field to the list of fields to display if -T fields is
      selected.
    - -R  <Read filter>

      Cause the specified filter (which uses the syntax of
      read/display filters, rather than that of capture filters) to be
      applied during the first pass of analysis. Packets not matching
      the filter are not considered for future passes. Only makes
      sense with multiple passes, see -2. For regular filtering on
      single-pass dissect see -Y instead.
    - -T  fields|pdml|ps|psml|text

      Set the format of the output when viewing decoded packet data.
      The options are one of:

      fields The values of fields specified with the -e option, in a
      form specified by the -E option.  For example,

        -T fields -E separator=, -E quote=d

      would generate comma-separated values (CSV) output suitable for
      importing into your favorite spreadsheet program.
*** ref [[file:master.bib::tshark][Developers 2015: Linux Users Manual]]
** PART [[./@font-face.html][@font-face]]
*** summary
    use fonts from local or remote sources in document
*** quotes
    #+BEGIN_SRC css
      @font-face {
        font-family: MyHelvetica;
        src: local("Helvetica Neue Bold"),
        local("HelveticaNeue-Bold"),
        url(MgOpenModernaBold.ttf);
        font-weight: bold;
      }
    #+END_SRC
*** ref [[file:master.bib::moz-fontface][fscholz 2016]]
** PART [[~/chive/GG/it/js/Ecma-262.pdf][ECMAScript Language Specification]]
*** summary
    19. [@19] The Strict Mode of ECMAScript
        - =arguments= unassignable, callee and caller not accessible
        - reserved tokens, f.ex. =let=, =yield=, ...
        - =with=, octal numbers not allowed
        - assignment to undeclared, unresolvable or unwritable forbidden,
        - =eval= unassignable
        - =this= not type-coerced
        - =delete= strongly restricted
*** quotes
    13. [@13] Function Definition
        1. Strict Mode Restrictions
           - It is a SyntaxError if any Identifier value occurs more
             than once within a FormalParameterList of a strict mode
             FunctionDeclaration or FunctionExpression.

             It is a SyntaxError if the Identifier "eval" or the
             Identifier "arguments" occurs within a
             FormalParameterList of a strict mode FunctionDeclaration
             or FunctionExpression.

             It is a SyntaxError if the Identifier "eval" or the
             Identifier "arguments" occurs as the Identifier of a
             strict mode FunctionDeclaration or FunctionExpression.
*** ref [[file:master.bib::ecma][International 2011: ECMAScript Language Specification]]
** PART [[./reftex.pdf][RefTEX User Manual]]
*** summary
*** quotes
    1. Introduction
       1. Installation
          1. [@3] Entering RefTEX Mode
             - To turn RefTEX Mode on and off in a particular buffer,
               use M-x reftex-mode RET. To turn on RefTEX Mode for all
               LaTeX files, add the following lines to your ‘.emacs’
               file: =(add-hook ’LaTeX-mode-hook ’turn-on-reftex)=
       2. [@2] RefTEX in a Nutshell
          3. [@3] Citations
             - Typing =C-c [= (=reftex-citation=) will let you specify
               a regular expression to search in current BibTEX
               database files (as specified in the \bibliography
               command)
*** TODO ref
** PART [[./Coefficient of determination - Wikipedia, the free encyclopedia.html]]
*** summary
    0. [@0] Summary

       - coefficient of determination

       - R^2 or r^2

       - can be negative in some cases

       - "proportion of the variance in the dependent variable that is
         predictable from the independent variable"
** PART [[./rfc2068.txt][RFC 2068 - Hypertext Transfer Protocol -- HTTP/1.1]]
*** summary
    10. [@10] Status Code Definitions

        2. [@2] Successful 2xx

           1. 200 OK

              success, returned information depends on request type

           3. [@3] 202 Accepted

              just message that request arrived, noncommittal (process started etc)

        3. Redirection 3xx

           further action necessary by user agent
    19. [@19] Appendices

        7. [@7] Compatibility with Previous Versions

           1. Compatibility with HTTP/1.0 Persistent Connections

              - faulty implementation in 1.0: hangs on proxy use
*** quotes
    1. [@5] Request
       5. Request-Line
          - The Request-Line begins with a method token, followed by
            the Request-URI and the protocol version, and ending with
            CRLF.
            2. [@2] Request-URI
               - An example Request-Line would be:
              #+BEGIN_EXAMPLE
               GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1
              #+END_EXAMPLE

*** ref [[file:master.bib::rfc2068][Fielding et al. 1997: Hypertext Transfer Protocl]]
** PART [[./rfc7230.pdf][RFC 7230 - Hypertext Transfer Protocol (HTTP/1.1): Message Syntax and Routing]]
*** summary
*** quotes
    6. [@6] Connection Management
       4. [@4] Concurrency
          - this specification does not mandate a particular maximum
            number of connections but, instead, encourages clients to
            be conservative when opening multiple connections.
*** ref [[file:master.bib::rfc7230][R. & J. 2014: Hypertext Transfer Protocol HTTP]]
** PART RFC 2616 - HTTP/1.1
*** TODO refactor
*** quotes
    - Authors of services which use the HTTP protocol SHOULD NOT use
      GET based forms for the submission of sensitive data
** PART The Clean Code Talks: Don't look for things
*** quotes
*** summary
    1. Cost of Construction
       - example: html client with url-constructor, gets from server
       - problem: hard to test: set up server for test
       - better, constructor gets htmlclient and url say: "we need one
         of these [htmlclient] in order to get our work done"
       - problem: still doing work in constructor and doc does not
         care about client
       - better: documentfactory(htmlclient) with build, return doc
         and document(String html)
         - eases testing of printer.print(Document)
** PART git-push manual page
*** quotes
    - git push origin master:refs/heads/experimental

      Create the branch experimental in the origin repository by
      copying the current master branch. This form is only needed to
      create a new branch or tag in the remote repository when the
      local name and the remote name are different; otherwise, the ref
      name on its own will work.
** PART git manual page
*** quotes
    - git-ls-tree(1)

      List the contents of a tree object.
** PART date manual page
*** summary
*** quotes
    - Convert seconds since the epoch (1970-01-01 UTC) to a date

      $ date --date='@2147483647'
** PART rsync manual
*** summary
*** quotes
    - rsync -avuzb --exclude '*~' samba:samba/ .
** PART [[./DOM2-Events.pdf][Document Object Model (DOM) Level 2 Events Specification]]
*** summary
    1. Document Object Model Events

       6. [@6] Event module definitions

          5. [@5] HTML event types

             - test conformance via =hasFeature("HTMLEvents", "2.0")=

             - contextual information

             - load: object (document, frameset, image, etc)
               completely loaded
*** quotes
    1. Document Object Model Events
       6. [@6] Event module definitions
          5. [@5] HTML event types
             - load

               The load event occurs when the DOM implementation
               finishes loading [...]
*** ref [[file:master.bib::dom2-events][W3C 2000: Document Object Model]]
** PART [[./201335142847631.pdf][Lohr - Sampling: Design and Analysis]]
** PART [[/usr/lib/python2.7/dist-packages/Gnuplot/demo.py][haggerty - demo.py -- Demonstrate the Gnuplot python module]]
   :PROPERTIES:
   :ORDERED:  t
   :END:
*** summary
*** quotes
    - A straightforward use of gnuplot.  The `debug=1' switch is used
      in these examples so that the commands that are sent to gnuplot
      are also output on stderr.
      #+BEGIN_SRC python
        g = Gnuplot.Gnuplot(debug=1)
        g.title('A simple example') # (optional)
        g('set data style linespoints') # give gnuplot an arbitrary command
        # Plot a list of (x, y) pairs (tuples or a numpy array would
        # also be OK):
        g.plot([[0,1.1], [1,5.8], [2,3.3], [3,4.2]])
      #+END_SRC
    - Save what we just plotted as a color postscript file.
      #+BEGIN_SRC python
        g.ylabel('x^2') # take advantage of enhanced postscript mode
        g.hardcopy('gp_test.ps', enhanced=1, color=1)
      #+END_SRC
** PART [[/usr/lib/python2.7/dist-packages/Gnuplot/test.py][haggerty - test.py -- Exercise the Gnuplot.py module]]
*** summary
    - set gnuplot's =with= parameter via =with_=
      #+BEGIN_SRC python
        g.plot(Gnuplot.Data(d, with_='lp 4 4'))
      #+END_SRC
*** quotes
** PART [[./single-page.html][HTML 5]]
*** summary
*** quotes
    4. [@4] The elements of HTML
       5. [@5] Text-level semantics
          12. [@12] The code element
              - The code element represents a fragment of computer
                code. This could be an XML element name, a file name,
                a computer program, or any other string that a
                computer would recognize.
          31. [@31] Usage summary
              - bdi: Text directionality isolation

                The recommended restaurant is <bdi lang="">My Juice
                Café (At The Beach)</bdi>.
              - wbr: Line breaking opportunity

                www.simply<wbr>orange<wbr>juice.com
*** ref [[file:master.bib::html5][Hickson et al. 2014: HTML5]]
** PART [[file:///usr/share/doc/python-matplotlib-doc/html/index.html][matplotlib]]
*** summary
    - [[file:///usr/share/doc/python-matplotlib-doc/html/api/pyplot_api.html][plot]]
      - with one data array, setting color from color_cycle, and transparency
        #+BEGIN_SRC python
          plt.plot(plot_list[0], color=(1.0, 0.5, 0.0), alpha=0.5)
          plt.show()
        #+END_SRC
*** quotes
** PART [[./pam2014-tor-nfattack.pdf][On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow Records]]
*** summary
    0. [@0] Abstract

       - using NetFlow records (Cisco)

       - active attack: change traffic at one end

       - high accuracy

    1. Introduction
*** quotes
*** ref [[file:master.bib::nfattackpam14][Chakravarty et al. 2014: Effectiveness Traffic Analysis]]
** PART [[info:elisp#Top][elisp]]
*** quotes
    3. [@3] [[info:elisp#Numbers][Numbers]]
       6. [@6] [[info:elisp#Arithmetic%20Operations][Arithmetic Operations]]
          - Function: * &rest numbers-or-markers

            This function multiplies its arguments together, and
            returns the product.
          - Function: / dividend divisor &rest divisors

            This function divides DIVIDEND by DIVISOR and returns the
            quotient.
       7. [[info:elisp#Rounding%20Operations][Rounding Operations]]
          - Function: fround float

            This function rounds FLOAT to the nearest integral value,
            and returns that value as a floating-point number.
** PART Grundgesetz für die Bundesrepublik Deutschland
*** quotes
*** summary
    10. [@10]
	1. Brief-, Post- und Fernmeldegeheimnis
	2. Beschränkung: wenn Staatsschutz, dann geheim
** PART libsvm READMEs
*** [[../sw/p/libsvm-3.20-src/README][./README]]
**** quotes
     2. [@2] Installation and Data Format
        - The format of training and testing data file is:

          <label> <index1>:<value1> <index2>:<value2> ...
     3. `svm-train' Usage
        - Usage: svm-train [options] training_set_file [model_file]
        - -t kernel_type : set type of kernel function (default 2)

	  2 -- radial basis function: exp(-gamma*|u-v|^2)
          4 -- precomputed kernel (kernel values in training_set_file)
        - -g gamma set gamma in kernel function (default 1/num_features)
        - -c cost

          set the parameter C of C-SVC, epsilon-SVR, and nu-SVR
          (default 1)
        - -m cachesize : set cache memory size in MB (default 100)
        - -v n: n-fold cross validation mode
        - -q : quiet mode (no outputs)
     4. `svm-predict' Usage
        - Usage: svm-predict [options] test_file model_file output_file
     5. `svm-scale' Usage
        - Usage: svm-scale [options] data_filename
          options:
          - -l lower : x scaling lower limit (default -1)
*** [[../sw/p/libsvm-3.20-src/tools/README][./tools/README]]
**** quotes
     2. [@2] Part II: Parameter Selection Tools
        1. Introduction
           - grid.py is a parameter selection tool for C-SVM
             classification using the RBF (radial basis function)
             kernel.
           - Usage: grid.py [grid_options] [svm_options] dataset
           - grid_options
             - -log2c {begin,end,step | "null"} : set the range of c
               (default -5,15,2)

               begin,end,step -- c_range = 2^{begin,...,begin+k*step,...,end}
               "null"         -- do not grid with c
             - -gnuplot {pathname | "null"} :

               pathname -- set gnuplot executable path and name
               "null"   -- do not plot
             - -resume [pathname] : resume the grid task using an
               existing output file (default pathname is dataset.out)
           - svm_options : additional options for svm-train
        2. Example
           #+BEGIN_SRC python
             python grid.py -log2c -5,5,1 -log2g -4,0,1 -v 5 -m 300 heart_scale
           #+END_SRC
** PART [[file:master.bib::time2-manual][time Linux Programmers Manual]]
*** quotes
    - When tloc is NULL, the call cannot fail.
** PART [[shell: unison -doc all][unison]]
*** summary
    1. Overview

       - variety of systems, user-level
*** quotes
    0. [@0] topics
       Documentation topics:
          about About Unison
         people People
          lists Mailing Lists and Bug Reporting
         status Development Status
        copying Copying
            ack Acknowledgements
        install Installation
       tutorial Tutorial
         basics Basic Concepts
       failures Invariants
        running Running Unison
            ssh Installing Ssh
           news Changes in Version 2.48.3

      Type "unison -doc <topic>" for detailed information about
       <topic> or "unison -doc all" for the whole manual
    2. [@2] Development Status
       - Reports of bugs affecting correctness or safety are of
         interest to many people and will generally get high priority.
       - Patches are even more welcome.
    3. [@x] A Basic Profile
       - ignore = Path */pilot/backup/Archive_*
    4. [@y] Path specification
       - Each pattern can have one of three forms. The most general
         form is a Posix extended regular expression introduced by the
         keyword Regex. (The collating sequences and character classes
         of full Posix regexps are not currently supported).

               Regex regexp

         For convenience, three other styles of pattern are also
         recognized:
       - Name name

         matches any path in which the last component matches name,
       - Path path

         matches exactly the path path,
       - BelowPath path

         matches the path path and any path below.
       - a * matches any sequence of characters not including / (and
         not beginning with ., when used at the beginning of a name)
** PART [[./2016-jmlr.pdf][Harry: A Tool for Measuring String Similarity]]
*** summary
    0. [@0] Abstract

       - similarity measures, fast, efficient

    1. Introduction

       - old problem

       - multi-threaded and distributed

       - complements Sally tool

    2. A Brief Overview of Harry

       compare strings, measure similarity

       1. Supported Similarity Measures

          see table 1

       2. Scalable Computation
*** quotes
*** ref available at http://www.mlsec.org/harry/docs/2016-jmlr.pdf
** PART [[file:~/chive/GG/it/emacs/GitHub%20-%20tbanel_orgtbljoin:%20Enrich%20an%20Org-table%20with%20a%20reference%20table.see_title.html][orgtbl-mode README.org]]
*** summary
    - Example

      - have one table with =#+tblname: nut= (or sth instead of nut)

      - select table to be expanded

      - M-x orbtbl-join

      - select reference table (here: =nut=) and common column
*** ref https://github.com/tbanel/orgtbljoin/blob/master/README.org
** PART [[./VASfwxyB.html][Ubuntu 16.04: Auto apt update and apt upgrade]]
*** quotes
    7. [@7] Disable auto update and upgrade
         #+BEGIN_SRC sh
           sudo systemctl disable apt-daily.service # disable run when system boot
           sudo systemctl disable apt-daily.timer   # disable timer run
         #+END_SRC
*** ref from http://www.hiroom2.com/2016/05/18/ubuntu-16-04-auto-apt-update-and-apt-upgrade/
** PART [[./org-plot/worg/org-tutorials/org-plot.html][Plotting tables in Org-Mode using org-plot]]
*** summary
    1. Introduction: install, options

    2. Getting Set up: gnuplot, gnuplot-mode, org-plot (with org-mode \ge 6.07)

    3. Examples

       1. 2d plots (lines and histograms)

          - key sequence C-M-g
*** quotes
*** ref http://orgmode.org/worg/org-tutorials/org-plot.html              :ML:
** onion routing project
*** [[~/da/git/docs/onion-routing_pet2000.pdf][Towards an Analysis of Onion Routing Security]]
**** ref: [[file:master.bib::onion-routing:pet2000][Syverson et al. 2000: Towards Analysis Onion Routing Security]]
*** [[~/da/git/docs/onion-routing_ih96.pdf][Hiding Routing Information]]
**** ref: [[file:master.bib::onion-routing:ih96][Goldschlag et al. 1996: Hiding Routing Information]]
** [[./1999-35.pdf][A Behavioral Model of Web Traffic]]
   old, original paper from 1999
*** summary
*** quotes
*** ref [[file:master.bib::DBLP:conf/icnp/ChoiL99][Choi & Limb 1999: Behavioral Model Web Traffic]]
** [[./sally/Example3.html]]
*** summary
    - sally -c example3.cfg jrc.zip jrc.mat
    - load jrc.mat
    - X = [fvec.data]
    - Y = [fvec.label]
    - [Y idx] = sort(Y);
    - K = X(:,idx)' * X(:,idx);
    - imagesc(K);
    - colormap(hot);
    - colorbar;
** [[./06vect.pdf][Scoring, term weighting and the vector space model]]
** [[./ch1.pdf][Data Mining]]
** [[./skl/working_with_text_data.html][Working With Text Data]]
** [[./Z5280B.pdf][Dirty Rotten Strategies - How We Trick Ourselves and Others into Solving the Wrong Problems Precisely]]
** [[./Cross-validation.leave-one-out_cross-validation.html]]
** [[./85331-CS.pdf][Improved Inapproximability Results for the Shortest Superstring and Related Problems]]
** SO: Sklearn kNN usage with a user defined metric
*** quotes
    - You pass a metric as metric param, and additional metric
      arguments as keyword paramethers to NN constructor:
      #+BEGIN_SRC python
        >>> def mydist(x, y):
        ...     return np.sum((x-y)**2)
        ...
        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

        >>> nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree',
        ...            metric='pyfunc', func=mydist)
      #+END_SRC
** [[./Gamma distribution - Wikipedia, the free encyclopedia.html]]
*** summary
    0. [@0] Abstract

       - two-parameter

       - parametrization

         - *shape* k, *scale* \theta

         - a, b with a == k and b == 1/\theta

         - k, \mu == k / b ==? k \cdot \theta

         - positive real numbers

       - if k is integer == erlang distribution

       - maximum entropy probability distribution

       - Mean: E[X] = k * \theta

       - Variance: Var[X] = k * \theta^2

    1. Characterization using shape k and scale θ

       - pdf: f(x;k,\theta) =  \frac{x^{k-1}e^{-\frac{x}{\theta}}}{\theta^k\Gamma(k)}

    2. Characterization using shape α and rate β

       - pdf: g(x;\alpha,\beta) = \frac{\beta^{\alpha} x^{\alpha-1} e^{-x\beta}}{\Gamma(\alpha)}
*** quotes
    - The common exponential distribution and chi-squared distribution
      are special cases of the gamma distribution.
    - Erlang distribution: the sum of k independent exponentially
      distributed random variables, each of which has a mean of θ
      (which is equivalent to a rate parameter of 1/θ).
    - (ends 0)
    - The skewness is equal to 2/\sqrt{k} , it *depends only on the
      shape parameter* (k) and approaches a normal distribution when k
      is large (approximately when k > 10).
    - (ends 3)
    - If X ~ Gamma(1, 1/λ) (shape -scale parametrization), then X has
      an exponential distribution with rate parameter λ.
    - (ends 6.4)
*** ref: [[file:master.bib::wiki:gamma][Wikipedia 2015: Gamma]]
** [[./howtoread.pdf][How to Read a Book, v5.0]]
** [[./mozsign/Add-ons_Extension Signing - MozillaWiki.html][Add-ons/Extension Signing]]
*** quotes
    - How do I get my add-ons signed if they are not hosted on
      addons.mozilla.org (AMO)?
      - You will need to create an AMO account and submit your
        add-on. There will be an option where you indicate the add-on
        won't be listed on AMO, and you'll be able to submit your
        add-on files without having them published on the site. Please
        read the Distribution Policy for more details.
      - You can also use the jpm sign command to generate a signed XPI
        that can be self-hosted.
      - There is an API you can use for signing.
** [[./Traffic generation model - Wikipedia, the free encyclopedia.html]]
** [[./dana_vol_11_2__pp_65_130.pdf][Predator foraging in patchy environments: the interrupted Poisson process (IPP) model unit]]
** [[./ide883.pdf][Superposition of Interrupted Poisson Processes and Its Application to Packetized Voice Multiplexers]]
** [[./all - jStat Documentation.html]]
** [[./generate_poisson.html]]
** [[./tcpdump - Wikipedia, the free encyclopedia.html]]
*** ref: [[file:master.bib::wiki:tcpdump][Wikipedia 2015: Tcpdump]]
** [[file:Marionette%20Python%20Tests%20-%20Mozilla%20|%20MDN.html][Marionette Python Tests]]
** [[file:Marionette%20-%20Mozilla%20|%20MDN.html::<!DOCTYPE%20html][Marionette (main page)]]
** [[./tcpdump.pdf]]
** [[shell:man npm-faq][man npm-faq]]
*** quotes
    - How do I install node with npm?
       You don't.  Try one of these node version managers:

       Unix:
       · http://github.com/isaacs/nave
       · http://github.com/visionmedia/n
       · http://github.com/creationix/nvm

       Windows:

       · http://github.com/marcelklehr/nodist
       · https://github.com/coreybutler/nvm-windows
       · https://github.com/hakobera/nvmw
       · https://github.com/nanjingboy/nvmw
** [[./reftex.pdf][RefTeX Manual]]
** [[./Wang_Tao.pdf][Wang - Website Fingerprinting: Attacks and Defenses]]
** [[./Huhta14-UCL-Msc.pdf][LINKING TOR CIRCUITS]]
** Wireshark-filter manual page
*** quotes
    - FILTER SYNTAX
      - Comparison operators
        Fields can also be compared against values.  The comparison operators
        can be expressed either through English-like abbreviations or through
        C-like symbols:

        eq, ==    Equal
        ne, !=    Not Equal
*** ref
** [[./Nicholas Piël » Benchmark of Python Web Servers.html]]
*** summary
*** quotes
*** ref [[file:master.bib::nicholas][Piël 2010: Nicholas Piël Benchmark]]
** Wireshark · Display Filter Reference: Transmission Control Protocol.html
*** summary
*** quotes
    | tcp.srcport | Source Port      | Unsigned integer, 2 bytes |
    | tcp.dstport | Destination Port | Unsigned integer, 2 bytes |
** [[./PEP 257 -- Docstring Conventions.html]]
** [[./leastsquares-pets12.pdf][Understanding Statistical Disclosure: A Least Squares approach]]
*** summary
*** quotes
*** ref [[file:master.bib::leastsquares-pets12][Perez-Gonzalez & Troncoso 2012: Understanding Statistical Disclosure]]
** [[./oakland05torta.pdf][Low-Cost Traffic Analysis of Tor]]
*** summary
*** quotes
*** ref [[file:master.bib::Murdoch05low-costtraffic][Murdoch & Danezis 2005: Low Cost Traffic]]
** [[./RP03-1.pdf][Practical Anonymity for the Masses with Mix-Networks]]
*** summary
*** quotes
*** ref [[file:master.bib::RP03-1][Rennhard & Plattner 2003: Practical Anonymity Masses Mix Networks]]
** [[./tarzan_ccs02.pdf][Freedman - Tarzan: A Peer-to-Peer Anonymizing Network Layer]]
*** summary
*** quotes
*** ref [[file:master.bib::tarzan:ccs02][Freedman & Morris 2002: Tarzan]]
** [[./WebAppSideChannel-final.pdf][Side-Channel Leaks in Web Applications: A Reality Today, a Challenge Tomorrow]]
*** summary
*** quotes
*** ref [[file:master.bib::DBLP:conf/sp/ChenWWZ10][Chen et al. 2010: Side Channel Leaks Web Applications]]
** [[/home/chive/IT-gg/python/PEP 8 - Style Guide for Python Code | Python.org.non-public++.html]]
*** summary
    1. [@3] Code lay-out

       1. Indentation

          - make it visually distinguishable, then quite a few
            different options (see quotes f.ex.)
*** quotes
    3. [@3] Code lay-out
       1. Indentation
          - [if example one style]
            #+BEGIN_SRC python
              if (this_is_one_thing and
                  that_is_another_thing):
                  do_something()
            #+END_SRC
** [[./Tor Project: manual.html]]
*** summary
*** quotes
    - Bridge [transport] IP:ORPort [fingerprint]

      When set along with UseBridges, instructs Tor to use the relay
      at "IP:ORPort" as a "bridge" relaying into the Tor network. [...]

      If "transport" is provided, and matches to a
      ClientTransportPlugin line, we use that pluggable transports
      proxy to transfer data to the bridge.
    - BridgeRelay 0|1

      Sets the relay to act as a "bridge" with respect to relaying
      connections from bridge users to the Tor network. It mainly
      causes Tor to publish a server descriptor to the bridge
      database, rather than to the public directory authorities.
    - ClientTransportPlugin transport socks4|socks5 IP:PORT
      ClientTransportPlugin transport exec path-to-binary [options]

      In its first form, when set along with a corresponding Bridge
      line, the Tor client forwards its traffic to a SOCKS-speaking
      proxy on "IP:PORT". It’s the duty of that proxy to properly
      forward the traffic to the bridge.

      In its second form, when set along with a corresponding Bridge
      line, the Tor client launches the pluggable transport proxy
      executable in path-to-binary using options as its command-line
      options, and forwards its traffic to it. It’s the duty of that
      proxy to properly forward the traffic to the bridge.
    - ORPort [address:]PORT|auto [flags]

      Advertise this port to listen for connections from Tor clients
      and servers. This option is required to be a Tor server. Set it
      to "auto" to have Tor pick a port for you. Set it to 0 to not
      run an ORPort at all. This option can occur more than
      once. (Default: 0)
    - ServerTransportPlugin transport exec path-to-binary [options]

      The Tor relay launches the pluggable transport proxy in
      path-to-binary using options as its command-line options, and
      expects to receive proxied client traffic from it.
    - SOCKSPort [address:]port|unix:path|auto [flags] [isolation flags]

      Open this port to listen for connections from SOCKS-speaking
      applications. Set this to 0 if you don’t want to allow
      application connections via SOCKS. Set it to "auto" to have Tor
      pick a port for you. This directive can be specified multiple
      times to bind to multiple addresses/ports. (Default: 9050)
    - UseBridges 0|1

      When set, Tor will fetch descriptors for each bridge listed in
      the "Bridge" config lines, and use these relays as both entry
      guards and directory guards. (Default: 0)
*** ref [[file:master.bib::tor-manual][Tor 2015: Linux Users Manual]]
** [[./How may I run multiple Tor relay instances in a single Linux machine.html]]
*** summary
    1. How may I run multiple Tor relay instances in a single Linux
       machine?

    2. by ...

       1. install tor

       2. create data dirs

       3. config files

       4. start

       5. firewall
*** quotes
*** ref http://tor.stackexchange.com/questions/327/how-may-i-run-multiple-tor-relay-instances-in-a-single-linux-machine
** [[./Analysis_of_Web_Data_Compression_and_its_Impact_on_Traffic_and_Energy_Consumption.pdf][Analysis of Web Data Compression and its Impact on Traffic and Energy Consumption]]
** [[./OverlierSyverson2006b.pdf][Locating Hidden Servers]]
** [[./arc97.pdf][ARCING CLASSIFIERS]]
** [[./DOM2-HTML.pdf][Document Object Model (DOM) Level 2 HTML Specification]]
*** summary
    1. Document Object Model HTML

       1. Introduction

          - only functionality specific to HTML, such as HTMLDocument
            and HTMLElement

          - assumes DTD: HTML 4.01 transitional or frameset or XHTML 1.0

       2. HTML Application of Core DOM
*** quotes
*** ref
** [[./ColorFAQ.pdf][Frequently Asked Questions about Color]]
*** summary
*** quotes
*** ref [[file:master.bib::color_faq][Poynton 1997: Frequently Asked Questions Color]]
** [[./danezis_pet2004.pdf][The Traffic Analysis of Continuous-Time Mixes]]
*** summary
*** quotes
*** ref [[file:master.bib::danezis:pet2004][Danezis 2004: Traffic Analysis Continuous Time Mixes]]
** [[./flow-correlation04.pdf][On Flow Correlation Attacks and Countermeasures in Mix Networks]]
*** summary
*** quotes
*** ref [[file:master.bib::flow-correlation04][Zhu et al. 2004: Flow Correlation Attacks]]
** [[./Hypertext Style: Cool URIs don't change..html][Berners-Lee - Cool URIs don't change]]
*** ref [[file:master.bib::cooluri][Berners-Lee 1998: Cool URIs]]
** [[./comp.ai.neural-nets FAQ, Part 3 of 7 GeneralizationSection What are cross-validation and bootstrapping.html][Sarle - comp.ai.neural-nets FAQ, Part 3 of 7: Generalization Section - What are cross-validation and bootstrapping?]]
** Beck - Extreme Programming planen
*** summary
    0. [@0] Vorwort

       - Krieg: Pendel zwischen Rüstung und Mobilität

       - IT: Pendel zwischen Prozess und Mobilität

       - xp: mehr mobilität

    0. [@0] Einleitung

       - Planung wichtig, Pläne nicht (Eisenhower)

       - jeden Tag ein neues Problem zu lösen

    1. Warum planen?

       0. [@0]

          - Pläne fokussieren, zeigen Richtung

          - reduzieren Panik

       1. Warum wir planen sollten

          - am wichtigsten arbeiten

          - zusammenarbeit koordinieren

          - Folgen von unerwarteten Vorfällen abschätzen

       2. Was wir zum Planen brauchen

          - Pläne helfen bei Abschätzung von benötigter Zeit,
            Koordination von Abläufen, erlauben Verlaufsplanung

          - müssen ehrlich sein, und alle aktuellen Informationen enthalten

       3. Die Planungsfalle

    6. [@6] Zu viel zu tun

       - statt "Ich habe nicht genug Zeit": "Ich habe zu viel zu tun."

       - Lösung

         - Prioritäten, manches nicht tun

         - Ausmaß mindern

         - jemand anderes bitten, Aufgaben zu übernehmen
*** quotes
    6. [@6] Zu viel zu tun
       - Zu viel zu erledigen zu haben, schafft Hoffnung.
       - wenigstens wissen wir, was wir tun müssen.
** Chang - LIBSVM: a library for support vector machines
*** ref [[file:master.bib::libsvm][Chang & Lin 2011: LIBSVM]]
** [[./2011-dmkd.pdf][Rieck - Similarity Measures for Sequential Data]]
*** quotes
*** ref available at http://www.mlsec.org/harry/docs/2011-dmkd.pdf
** [[./rfc7540.txt][Belshe - Hypertext Transfer Protocol Version 2 (HTTP/2)]]
*** ref [[file:master.bib::rfc7540][Belshe et al. 2015: Hypertext Transfer Protocol]]
** PART GIT-RESET Git Manual
*** quotes
    5. [@5] EXAMPLES
       - Undo a commit and redo

               $ git commit ...
               $ git reset --soft HEAD^      (1)
               $ edit                        (2)
               $ git commit -a -c ORIG_HEAD  (3)

           1. This is most often done when you remembered what you just
           committed is incomplete, or you misspelled your commit message, or
           both. Leaves working tree as it was before "reset".
           2. Make corrections to working tree files.
           3. "reset" copies the old head to .git/ORIG_HEAD; redo the commit
           by starting with its log message. If you do not need to edit the
           message further, you can give -C option instead.
** [[./taxonomy-dummy.pdf][Diaz - Taxonomy of Mixes and Dummy Traffic]]
