** [[./2015-asiaccs.pdf][Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]]
*** quotes
    - (a) web pages can be easily manipulated to load content from
      untrusted origins and (b) despite encryption, low-latency
      anonymization networks cannot effectively hide the size of
      request-response pairs.
    - A large body of work has studied passive attacks based on
      traffic analysis, most notably website fingerprinting and
      traffic confirmation attacks.
    - First, web pages can often be manipulated to load content from
      untrusted origins, for example, using advertisements or
      user-provided content.
    - deanonymize Tor users in a short period of time.
*** summary
    request packets per JS. they have sizes of 2k, 4k, 6k, 8k = 2bit
    information. Wait after page load, trigger js xmlhttprequest.

    0) [@0] 
       deanonymization via traffic injection and identification of
       request-response pairs
    1) user's browser does side-channel communication via
       user-submitted content or advertising, this is detected between
       entry node and onion proxy.
    2) background
       1) low-latency anonymization
       2) active attacker: remote markers via embed (ads) or local (at server)
       3) side-channel (see 3)
    3) 
       1) preprocessing:
          1) not ip, but TLS (removes *1 layer* of retransmissions)
	  2) empty out, merge adjacent, filter control (discard 512b-cells), merge again, sizes as multiples fo 2k
       2) side channel: messages: 2k, 4k, 6k, 8k bytes, 4 bits,
       3) transmission:
	  1) http via xmlhttprequest: with random parameter to avoid caching
	  2) use for web page markers, single easy, else 20byte sha1 (hammingdist)
       4) detection: sliding window (faster), svm, see [1]
    4) Evaluation: fixed set vs 60000 vs live
       1) Data Collection via Selenium with Firefox, if not loaded, discard
       2) extracting via Sally, learning via LibSVM, generate, record 50 transmissions, train
       3) closed-world: 
          reverse proxy, inject js, delay 30secs, 120secs on slow load
	  transmission takes 12-20 secs, compare to fingerprinting, 95%
       4) open-world: 91%, no false positives
       5) users: 31 of 34, no false positives
    5) limitations and defenses
       detect web page markers
       chaff
    6) related
       liberatore: look at traffic
       both ends: better
    7) conclusion
       owie, detect as first countermeasure
** [[./2014-torben.pdf][Torben: Deanonymizing Tor Communication using Web Page Markers]]
*** quotes
    - As a result, the few known cases of deanonymization of Tor have
      been reported to instead make use of advertisement networks or
      rely on vulnerabilities in browser implementations [29, 30] and
      are thus unrelated to insecurities of Tor in general.
    - As a consequence, there is an urgent need for defenses in
      anonymization services protecting users from active attacks at
      the application layer.
    - A web page[sic: r] marker is implanted using embedded or user-provided
      content, such as an advertisement or an image link. The marker
      induces a traffic pattern visible at the entry node, for
      example, using a chain of HTTP redirects or JavaScript code
      generating HTTP requests.
    - each circuit is only used for 10 minutes until a new circuit is
      created
    - For example, the Walsh-Hadamard code can be used to encode
      messages of length k ≤ 7 as code words of length 2 k with
      maximized minimum Hamming distances.
    - eight different sizes corresponding to the alphabet

      A = {− f(1, 1 ) , . . . , − f(0, 0 ) , f(0, 0 ) , . . . , f(1, 1 )} .
    - # ( x, p ) returns the number of occurrences of the positional
      n-gram p = ( s, i ) between the positions i and i + τ in the
      sequence x.
    - The vector φ ( x ) encodes information about the symbols, their
      order and their position in x—thus reflecting the basic
      properties of a web page marker.
    - This reliability rests on the design of the side channel that
      makes use of atypical request-response pairs for transmitting
      information (Section 3.2).
    - Whether such chaff traffic can be selectively injected to only
      destroy indicative traffic patterns is an interesting question
      for further research.
    - the attacker can expose the web pages a user visits within a
      couple of seconds.
*** summary
    0) [@0] Abstract: torben presented
    1) Introduction

       much research into passive attacks: high false positive,
       changing web content leads to problems, active attacks such as
       patch selection and watermarking

       instead: mostly browser vulnerabilities

       torben introduced (see also [[Torben: A Practical Side-Channel Attack for Deanonymizing Tor Communication]])
    2) Background
       1) The Tor Network: 

          bunch of routers, symmetric keys per hop in the circuit,
          onion encryption, each router only sees neighbors, each
          circuit only used for 10 minutes

       2) Attack Scenario:

	  Attacker can insert markers into web page of interest and
          analyze traffic between OP and guard.

    3) A Side-Channel Attack on Tor
       0) [@0]

	  - Tor ist low-latency
	  - request-response paare sichtbar im TLS Traffic (mit filtern)
	  - sollte der gegner die Website beeinflussen koennen
	    - via js oder http redirect (andere moeglichkeiten, css?)
	    - als direkte beeinflussung oder user content
	  - kann er versuchen, ueber diesen side-channel zu kodieren,
            welche websites besucht werden.
	  - Schritte
	    1) Preprocessing of network traces. (3.1)
	    2) Side channel design (3.2)
	    3) Transmission of web page markers (3.3).
	    4) Detection of web page markers. (3.4?)
       1) Vorverarbeitung:
	  merkmal: groesse der kontinuierlich in eine richtung uebermittelten daten
	  1) TCP statt IP analyse via tshark
	  2) Filtering and Merging TLS Records
	     a) filter non-tor-records (\le 100 bytes)
	     b) merge continuous to obtain amount of flow (packes sizes random)
	     c) filter control cells (512 bytes) and merge again (HTTP
                does not fit into 512 bytes)
	     d) normalize sizes, multitudes of 2000 bytes
       2) Side Channel Design
	  map two bits q = q_i, q_j to
          q_i, q_j \to ( q_i + 2q_j ) · s + c     (with s, c = 2000)
	  = q \cdot s + c
	  map four bits to request and response sizes, two bits each.
       3) Transmission

	  request: get with "random" parameter of fitting lenth
	  response: any page of acceptable size, *any host*

	  hash URLs to SHA-1 (optimal when fixed: walsh-hadamard code)
       4) Detection
	  1) 
	     - gelesene Sequenz gegeben (experiment: 100 symbols)
	     - A = {2,4,6,8}^2 (Torben-Alphabet, mit minus fuer request)
	       |A| = 16

	     - S=A^n alle n-gramme von A
	       |S| = 16^n

	       \to (eigenes) n = 40
	       == 2^160

	     - positional n-grams:

               P = S \times N, 
               mit Element p=(s,i) mit s \in S, i Position von s in Sequenz

	     - \varphi bildet von allen n-grammen A^* nach R^{|P|}.  
	       \varphi(x) \to (#(x,p))_{p \in P}

               Jedes n-gramm (s,i) hat seine Haeufigkeit zwischen i und
               i+\tau als wert

	     - \tau is toleranz-parameter

	  2) Probabilistic Classification

	     - SVM trainiert mit Sequenzen der Marker

	     - Riesiger Vektorraum, aber sparse

	     - P "nur" multiplikativ mit Fenstergroesse, nicht anders, puh

    4) Evaluation

       mehrere Experimente: cw, ow, users

       cw: unrealistisch, aber haeufig verwendet in website-fingerprinting, vergleich
       ow: 60000 webseiten

       1) Data

	  - Selenium WebDriver mit Tor bb

	  - wenn nicht load in 3 min, diese seite verwerfen

	  - remove similar, vergleich mit fingerprint (die failen)

       2) Detection
	  
	  - Sally verwandelt von Netzwerktraces in positional n-grams

	  - learning libsvm

	  - auf selbem rechner ausser Cai: cluster

	  - 100 marker, 50 uebertragungen jeweils gemessen

	  - n = 3,

	  - the tolerance to τ = 9 and

	  - the SVM regularization to C = 0.1

       3) Closed-World Evaluation
	  
	  - top 100 seiten je 50 mal

	  - jeweils im februar und april 2014

	  - js via reverse proxy

	  - marker nach 30 \to 120 sec delay

	  - transmission time 12-20 secs

	  - complete marker: 300 packets, \sim 390000 bytes

	  - vergleich mit Hermann.., Panchenko.. und Cai.. (mit
            Fingerprints vom Februar)

	  - torben imm 95%, die anderen schlechter

	  - false classification favors particular markers

       4) Open-World Evaluation

	  - 60000 von Alexa (top million \ top 100)

	  - few (as before, top 100) with markers
	    \to evaluate false positives

	  - detect 91% with no false positives

	  - reliable,
            due to atypical request-response-pairs

       5) Live Evaluation

	  - 4 users, 2 hours each

	  - if probability score below threshold of t=0.1, do not select

    5) Limitation and Defenses

       - torben works reliably

       - limitations?

       - detect web page markers: arms race: attackers change params,

       - chaff traffic: "might lower Tor’s overall performance."

    6) Related Work: first early, then active and passive vs low-latency

       1) Attacks on Encrypted Communication

	  http pattern of access detectible via tls

	  countermeasures fail to address size of data traffic

       2) Passive Attacks against Tor

	  - hermann: ip lengths

	  - panchenko: data sent before direction change,

	  - cai: ordering w/ displacements

	  - wang: tls

	  - high false-positives

	  - counter: morphing,

       3) Active Attacks against Tor

	  - passive: longer period

	  - solve: active attack

	  - 1: reveal communication path

	  - murdoch: similar, but path \to infeasible

	  - watermarking: inject specific patterns, inter-packet delays

	    - needs to control exit node, tcp level (not app)

    7) Conclusion
** [[./acmccs-wpes11-fingerprinting.pdf][Panchenko - Website Fingerprinting in Onion Routing Based Anonymization Networks]]
*** words
    - local eavesdropper

    - closed-world assumption:
      the victim retrieves only web pages from the predefined set and
      the attacker knows the set of possible web pages.

    - cross-validation:
      the data is split into n evenly large parts, the /folds/. Then,
      the entire process of training and testing is repeated n times,
      using one of the n folds as test data and the remaining n − 1
      folds as training data in turn. The results are averaged and
      therefore more solid and meaningful.

    - detection-rate: correct / all (in %)

    - true-positive: correct censored / all censored (in %)

    - false positive: uncensored as censored / all uncensored (in %)
*** summary

    0. [@0] Abstract: local website fingerprinting based on volume,
       time & direction

    1. INTRODUCTION:

       most attacks need some additionaly knowledge, f.ex. seeing both
       ends,

       - between OP and guard node easily observable

       - and gives 80% (73% open) against JAP and 55% against Tor

       - f.ex. ensure that censored /pages/ are not viewed

       - studies influence of supposed features,

       - propose camouflage

    2. RELATED WORKS

       - Hintz coined "website fingerprinting" in 2002 (paper)

       - 1996 Wagner/Schneier

       - 1998 Berkeley project about SSL traffic analysis

       - Bissias: IP packet sizes and inter-arrival times

       - Liberatore and Levine:
         - OpenSSH tunnels,
	 - Jaccard + naive Bayes classifier,
	 - consider only packet size of transmitted data,
	 - neglect timing and order information

       - Wright: morphing as countermeasure

       - Herrmann:
         - multinomial naive Bayes classifier
	 - OpenSSH, OpenVPN, Stunnel, Cisco IPsec-VPN, JAP, Tor
	 - 90%, 90%, 90%, 90%, 20%, 2.95%

    3. DATA SETS

       0) [@0]

	  - Firefox modification: No JS, Flash, Java, Cache

	  - Scripting Chickenfoot

	  - Data from Herrmann et al and Open-World-Dataset

       1) Closed-World Dataset

          - incoming size as positive, outgoing as negative

	  - only fully-loaded pages: users reload else \to load time to 600s

	  - "20 instances per website from our list of 775 sites"

       2) Open-World Dataset

	  - Alexa top 1000000,

	  - three censored: sexually explicit, top, random from alexa

       3) Countermeasure Dataset

	  - applied to closed-world (more difficult to camouflage ==
            easier to detect)

	  - if hampered, then also in open-world

	  - at the same time load a random website

       4)

	  - open: attacker has not seen the user's normal pattern

	  - separate dataset for tuning features and optimizing SVM

    4. A NEW APPROACH

       0) [@0] features, machine learning, compare to bayes, improve via SVM

       1) Features

	  facilitate subsequent classification, describes most relevant

	  Without Packets Sized 52: no ACKs

	  - *Size Markers* of uninterrupted flow (except ACK), grouped
            by 600 bytes

	  - *HTML Marker*: size of html document (first uninterrupted flow)

	  - *Total Transmitted Bytes*: grouped by 10000 bytes

	  - *Number Markers*: number of uninterruped packet flow in
            direction, grouped by 1,2,3-5,6-8,9-13,14

	  - *Occurring Packet Sizes*: grouped by 2, in/out

	  - *Percentage Incoming Packets* grouped by 5%

	  - *Number of Packets*, in/out grouped by 15

	  not working

	  - incoming/outgoing packets,

	  - leaving out frequent/rare sizes,

	  - including TLS/SSL record sizes,

	  - leaving empty TLS records,

	  - preserving packet order,

	  - rounding packet sizes,

	  - rounding packet frequencies

       2) Support Vector Classification

	  Uses SVM instead of Bayes classifier

	  SVMs try to separate the points via a hyperplane, maximizing
          the distance between the closes instances (= support
          vectors) and the plane

	  He uses a radial basis function kernel with parameters C=2^{17}
          and \gamma = 2^{-19}.

	  Finding these was the longest computation time.

    5. EXPERIMENTS AND RESULTS

       1. Experiments

	  closed-world: ten-fold stratified cross-validation

	  open-world: sufficient amount of data, not necessary

	  how:

	  - five censored, Sex Exp, Alexa Top, Alexa Random:
	    35 instances as training, 25 as test

	  - uncensored:
            4000 at random from top 1000000 for training,
            1000 for test, disjoint from training

	  - 20 times measured, each with new uncensored

       2. Results
	  0) [@0]
	     - Closed-World: recognition rates of 54.61% Tor, 80 % JAP

	     - Open-World: true positive rate of up to 73%

	  1) Results on Closed-World Dataset

	     - Final Result via checking if really loaded and removal
               of redirects

	     - JAP premium cascades worse in WF than free cascades

	  2) Results on Open-World Dataset
	     1) Experiment 1
		- 5 censored pages, 35 instances each, 4000 uncensored
		  pages, 1 each

		- test: 1000 which differ from the 4000

		- top ranked most easy to distinguish

	     2) Experiment 2

		- 5 censored, 20 training and 2 testing

		- uncensored variable: tp and fp both fall with more
                  examples of uncensored sites (measured up to 4000)

	     3) Experiment 3

		- censored from whole of alexa, varying number, 35
                  instances each

		- unsensored, 4000, 1 instance each

		- the more censored pages, the less clear the
                  classifier: fp rises, less impact on tp

	     4) Experiment 4

		- 5 censored, varying instance numbers

		- 4000 uncensored, 1 instance each

		- the more instances, the clearer, converges at about 35

	     5) Summary:

		- Your ISP could find out what you do online

    6. COUNTERMEASURES

       - padding works rather bad

       - camouflage: load randomly chosen web page simultaneously

       - used in both training and testing

       - to 3% where close to random would be optimal

    7. CONCLUSION AND FUTURE WORK

       Website Fingerprinting is possible in Tor and JAP, camouflage hampers.

       Next:

       detect: additional feature selection, active content, embedded
       links, analyse specific webpages,

       deter: browser plug-in, user feedback per page, parallel camouflage

*** quotes
    - We first define features for website fingerprinting solely based
      on volume, time, and direction of the traffic.

    - Finally, we show preliminary results of a proof-of-concept
      implementation that applies camouflage as a countermeasure to
      hamper the fingerprinting attack. For JAP, the detection rate
      decreases from 80% to 4% and for Tor it drops from 55% to about
      3%.

    - Several attacks against anonymization networks have been
      discovered, e.g., [6, 17, 19, 18], most notable the traffic
      confirmation attack.

    - Totalitarian regimes such as China or Iran usually do not have
      control over the communication party located in western
      countries precluding a traffic confirmation attack.

    - this attack is very realistic and anonymization networks must by
      all means be secure with respect to local attacks.

    - Instead of transforming websites, we obfuscate the page by
      loading another page in parallel.

    - In practice an attacker first retrieves a certain amount of
      relevant web pages by himself as training data for
      fingerprinting, using the anonymization network that he assumes
      his victim uses as well.

    - For each fetch of a URL, we store the sizes of packets in the
      observed order while recording incoming packets as positive,
      outgoing ones as negative numbers.

    - We once more achieve alarming detection rates motivating the
      need for additional countermeasures for anonymization networks

    - Successful countermeasures should decline the detection rates of
      all web pages to a level that is almost similar to random guess
      and at the same time cause only little performance losses.

    - We expect even better obfuscation for additional background
      pages as it will be more challenging for the attacker to extract
      the original statistics from the merged packets. Still, it has
      to be explored whether more sophisticated statistical measures
      can achieve this extraction.

*** ref
   #+BEGIN_SRC bibtex
     @inproceedings{panchenko,
       Author={Panchenko, Andriy and Niessen, Lukas and Zinnen, Andreas and Engel, Thomas},
       Booktitle={Proceedings of the 10th ACM Workshop on Privacy in the Electronic
     Society},
       Title={Website fingerprinting in onion routing based anonymization networks},
       Pages={103--114},
       Year={2011}
     }
   #+END_SRC
** [[https://blog.torproject.org/blog/experimental-defense-website-traffic-fingerprinting][Mike Perry - Experimental Defense for Website Traffic Fingerprinting]]
   - [[./experimental.html][local copy]]
*** summary
    - prior: belief that all was well (failed attempts)
    - panchenko: showed that not
    - disagree with background fetch: additional traffic
    - first attempt at mitigation: enable http pipelining and
      randomize pipeline size, request further research
    - other: http to spdy && ofbsp
*** quotes
    - Despite these early results, whenever researchers tried naively
      applying these techniques to Tor-like systems, they failed to
      come up with publishable results (meaning the attack did not
      work against Tor), due largely to the fixed 512 byte cell size,
      as well as the multiplexing of Tor client traffic over a single
      TLS connection.
    - We create it as a prototype, and request that future research
      papers do not treat the defense as if it were the final solution
      against website fingerprinting of Tor traffic.
    - However, the defense could also be improved. We did not attempt
      to determine the optimal pipeline size or distribution, and are
      relying on the research community to tweak these parameters as
      they evaluate the defense.
    - as these translations are potentially fragile as well as
      labor-intensive to implement and deploy, we are unlikely to take
      these measures without further feedback from and study by the
      research community.
** [[./article-2456.pdf][Juarez - A Critical Evaluation of Website Fingerprinting Attacks]]
*** summary
    0) [@0] ABSTRACT

       many WP papers do not use practical scenarios: browsing habits,
       location, version tbb,

    1) INTRODUCTION

       old studies did less about localization, tbb version and
       browsing habits, this addresses

       - evaluates these assumptions

       - what defeats the accuracy

       - how to reduce false positive rates

       - adversary's cost

    2) WEBSITE FINGERPRINTING

       find out which site or page is visited from network traffic only

       - first within single website

       - then within set of websites

       - then hintz's safeweb anonymizing web proxy ++

       - then Hermann: 3% success

       - Shi 50% for 20 pages, Panchenko 54% for Herrmann's dataset

       - cai et al, wang and goldberg: over 90%, *100 pages*

    3) MODEL

       passive local attack, targeted vs non-targeted

       1) Assumptions

	  listed by papers that explicitly mention assumptions

	  client-side, adversary, web assumptions

	  - client:

	    closed world: user may only visit certain pages, or only
            certain pages from a set are searched for

	    browsing behavior: users only have one tab open at a time,
            sequential browsing

	  - websites:

	    (?) all websites are built using templates

	    localized versions: but language of webpage is determined
            by exit node (really?)

	  - adversary:

	    page load parsing: page load start/stop are detectable

	    no background traffic: tor separable from other traffic

	    replicability: adversary can replicate user's setup (tbb
            version, OS, network connection)

    4) EVALUATION

       some assumptions distort the model

       1) Datasets

	  Alexa top sites and ALAD

       2) Data collection

	  - tbb with selenium

	  - dumpcap

	  - tor configured via stem

	  - circuit renewal to 600000 (? cf. wang/goldberg)

	  - disable UseEntryGuards

	  - batches: page 4 times, 5-10 batches of data per time

	  - 5 seconds before each crawl, 5 second pauses between each visit

	  - round-robin, hours apart

	  - two physical, three cloud-based virtual machines

	  - Linux Container based virtualization

	  - disabled OS updates (how about time, claws updates?)

	  - one crawler per machine at a time

	  - average CPU load low

       3) Methodology

	  - control crawl : default value

	  - test crawl: value of interest

	  - less controllable: time and tor-path-selection

	    - k-fold cross-validation and

	    - minimizing time gap control-to-test

	  - compared other papers

	  - chose the faster of the two by W[32]

	  - also own decision tree with panchenko "merkmale"

       4) Time

	  website fingerprints decay as time goes on: 50% after 9 days

       5) Multitab browsing

	  decays a lot, halved when only one of them counts as success

	  delays (0.5, 3, 5 sec) matter very little

       6) Tor Browser Bundle Versions

	  2.4.7 dissimilar to others

	  3.5 similar to 3.5.2.1

	  accuracy greatest for NumEntryGuards = 1, UseEntryGuards = 1

	  lowest for UseEntryGuards = 0, +2% for NumEntryGuards =3 (default)

       7) Network

	  differences in where the puter is matter greatly: backbone
          gives different pattern

       8) The importance of false positives

	  - Open-world: 

	    4 top sites vs 32710 other sites.

	  - The base rate fallacy

	    If there is a low chance that the user visits the
            fingerprinted websites, then the occurrence of false
            positives relative to true positives rises.

	  - User’s browsing habits

	    three random users from ALAD, 100 URLs each

	    tried to match everything, failed

    5) CLASSIFY-VERIFY

       probabilistic SVM

       with rejection if P_1 or P_1 - P_2 lower than threshold

       1) Evaluation and result

	  this combination greatly decreases the number of false
          positives

    6) MODELING THE ADVERSARY’S COST

       1) Data collection cost:

	  data D = n (training pages) \cdot m (versions) \cdot i (instances)

	  collection cost col(D) 

       2) Training cost:

	  train(D, F(=features), C(=classifier)) = D \cdot c

       3) Testing cost:

	  Test data T = v (=victims) \cdot p (=pages /victim /day)

	  test = col(T) + test(T, F, C)

       4) Updating cost:

	  update(D, F, C) / d(=days until change)

       5) Total cost:

	  init(D,F,C,T) = col(D) + train(D,F,C) + col(T) + test(T,F,C)

	  cost(D,F,C,T,d) = init(D,F,C,T) + update(D,F,C)/d

    7) CONCLUSION AND FUTURE WORK

       practical scenarios
*** quotes
    - The main objective of an adversary in a typical WF scenario is
      to identify which page the user is visiting.
    - Wang and Goldberg concluded that sites that change in size are
      hard to classify correctly
    - Over 50% sites were pages other than the front page
    - Classifiers designed for WF attacks are based on features
      extracted from the length, direction and inter-arrival times of
      network packets, such as unique number of packet lengths or the
      total bandwidth consumed.
    - In most cases, classifier W performed better than the others.
    - the accuracy drops extremely fast over time.
    - We observe a dramatic drop in the accuracy for all the
      classifiers with respect to the accuracy obtained with the
      control crawl
    - This might imply that the specific learning model is not as
      important for a successful attack as the feature selection.
    - The average page load for the test crawl for the 5 second gap
      experiment is 15 seconds, leaving on average 30% of the original
      trace uncovered by the background traffic. Even in this case,
      the accuracy with respect to the control crawl drops by over
      68%.
    - In practice, many TBB versions coexist, largely because of the
      lack of an auto-update functionality. (*new versions include updater*)
    - Even though we fix the entry guard for all circuits in a batch,
      since we remove the Tor data directory after each batch, we
      force the entry guard to change. On the other hand, allowing Tor
      to pick a different entry guard for each circuit results in a
      more balanced distribution because it is more likely that the
      same entry guards are being used in each single batch, thus
      there is lower variance across batches. We must clarify that
      these results are not concluding and there may be a different
      explanation for such difference in standard deviation.
    - the accuracy drop between the crawls training on Leuven and
      testing in one of the other two locations is relatively greater
      than the accuracy drop observed in the experiments between
      Singapore and New York. Since the VM in Leuven is located within
      a university network and the other two VMs in data centers
      belonging to the same company
    - One possible reason for low TPR is due to the effect of inner
      pages.
    - Bayesian detection rate [...] is defined as the probability that
      a traffic trace actually corresponds to a monitored webpage
      given that the classifier recognized it as monitored.[...]

      P (M | C) 
      = [P (C | M) P (M)] / [P (M) P (C | M) + P (¬M) P (C | ¬M)]
    - The results show that the BDR doubles when we use the
      Classify-Verify approach.
    - 10-fold cross-validation, where a threshold is determined by
      using 90% of the data and then tested on the remaining 10%.
    - train with different localized versions
    - When each of these assumptions are violated, the accuracy of the
      system drops significantly, and we have not examined in depth
      how the accuracy is impacted when multiple assumptions are
      violated.
    - it seems that the non-targeted attack is not feasible given the
      sophistication level of current attacks.
    - We believe that further research on evaluating the common
      assumptions of the WF literature is important for assessing the
      practicality and the efficacy of the WF attacks.

** [[./cacr2014-05.pdf][Wang - Effective Attacks and Provable Defenses for Website Fingerprinting]]
*** summary
    0) [@0] Abstract

       effective for seldomly visited pages

       85% tpr vs 0.6% fpr

    1) Introduction

       tor, ssh tunnels, vpn, ipsec are vulnerable to website
       fingerprinting

       contributions:

       - better attack

       - large open-world setting

       - best defense: supersequences over anonymity sets

    2) Basics

       1) Website Fingerprinting on Tor

	  two assumptions:

	  - clear start and end of trace

	  - no other activity

       2) Classification

	  kNN is multi-modal: different settings yield different
          traces for the same page

    3) Related Work

       HTTP/1.0 (resource lengths)
       
       \to HTTP/1.1, VPN, SSH-Tunnel (packet lengths)

       \to TOR (padded packet lengths)

       1) Resource length attacks

	  HTTP/1.0: each resource a separate tcp connection

       2) Unique packet length attacks

	  HTTP/1.1: combined in tcp connection, yet packets length
          distinguishable

       3) Hidden packet length attacks

	  extract features:

          - burst patterns

	  - main document size

	  - ratio incoming/outgoing

	  - total packet counts

	  use SVN

	  Dyer: less features, n-grams

	  Cai: edit distance of packet sequences, modified kernel of SVM

	  W&G: modified edit distance algo

       4) Defenses

	  simulatable vs non-simulatable

	  - simulatable: transform packet sequence, does not look at
            contents, cheaper

	  - non-simulatable: in-browser, access to client data

	  deterministic vs random

	  - deterministic: always outputs the same sequence on the
            same input

	  - random: can differ
            |-----------------+-------------------------+-----------------|
            |                 | random                  | deterministic   |
            |-----------------+-------------------------+-----------------|
            | simulatable     | morphing & panchenko    | padding & BuFLO |
            | non-simulatable | Tor's packet reordering | parts of HTTPOS |

    4) Attack

       k-NN, large feature set with weight adjustment

       1) k-NN classifier

	  points with classes, "lowest distance chosen"

          lots of features, weighted & learned distance

	  similar to SVM

       2) Feature set

	  diverse:

	  - general features:

            - *total size*

	    - *total time*

	    - *number of incoming and outgoing packets*

	  - *unique packet lengths*: 1 if a size occurs, 0 if not, for
            each size incoming and outgoing (useless on Tor (?),
            similar to Liberatore and Herrmann)

	  - *packet ordering*: number of packets before each, number
            of incoming between this and last outgoing packet (burst,
            see Cai)

	  - *Concentration of outgoing packets*: number of outgoing in
            30-packet-chunks (non-overlapping span)

	  - bursts: sequence with no two adjacent incoming packets,

	    - *maximum burst length*

	    - *mean burst length*

	    - *number of bursts*

	  - *initial lengths*: length of first 20 packets

	  pads with special character X for empty values, such that
          d(X, y) == d(y, X) := 0

       3) Weight initialization and adjustment

	  R rounds of learning

	  focus on one point P_{train} \in S_{train}, do two steps

	  1. weight recommendation

	     1. compute distances to all other P' \in S_{train}

	     2. take closest k_{reco} points in S_{train} as S_{good}
		and closest k_{reco} points in all other classes S' as S_{bad}

	     3. with d(P, S) := { d(P, s) | s \in S } define:

		- d_{maxgood_i} := max({d_{f_i}(P_{train}, P) | P \in S_{good} }) and compute:

		- n_{bad_i} := |{P' \in S_{bad} | d_{f_i}(P_{train}, P') \le d_{maxgood_i} }|

	          number of classes "closer" by feature than worst good candidate
	          the closer to k_{reco}, the worse

	  2. weight adjustment

	     1. for features worse than the best,
                reduce by \Delta w_{i} = w_{i} \cdot 0.01

	     2. other features, afterwards, increase equally such that
                d(P_{train}, S_{bad}) remains the same

	     3. both:

	        - \Delta w_i \cdot n_{bad_i}/k_{reco}

	        - multiply by overall badness 0.2 + N_bad/k_{reco} with
                  N_{bad} = |{P' \in S_{bad} | d(P_{train}, P') \le d_{maxgood} }|

	  3. best results for k_{reco} = 5

	  4. random vector between 0.5 and 1.5

    5) Attack evaluation

       better than all others

       1) Attack on Tor

	  - 90 instances each of 100 sensitive pages

	  - 1 instance each of 5000 non-monitored pages

	  - regular circuit resetting, no caches and time gaps between
            multiple loads of the same page

	  - weight adjustment: 6000 rounds, 100 pages, 60 instances
            (each instance once)

	  - only classified if all k neighbors agree, varying 1 \le k \le 15

	  - W&G has 10x higher FPR

	  - accuracy levels off after 800 rounds of weight adjustment

	  - 0.1 CPU seconds to test one instance,

       2) Training confidence

	  - FPR good for k=6, TPR good for k=2 (|C_{0}| = 500)

       3) Attack on Other Defenses

	  - evaluated defenses:

	    - traffic morphing,

	    - HTTPOS split,

	    - Panchenko decoy,

	    - BuFLO

	  - implemented as simulations.

    6) Defense

       Tamaraw++

       supersequences (provably best in sumulatable, deterministic
       class)

       approximation of optimal strategy

       1) Attacker’s upper bound

	  - Attacker: given observation (packet sequence p), find class C(p)

	  - trains on the same data

	  - optimal strategy: find class that occurs the most often

	  - with possibility set Q(p) := {C_{1}, C_{2}, ...} classes with
            the same observation p define

            Accuracy Acc(p):= |{C ∈ Q(p)|C = C_{max }}| / |Q(p)|

	  - non-uniform accuracy:= mean of accuracies Acc(p) (p \in S_{test})

	  - uniform accuracy:= maximum of accuracies Acc(p) (p \in S_{test})

	  - defense with optimal uniform accuracy leads to optimal
            non-uniform accuracy

       2) Optimal defense

	  - bandwidth-optimal, simulatable defense

	  - packet sequence as sequence of +1/-1

	  - anonymity set: set of packet sequences p_{i} with D(p_{i}) the
            same

       3) Anonymity set selection

	  - client cannot always choose freely:

	    - before page load

	  - levels of information

	    1. no information: all have to map to single set

	       solution: single supersequence

	    2. sequence end information: when is the query ended

	       solution: single supersequence with stopping points

	    3. class-specific: class is clear, but f.ex. multi-modal
               mode is not

	    4. full: prescience

	  - clustering to find anonymity sets

	    - except in level 1 or 2: one set

	    - level: cluster by distance for prefixes p',q' of length
              min(|p|,|q|):
              2 |f_{SCS}(p', q')| - |p'| - |q'|

	      step 2: stopping points by prefix

	    - level 4: only by distance with whole p,q

       4) SCS approximation

	  - NP-hard problem

	  - approximation algo:

	    - counters c_{i} for each packet sequence p_{i} of n, init at 1

	    - if p_{i}[c_{i}] outgoing for more than n/4-ths, add outgoing,
	      increment c_{i} where outgoing

	    - else, add incoming, increment c_{i} where incoming

	  - cannot have bounded error

    7) Defense evaluation

       best: only two supersequences, supersequence is way better,
       also than tamaraw, as it has uniform accuracy

    8) Discussion

       1) Realistically applying an attack

	  attacker's assumption: start/end is clearly defined and
          trace is from a single page load

       2) Realistic consequences of an attack

	  - many sensitive pages are among the top-100

	  - if local and temporal area is known, identifying get way
            easier

       3) Reproducibility of our results

	  attack, defense, other attacks & defenses and data available

    9) Conclusion

       - pages multi-modal

       - adjusting distance weights

       - knn very fast

       - performs well

       - powerful against all known defenses

       - provable defense

       - better overhead, same security level
*** quote
    - We found that our new attack is much more accurate than previous
      attempts, especially for an attacker monitoring a set of sites
      with low base incidence rate.
    - Privacy technologies are becoming more popular: Tor, a
      low-latency anonymity network, currently has 500,000 daily users
      and the number has been growing [21].
    - Only with a provably effective defense can we be certain that
      clients are protected against website
      fingerprinting. (*certainty* necessary?)
    - a training and testing time that is orders of magnitude lower
      than the previous best.
    - Tor developers remain unconvinced that website fingerprinting
      poses a real threat.
    - An attacker can deal with multi-modal data sets by gathering
      enough data to have representative elements from each mode.
    - Random defenses (noise) have the disadvantage that choosing a
      good covering is not guaranteed,
    - Implementation of random defenses must be careful so that noise
      cannot be easily distinguished from real packets.
    - We then train the attack to focus on features which the defense
      fails to cover and which therefore remain useful for
      classification.
    - The k-NN classifier needs a distance function d for pairs of
      packet sequences. The distance is non-trivial for packet
      sequences.
    - (ends 4.1)
    - the general features are amongst the strongest indicators of
      whether or not two packet sequences belong to the same mode of a
      page,
    - The total number of features is close to 4,000 (3,000 of which
      are just for the unique packet lengths).
    - (ends 4.2)
    - Note that we are not claiming these particular choices of
      parameters and constants yield an optimal attack
    - Our list of 100 monitored pages was compiled from a list of
      blocked web pages from China, the UK, and Saudi Arabia.
    - After |C_{0} | > 2500 [non-monitored pages], we could not see a
      significant benefit in adding more elements.
    - if the base incidence rate of the whole sensitive set is 0.01
      (99% of the time the client is visiting none of these pages),
      and our new classifier claims to have found a sensitive site,
      the decision is correct at least 80% of the time, the rest being
      false positives.
    - The testing time amounts to around 0.1 CPU seconds to classify
      one instance for our classifier and around 800 CPU seconds for
      Wang and Goldberg’s classifier, and 450 CPU seconds for that of
      Cai et al.
    - almost all of the graph in Figure 1 can be drawn only by varying
      k with |C_{0}| = 5000, suggesting that it is advantageous for the
      attacker to have a large number of non-monitored training pages.
    - Then, we must determine the SCS of all the packet sequences in
      the anonymity set. This is in general NP-hard. [13]
    - In fact, it is known that any polynomial-time approximation
      algorithm of shortest common supersequences cannot have bounded
      error [13].
    - It is possible that a clever clustering strategy for class-level
      information could achieve lower bandwidth overheads.
    - the start of a packet sequence generally contains around 3 times
      more outgoing packets than the rest of the sequence. If the user
      is accessing a page for which she does not have a current
      connection (i.e. most likely the user is visiting a page from
      another domain), then the user will always send one or two
      outgoing connections (depending on the browser setting) to the
      server, followed by acceptance from the server, followed by a
      GET request from the main page, and then by data from the
      server. This particular sequence is easily identifiable.
    - On Tor, users are discouraged from loading videos, using
      torrents, and downloading large files over Tor, which are types
      of noise that would interfere with website fingerprinting.
*** questions
    - features perfectly covered by a defence (such as unique packet
      lengths in Tor) will always have n_{bad_i} = k_{reco} , its maximum
      possible value.

      why *always*?

    - Then, accuracy is computed over the remaining 30 instances each,
      on which we perform all-but-one cross validation.

    - As we work with Tor cells, in the following a packet sequence
      can be considered a sequence of -1’s and 1’s (downstream and
      upstream packets respectively),

      so timing information is omitted?
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2014-critical,
        title = {A Critical Evaluation of Website Fingerprinting Attacks},
        author = {Marc Juarez and Sadia Afroz and Gunes Acar and Claudia Diaz and Rachel
              Greenstadt},
        booktitle = {Proceedings of the 21th ACM conference on Computer and Communications
              Security (CCS 2014)},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.cosic.esat.kuleuven.be/publications/article-2456.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** [[./ccs14.pdf][Cai 2014 - A Systematic Approach to Developing and Evaluating Website Fingerprinting Defenses]]
*** summary
    0) [@0] ABSTRACT

       - systematic analysis of features

       - proven lower bounds of bandwidth cost

       - mathematical framework for open-world given close-world

       - tamaraw, better than BuFLO

    1) INTRODUCTION

       fingerprinting attacks

       - dyer: 80%, which of 128 pages (5)

       - cai: 75% against countermeasures (3)

       - Cai: bundle defenses inffective (13)

       - Luo: HTTPOS (11)

	 - Cai: little benefit

       - Wright: traffic morphing (19)

         - Dyer, Cai: little protection

       - Dyer: BuFLO

       - real world vs close-world (14)

       - danger in real world

       - state-of-the-art: only lower bound

       - ideal attacker: websites distinguishable unless exact same
         pattern

       - abstract model:

	 - how far from optimal,

       - which traffic features leak most information

       - provably secure: tamaraw

       - evaluate tamaraw with above techniques

    2) WEBSITE FINGERPRINTING ATTACKS

       - cai and chen aim at identifying web sites instead of web
         pages

       - wf explained

         - only encrypted proxy

	 - page has characteristic dl/ul traffic pattern

       - two assumptions retained

	 - page start noticeable

	 - no background traffic (file downloads, music streaming, etc)

    3) FEATURES AND METHODOLOGY

       wf tries to classify by features, defense tries to hide them

       1) Packet Sequences and their Features

	  - time and length (positive for outgoing, negative for incoming)

	  - unique packet lengths (problem with tor)

	    (∃L ∈ P_{\ell} | L \not∈  P'_{\ell}) ∨ (∃L ∈ P_{\ell}' | L \not∈ P_{ell}' )

	    exists a length L
            which is in P, but not P'
            or in P', but not P

	  - packet length frequency (how often packet length occurs)

	    \exists L | n_{L}(P_{l}) \neq n_{L}(P_{l}') \wedge n_{L}(P_{l}) > 0 \wedge n_{L}(P_{l}') > 0

	    exists a length L
	    which occurs n_L times in P and not n_L times in P'
	    and with both occurances greater than 0

	  - packet ordering:

	    for the multiset of packet lengths M(P)
	    M(P) = M(P')
	    and P \ne P'

	  - interpacket timing:

	    two packets cannot be dependent, if their interpacket
            times is less than one RTT

	    exists 1 \le i \le min(|P|, |P'|)
	    such that the timing t(P_i) \ne t(P'_i)

	  - this is a complete feature set (fact 1) (?td: think?)

	  - features are rather independent (fact 2) (?)

       2) Comparative Methodology

	  - "To determine if a defense is able to hide a feature, we
            apply the defense to two classes, C and C 0 , which differ
            only by that feature. Then, we say that a defense is
            successful in hiding the feature if after applying the
            defense, there is no discernible difference between C and
            C 0."

	  - several generators

	    1. small changes G_{1}: length + v, upto MTU
	    2. large changes G_{2}: length + 1000, upto MTU
	    3. length diffusion G_{3}: increased by position i/5, upto MTU
	    4. append incoming packets G_{4}: length MTU
	    5. append outgoing packets G_{5}: length first outgoing
	    6. insert incoming packets G_{6}: length MTU, one per 5 packets
	    7. Adjacent Transpositions: "v packets are transposed with
               the previous packet"
	    8. Short-Distance Transpositions: v packets are transposed
               with the packet 4 elements ago.
	    9. Long-Distance Transpositions: v packets are transposed
               with the packet 19 elements ago.
	    10. Delays: Each packet is delayed by a linearly
                increasing amount of time, multiplied by v.

       3) Classification and Experimental Setup

	  C = 400 samples of bbc.co.uk
	  C' = generator(C)

	  200 training, 200 testing

	  4 feature classifiers

	  - Unique Packet-Lengths: (like jaccard of Liberatore)

	  - Packet-Length Frequencies: mean, std of (bytes and
            packets) (incoming and outgoing)

	    scored separately, multiplied (like naive bayes of Liberatore)

	  - Packet Ordering: each position: length compared to mean of
            all training packet length  (like bissias/liberatore)

	  - Interpacket Timing: total elapsed time

	  defense applied to each element c and c'

	  measured by the differences between C and c' before
          classifier can distinguish

	  setup: 100mbps ethernet, mtu 1500, imacros 9.00 firefox
          23.0, tcpdump

       4) COMPARISON OF DEFENSES

	  state-of-the-art defenses, simulated

	  1) Simulated Defenses

	     - Maximum Packet Padding (PadM): pads all to mtu

	     - Exponential Packet Padding (PadE): pad to closest power of 2

	     - Traffic Morphing (Wr-Morph): mimic target page

	     - HTTP Obfuscation (HTTPOS): client-side only, tcp
               advertised windows, http ranges, control sizes of
               outgoing and incoming

	       (here: just split packet without extra packets)

	     - Background Noise (Pa-Decoy): load decoy in background

	       (here: alexa top 800)

	     - Buffered Fixed Length Obfuscator (BuFLO): packets at
               fixed intervals with fixed lengths

	  2) Comparative Results

	     - "The full results are given in Table 3"

	     - v from 1 to 180,

	       - best feature classifier

	       - minimum value v for 55 % accuracy

	       - minimum value v for 75 % accuracy

	       - * means unable to

	     - PadM covers: unique packet lengths and orderings,
               better than PadE

	       - both beaten by frequency analysis

	     - HTTPOS broken (f.ex. packet ordering)

	     - PaDecoy, BuFLO work against Panchenko and frequency attacks

	     - Pa-decoy does not completely cover total time (fails
               half the time)

	     - BuFLO similar over 10seconds

	     - HTTPOS client-only

       5) THEORETICAL FOUNDATIONS

	  Model of WF attacks, lower bounds for bandwidth overhead.

	  1) Security vs. Overhead Trade-Off

	     dissimilarity of websites increases overhead

	     offline version

	     1) Definitions

		- w: website

		- t: packet trace

		- W: random variable for w (attacker knows distribution)

		- T_{w}^{D}: random variable for t with defense (attacker knows d.)

		- T_{w}: random variable for t without defense

		- A(t) = argmax_{w} Pr[W = w] Pr[T_{w}^{D} = t]

		  attacker output (determine website w)

		- D *non-uniformly \epsilon-secure* for W iff Pr A(T_{W}^{D}) = W ≤ \epsilon.

		- D *uniformly \epsilon-secure* if max_{w} Pr A(T_{W}^{D}) = w ≤ \epsilon.

		- B(t): total number of bytes transmitted in trace t.

		- BWRatio_{D}(W): E[B(T_{W}^{D})] / E[B(T_{W}^{})]

                  bandwidth ratio of defense D

	     2) Bandwidth Lower Bounds

		- THEOREM 1. Suppose n is an integer. Let W be a
                  random variable uniformly distributed over w_{1}, ... ,
                  w_{n}, i.e. W represents a closed-world
                  experiment. Suppose D is a defense that is
                  \epsilon-non-uniformly-secure against A_{S} on
                  distribution W. Then there exists a monotonically
                  increasing function f from S = {s_{1} , ... , s_{n}} to
                  itself such that

		  - |f(S)| ≤ \epsilon n
		  - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).

		- A_{S}(t) = argmax_{w} Pr[B(T_{w}^{D}) = B(t)]

		  optimal, looks only at total size

		- "Such an f is equivalent to a partition S_{1}, ... , S_{k}
                  of S satisfying k ≤ \epsilon n and minimizing
                  \sum_{i=1}^{k} |S_{i}| max_{s \in S_{i}} s.

		- THEOREM 2. Let W be uniformly distributed over w_{1},
                  ... , w_{n}, i.e. W represents a closed-world
                  experiment. Suppose D is a deterministic defense
                  that is uniformly-\epsilon-secure against A_{S} on
                  distribution W. Then there exists a monotonically
                  increasing function f from S = {s_{1} , ... , s_{n}} to
                  itself such that

		  - min_{i}|f^{-1}(s_{i})| \ge  1/ \epsilon
		  - \sum_{i=1}^{n} f(s_{i}) / \sum_{i=1}^{n} s_{i} \le BWRatio_{D} (W).

	  2) From Closed to Open World

	     - "researchers need only perform closed-world experiments
               to predict open-world performance."

	     - single w^{*}, find out if visited or not

	     - construct open-world from closed-world by selecting
               websites w_{2}, ..., w_{n} and determining if A(t) = w^{*

	     - compute false-positive rate by (p_{i} probability of w_{i})

	     - R_{n} = 1/n \cdot Pr[A(T_{w*}^{D}) = w^{* }] + \sum_{i=2}^{n} Pr[A(T_{wi}^{D}) = w_{i}^{}]
	       "the average success rate of A in the closed world"

	     ... compute FPR, TPR, TDR (true-discovery rate)

	     - algorithm

       6) TAMARAW: A NEW DEFENSE

	  theoretically provable BuFLO

	  1) Design

	     1) Strong Theoretical Foundations:

		optimal partitioning and feature hiding against A_{S}
                attackers

	     2) Feature coverage:

		not only total size, but all features (except for total
                downstream transmission size)

	     3) Reducing Overhead Costs:

		reduces BuFLO's overhead (bandwidth and time)

	     differences to BuFLO:

	     - 750 bytes, not MTU (most packets)

	     - distinguish incoming/outgoing

	     - time to next supersequence, not fixed

	     Tamaraw as follows:

	     - "We denote the packet intervals as ρ_{out} and ρ_{in}
               (measured in s/packet)."

	     - "In Tamaraw, however, the number of packets sent in
               both directions are always padded to multiples of a
               padding parameter, L"

	  2) Experimental Results

	     0) [@0]

		- "our objective in the choice of ρ_{in} and ρ_{out} is to
		  minimize overhead."

		- "as ρ in and ρ out increased, size overhead decreased
		  while time overhead increased"

		- padm better in some accounts

	     1) An Ideal Attacker

		- "evaluate the partitions produced by Tamaraw"

		- "For a partition of size |S|, the attacker can at
                  best achieve an accuracy of 1/|S| on each site in
                  the partition."

	     2) Closed-world Performance

		much better overhead ratio than BuFLO (configurable)

	     3) Open-world Performance

		Much better than agains Tor, BuFLO

       7) CODE AND DATA RELEASE

	  all available (notes: ask)

       8) CONCLUSIONS

	  classify and qualify WF defenses

	  tamaraw

       9) ACKNOWLEDGMENTS

	  Panchenko talked
*** quote
    - the Tor project now includes both network- and browser-level
      defenses against these attacks
    - an attacker could infer, with a success rate over 80%, which of
      128 pages a victim was visiting, even if the victim used
      network-level countermeasures.
    - In our ideal attack, two websites are distinguishable unless
      they generate the exact same sequence of network traffic
      observations.
    - The structure of a page induces a logical order in its packet
      sequence.
    - BuFLO unnecessarily wastes bandwidth hiding the number of
      upstream packets and does not adequately hide the total number
      of downstream packets.
    - This means that the attacker is weak, but is also resource-light
      and essentially undetectable
    - We indicate the packet length as a positive value if the packet
      is outgoing and as a negative value if it is incoming.
    - Packets are sent at fixed intervals with fixed length, and if no
      data needs to be sent, dummy packets are sent instead.
    - Pa-Decoy fails to completely cover interpacket timing because it
      only covers the total transmission time roughly half the time
      (i.e., when the decoy page takes longer to load than the desired
      page)
    - a set of similar websites can be protected with little overhead,
      a set of dissimilar websites requires more overhead.
    - show how to derive open-world performance from closed-world
      experimental results
    - DEFINITION 1. A fingerprinting defense D is *non-uniformly
      \epsilon-secure* for W iff Pr A(T_W^D) = W ≤ \epsilon. Defense D is *uniformly
      \epsilon-secure* for W if max_w Pr A(T_w^D ) = w ≤ \epsilon.

      These are information-theoretic security definitions – A is the
      optimal attacker described above. The first definition says that
      A’s average success rate is less than, but it does not require
      that every website be difficult to recognize. The second
      definition requires all websites to be at least \epsilon difficult to
      recognize. All previous papers on website fingerprinting attacks
      and defenses have reported average attack success rates in the
      closed-world model, i.e. they have reported non-uniform security
      measurements.
    - if the fingerprinting attacker is a government monitoring
      citizens Tor usage, then W would be distributed according to the
      popularity of websites among that nation’s Tor users.
    - Cai, et al., showed that the Alexa top 100 websites were about
      as similar as 100 randomly chosen websites [3], i.e. that the
      most popular websites are not particularly similar to eachother.
    - true-discovery rates for the open-world attack and defense
      evaluations in this paper. Given an open-world classifier, C,
      its true-discovery rate is defined as TDR(C) = Pr[W = w^∗ |
      C(T_W^D) = 1]. Intuitively, the true-discovery rate is the
      fraction of alarms that are true alarms.
    - In our implementations of BuFLO and Tamaraw, we pessimistically
      required that the original logical ordering of the real packets
      must be maintained.
    - A practical implementation could achieve a lower size and time
      overhead as re-ordering is possible for both defenses when
      subsequence is not consequence;
    - we eliminate the network variability and make the defense system
      deterministic, which, as shown in the Appendix, does not reduce
      the security of the defense.
    - at a size overhead of 130%, there are 553 partitions
      (non-uniform security of 69%) in BuFLO (τ = 9) and 18 partitions
      (non-uniform security of 2.25%) in Tamaraw.
    - By showing that the TDR becomes extremely low when attacking
      Tamaraw, even for the first 100 websites, we show that it’s
      extremely low for all websites.
    - The lower bounds of bandwidth costs are surprisingly low,
      suggesting that it may be possible to build very efficient
      defenses.
*** code
**** [[../sw/attacks/svm.py][svm.py]]
     #+BEGIN_SRC python
       #data is in this format:
       #each data[i] is a class
       #each data[i][j] is a standard-format sequence
       #standard format is: each element is a pair (time, direction)
     #+END_SRC
     - str_to_sinste: helper function, splits string
     - load_one: appends lines to data, returns
     - load_all: appends load_one to data, returns
     - extract: extracts features from data
       - sizemarkers: pad to 300 with 0
       - html size: my naive approach
       - total transmitted: sums up
       - number markers: pads to 300
       - unique packet: unique lengths (-/+)
       - percentage incoming
       - number of packets
     - "main"
       - splits data in test and training
       - saves test and training files
***** problemsmaybe:
      - unique packet no fixed length
**** [[file:~/da/git/sw/attacks/svm-run.py]]
     runs
     - python svm.py i
     - svm-train -c c -g g svm.train svm.model
     - svm-predict svm.test svm.model svm.resultst >> temp-acc
     for i folds from 1 to 10
**** [[file:~/da/git/sw/attacks/svmdotest.rb]]
     cleans up, runs
     - clgen_stratify cltor_matrix 36 40
     - svm-train -t 4 -c 1024
     - svm-predict
*** problemsmaybe
    - append small packets generator lacking
    - "Essentially, these two assumptions are equivalent to assuming
      that w^{∗} is not particularly difficult or easy for A to
      recognize."
    - We also show that, in some contexts, randomized defenses offer
      no security or overhead advantage compared to deterministic
      defenses.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2014-fingerprinting,
        title = {A Systematic Approach to Developing and Evaluating Website Fingerprinting
              Defenses},
        author = {Xiang Cai and Rishab Nithyanand and Tao Wang and Rob Johnson and Ian Goldberg},
        booktitle = {Proceedings of the 21th ACM conference on Computer and Communications
              Security (CCS 2014)},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.cs.stonybrook.edu/~rob/papers/ccs14.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** [[./guide_libsvm.pdf][A Practical Guide to Support Vector Classification]]
*** summary
    0) [@0] Abstract

       SVM cookbook

    1) Introduction

       0) [@0]

	  - separate into training and testing sets

	  - training set instance:

	    - "target value" = class label

	    - "attributes" = features or observed variables

	  - goal: produce model which predicts target values of test data
            given only its attributes

	  - four basic kernels (other developed)

	    - linear

	    - polynomial

	    - radial basis: exp(-γ || x_{i}- x_{j} ||^{2} )

	    - sigmoid

       1) Real-World Examples

	  data by users

       2) Proposed Procedure

	  - transform data for input

	  - scale

	  - with rbf:

	    - find C,\gamma by cross-validation

	    - train whole training set

	  - test

    2) Data Preprocessing

       1) Categorical Feature

	  - use m numbers to represent a m-category attribute
	    one is one, others are zero

	    +: more stable

       2) Scaling

	  +: avoid attributes in greater numeric ranges dominating
          those in smaller numeric ranges

	  +: avoid numerical difficulties

	  how: linearly scale to [-1, +1] or [0,1]

	  care: same scale for training and testing (which might then
          be [-1.1, +0.8])

    3) Model Selection

       1) RBF Kernel

	  - includes linear kernel

	  - sigmoid similar for certain parameters, yet sometimes invalid

	  - polynomial has more hyperparameters

	  - fewer numerical difficulties: goes to 0

	  - large features: linear kernel

       2) Cross-validation and Grid-search

	  - high training accuracy not useful \to cross-validation

	  - avoids overfitting better

	  - grid-search: all pairs of e.g.

            - \gamma \in {2^{-15}, 2^{-13}, ..., 2^{3}}
            - C \in {2^{3}, ..., 2^{-13}, 2^{-15}}

	  - advantages: parallelizable, better feeling

	  - first coarse grid, then finer grid

    4) Discussion

       - many features \to select which ones to use

    5) Appendix

       A) Examples of the Proposed Procedure

	  there are automated scripts easy.py and grid.py

	  first scale, then grid, then test \to better, automatic with scripts

       B) Common Mistakes in Scaling Training and Testing Data

	  - use the same scaling factors

	    $ ../svm-scale -l 0 -s range4 svmguide4 > svmguide4.scale
	    $ ../svm-scale -r range4 svmguide4.t > svmguide4.t.scale

       C) When to Use Linear but not RBF Kernel

	  RBF \ge linear only after searching (C, \gamma) space

	  1) Number of instances << number of features

	     linear kernel 98.6111 vs rbf kernel 97.2222

	  2) Both numbers of instances and features are large

	     liblinear faster and more accurate

	  3) Number of instances >> number of features

	     use liblinear -s 2, way faster than default -s 1
** [[./skl/tutorial.html][An introduction to machine learning with scikit-learn]]
*** summary
    1) Machine learning: the problem setting

       - supervised learning

	 - classification: classes

	 - regression: continuous variables

       - unsupervised learning:

         - clustering: similar examples within the data

	 - distribution of data: density estimation

       - training set and testing set:

	 - training: learn properties

	 - testing: test properties

    2) Loading an example dataset

       - from sklearn import datasets

       - digits = datasets.load_digits()

       - digits.data: features

       - digits.target: class

       - digits.images[0] (here)

    3) Learning and predicting

       - =estimator= offers =fit(X, y)= and =predict(T)=

       - from sklearn import svm

       - clf = svm.SVC(gamma=0.001, C=100.)

       - clf.fit(digits.data[:-1], digits.target[:-1])

       - clf.predict(digits.data[-1])

    4) Model persistence

       - from sklearn.externals import joblib

       - joblib.dump(clf, 'filename.pkl')

       - clf2 = joblib.load('filename.pkl')
	 
	 followed by clf2.predict(...)
** [[./paper-ssl-revised.pdf][Analysis of the SSL 3.0 protocol]]
*** summary
    0) [@0] Abstract
       
       some minor flaws, yet easily corrected, good stuff

    1) Introduction

       cryptographic security of SSL 3.0

       background, possible attacks, cryptographic protection, high-level view

    2) Background

       SSL consists of record layer and (connection) layer

       SSLv2 had key weaknesses, end deletion, and protocol
       degradation

    3) The record layer

       standard crypto problems, ok

       1) Confidentiality: eavesdropping

	  lots of known plaintext, but should be ok

       2) Confidentiality: traffic analysis

	  possible to determine request length, response lengths,
          determine which URL was visited

	  supports random padding for block ciphers, but not stream
          ciphers (more common)

       3) Confidentiality: active attacks

	  cut-and-pasted exchanges blocks of ciphertext, trying to
          leak the plaintext

	  short-block-attacks determine the last plaintext block: see
          when the ack is returned

       4) Message authentication

	  uses (old) HMAC, but still HMAC

       5) Replay attacks

	  includes sequence number in MACed data

       6) The Horton principle

	  is all the meaning validated?

	  (SSLCiphertext.ProtocolVersion is not), but in general, yes

       7) Summary

	  ok, minor concerns

    4) The key-exchange protocol

       better, but some scars

       1) Overview of the handshake flow

	  exchange data, compute secret, authenticate sent messages

       2) Ciphersuite rollback attacks

	  negotiation, change cipher spec, finished

       3) Dropping the change cipher spec message

	  in authentication-only mode, the change cipher spec-message
          can be untransmitted by the adversary, which allows her to
          always strip the authentication part

	  if weak encryption is used, this might allow for a online
          key search, with 4-12 (stream - block) bytes of known
          plaintext

       4) Key-exchange algorithm rollback

	  middleman tells each different ciphersuites, as this is not
          protected by hash

	  (horton principle violated)

       5) Anonymous key-exchange

	  specification unclear in what should be signed in anonymous
          mode

       6) Version rollback attacks

	  Mallory might exchange version 3 for version 2 session
          initiation to exploit the weaknesses of SSLv2. There is a
          proposed defense, which sets some padding bytes to fixed
          values.

	  There might be the danger of session resumption leading to
          use of v2. (room for further examination)

       7) Safeguarding the master secret

	  A nonce is hashed with the master secret on every session
          resume. Mallory can get a bit number of data thus hashed.

	  Replay attacks might work for that, too.

       8) Diffie-Hellman key-exchange

	  good idea, watch out to avoid server trapdooring

       9) The alert protocol

	  signify problems, mostly tear down the connection

       10) MAC usage

	   should consistently use HMACs

       11) Summary

	   some weaknesses in implementations possible

    5) Conclusion

       passive only recommendation: padding to avoid get request
       length analysis

       active: change cipher spec dropping and
       KeyExchangeAlgorithm-spoofing

       good step, minor patches recommended
*** quotes
    - We conclude that, while there are still a few technical wrinkles
      to iron out, on the whole SSL 3.0 is a valuable contribution
      towards practical communications security.
    - The SSL record layer provides confidentiality, authenticity, and
      replay protection over a connection-oriented reliable transport
      protocol such as TCP.
    - The only change to SSL’s protection against passive attacks
      worth recommending is support for padding to stop traffic
      analysis of GET (v5)
    - Diffie-Hellman is the only public key algorithm known which can
      efficiently provide perfect forward secrecy
    - To avoid server-generated trapdoors, theclient should be careful
      to check that the modulus and generator are from a fixed public
      list of safe values.
** [[./topranked.html][Does Alexa have a list of its top-ranked websites?]]
*** url https://support.alexa.com/hc/en-us/articles/200449834-Does-Alexa-have-a-list-of-its-top-ranked-websites-
*** summary
    top 1m sites at http://s3.amazonaws.com/alexa-static/top-1m.csv.zip

    updated daily
** [[./tf-idf.html][term frequency–inverse document frequency]]
*** quotes
    - tf–idf is the product of two statistics, term frequency and
      inverse document frequency. Various ways for determining the
      exact values of both statistics exist.
** [[/home/chive/import/pakdoc/rfc1928.socks5.txt][SOCKS Protocol Version 5]]
*** summary
    1. Introduction

       - firewall traversal with authentication

       - does not forward ICMP

       - both TCP and UDP

    2. Existing practice

       - SOCKS Version 4: unsecured, TCP-based

       - extends to include UDP, strong authentication, domain-name, IPv6

    3. Procedure for TCP-based clients

       1. client opens connection to SOCKS port,

       2. authentication negotiation

    4. Requests

       - connect OR bind OR udp associate

       - address type

       - address

       - port

    5. Addressing

       domain name has as first octet the name octets

    6. Replies

       ...
*** quotes
    - Compliant implementations MUST support GSSAPI and SHOULD support
      USERNAME/PASSWORD authentication methods.
** [[./notes]]
*** quotes
    - While we did some editing and customization to Firefox to enable
      data collection, in the newest version of Tor Firefox this is no
      longer necessary. It is possible to run it using just
      <torbrowserfolder>/firefox <sitename>, and we recommend this.
** [[./fp.pdf][Cai 2012 - Touching from a Distance: Website Fingerprinting Attacks and Defenses]]
*** summary
    0) [@0] ABSTRACT

       web-page (!) fingerprinting, 50% regardless of defense scheme

       \to web-site fingerprinting, 90% accuracy

    1) INTRODUCTION

       - "effective attacks against HTTPOS, randomized pipelining, and
         several other defenses."

       - "Even with a 1-to-1 ratio between cover traffic and real
         traffic, our attack could identify the victim’s web page over
         50% of the time."

       - "the first demonstration that application-level defenses,
         such as HTTPOS and randomized pipelining, are not secure."

       - levenshtein-based wf, extended to web sites via hmm

       - others are broken

       - we do better

    2) RELATED WORK

       0) [@0] attack classes

	  - identify user

	  - identify server

	  - identify path

	  - user most applicable

       1) Fingerprinting attacks on encrypting tunnels

	  beginning: packets sizes

	  later: HMMs

       2) Fingerprinting attacks on Tor

	  - hermann et al: multinomial naive bayes,

	  - shi et al: cosine similarity

	  - panchenko: http-specific with svm

	  - reimplementation: 65% success rate, 100 web pages

       3) Proposed traffic analysis defenses.

	  - "padding packets, splitting packets into multiple packets,
            and inserting dummy packets"

	  - Fu et al: theoretical: constant-rate, fixed-rate

	    - random intervals better

	  - wright et al: morphing

	  - lu et al: morphing extension to distribution of size-ngrams

	  - luo et al: HTTPOS:

	    - TCP: size and ordering of packets

	    - HTTP: multiple possibly overlapping requests, pipelinig,
              extra unnecessary requests, get extra data

	    - defeatable by OP

	  - Tor: randomized pipelining

	    - worse not better

       4) Other related work.

	  - Wright et al: HMM protocol classification encrypted TCP

	  - White et al HMM partial plaintext of encrypted VoIP

    3) RECOGNIZING WEB PAGES

       - Damerau-Levenshtein edit distance

	 - best costs when "transpositions were 20 times cheaper than
           insertions, deletions, and substitutions"

	 - size rounding (up)

	 - normalization to d(t, t') / min(|t|, |t'|)

	 - several worse approaches

	   - cells instead of packets

	   - knn

	   - fixed-length via l_{2}-norm

    4) RECOGNIZING WEB SITES

       - HMM

	 - "each web page corresponds to an HMM state, and state
           transition probabilities represent the probability that a
           user would navigate from one page to another."

	 - uses classifier for probability

	 - web site template for huge pages (like amazon)

	 - AJAX: transition between different states

	 - *cold* pages: on first visit, vs

	 - *warm* pages: with some stuff cached

	 - back button as link to warm page

	 - one set of usage patterns (or a few distinct, or uniform)

    5) Congestion-Sensitive BUFLO

       - BuFLO with output queue

       - only outgoing, other ends needs CS-BuFLO as well

       - reveals

	 - maximum transmission rate T

	 - number of transmitted cells B

	 - (upstream too)

    6) EVALUATION

       1) Web page classifier

	  0) [@0] questions

	     - defenses: https, randomized pipelining, padding, morphing

	     - other classifiers:herrmann, panchenko

	     - if number of web pages goes up?

	     - if size of training set goes up?

	     - choice of web pages?

	     - state of the browser?

	  1) Experimental Setup

	     - default firefox with Tor

	     - "either 20 or 40 traces from each URL"

	  2) Attacks and Defenses

	     1) data sets

		- none: ssh

		- ssh + httpos

		- tor

		- tor + randomized pipelining

	     2) generate defenses

		- ssh + sample-based traffic morphing to flickr.com

		- ssh packet count remove packet size and direction information

		- tor + randomized pipelining + randomized cover traffic

		  only insert 1500 or -1500 at l random positions

		  *weaker than panchenko*

		- tor packet count: as ssh p-c above

	     3) Results

		- better in many cases than panchenko

       2) Web site classifier

	  1) Experimental Setup

	     - facebook:

	       - login page, user's home page, "friend profile page"

	       - warm and cold of home and profile pages

	     - imdb:

	       - home page, search results, movie, celebrity

	       - warm and cold for each page

	     - artificial transition probabilities

	     - facebook via fixed path

	  2) Results

	     - perfect for facebook,

	     - still very good for imdb

    7) DISCUSSION

       - "Existing defenses are inadequate."

       - "Traffic analysis can infer user actions through several
         different side channels."

	 Panchenko good results. Theirs good "even if all packet size
         information is removed from the trace"

       - "The DLSVM classifier generally outperforms other classifiers."

	 - more generic: trace passed "directly into the classifier"

       - "Defenses based on randomized requests and cover traffic are
         not likely to be effective."

	 with their special randomized request (random form of l
         \pm 1500)

       - "This attack is practical in real settings."

    8) CONCLUSION

       - HTTPOS, randomized pipelining, traffic morphing were weak

       - new defense

       - this ignores packet sizes

       - web site classifier,

         - sequence of page loads,

	 - HMM
*** quotes
    - Our attack converts traces into strings and uses the
      Damerau-Levenshtein distance to compare them.
    - (ends 1)
    - they are a good match for the attacker scenario faced by many
      Tor users today: they use Tor toevade censorship and persecution
      by a government or ISP that wants to know their browsing habits
      and has the ability to monitor their internet connection, but
      cannot easily infiltrate Tor nodes and web servers outside the
      country.
    - (ends 2.0)
    - these edits correspond to packet and request re-ordering,
      request omissions (e.g. due to caching), and slight variations
      in the sizes of requests and responses.
    - a better approach would be to learn optimal costs from the
      training data using the recently-proposed method of Bellet, et
      al.
    - also rounds all packet sizes *up* to a multiple of 600
    - Other normalization factors, such as |t| + |t_{0}| and
      max(|t|, |t_{0}|), yielded worse results.
    - The γ parameter is used to normalize L so that it’s outputs fall
      into a useful range. In our experiments, we found γ = 1 works
      well.
    - We tried representing traces as a sequence of Tor cells instead
      of as a sequence of packets. Classifier performance degraded
      slightly, suggesting that the Tor cells are often grouped into
      packets in the same way each time a page is loaded.
    - neighbor algorithm: to classify trace t, the attacker computed
      t^{∗} = argmin_{t'} L(t, t') over every trace in his database, and
      guessed that t was from the same web page as t^{∗}
    - Finally, we tried using a metric embedding to convert our
      variable-length trace vectors into fixed-length vectors in a
      space using the \ell_{2} - norm, and then used an SVM to classify
      these vectors. This performed substantially worse than the SVM
      classifier with distance-based kernel described above.
    - (ends 3)
    - for each *observation* o ∈ O and *HMM state* s, the probability,
      Pr[o|s], that the HMM generates observation o upon transitioning
      to state s.
    - pages p_{1} and p_{2} can be represented by a single state s only if
      Pr[o|p_{1}] ≈ Pr[o|p_{2}] for all observations o.
    - assumes that users all tend to navigate through a website in the
      same way.
    - ends (4)
    - A (d, ρ, τ ) BUFLO implementation transmits d-byte pack ets
      every ρ milliseconds, and continues this process for at least τ
      milliseconds.
    - (ends 5)
    - if a window had, say, 3 IMDB pages and 3 non-IMDB pages, we
      discarded it from the histogram.
    - (ends 6.2.2)
    - recently proposed randomized pipelining defense
    - has no ordering information
    - (ends 7)
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2012-fingerprinting,
        title = {Touching from a Distance: Website Fingerprinting Attacks and Defenses},
        author = {Xiang Cai and Xincheng Zhang and Brijesh Joshi and Rob Johnson},
        booktitle = {Proceedings of the 19th ACM conference on Computer and Communications
              Security (CCS 2012)},
        year = {2012},
        month = {October},
        www_tags = {selected},
        www_pdf_url = {http://www.cs.sunysb.edu/~xcai/fp.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** [[./authorsversion-ccsw09.pdf][Herrmann - Website Fingerprinting: Attacking Popular Privacy Enhancing Technologies with the Multinomial Naïve-Bayes Classifier]]
*** summary
    0. [@0] ABSTRACT

       - attack privacy-enhancing technologies via text-mining
         techniques

       - closed-world: 97% success

    1. INTRODUCTION

       - PET (=privacy enhancing technology) website fingerprint
         attack

       - by local ISP, local admin, secret services

       - multinomial naive bayes

    2. SCENARIO

       - between user and PET, records traffic, can link IP to victim

       - passive, local, external attacker

       - training phase: fingerprints for all (or set of observed) websites

       - testing phase: measure user traffic, compare to fingerprints

    3. RELATED WORK

       - HTTP traffic analysis

	 - Mystry, Cheng: determine URLs via encrypted SSL (single web
           server)

	   - not feasible anymore: pipelining and multiple
             simulatenous connections

	 - Hintz, Sun: HTTP proxy with SSL

	   - library of histograms of sizes of transferred files

	   - Sun: 100000 websites, Jaccard, 75% with FPR 1.5

	 - Bissias, Liberatore: improved, patterns in IP packets

	   - Liberatore

	     - neglects timing information and order,

	     - compare packet size histograms via Jaccard coefficient and Naive
               Bayes

	     - with kernel density estimation:

	     - 73%

	     - padding schemes evaluated: ip padding foils attack

	 - Kiraly: Traffic Flow Confidentiality (IPSEC extension:
           padding and packet clocking)

	   - not against WF

	 - Wright: Traffic Morphing: "thwart statistical traffic
           analysis algorithms by efficiently modifying traffic of a
           website in a way so that it looks like another one."

    4. METHODOLOGY

       1. Analysed Systems

	  - single-hop

	    - OpenSSH: offers SOCKS proxy, multiplexing, flow control

	    - OpenVPN: raw IP packets (routing mode)

	    - CiscoVPN: ESP via UDP

	    - Stunnel: TCP and TLS handshakes for each connection

	    - later also WiFi via WPA (same category)

	  - multi-hop

	    - Tor: short-lived circuit

	    - JonDonym: static cascade

	    - I2P not included: performance/stability & used mostly for
              inter-I2P-communication

       2. Research Assumptions

	  assumptions very good for adversary
	  
	  1) knows PET type

	  2) knows all pages = closed-world

	  3) similar internet access like victim

	  4) knows browser and configuration

	  5) browser configured easily (no caching, no prefetching, no
             querying for updates)

	  6) victim requests single pages one at a time

       3. Modelling the Classification Problem

	  - data mining problem: classification

	  - Attributes: number of packets of a certain size (multiset)

       4. Known Website Fingerprinting Techniques

	  1. Jaccard’s Classifier

	     s_{AB} = |A \cap B| / |A \cup B|

	     60% in Liberatore/Levine

	  2. Naïve Bayes Classifier with Kernel Density Estimation

	     naive bayes, better for padded, worse for unpadded than
             jaccard

       5. Our Novel Website Fingerprinting Method

	  text mining techniques

	  1. Multinomial Naïve Bayes (MNB) Classifier

	     - text mining

	       - spam

	     - tf-idf similar to packet frequency

	     - different from naive bayes

	  2. Application of Text Mining Transformations

	     - averaging the number of texts (f.ex. ACKs) via tf

	     - averaging total word occurrences via idf

	     - normalising lengths via cosine transform

    5. EVALUATION

       0. [@0]

	  - Weka with own Jaccard-classifier

	  - single hop easily deanonymized, multi-hop "some protection"

       1. Data Collection and Sampling

	  - school data: real users, 2000 domain names from log file
            by frequency, filtered to 775 (real domains)

	  - setup

	    - script based on firewatir

	      - and javascript shell

	    - ff 2.0

	    - start tcpdump before

	    - aborts after 90 seconds

	    - restart browser after 775 URLs download

	  - 2 (tor) to 17 (cisco) instances per day

	  - testing data (48h),

	  - training data from (48h + \Delta) later time (48h)

	  - \to stratified

	  - corrected resampled paired t-test

       2. Performance of the MNB Classifier

	  0. [@0]

	     - comparison to other OpenSSH-fingerprinting
               attacks. This relates to other systems as well.

	     - accuracy: found/total

	  1. Influence of Transformations

	     best result for (only) TF with normalization

	  2. Size of Training Set

	     4 training instances ("good compromise between necessary
             resources and achievable accuracy.")

	  3. Robustness

	     quite robust to changes over time

	     also adjusts to changes in content: concept drift

       3. Comparison of Website Fingerprinting Methods Against OpenSSH

	  - with transformation (tf + normalisation + cosine),
            significantly better

	  - also faster for training and testing

       4. Attacking Popular PETs Using the MNB Classifier

	  - single-hop all above 94% with tf-normalization

	  - multi-hop JonDonym 19.97, Tor 2.96% (normalization only)

	  - better for top k (3/10) classes

	  - multi-hop worse than theoretic, might be vulnerable

    6. DISCUSSION

       - setup constant, might change for different Operating Systems etc

       - caching decreased success from 96.65 % (with caching
         disabled) to 91.70 %

       - false alarm avoidance comes at a great cost: with ~1.4% false
         alarms, tp falls to 40% (78 interesting sites of 775)

    7. CONCLUSION

       - Multinomial Naïve Bayes

       - "operates on the frequency distribution of IP packet sizes"

       - "increased performance is mainly due to the normalisation of
         the packet size frequency vectors"
*** quotes
    - influence of the browser cache on accuracy.
    - The attack consists of two phases: in the training phase the
      attacker creates traffic fingerprints for a large number of
      sites (or for a small set of interesting sites) and stores them
      together with the site URLs in a database. In the testing phase
      the attacker records the encrypted traffic of the user, creates
      fingerprints of small traffic chunks and tries to match them
      with records in the database.
    - the browser cache has only a moderate impact on the accuracy in
      our sample
    - Note that our instances closely resemble the typical document
      representation in the domain of text mining, where instances are
      represented by term frequency vectors.
    - (ends 4.3)
    - Jaccard’s coefficient is a similarity metric for sets [31],
      which is often used for unsupervised learning tasks.
    - (ends 4.4.1)
    - [The Naive Bayes Classifier] naïvely assumes independence of
      attributes, which is often not the case for real-world problems.
    - operates directly on multiset instances,
    - (ends 4.4.2)
    - biased towards classes which contain many packets and/or packets
      with high frequencies [...] problem is addressed by a sublinear
      transformation of the frequencies: 

      f^{∗}_{x_{j}} = log(1 + f_{x_{j}}). 

      This is referred to as *term frequency (TF) transformation*.
    - some packet sizes (e. g. with the size of the MTU) are part of
      every instance and do not confer much information [...] is
      alleviated using the *inverse document frequency (IDF)
      transformation* [...] 

      f^{*}_{x_{j}} = f_{x_{j}} · log (n / df_{x_{j}})
    - normalising the lengths [...] by applying cosine normalisation
      to the attribute vectors, i. e. the transformed frequencies are
      divided by the Euclidean length of the raw vectors: 

      f^{norm}_{x_{j}}= (f^{* }_{x_{j}} / ||(f^{* }_{x_{1}}, ... f^{*}_{x_{n}})|| )
    - (ends 4.5.2)
    - From an information-theoretic viewpoint, even the multi-hop
      systems do not protect perfectly, though: the accuracies found
      for them are well above the accuracy achievable by randomly
      guessing the class without any context knowledge (1/775 ≈
      0.13%).
    - top k classes from the list of predicted classes (sorted in
      descending order by class membership probability). If the actual
      class was among the list of predicted classes, the test instance
      was counted as correctlyclassified, otherwise as incorrectly
      classified. For k = 3 and k = 10 the accuracy values for Tor
      increase to 16.69 % and 22.13 %, respectively, for JonDonym they
      increase to 31.70 % and 47.53 %.
    - (ends 5)
    - dependent to some degree on the operating system, the type of
      the Internet connection and the browser and its
      configuration. We therefore expect that the accuracy of website
      fingerprinting attacks is degraded in case training and testing
      instances are not recorded in the same environment.
    - footnote to /false alarms/: Please note that the term /false
      positives/ is intentionally not used here, as it is used to
      convey another meaning in classical data mining.
    - (ends 6)
    - the development and implementation of efficient countermeasures
      becomes an important task for the PET community.
*** vocabulary
    - website fingerprinting: learn the identity, i. e. the URLs, of
      websites that are downloaded over an encrypted tunnel by
      comparing the observed traffic to a library of previously
      recorded fingerprints.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccsw09-fingerprinting,
        title = {Website fingerprinting: attacking popular privacy enhancing technologies with
              the multinomial na\"{\i}ve-bayes classifier},
        author = {Dominik Herrmann and Rolf Wendolsky and Hannes Federrath},
        booktitle = {Proceedings of the 2009 ACM workshop on Cloud computing security (CCSW
              '09)},
        year = {2009},
        month = {October},
        address = {New York, NY, USA},
        location = {Chicago, Illinois, USA},
        pages = {31--42},
        publisher = {ACM},
        doi = {http://doi.acm.org/10.1145/1655008.1655013},
        isbn = {978-1-60558-784-4},
        www_section = {Traffic analysis},
        www_tags = {selected},
        www_pdf_url = {http://epub.uni-regensburg.de/11919/1/authorsversion-ccsw09.pdf},
      }
    #+END_SRC
** [[./ieee-icc15.pdf][A First-Hop Traffic Analysis Attack Against Tor]]
*** summary
    0) [@0] Abstract

       timing-information only

       68% success

    1) INTRODUCTION

       0) [@0]

	  - only timing information

	  - padding defeats size info

	  - packet counting need partitioning

	  - this does not

       1) Related Work

	  - Hintz: SafeWeb

	    - sequential page fetches

	    - port/direction/size observed

	    - 75% success rate

	  - Bissias:

	    - also sequential

	    - size/direction/time observed

	    - 23% for 100 pages, 40% for fewer

       2) ANATOMY OF A WEB PAGE FETCH

	  - assume padding

	  - direction/timing observed

	  - enough: uplink traffic timestamps

	  - how web page fetched

	    - third-party: new delay (TCP handshake)

	    - AJAX: large inter-arrival times \to signature

	    - number of fin/finack/ack dependent on "number of
              distinct locations"

       3) COMPARING SEQUENCES OF PACKET TIMESTAMPS

	  how to compare different-length sequences

	  1. Derivative Dynamic Time Warping

	     - warping path (see quotes)

	     - cost function C: sum

	     - with cost per single difference given by derivative

	  2. F -Distance Measure

	     with two paths and a warping path, sum the stretches
             where only one of them increments (non-parallel), divide
             by total length of both paths

       4) DE - ANONYMISING WEB FETCHES OVER TOR

	  0) [@0]

	     - 20 health/ 20 finance websites a 100 "fetches"

	     - watir-webdriver script

	     - Firefox 21.0

	  1) Hardware/Software Setup

	     3 ghz core 2 duo, 2 gb ram, ubuntu 12.04 lts

	  2) Classifying Measured Timestamp Sequences

	     - K-NN with F-distance

	     - better than naive Bayes

	     - top 5 per web page to represent

	  3) Randomised Routing

	     - "Abrupt, substantial changes in the mean RTT are
               evident"

	  4) Classification Performance

	     - 67.7% on 40 sites a 100 samples

	     - 93% without Tor

	     - K=1 is best here

	  5) Finding a web page within a sequence of web requests

	     - 3 out of 5:

	     - find

	       - stream,

	       - cut by 10,

	       - use 3-instance exemplar to match,

	       - analyze using all

	     - 69% success with position \pm 65 packets

       5) SUMMARY AND CONCLUSIONS

	  - "The attack makes use only of packet timing information on
            the uplink"

	  - effective (68% accuracy on 40 sites)
*** quotes
    - define a *warping path* p to be a sequence of pairs,

      {(p_{k}^{i}, p_{k}^{j})}, k = 1, 2, ··· ,l with (p_{k}^{i}, p_{k}^{j}) ∈ V
      := {1, ... , n}× {1, ..., m}

      satisfying boundary conditions

      p^{i}_{1} = 1 = p^{j}_{1} ,

      p^{i}_{l} = n, p^{j}_{l} = m

      and step-wise constraints

      (p^{i}_{k+1} , p^{j}_{k+1} ) ∈ V p^{i}_{k} ,p^{j}_{k} := {(u, v) :
        u ∈ {p^{i}_{k} , p^{i}_{k + 1}} ∩ {1, . . . , n},
        v ∈  {p^{j}_{k} , p^{j}_{k + 1}} ∩ {1, . . . , n}},
      k = 1, · · · , l−1.
    - where D_{t}(i) = (t_{i} - t_{i^{-}}) + (t_{i^{+}} - t_{i^{-}}) / 2,
      i^{-} = max{i-1, 1} and
      i^{+ }= min{i+1, |t|}
    - (ends 3.1)
    - This suggests using the fraction of the overall warping path
      which is parallel to the x- or y-axes as a distance measure,
      which we refer to as the *F-distance*.
    - define κ 1 := 0 < κ 2 < · · · < κ r−1 < κ r := l such that for
      each s = 1, · · · , r − 1
      a) either p ik 1 = p ik 2 ∀k 1 , k 2 ∈ {κ s + 1, · · · , κ s+1 }
         or p jk 1 = p jk 2 ∀k 1 , k 2 ∈ {κ s + 1, · · · , κ s+1 } and
      b) either κ s+1 = l or condition (a) is violated for some k 1 ,
         k 2 ∈ {κ s , · · · , κ s+1 + 1} i.e. each subsequence is
         maximal.
    - define the *F-distance* measure between timestamp sequences t and
      t′ , namely:

      φ(t, t') := \sum_{s=1}^{r−1} (κ_{s+1} − (κ_{s} + 1)) / n+m (4)
    - (ends 3)
    - congestion window growth slows with increasing RTT.
    - (ends 4.D (=4.4))
** TODO [#A] [[./1512.00524v1.pdf][WTF-PAD: Toward an Efficient Website Fingerprinting Defense for Tor]]
*** summary
    0) [@0] Abstract

       - lightweight defense

       - tool to evaluate of pluggable transports

    1) INTRODUCTION

       - wf attacks mostly successful in lab

       - avoiding time delays crucial

       - contributions

	 - other defenses unsuitable

	 - framework for link-padding implementations

	 - lightweight defense

	 - realistic evaluation

    2) WEBSITE FINGERPRINTING (WF)

       how wf works

       1) Attacks

	  - infer from traffic which websites visited

	  - real world applicability questionable

       2) Defenses

	  - High-level (application) defenses:  HTTPOS,
            randomized pipelining

	  - supersequence etc and traffic morphing: require large database

	  - low-level (network): padding, morphing, BuFLO (constant
            rate), Tamaraw Cs-BuFLO, high latency addon

    3) SYSTEM MODEL

       threat and network model

       realistic defense requirements

       1) Adversary Model

	  here: bridge to connect

	  open-world: adversary tries to detect whether client
          downloads "one of a small set of target pages"

       2) Defense Model

	  - assume trusted bridge

       3) Defense Requirements

	  - Effectiveness: prevent WF ("need only provide this
            protection in a realistic setting")

	  - Usability: delay minimized, average bw acceptable

	  - Efficiency: not excessive bandwidth

	  - No server-side cooperation

	  - No databases: big files hard to distribute

    4) FRAMEWORK

       - evaluate "broad range of traffic analysis defenses."

       - can do

	 - crawl traces with defense enabled

	 - apply defense to traces

	 - simulate defense

       - parts: crawler, replayer, simulator

    5) ADAPTIVE PADDING

       no latency padding, 50% overhead often sufficed

       1) Design Overview

	  0) [@0] 

	     - burst and gap mode,

	     - random bursts of traffic in burst mode,

	     - in gap mode, waiting

	  1) AP algorithm.

	     - S

	     - (on arrival) Burst mode U, sample time

	       - (on delay without packet) Gap mode G, sample time

	       - (on packet receipt) resample time

	     - (on send) Gap mode G, sample time

	       - (on expired) send packet

	       - (on infinite sample) to U

	     - histogram bins exponentially

	       - better bin web traffic

	       - last to \infty

	     - tokens in bin,

	       - select token randomly

	       - when expires, remove token

	       - when arrives, remove arrival time bin token

	  2) Burst mode.

	     - if inter-arrival time longer than typical, go to G state

	  3) Gap mode

	     - "*within* a burst in traffic collected for a large sample
               of sites."

	     - (rather see timing...)

       2) WTF-PAD

	  - send / receive padding

	  - send histograms via control messages

	  - beginning of transmission

	  - soft stopping as with adaptive padding

       3) Interarrival time distribution
*** quotes
    - lightweight defenses that provide a sufficient level of security
      against website fingerprinting
    - a tool for evaluating the traffic analysis resistance properties
      of Tor’s Pluggable Transports
    - (ends 0)
    - bandwidth and latency increases come at a cost to usability and
      deployability,
    - While the study does not dismiss WF attacks as impractical, it
      argues that effective defenses may be feasible with bandwidth
      and latency overheads that are much lower than what is required
      in artificial lab conditions.
    - the Tor network has spare bandwidth on its entry edges
    - (ends 1)
    - the WF problem is treated as a supervised classification problem
    - Our own findings with their [W&G] classifier (see Section VI),
      however, indicate that these open-world findings may be incorrect.
    - (ends 2)
    - equivalent for a client connecting directly to Tor without a
      bridge, but the bridge-based model fits our framework.
    - hard to justify the large-scale deployment of WFcountermeasures
      with potentially heavy overheads that would significantly impact
      the quality and usability of the network as a
      whole. Implementing WF defenses as PTs instead allows
      researchers to evaluate these defenses outside the laboratory
      without introducing excessive overheads in the Tor network.
    - We argue that a defense against WF in Tor should satisfythe
      following requirements:
    - confound classifiers by reducing inter-class variance and
      increasing the intra-class variance
    - The defense should be implemented without requiring a large
      database profiling many websites.
    - (ends 3+4)
    - with a 50% padding rate, the accuracy of end-to-end timing-based
      traffic analysis is significantly degraded.
    - Adaptive Padding (AP) works differently. It does not delay
      application data; rather, it sends it immediately.
    - pads the gaps between data packets so that the interarrival
      packet timings follow a certain distribution, rather than
      imposing a constant rate.
    - (ends 5.0)
    - until transmission starts and it *receives* a real packet
    - (ends 5.1)
    - a fake burst interrupted by a real burst (right).
    - While we are in a burst, the delays we sample from H_{U} will not
      expire until we find an inter-arrival time that is longer than
      typical within a burst, which will make the delay expire and
      trigger the G state.
    - (ends 5.2)
    - In burst mode, the algorithm essentially assumes there is a
      burst of real data and consequently waits for a longer period
      before sending any padding. In gap mode, the algorithm assumes
      that there is a gap between bursts and consequently aims to add
      a fake burst of padding with short delays between packets.
    - In this paper, we follow Shmatikov and Wang and define a burst
      in terms of bandwidth. In particular, a burst is a sequence of
      packets that has been sent in a relatively short time.
    - It also adds padding randomly to some real bursts.
    - burst mode may randomly switch to gap mode in the middle of a
      burst, leading to a fake burst in the middle of a real burst
    - (ends AP algorithm..Beginning of transmission)
    - WTF-PAD does not require an explicit mechanism to conceal the
      total time of the transmission.
    - Inter-arrival times in seconds (*log scale*)
    - (ends 5.4)
    - simulate WTF-PAD on the traces of our dataset.
    - used the Kolmogorov-Smirnov test to evaluate the goodness of
      fit.
    - use normal and log-normal distributions for the sake of
      simplicity,
    - (ends 6)
    - In both multi-tab scenarios, we found that k-NN could detect the
      background page (the first tab) with much greater accuracy than
      the foreground page (the second tab). Additionally, adding
      significant delay before loading the foreground page increases
      the accuracy of classifying it. The likely cause of these
      findings is that the first few packets in a page often hold
      important features.
    - (ends 7)
    - only the remaining 60% of traffic needs to be covered by this
      defense.
    - distribution of inter-arrival times depends on the average
      bandwidth of the client. We have built our statistical model
      based on the inter-arrival times observed in our
      dataset. However, AP lacks a systematic method to set the
      optimal configuration for each client. We propose to have
      WTF-PAD bootstrap these values during installation.
    - (ends 8)
*** questions
    - burst mode may randomly switch to gap mode in the middle of a
      burst, leading to a fake burst in the middle of a real burst
      (see Figure 3).
      - not seen in figure, can this also happen?
** TODO [#A] [[./tor14design.pdf][Tor: The Second-Generation Onion Router (2014 DRAFT v1)]]
*** summary
    0) [@0] Abstract

       - real-world experiences

       - open problems

    1) Overview

       - Better than original onion routing by:

         - perfect forward secrecy:

           "subsequently compromised nodes cannot decrypt old traffic"

	 - Separation of “protocol cleaning” from anonymity

	   just uses SOCKS for applications to connect. (protocol
           cleaning is done f.ex. by addon or proxy)

	 - No mixing, padding, or traffic shaping (yet):

	   no usable concepts/implementations, high overhead

	 - Many TCP streams can share one circuit:

	   allows for multiple streams to have same circuit (with user
           control)

	   less crypto, less vulnerability (see section 9)

	 - Leaky-pipe circuit topology:

	   traffic can exit at any place in the circuit (how about
           exit node policies?)

	 - Congestion control:

	   end-to-end acks, active research

	 - Directory authorities:

	   instead of flooding the network, trusted nodes provide
           network info

	 - Variable exit policies:

	   exit node operators select which traffic to allow to which
           hosts

	 - End-to-end integrity checking:

	   in addition to crypto

	 - Rendezvous points and hidden services:

	   negotiation of rendezvous points (instead of "reply onions")

	 - Censorship resistance:

	   bridges (unlisted guard nodes) and HTTPS similarity

	 - Modular architecture:

	   - vidalia (control port)

	   - pluggable transports

	   - no OS patches, but only TCP possible

    2) Related work

       - Chaum: Mix-Net

       - Babel, Mixmaster, Mixminion: maximum anonymity, large latency

       - tor low-latency

       - single-hop: anonymizer, etc

       - JonDo: fixed cascades: routes that aggregate traffic

       - PipeNet: multi-hop, weaknesses

       - p2p:

         - tarzan, morphmix, layered encryption

         - crowds: all nodes can read

	 - hordes: crowds with multicast responses

	 - herbivore and P^{5}: crowds with broadcast responses

       - freedom, i2p: circuits all at once

       - cebolla, anonymity network: build in stages

       - circuit-based: which circuit? IP, TCP, HTTP?

       - TCP middle-approach,

         - can transfer all TCP streams

         - avoid TCP-TCP inefficiencies

       - censorship-resistance like eternity, free havfen, publius,
         tangler

    3) Design goals and assumptions

       0) [@0]

	  - Goals

	    - Deployability: cheap, rather legal, easy to implement

	    - Usability: usable by mose applications

	    - Flexibility: specified, replaceable

	    - Simple design: kiss: well-understood, accepted approaches

	    - Resistant to censorship: both by IP and protocol fingerprinting

	  - Non-goals

	    - Not peer-to-peer: "still has many open problems"

	    - Not secure against end-to-end attacks:

	      "Some approaches, such as having users run their own onion
              routers, may help;"

	    - No protocol normalization: needs to be added via Privoxy f.ex.

       1) Threat Model

	  adversary like [[*%5B%5B./tor-design.pdf%5D%5BTor:%20The%20Second-Generation%20Onion%20Router%5D%5D][Tor: The Second-Generation Onion Router*]]

    4) The Tor Design

       0) [@0]

	  - atop TLS

	  - onion routers

	    - TLS connection to other ORs

	    - 2 (+1) keys

	      - long-term identity: signs router descriptor and TLS
                certificates

	      - short-term onion key: decrypt circuit requests,
                negotiates keys

	  - onion proxy

	    - fetch directories

	    - establish circuits

	    - handle connections from users

       1) Cells

	  - fixed size vs variable size

	    - versions, vpadding, certs, auth_challenge, authenticate,
              autorize

	  - command vs relay vs relay_early

	    - relay: streamid, digest, length

       2) TLS details

	  - previously, TLS handshake identified Tor

	  - nowadays, in-TLS handshake using /versions/ cell

       3) Circuits and streams

	  Tor constructs circuits preemptively, routes several
          application streams via them

	  except if the user signals that she wants a separation

	  - Constructing a circuit

	    1. Alice to Bob: create e_{bob}(g^x)

	    2. Bob to Alice: created hash(K = g^xy), g^y)

	    3. Alice to Carol via Bob: relay extend g^{x_2}

	    4. Bob to Carol: create e_{carol}(g^{x_2})

	    5. Carol to Bob: created (...)

	    6. Bob to Alice: relay extended

	    also /create fast/ possible instead of create, which
            relies on TLS security and avoids the RSA overhead

	  - Relay cells

	    Cells sent forward from the host

	    - if digest valid, this OR is meant, process instructions

	    - else send on

	    - leaky circuit

	    - /destroy/ and /relay truncate/

       4) Choosing nodes for circuits

	  0) [@0]

	     - (bandwidth / capabilities) weighted distribution

	     - bandwidth measured, distributed by consensus

	  1) Guard nodes

	     increased (little) risk of more deanonymization,
             decreases (bit higher) risk of some deanonymization

	  2) Avoiding duplicate node families in the same circuit

	     - attack: control entry and exit node

	     - defense: avoid both from /16, also from (mutual) families

       5) Opening and closing streams

	  - create or select circuit

	  - use last hop or intermediate as exit

	  - /relay begin/ with random /streamID/

	  - /relay connected/

	  - client sends TCP with /relay data/

	  - SOCKS problems

	    - DNS data leak

	  - firefox problems

	    - cookies, DOM torage

	    - TLS session IDs

	    - browser characteristics

	    - plugins

	    - privoxy weak against HTTPS

	  - /relay teardown/ \sim TCP RST

	  - /relay end/ ~ TCP FIN (allows TCP half-closed conns)

       6) Integrity checking on streams
*** quotes
    - most designs protect primarily against traffic analysis rather
      than traffic confirmation
    - distributed-trust, circuit-based anonymizing systems
    - (ends 2)
    - adding unproven techniques to the design threatens
      deployability, readability, and ease of security analysis.
    - like all practical low-latency systems, Tor does not protect
      against such a strong adversary [a global passive adversary]
    - (ends 3)
    - some control cells are variable length, where the ability of an
      attacker to detect their presence doesn’t affect security.
    - Most traffic passes along these connections in fixed-size
      cells. (A few cell types, notably those used for connection
      establishment, are variable-sized.)
    - To determine that this newer version of the link protocol
      handshake is to be used, the initiator avoids using the exact
      set of ciphersuites used by early Tor versions, and the Tor
      responder uses an X.509 certificate unlike those generated by
      earlier versions of Tor. This may be too clever for Tor’s own
      good; we mean to eliminate it once every supported version of
      Tor supports this version of Tor’s link protocol.
    - (ends ?4.2?)
    - This circuit-level handshake protocol achieves unilateral entity
      authentication (Alice knows she’s handshaking with the OR, but
      the OR doesn’t care who is opening the circuit — Alice uses no
      public key and remains anonymous)
    - Preliminary analysis with the NRL protocol analyzer [33] shows
      this protocol to be secure (including perfect forward secrecy)
      under the traditional Dolev-Yao model.
    - (ends 4.3.Constructing a circuit)
    - As mentioned above, if the first and last node in a circuit are
      controlled by an adversary, they can use traffic correlation
      attacks to notice that the traffic entering the network at the
      first hop matches traffic leaving the circuit at the last hop,
      and thereby trace a client’s activity with high
      probability. Research on preventing this attack has not yet come
      up with any affordable, effective defense suitable for use in a
      low-latency anonymity network.
    - (ends 4.4.2)
** TODO [#B] [[./ShWa-Timing06.pdf][Timing analysis in low-latency mix networks: attacks and defenses]]
*** summary
    0) [@0] Abstract

       Analyze correlation attacks on low-latency mix networks.

       previous defenses ineffective or huge latency or huge overhead

    1) Introduction

       - low-latency mix networks

       - path establishment not researched

       - realistic traffic model based on HTTP traces

       - adaptive padding

	 - fill statistically unlikely gaps

	 - also against active attackers

       - compare to constant rate dummy traffic

    2) Related work

       - Venkatraman: mathematical model, defense requiring complete knowledge

       - Timmerman: traffic masking: assure certain profile via cover
         traffic and artificial delays

       - Berthold: high-latency, intermediate dummy, global,

         - CONTRA: not fine-grained

       - Rennhard: artificially delay traffic

         - CONTRA: latency

       - Fu: constant rate is vulnerable to analysis of inter-packet
         arrivals,

         - SOLUTION: variable inter-packet intervals

       - defensive-dropping: drop at intermediate, same constant rate,

         - CONTRA: latency

       - Overlier: trusted entry node,

	 - PRO: combines with this

    3) Model and metrics

       1) Network

	  sender, path of N (small: {2,3}, large: {5..8}) mixes, destination

	  like tarzan

	  (end-to-end TCP?)

       2) Timing analysis

	  - attacker measures inter-packet intervals

	  - active: also impose unique timing signature

	  - simple model: ignores TCP effect of drops and bursts

       3) Defense metric.

	  - adversary: correlate packet counts

	  - crossover rate: see "timing...systems"

       4) Negative impact on network performance.

	  - metric: padding ratio

	  - metric: extra delay

    4) Adaptive padding

       create statistic traffic, send if no packet in next drawn
       sample time

       1) Data structures: bins, indexed exponentially

       2) Adaptive padding algorithm

	  1. take token

	     - if packet arrives before time, remove token for packet
               arrival time, replace token into bin

	     - else, send dummary after token's time

	  2. until token buckets empty

       3) Destroying natural fingerprints

	  - 2 modes

	    - burst: packet received, take expected high-duration

	    - gap: dummy sent, take expected low-duration

    5) Experimental evaluation

       0) [@0] 

	  - 4 schemes

	    - undefended

	    - defensive dropping

	    - defensive dropping variant with constant rate cover traffic

	    - adaptive padding

	  - exponential distribution

       1) Attack model

	  - observes 60 seconds

	  - does cross-correlation for 1-second-steps

	  - sets threshold t to equals FPR and FNR

	  - defender's goal: high t

       2) Evaluation results

	  - constant rate dummies and no cover fails

	  - defensive dropping works by upping other streams' correlation

	  - adaptive padding works by lowering this stream's correlation

       3) Short paths

	  2-3 mixes

	  - defensive dropping works worse

	  - adaptive padding works better

    6) Active attacks
*** quotes
    - Many mix networks are specifically intended to provide anonymity
      against attackers who control the communication medium.
    - Traffic analysis is an especially serious threat for low-latency
      mix networks because it is very difficult to hide statistical
      characteristics of the packet stream and satisfy the stringent
      latency requirements imposed by interactive applications.
    - The standard measure of success is the /crossover error rate/, at
      which the attacker’s false positive rate is equal to his false
      negative rate.
    - (Maximum crossover error rate is 0.5, which corresponds to
      random guessing.)
    - recognized as a serious threat to low-latency mix networks, but
      few defenses have been proposed to date.
    - (ends 2?)
    - An active attacker can also impose his own unique timing
      signature (by dropping packets or introducing artificial bursts)
      onto the flow he is interested in, and then attempt to identify
      this signature on other network links.
    - (ends 3)
    - samples from the statistical distribution of inter-packet
      intervals. If the next packet arrives before the chosen interval
      expires, it is forwarded and a new value is sampled. To avoid
      skewing resulting intervals towards short values, the
      distribution is modified slightly to increase the probability of
      drawing a longer interval next time.
    - rough statistical distribution of inter packet intervals for a
      “normal” flow [...] can be pre-computed from traffic
      repositories such as NLANR [21]
    - increasing the range represented by each bin exponentially with
      the bin index works well.
    - (ends 4.Data structures)
    - We do not model TCP acknowledgements, re-transmissions,
      exponential backoff in response to dropped packets, and other
      TCP features that may be exploited by a sophisticated attacker.
    - (ends 5.0)
    - We conjecture that long-term attacks cannot be prevented without
      assuming that some senders or mixes emit cover traffic in
      perfect synchrony, which cannot be achieved by any real system.
    - The attacker chooses t [threshold for correlation r(d)] so that
      the false positive and false negative rates are equal. This is
      the attacker’s crossover error rate. High crossover rate means
      that the defense is effective.
    - (ends 5.1)
*** questions
    - 
** TODO [#A] [[./cacr2015-08.pdf][Wang - Walkie-Talkie: An Effective and Efficient Defense against Website Fingerprinting]]
*** summary
    0) [@0] ABSTRACT

       other defenses: too much time: 1/2 minute wait

       here: half-duplex

       9% time, 32% bw \to 5 % fpr

       9% time (?), 55% bw \to 10%fpr

    1) INTRODUCTION

       - tor breakable by wf

       - walkie-talkie: half-duplex defense, against perfect attacker

    2) RELATED WORK

       0) [@0]

	  1998 wf first mentioned

       1) Attacks

	  evolution

       2) Defenses

	  0) [@0]

	     - limited: against specific WF attack

	     - general: against perfectly accurate attacker

	  1) Limited defenses

	     - Wright's morphing

	       - addressed Liberatore and Levine's attack (relies on
                 unique packet lengths)

	       - randomly padded packets

	       - broken by effective's packet ordering attack

	     - Luo's HTTPOS

	       - broken by touching and effective

	     - Tor's randomised pipelining

	       - updated in critique

	       - both versions ineffective as of touching, effective
                 and "improved wf on tor"

	  2) General defenses

	     - "different web pages should, with sufficient
               likelihood, produce the exact same packet sequence."

	     - Dyer: BuFLO: fixed, constant rate, dummy packets

	       - padded upt o 10 seconds

	     - Cai: Tamaraw

	     - Wang: Supersequence

       3) Moving Forward

	  - Problems:

	    - Overhead:

	      - time 170% to 240%

	      - bandwdith 90% to 180%

	    - Congestion:

	      - not congestion-aware

	  - Solution: half-duplex communication

    3) IMPLEMENTING HALF-DUPLEX COMMUNICATION

       0) [@0]
	  - half-duplex:

	    - bursts by each side

	    - small time overhead

	    - no bandwidth overhead

	    - easy to implement

       1) How browsers work

	  - HTTP: client connects, retrieves or sends data

	  - Firefox (other browsers similar): connectionmanager uses
            up to fixed maximum of connections, sends requests via
            these

	  - for half-duplex: modify connection manager

       2) Implementing half-duplex communication

	  - walkie: idle

	  - talkie: sent data, receiving replies

	  - proof:

	    - client sends only when no active connections, \to server not talking

	    - server talking: client sends no data \to client not talking

       3) Optimistic data

	  - sending TCP connection request and GET at the same time

	    - one less RTT

	    - reduces number of bursts, thus padding

       4) Other Implementation Details

	  - pipelining disabled

	  - tls: delay, but not a weakness

	  - speculative connections: disabled

    4) PADDING

       0) [@0]

	  - assume padding of data

	  - half-duplex reduces further to number of packets in each
            burst [+timing?]

	    to burst pair (incoming number, outgoing number)

	  - perfect attacker can detect except for collision

       1) Deterministic Padding

	  - round_Y(a) up to rounding set Y

	  - pad number of bursts to round_Y(bursts)

	  - overhead-optimal rounding set by algorithm

	  - randomized test 2 \le |Y| \le 9

       2) Random padding

	  - client and cooperator send dummy packets when request has
            started

	  - adding from uniform distribution to real burst

	  - adding from (test uniform and normal)

    5) EVALUATION

       0) [@0]

	  test against known attacks

	  expect perfect attacker, which can do perfect table lookup

       1) Evaluation of deterministic defenses

	  - "The objective of the defense is to maximize collisions."

	  - "To classify any sequence belonging to c, the attacker’s
            optimal strategy (optimizing accuracy) is to guess that
            all elements in the collision set belong to the class with
            the highest rate of occurrence in that set. The attacker’s
            overall accuracy over c is therefore max_{i} n_{i} /|c|.

       2) Evaluation of random defenses

	  - determine maximum probability instead of "highest rate of
            occurrence"

       3) Overhead
*** quotes
    - design goals:
      1. General: The defense succeeds against all possible
         classification attacks by causing collisions, where the
         defense outputs the same packet sequence given input packet
         sequences from different web pages. Even a perfect attacker
         would not be able to tell which page it came from.
      2. Easy to use: The client does not need to configure the
         defense. The defense is ready to use out of the box, and can
         be deployed incrementally as it does not depend on other
         clients using the same defense.
      3. Decentralized: The defense should not require some central
         server, with a shared database, to operate. We want to match
         the decentralized model of anonymity networks.
    - (ends 1)
    - Half-duplex communication reduces the attacker’s potential
      feature set to only a series of burst traffic sizes, which makes
      WF difficult.
    - (ends 2/3?)
    - The client adds outgoing dummy packets, while a cooperator adds
      incoming dummy packets.
    - Due to the low base rate of visits to each single web page, a
      client protecting her page accesses against an attacker may only
      need a false positive rate over 5% [18,21] even if the
      attacker’s true positive rate was 100%.
    - (ends 4.0)
*** questions
    - Further, we assume this encrypted channel pads all packets to a
      fixed size so as to remove packet size as a possible
      feature. (Note that Tor delivers all data in fixed-size cells.)

      How about other cells? Are they irrelevant?

      Probably, as they are only the versions, tls, etc

      Maybe not, handshake etc...
    - client reduces the attacker’s possible feature set even further,
      to simply counts for thenumber of packets in each burst of
      traffic.

      How about timing data?
** TODO [#A] [[./morphing09.pdf][Traffic Morphing: An Efficient Defense Against Statistical Traffic Analysis]]
*** summary
    0. [@0] Abstract

       morphing changes traffic to look like another site

    1. Introduction

       convex optimization to simulate (morph to) other site

       split etc packets to create the appearance of another site

       here: only packet size features obfuscated

    2. Related Work

       - Song: SSL timing analysis for login password length

       - Sun: SSL - use size of HTML objects to identify web pages

       - Liberatore: with persisten connections and SSH port forwarding

       - Wright: VoIP language spoken

       - Saponas: identify movies within encrypted connections

       - Wright: identify spoken phrases in encrypted VoIP

       - Wagner/Dean: mimicry attack vs IDS

       - Soto: against pH IDS

       - Tan: against stide IDS

       - Fogla: packet payloads, hide malicious payload

       - many: fake training data

    3. Traffic Morphing

       0) [@0]
    
	  - alter "distribution of packet sizes"

       1) What is the Matrix?

	  - morphing matrix: given input probability sizes X and
            output probability sizes Y, find matrix A such that Y=AX

	  - receive packet of size s_j, sample target size s_i from j-th
            column of A via cumulative probability

	  - pads with zeros if s_i > s_j, split otherwise

	  - convex optimization to get minimal overhead

       2) Morphing via Convex Optimization

	  - n^2 variables, 2n bounds \to infinite possibilities

	  - cost function example: number of additional bytes

       3) Additional Morphing Constraints
*** quotes
    - morphing one class of traffic to look like another class.
    - show how to optimally modify packets in real-time to reduce the
      accuracy of a variety of traffic classifiers while incurring
      much less overhead than padding.
    - (ends 0)
    - For the remainder of this paper, we focus on the use of our
      morphing techniques in thwarting traffic classifiers that
      utilize features based on packet sizes.
    - (ends 1 + 2)
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{morphing09,
        title = {Traffic Morphing: An efficient defense against statistical traffic analysis},
        author = {Charles Wright and Scott Coull and Fabian Monrose},
        booktitle = {Proceedings of the Network and Distributed Security Symposium - {NDSS} '09},
        year = {2009},
        month = {February},
        publisher = {IEEE},
        www_tags = {selected},
        www_pdf_url = {http://freehaven.net/anonbib/papers/morphing09.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#A] [[./802163c-01_30r1.pdf][Traffic Model for 802.16 TG3 MAC/PHY Simulations]]
*** summary
    1) Introduction

       - model (wireless WAN)

       - one direction only

       - different types available

       - user: different type and different volume

    2) Description of the HTTP/TCP and FTP Model
*** quotes
** TODO [#A] [[./HTTP Traffic Model_v1 1 white paper.pdf]]
*** summary
    0) [@0] ABSTRACT

       - squid log analyzer

       - well-matching http model and generator

    1) INTRODUCTION
*** quotes
    - It is possible that the ON state lasts over multiple Web-Request
      periods when the downloading of the last embedded object and the
      next HTML object overlaps.
    - Table 2: Analysis Result of HTTP Model Parameters
      | Parameters    | Mean            |   S.D. | Best Fit                     |
      |---------------+-----------------+--------+------------------------------|
      | <13>          | <15>            |        | <28>                         |
      | HTML Object size | 11872 (Max 2MB) |  38306 | Truncated Lognormal (μ=7.90272, σ=1.7643) |
      | Embedded ObjectSize | 12460 (Max 6MB) | 116050 | Truncated Lognormal (μ=7.51384, σ 2.17454) |
      | Number of Embedded objects | 5.07 (Max 300)  |        | Gamma (κ=0.141385, θ=40.3257) |
      | Parsing Time  | 3.12 (median 0.30)(max 300 sec) |  14.21 | Truncated Lognormal (μ=-1.24892, σ=2.08427) |
      | Embedded Object IAT | 0.83            |    8.4 | Weibull (α=0.2089, β=0.376)  |
      | Reading Time  | 39.70 (max 10,000 sec) | 324.92 | Lognormal (μ=-0.495204, σ=2.7731) |
      | Request Size  | 318.59          | 179.46 | Uniform (350 Bytes)          |
*** questions
    - correlation of features (high object count \to high page size)?
** TODO [#B] [[./LZCLCP_NDSS11.pdf][HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows]]
*** summary
    0) [@0] Abstract

       - defense against HTTPS information leakage

    1) Introduction

       HTTPOS (http[s] obfuscation) modifies traffic.

       was very successful in their tries against single-hop attacks

    2) Threat models

       - web sites and pages (at page level, IP visible)

       - ajax-interaction regarded as new pages

       - HTTP proxy

	 - can use TCP features

    3) Defending against traffic-analysis attacks

       analyze classification and propose defenses

       1) The state-of-the-art attacks
*** quotes
    - Extensive evaluation of HTTPOS on live web traffic shows that it
      can successfully prevent the state-of-the-art attacks from
      inferring private information from encrypted HTTP flows.
    - (ends 0)
    - the efficacy of these methods has not been validated thoroughly
      based on actual implementations and live HTTP traffic.
    - TCP (e.g., Maximal Segment Size (MSS) negotiation and
      advertising window) and HTTP (e.g., HTTP Range and HTTP
      Pipelining).
    - (ends 1+2)
    - four basic features that can affect the information used by
      those traffic-analysis attacks: packet size, timing of packets,
      web object size, and flow size.
    - (ends 3.0)
** TODO [#B] [[./oakland2012-peekaboo.pdf][Dyer - Peek-a-Boo, I Still See You: Why Efficient Traffic Analysis Countermeasures Fail]]
*** summary
    0) [@0] Abstract

       - coarse features

       - analysis of TA countermeasures

	 - standardized in TLS, SSH, IPsec

	 - wright

    1) INTRODUCTION

       - wf is a problem

       - most defenders try to eliminate just one feature

       - 9 defenses, 7 attacks, two datasets

       - tested countermeasures ineffective

       - hiding length is not sufficient

       - Coarse information is unlikely to be hidden efficiently.

       - Relevance to other settings: real-world attacks harder, yet
         wf defenses should cover this

       - did not try Camouflage and HTTPOS

    2) EXPERIMENTAL METHODOLOGY

       - herrmann and levine datasets

       - each classifier, each defense

       - select privacy set k \le n of websites

       - for each websites, 20 traces, 16 of which training, 4 test

       - many trials

       - downloadable python code http://www.kpdyer.com/

    3) TRAFFIC CLASSIFIERS

       supervised learning, testing and training data

       1) Liberatore and Levine Classifier

	  - naive Bayes

	  - direction and length of packets

       2) Herrmann et al. Classifier

	  - multinomial naive Bayes

	  - normalized counts

	  - tf + cosine

       3) Panchenko et al. Classifier

	  - SVM

	  - panchenko's parameters C=2**17, \gamma=2**-19

	  - remove ACKs

	  - number of features

    4) COUNTERMEASURES
*** quotes
    - in the context of website identification, it is unlikely that
      bandwidth-efficient, general-purpose TA countermeasures can ever
      provide the type of security targeted in prior work.
    - (ends 0)
    - This implies that any effective countermeasure must produce
      outputs that consume indistinguishable amounts of bandwidth.
    - (ends 1)
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{oakland2012-peekaboo,
        title = {Peek-a-Boo, {I} Still See You: Why Efficient Traffic Analysis Countermeasures
              Fail},
        author = {Kevin P. Dyer and Scott E. Coull and Thomas Ristenpart and Thomas Shrimpton},
        booktitle = {Proceedings of the 2012 IEEE Symposium on Security and Privacy},
        year = {2012},
        month = {May},
        www_pdf_url = {http://kpdyer.com/publications/oakland2012.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#B] [[./ESLII_print4.pdf][The Elements of Statistical Learning]]
*** summary
    1) Introduction

       examples

    2) Overview of Supervised Learning

       1) inputs

          == predictors (statistics)

          == independent variables (statistics)

	  == features (pattern recognition)

	  outputs

	  == responses (statistics)

	  == dependent variables (statistics)

       2) Variable Types and Terminology

	  - qualitative aka discrete aka categorical variables aka factors

	    example: handwritten digits, fisher's irises

	  - regression for quantitative outputs Y, classification for
            qualitative outputs G

       3) Two Simple Approaches to Prediction: Least Squares and
          Nearest Neighbors

	  0) [@0]

	     - linear model

	       - "stable but possibly inaccurate predictions"

	     - k-nearest neighbors

	       - "often accurate but can be unstable"

	  1) Linear Models and Least Squares

	     "solve" the quadratic equation in quotes by finding a
             minimum

	  2) Nearest-Neighbor Methods

	     - majority of the closest (by specific metric) k neigbors
               are of the class

	     - k=1 - might lead to overfitting

	  3) From Least Squares to Nearest Neighbors

	     - defined later

	       - variance: ca schwankung

	       - bias: ca vorannahmen

	     - ls: high bias, low variance

	     - nn: low bias, high variance

	     - knn - degrees of freedom: N / k

	     - many models today are variations of knn and the linear
               model

       4) Statistical Decision Theory

	  - minimize expected loss

	    - knn: localized minimization

	    - linear model: assume linearity

	    - expected loss (for regression):

	      - squared (L2) most often,

	      - L1 has discontinuties in derivative (=nicht stetig diffbar)

	    - expected loss for classification:

	      - often zero-one loss function

	      - results in Bayes classifiers

	    - classification similar to regression

	      - knn: localized conditional probability, estimated by
                training sample proportions

	      - linear: map to dummy variable

       5) Local Methods in High Dimensions

	  - unbiased methods like problematic supposedly more
            problematic, as dimension rises

       6) Statistical Models, Supervised Learning and Function
          Approximation

	  0) [@0]

	     Try to approximate the I/O relationship f(x) via f\circ(x)

	     1) A Statistical Model for the Joint Distribution Pr(X, Y )
*** quotes
    - Chapter 18 explores the “p ≫ N ” problem, which is learning in
      high-dimensional feature spaces. These problems arise in many
      areas, including genomic and proteomic studies, and document
      classification.
    - (Ends preface to the Second Edition)
    - 7 Model Assessment and Selection
    - 7.10.2 The Wrong and Right Way to Do Cross-validation
    - 15 Random Forests
    - 18 High-Dimensional Problems: p ≫ N
    - 18.3.3 The Support Vector Classifier
    - (ends Contents)
    - Average percentage of words or characters in an email message
      equal to the indicated word or character. We have chosen the
      words and characters showing the largest difference between spam
      and email.
      |       | george |  you | your |   hp | free |  hpl |    ! |  our |   re |  edu | remove |
      |-------+--------+------+------+------+------+------+------+------+------+------+--------|
      | spam  |   0.00 | 2.26 | 1.38 | 0.02 | 0.52 | 0.01 | 0.51 | 0.51 | 0.13 | 0.01 |   0.28 |
      | email |   1.27 | 1.27 | 0.44 | 0.90 | 0.07 | 0.43 | 0.11 | 0.18 | 0.42 | 0.29 |   0.01 |
    - This is a classification problem for which the error rate needs
      to be kept very low [...] In order to achieve this low error
      rate, some objects can be assignedto a “don’t know” category,
      and sorted instead by hand.
    - splines, wavelets and regularization/penalization methods for a
      single predictor, [...] kernel methods and local
      regression. Both of these sets of methods are important building
      blocks for high-dimensional learning techniques.
    - At the end of each chapter we discuss computational considerations
    - Chapters 1–4 be first read in sequence. Chapter 7 should also be
      considered mandatory, as it covers central concepts that pertain
      to all learning methods.
    - (ends 1)
    - predict the output Y via the model

      Ŷ = β̂_{0} + \sum_{j=1}^{p} X_{j} β̂_{j} . (2.1)
    - in vector form as an inner product

      Ŷ = X^{T} β̂, (2.2)
    - least squares. In this approach, we pick the coefficients β to
      minimize the residual sum of squares

      RSS(β) = \sum_{i=1}^{N} (y_{i} − x^{T}_{i} β)^{2}. (2.3)
    - in matrix notation. We can write

      RSS(β) = (y − Xβ)^{T} (y − Xβ), (2.4)
    - the unique solution is given by

      β̂ = (X^{T} X)^{−1} X^{T} y, (2.6)
    - In the case of one Gaussian per class, we will see in Chapter 4
      that a linear decision boundary is the best one can do, and that
      our estimate is almost optimal. The region of overlap is
      inevitable, and future data to be predicted will be plagued by
      this overlap as well.
    - (ends 2.3.1)
    - linear decision boundary [...] has low variance and potentially
      high bias.
    - k-nearest-neighbor [...] high variance and low bias.
    - 1-nearest-neighbor, the simplest of all, captures a large
      percentage of the market for low-dimensional problems.
    - Kernel methods use weights that decrease smoothly to zero with
      distance from the target point, rather than the effective 0/1
      weights used by k-nearest neighbors.
    - Projection pursuit and neural network models consist of sums of
      nonlinearly transformed linear models.
    - (ends 2.3.3)
    - X ∈ R^{p} input
    - Y ∈ R output
    - function f(X) for predicting Y
    - /squared error loss/: L(Y, f(X)) = (Y − f (X))^{2} .
    - EPE(f) = E(Y − f (X))^{2} (2.9)

      = \int [y − f (x)]^{2} Pr(dx, dy) (2.10)

      the /expected (squared) prediction error/
    - The solution is

      f (x) = E(Y |X = x), (2.13)

      the conditional expectation, also known as the regression
      function. Thus the best prediction of Y at any point X = x is
      the conditional mean, when best is measured by average squared
      error.
    - (ends 2.4)
    - In ten dimensions we need to cover 80% of the range of each
      coordinate to capture 10% of the data.
    - (ends 2.5)
    - Thus it is not surprising that the analysis of high-dimensional
      data requires either modification of procedures designed for the
      N > p scenario, or entirely new procedures
    - (ends 2.6.0)
    - (ends 18.1)
** TODO [#B] [[./thebook.pdf][INTRODUCTION TO MACHINE LEARNING]] [0/2]
*** TODO summary
    1) Introduction

       often in background

       1) A Taste of Machine Learning

	  applications, formalize

	  1) Applications

	     - page rank

	     - collaborative filtering: amazon recommendations

	     - automatic translation

	     - face recognition vs face verification

	     - named entity recognition: from documents etc some named thing

	     - speech recognition: annotate voice sample

	       - and similarly: handwriting, etc

	     classification: yes/no answer

	  2) Data

	     types of data

	     - Lists

	     - Sets

	     - Matrices

	     - Images

	     - Videos

	     - Trees and Graphs

	     - Strings

	     - Compound structures: combine these

	  3) Problems

	     1) Binary Classification:

		- online learning: instantaneous determination of y for x

		- batch learning: group of stuff

		- transduction: know X' (testing data)

		- active learning: choose X

		- estimation with missing variables

		- covariate shift correction: X and X' from different
                  data sources

		- co-training: related but different problems

		- loss functions

	     2) Multiclass Classification

	     3) Structured Estimation

	     4) Regression

	     5) Novelty Detection: new and unusual observations

       2) Probability Theory

	  1) Random Variables

	     X takes on values in f.ex. \cal X = {1,..., 6} \ni x

	  2) Distributions

	     - discrete values: probability mass function, PMF

	     - continuous values: probability density function, PDF
	       integral: CDF

	     - Pr(a \le X \le b) = \int_{a}^{b} dp(x) = F(b) - F(a)

	     - Quantile: value x' for which Pr(X < x') \le q, Pr(X > x') \le 1-q
	       is /q-quantile/. q=0.5: /median/

	  3) Mean and Variance

	     - mean: E[x] = \sum_{x} xp(x) and \int x dp(x)
	       E[f(x)] =  \int f(x) dp(x)

	     - variance Var[X] = E[(X - E(X))^{2}]
	       Var[f(x)] =  E[(f(X) - E(f(X)))^{2}]

	  4) Marginalization, Independence, Conditioning, and Bayes Rule

	     0) [@0]

		- marginalization, independence, conditioning, iid, bayes
		  rule: see quotes

		- Bayes Rule holds because p(x,y) = p(x|y)p(y) = p(y|x)p(x)

		- "The key consequence of (1.15) is that we may
                  reverse the conditioning between a pair of random
                  variables."

	     1) An Example

		HIV testing,

		- p(T = HIV+ | X = HIV+) = 1

		- p(T = HIV+ | X = HIV-) = 0.01

		- p(X = HIV+) = 0.0015
                  \to  p(X = HIV-) = 0.9985

		- then p(X = HIV+ | T = HIV+) is (shorthand P(x+, t-) etc)
		  = p(t+ | x+) p(x+) / p(t+)

		- p(t+) = p(x+, t+) + p(x-, t+)
		  = p(t+ | x+ ) p(x+) + p(t+ | x-) p(x-)
		  = 1 * 0.0015 + 0.01 * 0.9985

		- back to p(x+ | t+)
		  = 1 * 0.0015 / (1 * 0.0015 + 0.01 * 0.9985)

		- increase test accuracy by further information (f.ex. age)

		- and further, conditionally independent, tests

       3) Basic Algorithms

	  0) [@0]

	     - features X, labels y

	     - /bag of words/: "Assume we have a list of all possible
               words occurring in X, that is a dictionary, then we are
               able to assign a unique number with each of those words
               (e.g. the position in the dictionary). Now we may
               simply count for each document x_{i} the number of times
               a given word j is occurring. This is then used as the
               value of the j-th coordinate of x_{i}."

	  1) Naive Bayes

	     use bayes formula with approximations

	     - replace p(x) by likelihood ratio (adjusted)

	     - replace p(x|y) by independent product of parts (spam: words)

	  2) Nearest Neighbor Estimators

	     - distance to (k-) nearest point(s).

	     - whichever class is closest

	     - distance-dependent

	     - works well if work was put into distance

	  3) A Simple Classifier

	     1. use means of classes

	     2. compute distance of point to means

	     3. extension: line boundary \to geometric boundary

	  4) Perceptron

	     - online learning

	     - "data points x t ∈ R d , and labels y t ∈ {±1}"

	       - if wrong, updates w by x_{t}y_{t}

	       - kernelizeable: x replaceable by \phi(x)

	     - converges if convergeable

	     - margin describes how thin the border is

	     - novikoff's theorem gives perceptron's accuracy bounded
               by margin (+ data set diameter)

	     - proof by two limits per step, induction, cauchy-schwartz

	     - perceptron was used in (a)nn

	     - here, more complex feature maps and "associated kernel"s

	  5) K-Means

	     - cluster

	     - start with k /prototype vector/s,

	     - approximate classes

	     - approximate vectors

	     - repeat

    2) Density Estimation

       1) Limit Theorems

	  0) [@0] Casino

	     - example: 100 die (1w6) tosses at casino yields only 11 6es
               (instead of 17, as expected)

	     - (^{m}_{n}) = m! / n!(m-n)! different sequences with
               proportions n="6" and m="not 6" respectively

	     - only 11 '6'es for 100 die tosses: \sim 7.0%

	       probability[X \le 11] = \sum_{i=0}^{11} (^{100}_{i}) (1/6)^{i}  (5/6)^{100-i}

	  1) Fundamental Laws

	     - weak law: irgendwann ist summe durch \epsilon beschraenkt

	     - strong law: wkt der konvergenz ist 1

	     - central limit theorem: id vars converge to (var-sth)
               normal distribution (all - \mu)

	     - slutsky: continuous functions can be pulled into E

	     - delta method: see later

	  2) The Characteristic Function

	     - Fourier transform

	     - Characteristic functions

	     - Sum of random variables and convolutions

	       - Proof via Y=z-x, joint distribution p(z, x)

	     - proofs of 1)

	     - moments \to function

	  3) Tail Bounds

	     - Gauss inequality: proof via

	       - 1 \le x/\epsilon (auf [\epsilon, \infty))

	       - 1/\epsilon herausziehen, auf [0, \infty) erweitern

	       - auf dem Intervall ist x durschnittlich kleiner \mu

	  4) An Example

	     - assume that all production steps are iid, have the same \mu_{B}

	     - X_{i} for wafer i

	     - X̄_{m} the average of the yields of m wafers using process
               ’B’

	     - interested in is the accuracy \epsilon for which the
               probability

	       δ = Pr(| X̄_{m} − μ_{B} | > \epsilon) satisfies δ ≤ 0.05.

	     - solve

	       - chebyshev: 40000

	       - hoeffding: 738

	       - central limit theorem: 55

       2) Parzen Windows

	  1) Discrete Density Estimation

	     estimate by actual occurrences in sample data

	     problem: some numbers might not appear

	     solution: laplace smoothing

	  2) Smoothing Kernel

	     - "proper density estimation."

	     - parzen windows estimate (see quotes)

	     - different kernels

	  3) Parameter Estimation

	     - width of kernel function is important, shape less

	     - to maximize likelihood, sum over log-likelihood to
               avoid float overflow

	     - maximizing likelihood on all of data leads to
               overfitting

	     - avoid by using a Validation Set, n-fold
               Cross-validation or a Leave-one-out Estimator

	  4) Silverman’s Rule

	     - locality-dependent kernel width

	       r_{i} = c/k \sum_{x \in kNN(x_i)} ||x - x_{i}||

	     - locality-dependent parzen rule

	       p\circ(x) = 1/m \sum_{i=1}^{m} r^{-d}h(x - x_{i}/r_{i})

	  5) Watson-Nadaraya Estimator

	     - non-parametric data

       3) Exponential Families

	  0) [@0]

	     - "Gaussians, Poisson, Gamma and Wishart distributions all
               form part of the exponential family"

	     - "lead to convex optimization problems"

	  1) Basics

	     - Defined by

               p(x; θ) := p_{0}(x) exp (<φ(x), θ> − g(θ)) . (2.40)

	     - with g(\theta) weighting term to get density function

	     - \varphi map from parameters to statistics

	  2) Examples

	     Many distributions can be seen as member of the exponential family

	     - Gaussian

	     - Multinomial

	     - Poisson

	     - Beta
*** TODO quotes
    - much of the art of machine learning is to reduce a range of
      fairly disparate problems to a set of fairly narrow
      prototypes.
    - A rather related application is collaborative
      filtering. Internet book stores such as Amazon, or video rental
      sites such as Netflix use this information extensively to entice
      users to purchase additional goods (or rent more movies).
    - Many security applications, e.g. for access control, use face
      recognition as one of its components.
    - The overarching theme of learning problems is that there exists
      a nontrivial dependence between some observations, which we will
      commonly refer to as x and a desired response, which we refer to
      as y, for which a simple set of deterministic rules is not
      known. By using learning we can infer such a dependency between
      x and y in a systematic fashion.
    - (ends 1.1.1)
    - One of the challenges in dealing with vectors is that the scales
      and units of different coordinates may vary widely.
    - One way of dealing with those issues in an automatic fashion is
      to normalize the data. We will discuss means of doing so in an
      automatic fashion.
    - In some cases the vectors we obtain may contain a variable
      number of features.
    - (ends 1.1.2)
    - 3-class classification. Note that in the latter case we have
      much more degree for ambiguity.
    - a sequence of (x_{i} , y_{i} ) pairs for which y i needs to be
      estimated in an instantaneous online fashion. This is commonly
      referred to as online learning.
    - know X 0 already at the time of constructing the model. This is
      commonly referred to as transduction.
    - (ends 1.1.3)
    - For more details and a very gentle and detailed discussion see
      the excellent book of [BT03].
    - (ends 1.2.0)
    - Formally, [...] 1 occurs with probability 1/6
    - notational convention [...] use uppercase letters, e.g., X, Y
      etc to denote random variables and lower case letters, e.g., x,
      y etc to denote the values they take.
    - (ends 1.2.1)
    - If the random variable is discrete, i.e., it takes on a finite
      number of values, then this assignment of probabilities is
      called a /probability mass function/ or PMF for short.
    - slightly informal notation p(x) := Pr(X = x)
    - continuous random variable the assignment of probabilities
      results in a probability density function or PDF for short.
    - Closely associated with a PDF is the indefinite integral
      over p. It is commonly referred to as the cumulative
      distribution function (CDF).
    - (ends 1.2.2)
    - if f: R → R is a function, then f(X) is also a random
      variable. Its mean is mean given by

      E[f(X)] := \int f(x)dp(x).
    - (ends 1.2.3)
    - /joint density/ p(x, y).
    - /marginalization/: recover p(x) by integrating out y:
      p(x) = \int dp(x, y).
    - /X and Y are independent/: p(x,y) = p(x) p(y)
    - /iid random variables/: independently and identically distributed
    - /conditional probabilities/:
      p(x|y) := p(x, y) / p(y)
    - /Bayes Rule/: p(y|x) = p(x|y)p(y) / p(x)
    - (ends 1.2.4.0)
    - The physician recommends a test which is guaranteed to detect
      HIV-positive whenever a patient is infected.
    - On the other hand, for healthy patients it has a 1% error
      rate. That is, with probability 0.01 it diagnoses a patient as
      HIV-positive even when he is, in fact, HIV-negative.
    - Moreover, assume that 0.15% of the population is infected.
    - Denote by X and T the random variables associated with the
      health status of the patient and the outcome of the test
      respectively.
    - We are interested in p(X = HIV+|T = HIV+).
    - Note that often our tests may not be conditionally independent
      and we would need to take this into account.
    - p(X = HIV+|T = HIV+) = p(T = HIV+|X = HIV+) p(X = HIV+) / p(T = HIV+)
    - p(T = HIV+) = \sum p(T = HIV+, x) = \sum p(T = HIV+|x)p(x)
    - The corresponding expression yields:

      p(T = HIV+|X = HIV+, A)p(X = HIV+|A) / p(T = HIV+|A)

      Here we simply conditioned all random variables on A in order to
      take additional information into account.
    - What we want is that the diagnosis of T_{2} is independent of that
      of T_{2} given the health status X of the patient. This is
      expressed as
      p(t_{1} , t_{2} |x) = p(t_{1} |x)p(t_{2} |x). (1.16)
    - Random variables satisfying the condition (1.16) are commonly
      referred to as /conditionally independent/.
    - (ends 1.2.4.1)
    - assume that there is sufficiently strong dependence between x
      and y that we will be able to estimate y given x and a set of
      labeled instances X, Y.
    - (ends 1.3.0)
    - estimate of p(y), that is, the probability of receiving a spam
      or ham mail.
    - dispose of the requirement of knowing p(x) by settling for a
      likelihood ratio
      L(x) := p(spam|x) / p(ham|x) = p(x|spam)p(spam) / p(x|ham)p(ham)
    - treat the occurrence of each word in a document as a separate
      test and combine the outcomes in a naive fashion by assuming
      that
      p(x|y) = \Pi_{j=1}^{# of words in x} p(w^{j} |y)
    - p(w|y) can be obtained, for instance, by simply counting the
      frequency occurrence of the word within documents of a given
      class.
    - since we are computing a product over a large number of factors
      the numbers might lead to numerical overflow or underflow. This
      can be addressed by summing over the logarithm of terms rather
      than computing products.
    - need to address the issue of estimating p(w|y) for words w which
      we might not have seen before. One way of dealing with this is
      to increment all counts by 1.
    - perform surprisingly well
    - (ends 1.3.1)
    - Note that nearest neighbor algorithms can yield excellent
      performance when used with a good distance measure.
    - lemma by Johnson and Lindenstrauss [DG03] asserts that a set of
      m points in high dimensional Euclidean space can be projected
      into a O(log m/\epsilon 2 ) dimensional Euclidean space such that the
      distance between any two points changes only by a factor of (1 ±
      \epsilon). [...] The surprising fact is that the projection relies on a
      simple randomized algorithm.
    - (ends 1.3.2)
    - In general, we may pick *arbitrary maps* φ : X → H mapping the
      space of observations into a feature space H, as long as the
      latter is endowed with a dot product (see Figure 1.21). This
      means that instead of dealing with <x, x'> we will be dealing
      with <φ(x), φ(x')>.
    - (ends 1.3.3)
    - a user might want to have instant results whenever a new e-mail
      arrives and he would like the system to learn immediately from
      any corrections to mistakes the system makes.
    - the Perceptron is a linear classifier
    - if ŷ_{t} \neq y_{t} the weight vector is updated as
      w ← w + y_{t} x_{t} and b ← b + y_{t}
    - If the dataset (X, Y) is linearly separable, then the Perceptron
      algorithm eventually converges and correctly classifies all the
      points in X.
    - Definition 1.6 (*Margin*) Let w ∈ R d be a weight vector and let
      b ∈ R be an offset. The margin of an observation x ∈ R d with
      associated label y is

      \gamma(x, y) := y (<w, x> + b) (1.27)

      Moreover, the margin of an entire set of observations X with
      labels Y is

      γ(X, Y) := min_{i} γ(x_{i} , y_{i}) (1.28)
    - Geometrically speaking (see Figure 1.22) the margin measures the
      distance of x from the hyperplane defined by {x| <w, x> + b = 0}.
    - Theorem 1.7 (*Novikoff’s theorem*) Let (X, Y) be a dataset with
      at least one example labeled +1 and one example labeled −1. Let
      R := max_{t} ||x_{t}||, and assume that there exists (w^{∗}, b^{∗}) such
      that ||w^{∗}|| = 1 and γ_{t} := y_{t} (<w^{∗}, x_{t}>) + b^{∗} ) ≥ γ for
      all t. Then, the Perceptron will make at most (1+R^{2})(1+(b^{*})^2)/γ^{2}
      mistakes.
    - a similar bound can be shown for Support Vector Machines [Vap95]
    - (ends 1.3.4)
    - define prototype vectors μ_{1} , ... , μ_{k} and an indicator vector
      r_{ij} which is 1 if, and only if, x_{i} is assigned to cluster j. To
      cluster our dataset we will minimize the following *distortion
      measure*, which minimizes the distance of each point from the
      prototype vector:

      J(r, μ) := 1/2 \sum_{i=1}^{m} \sum_{j=1}^{k} r_{ij} ||x_{i} - \mu_{j}||^{2}

      where r = {r_{ij}}, μ = {μ_{j}}
    - adapt a two stage strategy
      - Stage 1 Keep the μ [prototype vectors] fixed and determine r.[...]
	The solution for the i-th data point x_{i} can be found by setting:

	r_{ij} = 1 if j = argmin_{j'} ||x_{i} − μ_{j'}||^{2} (1.30)

        and 0 otherwise.
      - Stage 2 Keep the r fixed and determine μ. [...]  [J] can be
        minimized by setting the derivative with respect to μ_{j} to be
        0:

	\sum_{j=1}^{m} r_{ij} (x_{i} - \mu_{j}) = 0 for all j. (1.31)

        Rearranging obtains \mu_{j} = \sum_{i} r_{ij}x_{i} / \sum_{i} r_{ij} (1.32)

	Since \sum_{i} r_{ij} counts the number of points assigned to cluster
        j, we are essentially setting μ_{j} to be the sample mean of the
        points assigned to cluster j.
      - Two issues [...]
	1. sensitive to the choice of the initial cluster centers μ.
	2. hard assignment of every point to a cluster center.
      - (ends 1.5)
      - Before crying foul you decide that some mathematical analysis
        is in order.
      - limit theorems [...] tell us by how much averages over a set
        of observations may deviate from the corresponding
        expectations and how many observations we need to draw to
        estimate a number of probabilities reliably.
      - (ends 2.0)
      - Theorem 2.1 (Weak Law of Large Numbers) Denote by X_{1} , ... ,
        X_{m} random variables drawn from p(x) with mean μ = E_{X_{i}} [x_{i}]
        for all i. Moreover let

        X̄_{m} := 1/m \sum_{i=1}^{m} X_{i} (2.2)

        be the empirical average over the random variables X_{i} . Then
        for any \epsilon > 0 the following holds

        lim_{m \to \infty} Pr (|X̄_{m} − μ| ≤ \epsilon = 1). (2.3)
      - Theorem 2.2 (Strong Law of Large Numbers) Under the conditions
        of Theorem 2.1 we have Pr(lim_{m→∞} X̄_{m} = μ) = 1.
      - Theorem 2.3 (Central Limit Theorem) Denote by X_{i} independent
        random variables with means μ_{i} and standard deviation σ_{i}. Then

	Z_{m} := m [ \sum_{i=1}^{m} \sigma_{i}^{2} ]^{- 1/2} [ \sum_{i=1}^{m} X_{}_{i} - \mu_{i}^{} ] (2.4)

	converges to a Normal Distribution with zero mean and unit
        variance.
      - Theorem 2.4 (Slutsky’s Theorem) Denote by X_{i} , Y_{i} sequences of
        random variables with X_{i} → X and Y_{i} → c for c ∈ R in
        probability. Moreover, denote by g(x, y) a function which is
        continuous for all (x, c). In this case the random variable
        g(X_{i}, Y_{i}) converges in probability to g(X, c).
      - Theorem 2.5 (Delta Method) Assume that X_{n} ∈ R^{d} is
        asymptotically normal with a_{n}^{-2}(X_{n} - b) \to N(0, \Sigma) for a_{n}
        → 0. Moreover, assume that g : R^{d} → R^{l} is a mapping which is
        continuously differentiable at b. In this case the random
        variable g(X_{n}) converges

	a_{n}^{-2}(g(X_{n}) - g(b)) \to N(0, [∇_{x} g(b)]Σ[∇_{x} g(b)]^{T}) (2.5)
      - The Fourier transform plays a crucial role [...] in
        statistics. For historic reasons its applications to
        distributions is called *the characteristic function*.
      - Definition 2.6 (*Fourier Transform*) Denote by f : R^{n} → C a
        function defined on a d-dimensional Euclidean space. Moreover,
        let x, ω ∈ R^{n} . Then the Fourier transform F and its inverse
        F^{-1} are given by

        F [f](ω) := (2π)^{−d/2} \int_{R^{n}} f(x) exp(-i<\omega, x>)dx (2.7)

        F^{-1} [g](ω) := (2π)^{−d/2} \int_{R^{n}} g(\omega) exp(i<\omega, x>)d\omega (2.8)
      - Definition 2.7 (*Characteristic Function*) Denote by p(x) a
        distribution of a random variable X ∈ R^{d}. Then the
        characteristic function φ_{X} (ω) with ω ∈ R^{d} is given by

        φ_{X} (ω) := (2π)^{d/2} F^{−1} [p(x)] = \int exp(i <ω, x>)dp(x). (2.9)
      - Theorem 2.8 (*Sums of random variables and convolutions*)
        Denote by X, Y ∈ R two independent random variables. Moreover,
        denote by Z := X + Y the sum of both random variables. Then
        the distribution over Z satisfies p(z) = p(x) ◦
        p(y). Moreover, the characteristic function yields:

        φ_{Z} (ω) = φ_{X}(ω)φ_{Y}(ω). (2.10)
      - ∇^{n}_{ω} φ_{X}(0) = i^{−n} E_{X} [x^{n}]. (2.12)

      - if we know the moments of a distribution we are able to
        reconstruct it directly since it allows us to reconstruct its
        characteristic function.
      - (ends 2.1.2)
      - Theorem 2.9 (*Gauss-Markov*) Denote by X ≥ 0 a random variable
        and let μ be its mean. Then for any \epsilon > 0 we have

	Pr(X ≥ \epsilon) ≤ \mu / \epsilon (2.13).

	This means that for random variables with a small mean, the
        proportion of samples with large value has to be small.
      - Theorem 2.10 (*Chebyshev*) Denote by X a random variable with
        mean μ and variance σ^{2} . Then the following holds for \epsilon > 0:

        Pr(|x − μ| ≥ \epsilon) \le \sigma^{2} / \epsilon^{2} (2.14)
      - Theorem 2.12 (*Hoeffding*) Denote by X_{i} iid random variables
        with bounded range X_{i} ∈ [a, b] and mean μ. Let X̄_{m} := m^{−1}
        \sum_{i=1}^{m} X_{i} be their average. Then the following bound holds:

        Pr(|X̄_{m} − μ| > \epsilon) ≤ 2 exp(-2m\epsilon^{2} / (b-a)^{2}) (2.15)
      - (ends 2.1.3)
      - The convergence theorems discussed so far mean that we can use
        empirical observations for the purpose of density estimation.
      - Example 2.4 (Curse of Dimensionality) Assume that X = {0, 1} d
        , i.e. x consists of binary bit vectors of
        dimensionality d. As d increases the size of X increases
        exponentially, requiring an exponential number of observations
        to perform density estimation. For instance, if we work with
        images, a 100 × 100 black and white picture would require in
        the order of 10^{3010} observations to model such fairly
        low-resolution images accurately. This is clearly utterly
        infeasible — the number of particles in the known universe is
        in the order of 10^{80} . Bellman [Bel61] was one of the first to
        formalize this dilemma by coining the term ’curse of
        dimensionality’.
      - (ends 2.2.1)
      - p\circ(x) = 1/m \sum_{i=1}^{m} r^{-d}h(x - x_{i}/r)

	This expansion is commonly known as the /Parzen windows/ estimate.
      - h(x) = (2π)^{−1/2} e^{−1/2 x^2} /Gaussian kernel/
      - h(x) = 3/4 max(0, 1 − x^2) /Epanechnikov kernel/
      - For practical purposes the Gaussian kernel (2.28) or the
        Epanechnikov kernel (2.30) are most suitable.
      - /compact support/ [...] for any given density estimate at
        location x we will only need to evaluate terms h(x_{i} − x) for
        which the distance ||x_{i} − x|| is less than r.
      - (ends 2.2.2)
      - choose r such that the /log-likelihood of the data/ is
        maximized. It is given by

	log \Pi_{j=0}^m p(x_{j}) = −m log m + \sum_{j=1}^m log (m \cdot p\circ(x_{j}))
      - (ends 2.2.3)
      - Provided that we are able to compute density estimates p(x)
        given a set of observations X we could appeal to Bayes rule to
        obtain

	p(y|x) = p(x|y)p(y)/p(x)
        = ((m_{y} / m) \cdot 1/m_{y} \sum_{i:y_i=y} r^{-d} h(x_{i}-x/r) / 1/m \sum_{i=1}^{m} r^{-d} h(x_{i}-x/r) ) (2.38)
      - For binary classification (2.38) can be simplified [...] the function

	f (x)
        = \sum_{i} y_{i} h(x_{i}-x/r) / \sum_{i} h(x_{i}-x/r)
	= \sum_{i} y_{i} (h(x_{i}-x/r) / \sum_{i} h(x_{i}-x/r))
	=: \sum_{i} y_{i} w_{i}(x) (2.39)

	can be used to achieve the same goal since

        f (x) > 0 ⇐⇒ p(y = 1|x) > 0.5
      - (ends 2.2.5)
      - 
      - The reason why many discrete processes follow the Poisson
        distribution is that it can be seen as the limit over the
        average of a large number of Bernoulli draws
** TODO [#B] [[./cacr2015-09.pdf][Wang - On Realistically Attacking Tor with Website Fingerprinting]]
*** summary
    0) [@0] Abstract

       - laboratory vs real

	 - training data similar to testing data

	 - single page each

	 - noise (not studied)

       - to real

	 - maintain fresh training set

	 - split traces

	 - users cannot generate noise (?)

    1) INTRODUCTION

       - 6 laboratory assumptions

	 1) template websites

	    but only used by cai

	 2) closed-world

	    but: expanded to open-world f.ex. by w&g

	 3) replicability

	    recording conditions and data freshness

	 4) behavior: sequential one-at-a-time browsing

	 5) parsing/splitting data: possible to split

	 6) no background traffic: adversary can filter

       - 3 addressed by

	 - needs only small amount of data

	 - easy to keep fresh

	 - update method

       - 4+5 addressed by splitting (time- and classification-based)

       - 6 hard problem, but hard to generate bg noise

       - no new classifier

    2) RELATED WORK
*** quotes
    - (begins 1)
    - it is very hard to generate sufficient background noise on Tor
      to disrupt WF due to the design of the Tor Browser.
    - (ends 1)
    - This is the hardest class to split as there is no noticeable gap
      nor a clear pattern of cells indicating the gap.
** TODO [#B] [[./usersrouted-ccs13.pdf][Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries]]
*** summary
    0) [@0] ABSTRACT

       Tor is more vulnerable than previously thought.

       - security evaluation based on

         - different user models

	 - ASes etc

	 - paths

    1) INTRODUCTION
*** quotes
    - Specific contributions of the paper include (1) a model of
      various typical kinds of users, (2) an adversary model that
      includes Tor network relays, autonomous systems (ASes), Internet
      exchange points (IXPs), and groups of IXPs drawn from empirical
      study, (3) metrics that indicate how secure users are over a
      period of time, (4) the most accurate topological model to date
      of ASes and IXPs as they relate to Tor usage and network
      configuration, (5) a novel realistic Tor path simulator (TorPS),
      and (6) analyses of security making use of all the above.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ccs2013-usersrouted,
        title = {Users Get Routed: Traffic Correlation on Tor by Realistic Adversaries},
        author = {Aaron Johnson and Chris Wacek and Rob Jansen and Micah Sherr and Paul
              Syverson},
        booktitle = {Proceedings of the 20th ACM conference on Computer and Communications
              Security (CCS 2013)},
        year = {2013},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://www.ohmygodel.com/publications/usersrouted-ccs13.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#B] [[~/da/git/docs/timing-fc2004.pdf][Timing Attacks in Low-Latency Mix Systems]]
** TODO [#B] [[./webtraffic-imc11.pdf]]
** TODO [#C] [[./Oya.pdf][Do dummies pay off ? Limits of dummy traffic protection in anonymous communications]]
*** summary
    0) [@0] Abstract

       Communication patterns recognizable

       Dummy evaluated

    1) Introduction

       - least squares approach

       - model adversary error

       - design guidelines

    2) System and Adversary Model

       - mix-based

       - 4 stages

	 1) discard dummies

	 2) Pool: select the ones to send

	 3) Mix: change from input

	 4) add new dummies

       - Global passive adversary

	 - goal: determine communication channel probabilities

    3) A Least Square Profile Estimator for Dummy-basedAnonymization
       Systems
*** quotes
    - In practice, user behavior and latency constrain the composition
      of anonymity sets
    - Another shortcoming of previous works [5, 9, 10] is that the
      proposed evaluation strategies cannot be used to guide the
      design of effective dummy generation strategies, which is
      recognized to be a hard problem [11]. This has lead the deployed
      high latency anonymous communication systems to either implement
      arbitrary dummy strategies [12] or no dummy traffic at all [11].
    - (ends 1)
    - capital letters to denote random variables andlower-case letters
      to denote their realizations.
    - high-latency mix-based anonymous communication channel
    - (ends 2)
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{pets14-dummy-traffic,
        title = {Do dummies pay off? Limits of dummy traffic protection in anonymous
              communications},
        author = {Simon Oya, Carmela Troncoso and Fernando P{\'e}rez-Gonz\'alez},
        booktitle = {Proceedings of the 14th Privacy Enhancing Technologies Symposium (PETS
              2014)},
        year = {2014},
        month = {July},
        www_pdf_url = {https://www.petsymposium.org/2014/papers/Oya.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis,Anonymous communication},
      }
    #+END_SRC
** TODO [#C] [[./webfingerprint-wpes.pdf][Improved Website Fingerprinting on Tor]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{wpes13-fingerprinting,
        title = {Improved Website Fingerprinting on Tor},
        author = {Tao Wang and Ian Goldberg},
        booktitle = {Proceedings of the Workshop on Privacy in the Electronic Society (WPES
              2013)},
        year = {2013},
        month = {November},
        location = {Berlin, Germany},
        publisher = {ACM},
        www_tags = {selected},
        www_pdf_url = {http://www.cypherpunks.ca/~iang/pubs/webfingerprint-wpes.pdf},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [#C] [[./irbookonlinereading.pdf][An Introduction to Information Retrieval]]
*** summary
    0. [@0] Preface

       - from specialized academic study to www

       - Book organization and course development

	 - chapter: 75 to 90 minutes

	 - 1-8 foundation topics

       - Prerequisites

       - Book layout

	 - pencil: worked examples

	 - scissors: advanced, can leave out

	 - ? exercise, with difficulty * to ***

       - Acknowledgments

    1. Boolean retrieval
*** quotes
    - Chapter 10 considers information retrieval from documents that
      are structured with markup languages like XML and HTML.
    - (ends 0)
    - Most commonly, the edit operations allowed for this purpose are:
      (i) insert a character into a string; (ii) delete a character
      from a string and (iii) replace a character of a string by
      another character; for these operations, edit distance
      is sometimes known as *Levenshtein distance*.
    - EDIT DISTANCE ( s_{1} , s_{2} )
      1. int m [ i, j ] = 0
      2. for i ← 1 to | s_{1} |
      3. do m [ i, 0 ] = i
      4. for j ← 1 to | s_{2} |
      5. do m [ 0, j ] = j
      6. for i ← 1 to | s_{1} |
      7. do for j ← 1 to | s_{2} |
      8. do m [ i, j ] = min { m [i−1, j−1]
	 + if ( s1[i] = s2[j]) then 0 else 1 fi,
      9. m [ i − 1, j ] + 1,
      10. m [ i, j − 1 ] + 1 }
      11. return m [| s_{1} | , | s_{2} |]
    - (ends 3.3)
    - *Contiguity hypothesis*. Documents in the same class form a
      contiguous region and regions of different classes do not
      overlap.
    - Nonlinear models have more parameters to fit on a limited amount
      of training data and are more likely to make mistakes for small
      and noisy data sets.
    - (ends 14.0)
** TODO [#C] [[~/da/git/docs/onion.pdf][Anonymous Connections and Onion Routing]]
*** summary
    0) [@0] Abstract

       - "Onion Routing is an infrastructure for private
	 communication over a public network.

       - Eavesdropping and traffic analysis are hampered by /onion
	 routing/.

    1) Introduction

       - traffic analysis extracts data from who talks to whom
*** quotes
    - web based shopping or browsing of public databases should not
      require revealing one's identity
    - our implementation of anonymous connections, onion routing,
      provides protection against eavesdropping as a side effect
*** ref

*** [[~/da/git/docs/onion-discex00.pdf][Onion Routing Access Configurations]]
    see also bibtex
** TODO [#C] [[./xxx-multihop-padding-primitives.txt][Multihop Padding Primitives]]
** TODO [#C] [[./wfpadtools-spec.txt][WFPadTools Protocol Specification]]
** TODO [#C] [[./ethical.html][Ethical Tor Research: Guidelines]]
*** summary
*** quotes
*** ref
    #+BEGIN_SRC bibtex
      @misc{ethical,
        tag = "The Tor Project",
        title = "Ethical Tor Research: Guidelines",
        url = "\url{https://blog.torproject.org/blog/ethical-tor-research-guidelines}",
        note = "[Online; accessed 11-November-2015]"
      }
    #+END_SRC
** TODO [#C] [[/home/chive/own/tor/doc/read/tor-spec.txt][Tor Protocol Specification]]
*** summary
    0) [@0] Preliminaries: MUST, etc. keywords like RFC
       1) Notation: PK, SK, K, a|b, [a b c], H(m)
	  1) all multibyte values are big-endian
       2) Security: KEY_LEN, PK_ENC_LEN, PK_PAD_LEN, DH_LEN,
          DH_SEC_LEN, HASH_LEN, PAYLOAD_LEN, CELL_LEN(v)
       3) Ciphers
	  - stream: 128bit counter-mode AES with IV=0x0...0
	  - pk: 1024 bit AES with exponent of 65537
	    0aef-mgf1 padding, sha1 digest, label unset
	  - ntor: curve25519
	  - dh generator 2, modulus from rfc2409
	    SHOULD private keys of 320 bits, no reuse
	  - hybrid encryption
    1) System overview: TOR low-latency tcp distributed overlay
       1) Keys and names
	  - long-term *identity* key, signing only
	  - medium-term *onion*-key, keep at least one week after
            advertisement
	  - (short-term TLS *connection* key, rotate at least once a day)
    2) Connections
       - link layer: 3 ways
         1. "certificates up-front"
	    both send two-certificate chain
  	    - init: short-term X.509 cert + self-signed identity X.509
	    - resp: similar
	    - MAY ONLY INCLUDE
	      - TLS_DHE_RSA_WITH_AES_256_CBC_SHA
	      - TLS_DHE_RSA_WITH_AES_128_CBC_SHA
	      - SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA
	      jeder zwei x.509 Zertifikate: eines fuer connection, eines
              fuer identity DARF NUR:
              - TLS_DHE_RSA_WITH_AES_256_CBC_SHA
              - TLS_DHE_RSA_WITH_AES_128_CBC_SHA
              - SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA
	 2. "renegotiation" (tor >= 0.2.0.21)
   	    responder single certificate, 
	    initiator renegotiates
	    - init: no cert
	    - resp: connection cert
	    - ... (continue handshake)
	    - init: renegotiate
	    - MUST HAVE ONE NOT IN ABOVE
	 3. "in-protocol" (tor >= 0.2.3.6-alpha)
	    distinguish vs v2 via one of:
	    - self-signed cert
	    - commonName ends other than ".net"
	    - public key modulus >= 1024 bits
	    then: send VERSIONS cell, ...
       - several security /accessibility features:
         - fixed protocol list
         - fixed choice of response
         - no server without valid certificates
    3) Cell Packet Format
       ...
    4) 
    5) 
       1) 
       2) 
       3) 
       4) 
       5) Routing relay cells
	  checks circID, crypts payload, inspect payload (6.1), work it
    6) Application connections and stream management
       1) Relay cells
       2) 
	  ...
    7) FLow control
       1) Link throttling
*** quotes
   - Tor relays are also identified by "nicknames"; these are specified in
     dir-spec.txt.
   - <<O1>> All implementations MUST support the SSLv3 ciphersuite
     "SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA", and SHOULD support the TLS
     ciphersuite "TLS_DHE_RSA_WITH_AES_128_CBC_SHA" if it is
     available.
   - In all of the above handshake variants, certificates sent in the
     clear SHOULD NOT include any strings to identify the host as a
     Tor relay. In the "renegotiation" and "backwards-compatible
     renegotiation" steps, the initiator SHOULD choose a list of
     ciphersuites and TLS extensions to mimic one used by a popular
     web browser.
   - TLS connections are not permanent.
   - Each client or relay should do appropriate bandwidth throttling to
     keep its user happy. [...]
     The mainline Tor implementation uses token buckets (one for
     reads, one for writes) for the rate limiting.

*** up to
    - 4. Negotiating and initializing connections
    - read to
      - 4.4. AUTHENTICATE cells
** TODO [[./csbuflo.pdf][Cai - CS-BuFLO: A Congestion Sensitive Website Fingerprinting Defense]]
*** quotes
    - Our experiments find that Congestion-Sensitive BuFLO has high
      overhead (around 2.3-2.8x)
    - it is not currently known whether there exists any efficient and
      secure defense against website fingerprinting attacks.
    - all previously-proposed defenses provide little security.
*** summary
    0) [@0] Abstract:

       - fingerprint infers, even if tor,

       - previous defenses are ineffective,

       - spec of cs-buflo,

       - implementation

    1) Introduction

       - several website fingerprinting attacks, several defenses

       - 80%, which of 128 pages

       - BuFLO: over 400% bandwidth overhead

       - DLSVM fingerprinting attack greater than 75% success rate
         against numerous defenses, including application-level
         defenses, such as HTTPOS and randomized pipelining

       - CS-BuFLO, congestion avoidance, TCP-friendly,

       - here: adapt its transmission rate dynamically, and improve
         its stream padding: less bandwidth, hiding more

       - adapting too quickly can reveal info, solve: limit adaptation

       - alexa 200: 91% of web use

       - CS-BuFLO: 2.8 times as much bandwidth as SSH, only a 20% success rate

       - CS-BuFLO ratio 2.8. BuFLO ratio of 2.2.

    2) RELATED WORK

       - dyer: lists stuff like padM, padE, ...

       - wright: morphing

       - dyer defeats

       - Lu extends morphing

       - Dyer BuFLO

       - Fu: CPU load changes, recommend randomized intervals

       - HTTPOS

       - Tor randomized

       - Cai defeated

       - many attacks agains https, ipsec, vpn, etc

       - herrmann: tunnels attack, fails on tor

       - panchenko

       - dyer vng++

       - cai string edit distance

       - wang improved

       - danezis, yu, cai hmm to extend to web site fp

    3) WEBSITE FINGERPRINTING ATTACKS
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{wpes14-csbuflo,
        title = {{CS-BuFLO}: A Congestion Sensitive Website Fingerprinting Defense},
        author = {Xiang Cai and Rishab Nithyanand and Rob Johnson},
        booktitle = {{Proceedings of the 12th Workshop on Privacy in the Electronic Society
              (WPES)}},
        year = {2014},
        month = {November},
        www_tags = {selected},
        www_pdf_url = {http://pub.cs.sunysb.edu/~rob/papers/csbuflo.pdf},
        www_section = {Anonymous communication},
      }
    #+END_SRC
** TODO [[./Liberatore_2006.pdf][Inferring the Source of Encrypted HTTP Connections]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{Liberatore:2006,
        title = {{Inferring the Source of Encrypted HTTP Connections}},
        author = {Marc Liberatore and Brian Neil Levine},
        booktitle = {Proceedings of the 13th ACM conference on Computer and Communications
              Security (CCS 2006)},
        year = {2006},
        month = {November},
        pages = {255--263},
        www_tags = {selected},
        www_section = {Traffic analysis},
        www_pdf_url = {http://prisms.cs.umass.edu/brian/pubs/liberatore.ccs2006.pdf},
      }
    #+END_SRC
** TODO [[./ipccc12-tor-performance.pdf][Improving Performance and Anonymity in the Tor Network]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{ipccc12-performance,
        title = {Improving Performance and Anonymity in the Tor Network},
        author = {Andriy Panchenko and Fabian Lanze and Thomas Engel},
        booktitle = {Proceedings of the 31st IEEE International Performance Computing and
              Communications Conference (IPCCC 2012)},
        year = {2012},
        month = {December},
        www_tags = {selected},
        www_pdf_url = {http://lorre.uni.lu/~andriy/papers/ipccc12-tor-performance.pdf},
        www_section = {Anonymous communication,Tor Performance},
      }
    #+END_SRC
** TODO [[./pam2014-tor-nfattack.pdf][On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow Records]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{nfattackpam14,
        title = {On the Effectiveness of Traffic Analysis Against Anonymity Networks Using Flow
              Records},
        author = {S. Chakravarty and M. V. Barbera and G. Portokalidis and M. Polychronakis and
              A. D. Keromytis},
        booktitle = {Proceedings of the 15th Passive and Active Measurements Conference (PAM
              '14)},
        year = {2014},
        month = {March},
        www_pdf_url = {http://www.cs.columbia.edu/~sc2516/papers/pam2014-tor-nfattack.pdf},
        www_tags = {selected},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[./10.1.1.10.5823.pdf][Quantifying Traffic Analysis of Encrypted Web-Browsing]]
** TODO [[./hintz02.pdf][Fingerprinting Websites Using Traffic Analysis]]
*** summary
    0) [@0] Abstract

       Attack to find out whether user is visiting certain websites,
       even though he uses an encrypted proxy. 

       Plus discussion: better attack and defenses

    1) Introduction

       With normal encryption, metadata is visible. 

       With one-hop proxies, metadata is discoverable.

       Tere are several defenses.

    2) Definition of Traffic Analysis

       sender, receiver, amount of data transferred (ssl does not try
       to obfuscate)
*** quotes
    - The process of monitoring the nature and behavior of traffic,
      rather than its content, is known as traffic analysis.
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{hintz02,
        title = {Fingerprinting Websites Using Traffic Analysis},
        author = {Andrew Hintz},
        booktitle = {Proceedings of Privacy Enhancing Technologies workshop (PET 2002)},
        year = {2002},
        month = {April},
        editor = {Roger Dingledine and Paul Syverson},
        publisher = {Springer-Verlag, LNCS 2482},
        www_tags = {selected},
        www_html_url = {http://guh.nu/projects/ta/safeweb/safeweb.html},
        www_pdf_url = {http://guh.nu/projects/ta/safeweb/safeweb.pdf},
        www_ps_url = {http://guh.nu/projects/ta/safeweb/safeweb.ps},
        www_section = {Traffic analysis},
      }
    #+END_SRC
** TODO [[https://www.torproject.org/projects/torbrowser/design/#proxy-obedience][The Design and Implementation of the Tor Browser]]
   - [[./browser.html][local copy]]
*** summary
    1) Introduction

       adversary mode, design requirements and implementation of 4.5

       1) Browser Component Overview

	  based on firefox esr, patches, torbutton, pref changes

	  tor laucher addon: splash screen & progrss bar

	  https-everywhere, noscript, extension prefs changed:  https://gitweb.torproject.org/builders/tor-browser-bundle.git/tree/Bundle-Data/linux/Data/Browser/profile.default/preferences/extension-overrides.js

	  pluggable transports

    2) Design Requirements and Philosophy
*** quotes
    - Unfortunately, the bias in favor of compelling attack papers has
      caused academia to ignore this request thus far, instead
      publishing only cursory (yet "devastating") evaluations that
      fail to provide even simple statistics such as the rates of
      actual pipeline utilization during their evaluations, in
      addition to the other shortcomings and shortcuts mentioned earlier.
    - These shortcomings and fallback behaviors are the primary reason
      that Google developed SPDY as opposed to simply extending HTTP
      to improve pipelining.
** TODO [[./ronathan-heyning.pdf][Traffic Analysis of SSL Encrypted Web Browsing]]
** TODO [[https://trac.torproject.org/projects/tor/ticket/8470#comment:7][#8470 Request randomization a lot less random in FF17]]
   local: [[file:///home/w00k/da/docs/lessrand.html]]
*** quotes
    - [...] the browser is often too slow to be able to keep the
      pipeline full of requests, and requests aren't packed together
      in Tor cells. This might be important, because it may also be
      the case that a browser that is driven around by selenium in a
      VM might be similarly too slow for pipelining's request
      combining to happen.
** TODO [[./tor-design.pdf][Tor: The Second-Generation Onion Router]]
*** quotes
  - Clients choose a path through the network and build a circuit, in
    which each node (or “onion router” or “OR”) in the path knows its
    predecessor and successor, but no other nodes in the circuit.
  - Rather than using a single multiply encrypted data structure (an
    onion) to lay each circuit, Tor now uses an incremental or
    telescoping path-building design, where the initiator negotiates
    session keys with each successive hop in the circuit.
  - Tor multiplexes multiple TCP streams along each circuit to improve
    efficiency and anonymity.
  - Tor’s decentralized congestion control uses end-to-end acks to
    maintain anonymity while allowing nodes at the edges of the
    network to detect congestion or flooding and send less data until
    the congestion subsides.
  - Certain more trusted nodes act as directory servers: they provide
    signed directories describing known routers and their current
    state.
  - Variable exit policies: Tor provides a consistent mechanism for
    each node to advertise a policy describing the hosts and ports to
    which it will connect. These exit policies are critical in a
    volunteer-based distributed infrastructure, because each operator
    is comfortable with allowing different types of traffic to exit
    from his node.
  - End-to-end integrity checking
  - Modern anonymity systems date to Chaum’s Mix-Net design [10].
  - Tor as described herein, Tarzan, MorphMix, Cebolla [9], and
    Rennhard’s Anonymity Network [44] build circuits in stages,
    extending them one hop at a time. Section 4.2 describes how this
    approach enables <<perfect forward secrecy>>.
  - by treating application connections as data streams rather than
    raw TCP packets, they avoid the inefficiencies of tunneling TCP
    over TCP.
  - Distributed-trust anonymizing systems need to prevent attackers
    from adding too many servers and thus compromising user paths. Tor
    relies on a small set of well-known directory servers, run by
    independent parties, to decide which nodes can join.
  - Each onion router maintains a TLS [17] connection to every other
    onion router.
  - Actually, the negotiated key is used to derive two symmetric keys:
    one for each direction.
  - Preliminary analysis with the NRL protocol analyzer [35] shows
    this protocol to be secure (including perfect forward secrecy)
    under the traditional <<Dolev-Yao>> model.
  - Once Alice has established the circuit (so she shares keys with 
    each OR on the circuit), she can send relay cells.
  - (as an optimization, the first two bytes of the integrity check 
    are zero, so in most cases we can avoid computing the hash). 
  - Thus the “<<break a node and see which circuits go down>>” attack [4]
    is weakened.
  - (usually the last node, but maybe others due to exit policy
    conflicts; see Section 6.2.)
  - Because Tor uses TLS on its links, external adversaries cannot
    modify data. Addressing the insider malleability attack, however,
    is more complex.
  - given that the OP or OR tear down the circuit if they receive a
    bad hash.
  - a circuit’s edges can heuristically distinguish interactive
    streams from bulk streams by comparing the frequency with which
    they supply cells.
  - If enough users choose the same OR-to-OR connection for their cir-
    cuits, that connection can become saturated.
  - These arbitrarily chosen parameters seem to give tolerable
    throughput and delay; see Section 8.
  - This type of anonymity protects against distributed DoS attacks:
    attackers are forced to attack the onion routing network because
    they do not know Bob’s IP address.
  - thus hostnames take the form x.y.onion where x is the
    authorization cookie and y encodes the hash of the public key.
  - Rather than searching exit connections for timing and volume
    correlations, the adversary may build up a database of
    “fingerprints” contain- ing file sizes and access patterns for
    targeted websites. He can later confirm a user’s connection to a
    given site simply by consulting the database. This attack has been
    shown to be effective against SafeWeb [29].
  - On the other hand, an attacker who learns a node’s identity key
    can replace that node indefinitely by sending new forged descrip-
    tors to the directory servers.<<<permanent identity key>>>
  - The best defense here is robustness.
  - Our threat model explicitly assumes directory server operators
    will be able to ﬁlter out most hostile ORs.
  - once we have more experience, and assuming we can resolve the
    anonymity issues, we may partition traffic into two relay cell
    sizes: one to handle bulk traffic and one for interactive traffic.
  - Second, our end-to-end congestion control algorithm focuses on
    protecting volunteer servers from accidental DoS rather than on
    optimizing performance.
  - On the other hand, as our users remain satisﬁed with this
    increased latency, we can address our performance incrementally as
    we proceed with development.
  - [...] we still expect the network to support a few hundred nodes
    and maybe 10,000 users before we’re forced to become more
    distributed.
  - Throughout this paper, we have assumed that end-to-end trafﬁc
    conﬁrmation will immediately and automatically defeat a
    low-latency anonymity system.

*** summary
    1) 
    2) Related work
       
       The first design ever was called Mix-Net. From there, two
       directions diverged. The one was highly anonymized,
       high-latency. The other tried to minimize latency. Among these,
       there are IP, TCP and application-level (e.g. HTTP)
       filters. TOR does a TCP design that is mostly distributed but
       has some fixed directory structure.

    3) Design goals and assumptions

       TOR's design aims to be easy to use by providing deployability,
       usability, flexibility and a simple design. It does not try to
       hide its traffic, normalize protocols, be 100% p2p or prevent
       end-to-end analysis. It is assumed that an adversary can control
       some onion routers and thus analyze or disturb the network.

       1) Threat Model

	  adversaries can
          - observe fraction of traffic
	  - can generate, modify, delete traffic
	  - operate own ORs
	  - compromise fraction of other ORs

	  they are not global.

    4) The TOR design incorporates encapsulation over SSL (link-layer)
       links.  There are two cells types, for control and
       content. Checksumming, QOS-type rate limiting and congestion
       control are implemented.
       + Every OR has a TLS connection to every other OR. They send
	 each other fixed-size cells each encrypted via
	 quickly-changing TLS keys. The connection is signed via
	 mid-term onion keys. The identity is provided via [[permanent
	 identity key]].
       + TORs messages are either command/control/direct (2b circID, 1b
	 cmd, 509b data) messages to the next OR or relay (2b cirdID,
	 1b relay, 2b streamID, 6b digest, 2b len, 1b cmd, 498b data)
	 messages to some other host. The circuit is created
	 recursively using several hops, each of which creates its own
	 symmetric key pair via DH. Each circuit OR can be asked to
	 open a connection out. circuitID changes from hop to hop
       + 
       + when an OR receives a relay message, it attemps to decrypt the
	 relay header and read the digest. If the digest is correct, it
	 is accepted. Else, it is forwarded (or the stream is killed if
	 at the end)
       + for QoS and fairness, a token-bucket mechanism is used and
	 interactive streams get preferential treatment
       + congestion control is handled via two window mechanisms: one
	 for circuit-level, the other for stream-level-throttling
    5) hidden services in TOR are implemented via introduction pionts:
       - bob advertises these, alice creates a rendezvous point, leaves
	 a message at those intro points, bob connects to this
	 rendezvous point
       - both the server and the client can be used unmodified: the
	 server just behind tor, the client just using its onion proxy
       - there was some previous work, also on anonymizing cellphone
	 usage
    6) miscellany
       - dos might be possible but has not been observed yet
       - exit policies: 
	 - thoughts of adding headers
	 - mixed policies
       - directory servers
	 - cached statements at ORs,
	 - more efficient that just propagating messages
    7) attack & defense
       - passive attacks
	 - traffic confirmation attacks outside of design goal, but
	 - fingerprinting might be effective
	 - user content, option distinguishability, timing
	   correlations, size correlations, fingerprinting, latency
       - active attacks
	 - five different keys can be compromised:
	   tls session, tls cert, circuit session, circuit cert, identity
	 - compromise of the OR itself has to be done in tight timeframe
	 - run recipient, eases end-to-end attacks
	   - approached by privoxy
	 - run onion proxy
	   - no solution
	 - DoS non-observed nodes
	   - best defense robustness
	 - Run hostile OR, timing attacks, tagging cells, replace
	   content, replay attacks, smear tor's name, hostile code
	   (altered TOR software)
       - directory attacks
	 - destroy directory server, subvert directory server, subvert
	   majority, encourage dissent, insert hostile OR, do as if
	   working correctly
       - rendezvous attacks
	 - make many request, attack intro, compromise intro, compromise rend
    8) In da wild
       - works for variety of uses
	 - download fast, latency varies greatly
    9) open questions
       - which path length (static/dynamic), which renewal frequency?
       - cascade / hydra /own OR
       - what if some central directory server do not suffice,
	 non-clique aka restricted-route
       - w/o central authority, how to avoid bad nodes
       - anonymity gains from running own OR to lead to recommendation?
    10) future directions
	- scalability, 
	- bandwidth classes [DSL, T1, T3] for ORs
	- incentives, cover traffic, cachin gat exit nodes, better
          directory distribution, further spec review, multisystem
          interoperability, wider-scale deployment
    11) references
*** questions
    - exit policies for .onion
    - safeweb (? was soll das denn sein??)
    - privoxy email default message on plaintext connect
    - "Our threat model explicitly assumes directory server operators
      will be able to ﬁlter out most hostile ORs."
    - caching at exit nodes: Flos Gedanken
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{tor-design,
        title = {{Tor}: The Second-Generation Onion Router},
        author = {Roger Dingledine and Nick Mathewson and Paul Syverson},
        booktitle = {Proceedings of the 13th USENIX Security Symposium},
        year = {2004},
        month = {August},
        www_important = {1},
        www_tags = {selected},
        www_html_url = {https://svn.torproject.org/svn/projects/design-paper/tor-design.html},
        www_pdf_url = {https://svn.torproject.org/svn/projects/design-paper/tor-design.pdf},
        www_section = {Anonymous communication},
      }
    #+END_SRC
** TODO [[./marionette_client.pdf][Marionette Python Client Documentation]]
*** summary
    0. [@0]

       - remotely control firefox (gecko-based browser)

    1. Getting the Client

       - install via

         #+BEGIN_SRC sh
           pip install marionette_client
         #+END_SRC

       - use virtualenv

    2. Using the Client for Testing

       see Marionette tests section

    3. Session Management

       initialize via

       #+BEGIN_SRC python
         client = Marionette('localhost', port=2828)
         client.start_session()
       #+END_SRC

       session_capabilities holds what the session can do
       (f.ex. rotate window on FF OS)

    4. Context Management
*** quotes
** TODO [[./python-doc-howto-doanddont.pdf][Idioms and Anti-Idioms in Python]]
*** summary
    1) Language Constructs You Should Not Use

       1) from module import *

	  - Inside Function Definitions

	    invalid

	  -
*** quotes
    - [import * inside functions] made the function execution slower,
      because the compiler could not be certain which names were local
      and which were global.
    - You should try to use as few except clauses in your code as you
      can
** TODO [[./python-doc-tutorial.pdf][Python Tutorial]]
*** summary
    0) [@0]

       Python

       - easy

       - powerful

       - high-level data structures

       - dynamic typing

       - \to elegant

       - open source

       - extensible

       - tutorial incomprehensive

       - others: library, reference

    1) WHETTING YOUR APPETITE

       - automate

       - write, compile, test too slow

       - \to has interactive interpreter

       - compact, readable

       - \to very-high-level

       - modular

    2) USING THE PYTHON INTERPRETER

       1) Invoking the Interpreter

	  =python=

	  EOF (C-D) finishes, or =quit=

	  readline adds history features

	  =python -c command=

	  =python -m module=

	  =-i= enters interactive mode afterwards

	  1) Argument Passing

	     - =sys.argv=

	     - no script/arguments: ''

	     - -c: '-c'

	     - -m: module name

	  2) Interactive Mode

	     >>>, resp ...

       2) The Interpreter and Its Environment

	  1) Source Code Encoding

	     "It is possible to use encodings different than ASCII in
             Python source files. The best way to do it is to put one
             more special comment line right after the #! line to
             define the source file encoding:
             #+BEGIN_SRC python
               # -*- coding: encoding -*-
             #+END_SRC

    3) AN INFORMAL INTRODUCTION TO PYTHON

       1) Using Python as a Calculator

	  1) Numbers

	     - floor division: // (default with ints)

	     - ** powers
	       #+BEGIN_SRC python
                 2 ** 7 # 2 to the power of 7
	       #+END_SRC

	     - "In interactive mode, the last printed expression is
               assigned to the variable _."

	     - rounding
	       #+BEGIN_SRC python
                 round(13.0563, 2) # yields 13.05
	       #+END_SRC

	  2) Strings

	     - escape single quote
	       #+BEGIN_SRC python
                 'doesn\'t' # use \' to escape the single quote...
	       #+END_SRC

	     - print erases newline
	       #+BEGIN_SRC python
                 >>> s = 'First line.\nSecond line.' # \n means newline
                 >>> s # without print, \n is included in the output
                 'First line.\nSecond line.'
                 >>> print s # with print, \n produces a new line
                 First line.
                 Second line.
	       #+END_SRC

	     - raw strings
	       #+BEGIN_SRC python
                 >>> print r'C:\some\name' # note the r before the quote
                 C:\some\name
	       #+END_SRC

	     - string combination Strings can be concatenated (glued
               together) with the + operator, and repeated with * :
	       #+BEGIN_SRC python
                 >>> # 3 times 'un', followed by 'ium'
                 >>> 3 * 'un' + 'ium'
                 'unununium'
	       #+END_SRC

	     - combine strings via 'a' 'b':

               "This feature is particularly useful when you want to
               break long strings:"
	       #+BEGIN_SRC python
                 >>> text = ('Put several strings within parentheses '
                 'to have them joined together.')
                 >>> text
                 'Put several strings within parentheses to have them joined together.'
	       #+END_SRC

	     - "Note that since -0 is the same as 0, negative indices
               start from -1."

	     - Slice indices have useful defaults; an omitted first
               index defaults to zero, an omitted second index
               defaults to the size of the string being sliced.
	       #+BEGIN_SRC python
                 >>> word[:2] # character from the beginning to position 2 (excluded)
                 'Py'
                 >>> word[4:] # characters from position 4 (included) to the end
                 'on'
                 >>> word[-2:] # characters from the second-last (included) to the end
                 'on'
	       #+END_SRC

	  3) Unicode Strings

	     - The escape sequence \u0020 indicates to insert the
               Unicode character with the ordinal value 0x0020 (the
               space character) at the given position.
               #+BEGIN_SRC python
                 >>> u'Hello\u0020World !'
                 u'Hello World !'
               #+END_SRC

	  4) Lists

	     - All slice operations return a new list containing the
               requested elements. This means that the following slice
               returns a new (shallow) copy of the list:
	       #+BEGIN_SRC python
                 >>> squares[:]
                 [1, 4, 9, 16, 25]
	       #+END_SRC

	     - replace some values
	       #+BEGIN_SRC python
                 >>> letters[2:5] = ['C', 'D', 'E']
	       #+END_SRC

       2) First Steps Towards Programming

    4) MORE CONTROL FLOW TOOLS

       1) if Statements

	  - if, elif, else

       2) for Statements

	  over iterator, =[:]= for a copy

       3) The range() Function

	  does not include the argument,

	  start, stop, step

       4) break and continue Statements, and else Clauses on Loops

	  - "The break statement, like in C, breaks out of the
            smallest enclosing for or while loop."

	    if loop not broken, =else:= is executed

       5) pass Statements

	  does nothing

	  use cases:

	  - busy-wait

	  - empty class

	  - implement later

       6) Defining Functions

	  - var lookup order:

	    local \to surrounding locals \to global \to builtin

       7) More on Defining Functions

	  different number of parameters

	  1) Default Argument Values

	     #+BEGIN_SRC python
               def func(retries = 4):
	     #+END_SRC

	     if set, takes that, otherwise default

	  2) Keyword Arguments

	     - "When a final formal parameter of the form **name is
               present, it receives a dictionary (see typesmapping)
               containing all keyword arguments except for those
               corresponding to a formal parameter. This may be
               combined with a formal parameter of the form *name
               (described in the next subsection) which receives a
               tuple containing the positional arguments beyond the
               formal parameter list."

	  3) Arbitrary Argument Lists

	     *var sums all others into a tuple

	  4) Unpacking Argument Lists

	     "when the arguments are already in a list or tuple but
             need to be unpacked for a function call requiring
             separate positional arguments. [...] If they are not
             available separately, write the function call with the *
             -operator to unpack the arguments out of a list or tuple:
	     #+BEGIN_SRC python
               >>> args = [3, 6]
               >>> range(*args)
               [3, 4, 5]
	     #+END_SRC

	  5) Lambda Expressions

	     - "Small anonymous functions"

	     - "syntactically restricted to a single expression"

	     - "pass a small function as an argument:"
	       #+BEGIN_SRC python
                 >>> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]
                 >>> pairs.sort(key=lambda pair: pair[1])
                 >>> pairs
                 [(4, 'four'), (1, 'one'), (3, 'three'), (2, 'two')]
	       #+END_SRC

	  6) Documentation Strings

	     - "The first line should always be a short, concise
               summary of the object’s purpose. [...] This line should
               begin with a capital letter and end with a period."

       8) Intermezzo: Coding Style

	  - 4 space indentation

	  - 79 character lines

	  - blank lines to separate functions, classes, larger code blocks

	  - comments on line of their own if possible

	  - docstrings

	  - spaces right

	  - CamelClass,  function_underscore,  method(self, ...)

	  - ASCII

    5) DATA STRUCTURES

       1) More on Lists

	  0) [@0] all methods

	     - append,

	     - extend

	     - insert

	     - remove

	     - pop

	     - index

	     - count

	     - sort

	     - reverse

	  1) Using Lists as Stacks

	     via append() 
             and pop()

	  2) Using Lists as Queues

	     use collections.deque()

	  3) Functional Programming Tools

	     - filter(function, sequence) returns a sequence
               consisting of those items from the sequence for which
               function(item) is true.

	       #+BEGIN_SRC python
                 >>> def f(x): return x % 3 == 0 or x % 5 == 0
                 ...
                 >>> filter(f, range(2, 25))
                 [3, 5, 6, 9, 10, 12, 15, 18, 20, 21, 24]
	       #+END_SRC

	  4) List Comprehensions

	     - list of squares
	       #+BEGIN_SRC python
                 squares = [x**2 for x in range(10)]
	       #+END_SRC
               This is also equivalent to
	       #+BEGIN_SRC python
                 squares = map(lambda x: x**2, range(10))
	       #+END_SRC
               , but it’s more concise and readable.

	     - flatten a list using a listcomp with two 'for'
	       #+BEGIN_SRC python
                 >>> vec = [[1,2,3], [4,5,6], [7,8,9]]
                 >>> [num for elem in vec for num in elem]
                 [1, 2, 3, 4, 5, 6, 7, 8, 9]
	       #+END_SRC

       2) The del statement

	  - remove elements 
	    del a[3]

	  - remove slices
	    del a[3:]

	  - remove whole list
	    del a[:]

	  - remove variable
	    del a

       3) Tuples and Sequences

	  - tuples, strings and lists are sequences

	  - tuples for indexing, packing, ...

	  - immutable

	  - a = 'hello',
	    is a tuple

	  - /tuple packing/:
	    #+BEGIN_SRC python
              t = 12345, 54321, ’hello!’
	    #+END_SRC

	  - /sequence unpacking/:
	    #+BEGIN_SRC python
              x, y, z = t
	    #+END_SRC

       4) Sets

	  - "unordered collection with no duplicate elements"

	  - fast "membership testing and eliminating duplicate entries"

	  - union, intersection, difference, and symmetric difference
	    #+BEGIN_SRC python
              >>> a = set('abracadabra')
              >>> b = set('alacazam')
              >>> a
              # unique letters in a
              set(['a', 'r', 'b', 'c', 'd'])
              >>> a - b
              # letters in a but not in b
              set(['r', 'd', 'b'])
              >>> a | b
              # letters in either a or b
              set(['a', 'c', 'r', 'd', 'b', 'm', 'z', 'l'])
              >>> a & b
              # letters in both a and b
              set(['a', 'c'])
              >>> a ^ b
              # letters in a or b but not both
              set(['r', 'd', 'b', 'm', 'z', 'l'])
	    #+END_SRC

	  - set comprehension like list comprehension
	    #+BEGIN_SRC python
              >>> a = {x for x in 'abracadabra' if x not in 'abc'}
              >>> a
              set(['r', 'd'])
	    #+END_SRC

       5) Dictionaries

	  - see quotes:

	    - for key/value storage and retrieval

	    - dict via array, =, dict comprehension

	    - in

       6) Looping Techniques

	  - enumerate: index, item

	  - zip: combines

	  - reversed: returns reversed

	  - sorted: returns sorted

	  - iteritems: key, value

	  - modify: better create new list

       7) More on Conditions

	  - in: containment

	  - is: object equality

	  - comparison chaining

	    #+BEGIN_SRC python
              a < b == c
	    #+END_SRC

	  - not > and > or

	  - assign the result of a comparison or other Boolean
            expression to a variable.
	    
	    #+BEGIN_SRC python
              >>> string1, string2, string3 = '', 'Trondheim', 'Hammer Dance'
              >>> non_null = string1 or string2 or string3
              >>> non_null
              'Trondheim'
	    #+END_SRC

	  - not inside expressions (avoids comparison typo a = b+c)

       8) Comparing Sequences and Other Types

	  - same type: recursive

	  - different types: ordered by name (list < string)

    6) MODULES
*** quotes
    - it’s good practice to include docstrings in code that you write,
      so make a habit of it.
    - Thus, global variables cannot be directly assigned a value
      within a function (unless named in a global statement), although
      they may be referenced.
    - Actually, call by object reference would be a better
      description, since if a mutable object is passed, the caller
      will see any changes the callee makes to it (items inserted into
      a list).
    - Falling off the end of a function also returns None.
    - (example in and raw_input
      #+BEGIN_SRC python
        ok = raw_input(prompt)
        if ok in ('y', 'ye', 'yes'):
            return True
      #+END_SRC
    - Now enter the Python interpreter and import this module with the
      following command:
      #+BEGIN_SRC python
      >>> import fibo
      #+END_SRC
    - Finally, the least frequently used option is to specify that a
      function can be called with an arbitrary number of
      arguments. These arguments will be wrapped up in a tuple (see
      Tuples and Sequences). Before the variable number of arguments,
      zero or more normal arguments may occur.
      #+BEGIN_SRC python
        def write_multiple_items(file, separator, *args):
          file.write(separator.join(args))
      #+END_SRC
    - An optional ’:’ and format specifier can follow the field
      name. This allows greater control over how the value is
      formatted. The following example rounds Pi to three places after
      the decimal.
      #+BEGIN_SRC python
        import math
        print 'The value of PI is approximately {0:.3f}.'.format(math.pi)
      #+END_SRC
    - Class definitions play some neat tricks with namespaces, and you
      need to know how scopes and namespaces work to fully understand
      what’s going on.
    - >>> tel = {'jack': 4098, 'sape': 4139}
      >>> tel['guido'] = 4127
    - When *looping through dictionaries*, the key and corresponding
      value can be retrieved at the same time using the iteritems()
      method.
      #+BEGIN_SRC python
        >>> knights = {'gallahad': 'the pure', 'robin': 'the brave'}
        >>> for k, v in knights.iteritems():
        ...     print k, v
      #+END_SRC
    - [tuples] may be input with or without surrounding parentheses,
      although often parentheses are necessary anyway
    - (ends 5.4)
    - It is best to think of a dictionary as an unordered set of key:
      value pairs, with the requirement that the keys are unique
      (within one dictionary)
    - The main operations on a dictionary are storing a value with
      some key and extracting the value given the key.
    - The dict() constructor builds dictionaries directly from
      sequences of key-value pairs:
      #+BEGIN_SRC python
        >>> dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])
        {'sape': 4139, 'jack': 4098, 'guido': 4127}
      #+END_SRC
    - In addition, dict comprehensions can be used to create
      dictionaries from arbitrary key and value expressions:
      #+BEGIN_SRC python
        >>> {x: x**2 for x in (2, 4, 6)}
        {2: 4, 4: 16, 6: 36}
      #+END_SRC
    - (ends 5.5)
    - To loop over two or more sequences at the same time, the entries
      can be paired with the zip() function.
      #+BEGIN_SRC python
        >>> questions = ['name', 'quest', 'favorite color']
        >>> answers = ['lancelot', 'the holy grail', 'blue']
        >>> for q, a in zip(questions, answers):
        ...
        print 'What is your {0}? It is {1}.'.format(q, a)
        ...
        What is your name? It is lancelot.
        What is your quest? It is the holy grail.
        What is your favorite color? It is blue.
      #+END_SRC
    - It is sometimes tempting to change a list while you are looping
      over it; however, it is often simpler and safer to create a new
      list instead.
      #+BEGIN_SRC python
        >>> import math
        >>> raw_data = [56.2, float('NaN'), 51.7, 55.3, 52.5, float('NaN'), 47.8]
        >>> filtered_data = []
        >>> for value in raw_data:
        ...
        if not math.isnan(value):
        ...
        filtered_data.append(value)
        ...
        >>> filtered_data
        [56.2, 51.7, 55.3, 52.5, 47.8]
      #+END_SRC
** TODO [[./Tor Project: Overview.html][Tor: Overview]]
** TODO [[shell:ristretto ./malamud][Malamud:Privacy and Private States]]
*** summary
*** quotes
    - Primo Levi observed that "solitude in a Camp is more precious
      and rare than bread."
** TODO [[./www.ted.com/talks/glenn_greenwald_why_privacy_matters/transcript.html][Greenwald:privacy]]
*** summary
    - youtube videos of people who think they are unobserved, do sth,
      horror on being seen (singing, dancing, nose picking, ...
    - Snowden revealed: US spying on every internet user
    - common thinking: only bad people have a reason for privacy
    - this is self-deprecation
    - about this mentality: people do not believe that. they safeguard
      their privacy (passwords, locks on bedroom and bathroom doors
    - cnet found out stuff about eric schmidt via google, he
      subsequently forbid his employees from talking to cnet
    - zuckerberg bought own house plus all four adjacent houses to
      have a zone of privacy
    - tell people who say they do not care: give me all your
      email/social media-passwords, let me publish whatever I find
      interesting
*** quotes
    - mass, indiscriminate surveillance
    - People can very easily in words claim that they don't value
      their privacy, but their actions negate the authenticity of that
      belief.
    - There are dozens of psychological studies that prove that when
      somebody knows that they might be watched, the behavior they
      engage in is vastly more conformist and compliant.
    - crucial to this design was that the inmates could not actually
      see into the panopticon, into the tower, and so they never knew
      if they were being watched or even when.
*** ref
#+BEGIN_SRC bibtex
     @misc{privacy-g,
       author = "Glenn Greenwald",
       title = "Why Privacy Matters",
       url = "\url{www.ted.com/talks/glenn_greenwald_why_privacy_matters/transcript}",
       note = "[Online; accessed 15-October-2015]"
     }
#+END_SRC
** TODO [[./tor_overview.html][Tor Project: Overview]]
*** summary
    1) Overview

       - tor increases privacy and security

       - series of virtual tunnels

       - censorship circumvention

       - groups
	 - individuals: keep from tracking, connect to blocked
           websites, hidden services allow to publish, socially
           sensitive communication

	 - journalists: communicate with whistleblowers

	 - indymedia, eff recommend Tor, corporations for
           investigating competitors

	 - U.S. Navy: comint, teams,

	 - law enforcement: surveil websites, sting

    2) Why we need Tor

       - f.ex. foreign country, s/o could observe where you are connecting

    3) The solution: a distributed, anonymous network
** TODO [[./Levenshtein.html][Levenshtein-Distanz]]
*** quotes
    - ist die minimale Anzahl von Einfüge-, Lösch- und
      Ersetz-Operationen, um die erste Zeichenkette in die zweite
      umzuwandeln.
*** TODO ref in english
    #+BEGIN_SRC bibtex
      @misc{ wiki:xxx,
        author = "Wikipedia",
        title = "Levenshtein-Distanz --- Wikipedia{,} Die freie Enzyklopädie",
        year = "2014",
        url = "\url{https://de.wikipedia.org/w/index.php?title=Levenshtein-Distanz&oldid=133874990}",
        note = "[Online; accessed 22-Oktober-2015]"
      }
   #+END_SRC
** TODO [[./EdmanS09.pdf][AS-awareness in Tor Path Selection]]
*** ref
    #+BEGIN_SRC bibtex
      @inproceedings{DBLP:conf/ccs/EdmanS09,
        title = {{AS}-awareness in {T}or path selection},
        author = {Matthew Edman and Paul F. Syverson},
        booktitle = {Proceedings of the 2009 ACM Conference on Computer and Communications
              Security, CCS 2009},
        year = {2009},
        month = {November},
        location = {Chicago, Illinois, USA},
        pages = {380--389},
        editor = {Ehab Al-Shaer and Somesh Jha and Angelos D. Keromytis},
        publisher = {ACM},
        isbn = {978-1-60558-894-0},
        www_section = {Anonymous communication},
        www_pdf_url = {http://www.cs.rpi.edu/~edmanm2/ccs159-edman.pdf},
        www_tags = {selected},
        bibsource = {DBLP, http://dblp.uni-trier.de},
      }
    #+END_SRC
** TODO [[./2012-jmlr.pdf][Sally: A Tool for Embedding Strings in Vector Spaces]]
** TODO [[./supersequence.pdf][On the Approximation of Shortest Common Supersequences and Longest Common Subsequences]]
** TODO [[./projects.html][Software & Services]]
*** summary
    - Tor Browser

      - "everything you need to safely browse the Internet"

      - requires no installation

    - ...
*** ref
    #+BEGIN_SRC bibtex
     @misc{tor-ecosystem,
       tag = "The Tor Project",
       title = "Software & Services",
       url = "\url{https://www.torproject.org/projects/projects.html.en}",
       note = "[Online; accessed 21-October-2015]"
     }
   #+END_SRC
** TODO [[./python-doc-howto-logging.pdf][Logging HOWTO]]
*** summary
    1) Basic Logging Tutorial

       log(level[, data])

       1) When to use logging

	  normal output for the user: use =print=, else use logging.level()

	  WARNING is the default level

       2) A simple example

	  only warning and above are shown:

	  import logging
          logging.warning('Watch out!') # will print a message to the console
          logging.info('I told you so') # will not print anything

          WARNING:root:Watch out!

       3) Logging to a file

	  logging.basicConfig(filename='example.log',level=logging.DEBUG)

       4) Logging from multiple modules

	  import in every module

	  call basicConfig in main entry point

       5) Logging variable data

	  %s style, other options work maybe, too

       6) Changing the format of displayed messages

	  #+BEGIN_SRC python
            logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
	  #+END_SRC

       7) Displaying the date/time in messages
*** quotes
** PART [#A] [[./random.pdf][Computer Generation of Statistical Distributions]]
*** quotes
    5) [@5] PROBABILITY DISTRIBUTION FUNCTIONS
       1) Continuous Distributions
	  8) [@8] Exponential

	     - (1) Generate U ~ U(0, 1)
               (2) Return X = a − b ln U

	     - Source Code:
	       #+BEGIN_SRC C++
                 double exponential( double a, double b )
                 {
                   assert( b > 0. );
                   return a - b * log( uniform( 0., 1. ) );
                 }
	       #+END_SRC

	  15) [@15] Lognormal

	      - Mean:
		a + e^(\mu + \sigma^2/2)

	      - Variance:
		e^(2*mu+sig^2) * (e^(sig^2)-1)

	      - (1) Generate V ~ N ( μ , σ^2 )
		(2) Return X = a + e^{V}

	      - Source Code:
		#+BEGIN_SRC C++
                  double lognormal( double a, double mu, double sigma )
                  {
                    return a + exp( normal( mu, sigma ) );
                  }
		#+END_SRC

	  27) [@27] Weibull

	      - Algorithm:

		1. Generate U ~ U(0, 1)
		2. Return X = a + b(− ln U) 1/c

	      - Source Code:

		#+BEGIN_SRC c++
                  double weibull( double a, double b, double c )
                  {
                    assert( b > 0. && c > 0. );
                    return a + b * pow( -log( uniform( 0., 1. ) ), 1. / c );
                  }
		#+END_SRC

	      - Notes:
		1. When c = 1, this becomes the exponential
                   distribution with scale b.
		2. When c = 2 for general b, it becomes the Rayleigh
                   distribution.
       2) Discrete Distributions

	  3) [@3] Geometric

	     - "The geometric distribution represents the probability
               of obtaining k failures before the first success in
               independent Bernoulli trials, where the probability of
               success in a single trial is p."

	     - Algorithm:

	       1. Generate U ~ U(0, 1)

	       2. Return X = int ( ln U/ ln (1 − p) )

	     - Source Code:

	       #+BEGIN_SRC c++
                 int geometric( double p )
                 {
                   assert( 0. < p && p < 1. );
                   return int( log( uniform( 0., 1. ) ) / log( 1. - p ) );
                 }
	       #+END_SRC
** [#B] PART [[./user_guide_0.16.1.pdf][scikit-learn user guide]]
*** quotes
    - Despite its simplicity, nearest neighbors has been successful in
      a large number of classification and regression problems
    - SVC and NuSVC implement the “one-against-one” approach (Knerr et
      al., 1990) for multi- class classification. If n_class is the
      number of classes, then n_class * (n_class - 1) / 2 classifiers
      are constructed and each one trains data from two classes
    - Note that the LinearSVC also implements an alternative
      multi-class strategy, the so-called multi-class SVM formulated
      by Crammer and Singer, by using the option
      multi_class=’crammer_singer’. This method is consistent, which
      is not true for one-vs-rest classification. In practice,
      one-vs-rest classification is usually preferred, since the
      results are mostly similar, but the runtime is significantly
      less.
    - (begins 4)
    - RadiusNeighborsClassifier [...] For high-dimensional parameter
      spaces, this method becomes less effective due to the so-called
      “curse of dimensionality”.
    - (ends 4.6.2)
    - Currently, algorithm = ’auto’ selects ’kd_tree’ if k < N/2 and
      the ’effective_metric_’ is in the ’VALID_METRICS’ list of
      ’kd_tree’. It selects ’ball_tree’ if k < N/2 and the
      ’effective_metric_’ is not in the ’VALID_METRICS’ list of
      ’kd_tree’. It selects ’brute’ if k >= N/2. This choice is based
      on the assumption that the number of query points is at least
      the same order as the number of training points, and that
      leaf_size is close to its default value of 30.
    - (ends 4.6.4)
    - There are many efficient exact nearest neighbor search
      algorithms for low dimensions d (approximately 50). However
      these algorithms perform poorly with respect to space and query
      time when d increases. These algorithms are not any better than
      comparing query point to each point from the database in a high
      dimension (see Brute Force). This is a well-known consequence of
      the phenomenon called “The Curse of Dimensionality”. There are
      certain applications where we do not need the exact nearest
      neighbors but having a “good guess” would suffice. When answers
      do not have to be exact, the LSHForest class implements an
      approximate nearest neighbor search.
    - (ends 4.6.6)
    - DecisionTreeClassifier [...]
      #+BEGIN_SRC python
        from sklearn import tree
        X = [[0, 0], [1, 1]]
        Y = [0, 1]
        clf = tree.DecisionTreeClassifier()
        clf = clf.fit(X, Y)
      #+END_SRC
    - (ends 4.11)
    - As other classifiers, forest classifiers have to be fitted with
      two arrays: a sparse or dense array X of size [n_samples,
      n_features] holding the training samples, and an array Y of size
      [n_samples] holding the target values (class labels) for the
      training samples:
      #+BEGIN_SRC python
        from sklearn.ensemble import RandomForestClassifier
        X = [[0, 0], [1, 1]]
        Y = [0, 1]
        clf = RandomForestClassifier(n_estimators=10)
        clf = clf.fit(X, Y)
      #+END_SRC
    - (ends 4.11.2)
    - 
      #+BEGIN_SRC python
        from sklearn.ensemble import ExtraTreesClassifier
        [...]
        # Build a forest and compute the feature importances
        forest = ExtraTreesClassifier(n_estimators=250,
        random_state=0)
        forest.fit(X, y)
        importances = forest.feature_importances_
      #+END_SRC
    - (begins 12)
    - Firstly, many estimators take precomputed distance/similarity
      matrices, so if the dataset is not too large, you can compute
      distances for all pairs of inputs. If the dataset is large, you
      can use feature vectors with only one “feature”, which is an
      index into a separate data structure, and supply a custom metric
      function that looks up the actual data in this data
      structure. E.g., to use DBSCAN with Levenshtein distances:
      #+BEGIN_SRC python
        >>> from leven import levenshtein
        >>> import numpy as np
        >>> from sklearn.cluster import dbscan
        >>> data = ["ACCTCCTAGAAG", "ACCTACTAGAAGTT", "GAATATTAGGCCGA"]
        >>> def lev_metric(x, y):
        ...
        i, j = int(x[0]), int(y[0])
        # extract indices
        ...
        return levenshtein(data[i], data[j])
        ...
        >>> X = np.arange(len(data)).reshape(-1, 1)
        >>> X
        array([[0],
        [1],
        [2]])
        >>> dbscan(X, metric=lev_metric, eps=5, min_samples=2)
        ([0, 1], array([ 0, 0, -1]))
      #+END_SRC
      (This uses the third-party edit distance package leven.) Similar
      tricks can be used, with some care, for tree kernels, graph
      kernels, etc.
    - (ends 12.12)
    - (begins 42)
    - class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,
      weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2,
      metric=’minkowski’, metric_params=None, **kwargs)
      [...]

      metric : string or DistanceMetric object (default = ‘minkowski’)
      the distance metric to use for the tree. The default metric is
      minkowski, and with p=2 is equivalent to the standard Euclidean
      metric. See the documentation of the Distance-Metric class for
      a list of available metrics.
    - “braycurtis” | BrayCurtisDistance | sum(|x - y|) / (sum(|x|) + sum(|y|))
    - (ends 42.25.2)
    - class sklearn.pipeline.Pipeline(steps) Pipeline of transforms
      with a final estimator. Sequentially apply a list of transforms
      and a final estimator. Intermediate steps of the pipeline must
      be ‘transforms’, that is, they must implement fit and transform
      methods. The final estimator only needs to implement fit. The
      purpose of the pipeline is to assemble several steps that can be
      cross-validated together while setting different parameters. For
      this, it enables setting parameters of the various steps using
      their names and the parameter name separated by a ‘__’, as in
      the example below.

      Parameterssteps: list :List of (name, transform) tuples
      (implementing fit/transform) that are chained, in the order in
      which they are chained, with the last object an estimator.

      Examples
      #+BEGIN_SRC python
        >>> from sklearn import svm
        >>> from sklearn.datasets import samples_generator
        >>> from sklearn.feature_selection import SelectKBest
        >>> from sklearn.feature_selection import f_regression
        >>> from sklearn.pipeline import Pipeline
        >>> # generate some data to play with
        >>> X, y = samples_generator.make_classification(
        ...
        n_informative=5, n_redundant=0, random_state=42)
        >>> # ANOVA SVM-C
        >>> anova_filter = SelectKBest(f_regression, k=5)
        >>> clf = svm.SVC(kernel='linear')
        >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
        >>> # You can set the parameters using the names issued
        >>> # For instance, fit using a k of 10 in the SelectKBest
        >>> # and a parameter 'C' of the svm
        >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
        ...
        Pipeline(steps=[...])
        >>> prediction = anova_svm.predict(X)
        >>> anova_svm.score(X, y)
        0.77...
        #+END_SRC
      - (ends 42.29.1)
      - class sklearn.tree.DecisionTreeClassifier[...]

	feature_importances_ : array of shape = [n_features]

	The feature importances.
      - (ends 42.35.1)
      - It is however still interesting to check what’s happening
        inside the _nls_subproblem function which is the hotspot if we
        only consider Python code: it takes around 100% of the
        cumulated time of the module. In order to better understand
        the profile of this specific function, let us install
        line-prof and wire it to IPython:
	#+BEGIN_SRC sh
          pip install line-profiler
	#+END_SRC
      - (ends 47.3)
*** ref
    #+BEGIN_SRC bibtex
      @article{scikit-learn,
        title={Scikit-learn: Machine Learning in {P}ython},
        author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
               and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
               and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
               Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
        journal={Journal of Machine Learning Research},
        volume={12},
        pages={2825--2830},
        year={2011}
      }
    #+END_SRC
** PART [#C] [[./javascript_the_good_parts.pdf][JavaScript: The Good Parts]]
*** summary
    1. Good parts

       - some: functionaly programming, prototypal inheritance

       - very different from other languages

    2. Grammar
*** quotes
    - We appreciate, but do not require, attribution. An attribution
      usually includes the title, author, publisher, and ISBN. For
      example: “JavaScript: The Good Parts by Douglas
      Crockford. Copyright 2008 Yahoo! Inc., 978-0-596-51774-8.”
    - (ends 0+1)
** PART [[./sally/Example1.html]]
*** summary
    - input files example1.cfg reuters.zip
    - sally -c example1.cfg reuters.zip reuters.libsvm
** PART [[./wireshark-debian.html][wireshark/trunk/debian/README.Debian]]
*** summary
    capture as root, analyze as user

    or add users to the wireshark-group dpkg-reconfigure ... (see below)
*** quotes
    - The installation method can be changed any time by running:
      dpkg-reconfigure wireshark-common
** PART [[./python-doc-library.pdf][The Python Library Reference]]
*** quotes
    - doctest
      #+BEGIN_SRC python
        if __name__ == "__main__":
            import doctest
            doctest.testmod()
      #+END_SRC
    - os.walk(top, topdown=True, onerror=None, followlinks=False)

      Generate the file names in a directory tree by walking the tree
      either top-down or bottom-up. For each directory in the tree
      rooted at directory top (including top itself), it yields a
      3-tuple (dirpath, dirnames, filenames).  dirpath is a string,
      the path to the directory. dirnames is a list of the names of
      the subdirectories in dirpath (excluding ’.’ and
      ’..’). filenames is a list of the names of the non-directory
      files in dirpath. Note that the names in the listscontain no
      path components. To get a full path (which begins with top) to a
      file or directory in dirpath, do os.path.join(dirpath, name).
    - str.lstrip( [ chars ] )

      Return a copy of the string with leading characters removed. The
      chars argument is a string specifying the set of characters to
      be removed.
    - str.partition(sep)

      Split the string at the first occurrence of sep, and return a
      3-tuple containing the part before the separator, the separator
      itself, and the part after the separator. If the separator is
      not found, return a 3-tuple containing the string itself,
      followed by two empty strings.
    - (ends 5)
    - os.path.basename(path)

      Return the base name of pathname path.
    - staticmethod(function)

      Return a static method for function. A static method does not
      receive an implicit first argument. To declare a static method,
      use this idiom:
      #+BEGIN_SRC python
        class C(object):
            @staticmethod
            def f(arg1, arg2, ...):
                ...
      #+END_SRC
    - pdb.py can also be invoked as a script to debug other scripts. For example:

      #+BEGIN_SRC python
        python -m pdb myscript.py
      #+END_SRC
    - The typical usage to break into the debugger from a running
      program is to insert

      #+BEGIN_SRC python
        import pdb; pdb.set_trace()
      #+END_SRC

      at the location you want to break into the debugger.
    - (begins 20)
    - The SimpleHTTPServer module can also be invoked directly using
      the -m switch of the interpreter with a port number
      argument. Similar to the previous example, this serves the files
      relative to the current directory.

      python -m SimpleHTTPServer 8000
    - (ends 20.19)
    - virtual environment

      A cooperatively isolated runtime environment that allows Python
      users and applications to install and upgrade Python
      distribution packages without interfering with the behaviour of
      other Python applications running on the same system.
    - (ends Appendix A)
** PART [[/home/chive/IT-gg/python/scipy-ref-0.15.1.pdf][SciPy Reference Guide]]
*** quotes
    - (begins 1.13)
    - Freezing a Distribution

      Passing the loc and scale keywords time and again can become
      quite bothersome. The concept of freezing a RV is used to solve
      such problems.
      #+BEGIN_SRC python
        >>> rv = gamma(1, scale=2.)
      #+END_SRC
      By using rv we no longer have to include the scale or the shape
      parameters anymore. Thus, distributions can be used in one of
      two ways, either by passing all distribution parameters to each
      method call (such as we did earlier) or by freezing the
      parameters for the instance of the distribution.
    - (ends 1.13)
    - (begins 5)
    - scipy.spatial.distance.euclidean(u, v)

      Computes the Euclidean distance between two 1-D arrays.
    - Y = pdist(X, ’braycurtis’)

      Computes the Bray-Curtis distance between the points. The
      Bray-Curtis distance between two points u and v is

      d(u, v) = ∑_{i} u_{i} − v_{i} / ∑_{i} u_{i} + v_{i}
    - (ends 5.29)
    - scipy.special.btdtr(a, b, x) = <ufunc ‘btdtr’>

      Cumulative beta distribution. Returns the area from zero to x
      under the beta density function:

      gamma(a+b)/(gamma(a)*gamma(b)))*integral(t**(a-1) (1-t)**(b-1),
      t=0..x)
    - (ends 5.30)
    - Poisson distribution
      #+BEGIN_SRC python
        from scipy.stats import poisson
        import matplotlib.pyplot as plt
        fig, ax = plt.subplots(1, 1)
        # Calculate a few first moments:
        mu = 0.6
        mean, var, skew, kurt = poisson.stats(mu, moments=’mvsk’)
        # Display the probability mass function (pmf):
        x = np.arange(poisson.ppf(0.01, mu),
                      poisson.ppf(0.99, mu))
        ax.plot(x, poisson.pmf(x, mu), ’bo’, ms=8, label=’poisson pmf’)
        ax.vlines(x, 0, poisson.pmf(x, mu), colors=’b’, lw=5, alpha=0.5)

        plt.show()
      #+END_SRC
    - (ends 5.31)
** PART [[/home/chive/IT-gg/python/numpy-ref-1.9.1.pdf][NumPy Reference]]
*** quotes
    - numpy.random.exponential(scale=1.0, size=None)

      Exponential distribution. Its probability density function is

      f (x; 1/\beta) = 1/\beta exp(- x/\beta)

      for x > 0 and 0 elsewhere. β is the scale parameter, which is
      the inverse of the rate parameter λ = 1/\beta.
    - numpy.random.gamma(shape, scale=1.0, size=None)

      Draw samples from a Gamma distribution. Samples are drawn from a
      Gamma distribution with specified parameters, shape (sometimes
      designated “k”) and scale (sometimes designated “theta”), where
      both parameters are > 0.

      Parameters

      shape : scalar > 0 The shape of the gamma distribution.

      scale : scalar > 0, optional The scale of the gamma
      distribution. Default is equal to 1.

      [...]

      Draw samples from the distribution:
      #+BEGIN_SRC python
        shape, scale = 2., 2. # mean and dispersion
        s = np.random.gamma(shape, scale, 1000)
      #+END_SRC
      Display the histogram of the samples, along with the probability
      density function:
      #+BEGIN_SRC python
        import matplotlib.pyplot as plt
        import scipy.special as sps
        count, bins, ignored = plt.hist(s, 50, normed=True)
        y = bins**(shape-1)*(np.exp(-bins/scale) /
        (sps.gamma(shape)*scale**shape))
        plt.plot(bins, y, linewidth=2, color=’r’)
        plt.show()
      #+END_SRC
    - (ends 3.25)
    - numpy.zeros(shape, dtype=float, order=’C’)

      Return a new array of given shape and type, filled with zeros.

    - numpy.array(object, dtype=None, copy=True, order=None,
      subok=False, ndmin=0)

      Create an array.

    - numpy.ma.arange( [ start ] , stop [ , step ] , dtype=None)

      Return evenly spaced values within a given interval.  [...]
      When using a non-integer step, such as 0.1, the results will
      often not be consistent. It is better to use linspace for these
      cases.

    - numpy.linspace(start, stop, num=50, endpoint=True,
      retstep=False, dtype=None)

      Return evenly spaced numbers over a specified interval.
** MAYBE [[./rijsbergen79_infor_retriev.pdf][INFORMATION RETRIEVAL]]
*** quotes
    |X ∩ Y| / |X ∪ Y| Jaccard's coefficient
** onion routing project
*** [[~/da/git/docs/onion-routing_pet2000.pdf][Towards an Analysis of Onion Routing Security]]
**** ref
     #+BEGIN_SRC bibtex
       @inproceedings{onion-routing:pet2000,
         title = {{Towards an Analysis of Onion Routing Security}},
         author = {Paul Syverson and Gene Tsudik and Michael Reed and Carl Landwehr},
         booktitle = {Proceedings of Designing Privacy Enhancing Technologies: Workshop on Design
               Issues in Anonymity and Unobservability},
         year = {2000},
         month = {July},
         pages = {96--114},
         editor = {H. Federrath},
         publisher = {Springer-Verlag, LNCS 2009},
         www_important = {1},
         www_tags = {selected},
         www_section = {Anonymous communication},
         www_ps_gz_url = {http://www.onion-router.net/Publications/WDIAU-2000.ps.gz},
       }
     #+END_SRC
*** [[~/da/git/docs/onion-routing_ih96.pdf][Hiding Routing Information]]
**** ref
     #+BEGIN_SRC bibtex
       @inproceedings{onion-routing:ih96,
         title = {{Hiding Routing Information}},
         author = {David M. Goldschlag and Michael G. Reed and Paul F. Syverson},
         booktitle = {Proceedings of Information Hiding: First International Workshop},
         year = {1996},
         month = {May},
         pages = {137--150},
         editor = {R. Anderson},
         publisher = {Springer-Verlag, LNCS 1174},
         www_tags = {selected},
         www_section = {Anonymous communication},
         www_pdf_url = {http://www.onion-router.net/Publications/IH-1996.pdf},
         www_ps_gz_url = {http://www.onion-router.net/Publications/IH-1996.ps.gz},
       }
     #+END_SRC
** [[./sally/Example3.html]]
*** summary
    - sally -c example3.cfg jrc.zip jrc.mat
    - load jrc.mat
    - X = [fvec.data]
    - Y = [fvec.label]
    - [Y idx] = sort(Y);
    - K = X(:,idx)' * X(:,idx);
    - imagesc(K);
    - colormap(hot);
    - colorbar;
** [[./06vect.pdf][Scoring, term weighting and the vector space model]]
** [[./ch1.pdf][Data Mining]]
** [[shell:firefox ./skl/index.html.4 &][A tutorial on statistical-learning for scientific data processing]]
*** summary
    1) Statistical learning: the setting and the estimator object in scikit-learn

       data: often 2d-array
       estimator: estimates (fit, ..., params at object creation)

    2) Supervised learning: predicting an output variable from high-dimensional observations

       1) Nearest neighbor and the curse of dimensionality

	  toy problem: classifying irises

	  - import numpy as np
	  - from sklearn import datasets
	  - iris = datasets.load_iris()
	  - iris_X = iris.data
	  - iris_y = iris.target
	  - np.unique(iris_y)

	  - k-Nearest neighbors classifier

	    - simplest classifier

	  - The curse of dimensionality

	    the number of points needed scales exponentially with the
            dimension

       2) Linear model: from regression to sparsity

	  1) linear regression

	     fit line X \beta + \epsilon to data points

	  2) Shrinkage

	     problem with few data points per dimension, solve via
             Ridge (uses l_{2}-Norm)

	  3) Sparsity

	     it helps to reduce non-informative variables. Ridge
             reduces, Lasso sets to 0 (good on large datasets), as
             does LassoLARS (few observations)

	  4) Classification

	     for classification-regression, better use sigmoid instead
             of linear (LogisticRegression)

       3) Support vector machines (SVMs)

	  1) Linear SVMs

	     C is regularization parameter:

	     "a small value for C means the margin is calculated using
             many or all of the observations around the separating line
             (more regularization); a large value for C means the
             margin is calculated on observations close to the
             separating line (less regularization)."

	     thus: bigger C, less values considered (always those
             close to separating line)

	     "For many estimators, including the SVMs, having datasets
             with *unit standard deviation* for each feature is
             important to get good prediction."

	  2) Using kernels

	     different fitting functions: linear, poly, rbf

    3) Model selection: choosing estimators and their parameters

       1) Score, and cross-validated scores

	  - score: bigger is better

	  - k-fold cross-validation:

	    - import numpy as np
	    - X_folds = np.array_split(X_digits, 3)
	    - y_folds = np.array_split(y_digits, 3)
	    - scores = list()
	    - for k in range(3):
	      - X_train = list(X_folds)
	      - X_test  = X_train.pop(k)
	      - X_train = np.concatenate(X_train)
	      - y_train = list(y_folds)
	      - y_test  = y_train.pop(k)
	      - y_train = np.concatenate(y_train)
	      - scores.append(svc.fit(X_train, y_train).score(X_test, y_test))

       2) Cross-validation generators

	  X-validation (Kfold, Loo, etc) are available via

	  =sklearn.cross_validation=

	  - "To compute the score method of an estimator, the sklearn
            exposes a helper function:

	    #+BEGIN_SRC python
              cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)
	    #+END_SRC

       3) Grid-search and cross-validated estimators

	  1) Grid-search

	     automatic parameter search

	     #+BEGIN_SRC python
               from sklearn.grid_search import GridSearchCV
               Cs = np.logspace(-6, -1, 10)
               clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=-1)
               clf.fit(X_digits[:1000], y_digits[:1000])
               clf.best_score_
               clf.best_estimator_.C
	     #+END_SRC

	  2) Cross-validated estimators

	     with CV added to name: search automatically

	     (but not svm)

    4) Unsupervised learning: seeking representations of the data

       1) Clustering: grouping observations together

	  when categorization data not at hand

	  1) K-means clustering
	     #+BEGIN_SRC python
               >>> k_means = cluster.KMeans(n_clusters=3)
               >>> k_means.fit(X_iris)
               >>> print(k_means.labels_[::10])
	     #+END_SRC

	     iris: almost correct with 8 clusters, off on border with 3

	  2) Hierarchical agglomerative clustering: Ward

	     hierarchy of clusters

	     - agglomerative (bottom-up)

	       "This approach is particularly interesting when the
               clusters of interest are made of only a few
               observations. When the number of clusters is large, it
               is much more computationally efficient than k-means."

	     - divisive (top-down)

	       "For estimating large numbers of clusters, this
               approach is both slow (due to all observations starting
               as one cluster, which it splits recursively) and
               statistically ill-posed."
*** quotes
    - import pylab as pl
      pl.imshow
*** exercises
**** Exercise digits-knn-linear
     Try classifying the digits dataset with nearest neighbors and a
     linear model. Leave out the last 10% and test prediction
     performance on these observations.

     from sklearn import datasets, neighbors, linear_model

     digits = datasets.load_digits()
     X_digits = digits.data
     y_digits = digits.target
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, neighbors, linear_model
        import numpy as np

        digits = datasets.load_digits()
        X_digits = digits.data
        y_digits = digits.target

        np.random.seed(0)
        indices = np.random.permutation(len(y_digits))
        percent = int(len(y_digits) * 0.9)

        digits_X_train = X_digits[indices[:percent]]
        digits_X_test = X_digits[indices[percent:]]
        digits_y_train = y_digits[indices[:percent]]
        digits_y_test = y_digits[indices[percent:]]

        knn = neighbors.KNeighborsClassifier()
        knn.fit(digits_X_train, digits_y_train)
        knn.score(digits_X_test, digits_y_test)

        lin = linear_model.LogisticRegression()
        lin.fit(digits_X_train, digits_y_train)
        lin.score(digits_X_test, digits_y_test)
      #+END_SRC
**** Exercise iris-svm
     Try classifying classes 1 and 2 from the iris dataset with SVMs,
     with the 2 first features. Leave out 10% of each class and test
     prediction performance on these observations.
***** Solution
      #+BEGIN_SRC python
        from sklearn import datasets, svm
        import numpy as np

        iris = datasets.load_iris()
        X = iris.data
        y = iris.target

        # normalize std
        s = np.std(X, 0)
        for i in range(4):
            X[:,i] /= s[i]

        np.random.seed(0)
        indices = np.random.permutation(len(y))
        percent = int(len(y) * 0.9)

        X_train = X[indices[:percent]]
        X_test = X[indices[percent:]]
        y_train = y[indices[:percent]]
        y_test = y[indices[percent:]]

        svc = svm.SVC(kernel='linear')
        svc.fit(X_train, y_train)
        print svc.score(X_test, y_test)

        svcr = svm.SVC()
        svcr.fit(X_train, y_train)
        print svcr.score(X_test, y_test)
      #+END_SRC
**** Exercise kfold
     On the digits dataset, plot the cross-validation score of a SVC
     estimator with an linear kernel as a function of parameter C (use
     a logarithmic grid of points, from 1 to 10).
***** Solution
      #+BEGIN_SRC python
        import numpy as np
        from sklearn import cross_validation, datasets, svm

        C_s = np.logspace(-10, 0, 50)

        digits = datasets.load_digits()
        X = digits.data
        y = digits.target

        for C in C_s:
            svc.C = C
            score = cross_validation.cross_val_score(svc, X, y, n_jobs=1)
            print '%f, %f, %f' % (C, np.mean(score), np.std(score))
      #+END_SRC
**** Exercise grid-optimal
     On the diabetes dataset, find the optimal regularization
     parameter alpha.
***** Solution
      #+BEGIN_SRC python
        from sklearn import cross_validation, datasets, linear_model
        import numpy as np

        diabetes = datasets.load_diabetes()
        X = diabetes.data[:150]
        y = diabetes.target[:150]

        lasso = linear_model.Lasso()
        alphas = np.logspace(-4, -.5, 30)

        from sklearn.grid_search import GridSearchCV
        clf = GridSearchCV(estimator=lasso, param_grid=dict(alpha=alphas), n_jobs=-1)
        clf.fit(X, y)
        print clf.score(diabetes.data[150:], diabetes.target[150:])

        # for a in alphas:
        #     lasso.alpha = a
        #     score = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)
        #     print '%f, %s' % (a, np.mean(score))
      #+END_SRC
** [[./skl/working_with_text_data.html][Working With Text Data]]
** [[./Z5280B.pdf][Dirty Rotten Strategies - How We Trick Ourselves and Others into Solving the Wrong Problems Precisely]]
** [[./Cross-validation.leave-one-out_cross-validation.html]]
** [[./85331-CS.pdf][Improved Inapproximability Results for the Shortest Superstring and Related Problems]]
** [[~/da/git/docs/python-doc-reference.pdf][The Python Language Reference]]
*** quotes
    - For constructing a set or a dictionary Python provides special
      syntax called “displays”, each of them in two flavors:

      + either the container contents are listed explicitly, or
      + they are computed via a set of looping and filtering
        instructions, called a comprehension.

      Common syntax elements for comprehensions are:

      #+BEGIN_SRC python
        comprehension ::= expression comp_for
        comp_for      ::= “for” target_list “in” or_test [comp_iter]
        comp_iter     ::= comp_for | comp_if
        comp_if       ::= “if” expression_nocond [comp_iter]
      #+END_SRC

      [...] evaluating the expression to produce an element each time
      the innermost block is reached.

      Note that the comprehension is executed in a separate scope, so
      names assigned to in the target list don’t “leak” in the
      enclosing scope.
** TODO [[file:sarle/comp.ai.neural-nets%20FAQ,%20Part%201%20of%207:%20Introduction.html][Sarle Neural Network FAQ Part 1]]
*** summary
    1) Introduction

       - What is a neural network (NN)?

	 *Artificial* Neural Network:

	 parallelistic learning, experiential learning, asynchronous

       - Where can I find a simple introduction to NNs?

	 see chapter 4

       - Are there any online books about NNs?

*** quotes
    - Some care should be invested into a summary:
      - simple concatenation of all the answers is not enough: instead,
       redundancies, irrelevancies, verbosities, and errors should be
       filtered out (as well as possible)
      - the answers should be separated clearly
      - the contributors of the individual answers should be
        identifiable (unless they requested to remain anonymous [yes,
        that happens])
      - the summary should start with the "quintessence" of the
        answers, as seen by the original poster
      - A summary should, when posted, clearly be indicated to be one
        by giving it a Subject line starting with "SUMMARY:"
      Note that a good summary is pure gold for the rest of the newsgroup
      community
    - If trained carefully, NNs may exhibit some capability for
      generalization beyond the training data, that is, to produce
      approximately correct results for new cases that were not used
      for training.
*** ref
    Sarle, W.S., ed. (1997), Neural Network FAQ, part 1 of 7:
    Introduction, periodic posting to the Usenet newsgroup
    comp.ai.neural-nets, URL: ftp://ftp.sas.com/pub/neural/FAQ.html
** TODO [[file:Curse%20of%20dimensionality%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Curse of dimensionality - Wikipedia, the free encyclopedia.html]]
** TODO [[file:Regularization%20(mathematics)%20-%20Wikipedia,%20the%20free%20encyclopedia.html][file:~/da/git/docs/Regularization (mathematics) - Wikipedia, the free encyclopedia.html]]
** TODO [[~/da/git/docs/10.1.1.96.7225.pdf][A Guided Tour to Approximate String Matching]]
*** quotes
    - Despite the fact that most existing algorithms concentrate on
      the simple edit distance, many of them can be easily adapted to
      the generalized edit distance, and we pay attention to this
      issue throughout this work.
    - by allowing only insertions and deletions at cost 1, we can
      compute the *longest common subsequence* (LCS) between two
      strings.
    - simplification that has received a lot of attention is the
      variant that allows only substitutions (*Hamming distance*).
    - error correction. The physical transmission of signals is
      error-prone. To ensure correct transmission over a physical
      channel, it is necessary to be able to recover the correct
      message after a possible modification (error) introduced during
      the transmission. The probability of such errors is obtained
      from the signal processing theory and used to assign a cost to
      them.
    - Although this area has not developed much with respect to
      approximate searching, it has generated the most important
      measure of similarity, known as the Levenshtein distance
      [Levenshtein 1965; 1966] (also called “edit distance”).
    - (ends 2.2)
    - This paper is most concerned with the simple edit distance,
      which we denote ed(·). Although transpositions are of interest
      (especially in case of typing errors), there are few algorithms
      to deal with them. However, we will consider them at some point
      in this work (note that a transposition can be simulated with an
      insertion plus a deletion, but the cost is different). We also
      point out when the algorithms can be extended to have different
      costs of the operations (which is of special interest in
      computational biology), including the extreme case of not
      allowing some operations. This includes the other distances
      mentioned.
    - (ends 3.1)
** TODO [[~/da/git/docs/mlj12.pdf][Good edit similarity learning by loss minimization]]
** [[/home/chive/IT-gg/rfc7159.json-storage-and_xfer.txt][The JavaScript Object Notation (JSON) Data Interchange Format]]
*** summary
    0) [@0] Abstract

       "JavaScript Object Notation (JSON) is a lightweight, text-based,
       language-independent data interchange format"

    1) Introduction
*** quotes
*** ref
    #+BEGIN_SRC bibtex
      @techreport{rfc7159,
        author="T. Bray",
        title="{The JavaScript Object Notation (JSON) Data Interchange Format}",
        howpublished="Internet Request for Comments",
        TYPE="{RFC}",
        number=7159,
        PAGES = {1-16},
        year=2014,
        month=mar,
        publisher="{RFC Editor}",
        INSTITUTION = "{RFC Editor}",
        url="http://www.rfc-editor.org/rfc/rfc7159.txt",
      }
    #+END_SRC
** [[./Google%20Python%20Style%20Guide.html][Google Python Style Guide]]
*** summary
    1) Python Language Rules
       1) Lint

	  "Run pylint over your code."

	  disable warnings by-case using symbolic names

       2) Imports

	  "Use imports for packages and modules only."

	  - "Use import x for importing packages and modules."

	  - "Use from x import y where x is the package prefix and y
            is the module name with no prefix."

	  - "Use from x import y as z if two modules named y are to be
            imported or if y is an inconveniently long name."

       3) Packages
*** quotes
    - Avoid using the + and += operators to accumulate a string within
      a loop. Since strings are immutable, this creates unnecessary
      temporary objects and results in quadratic rather than linear
      running time. Instead, add each substring to a list and ''.join
      the list after the loop terminates (or, write each substring to
      a =io.BytesIO= buffer).
** [[./critique.html][A Critique of Website Traffic Fingerprinting Attacks]]
*** summary
*** quotes
*** ref
    #+BEGIN_SRC bibtex
      @misc{critique,
        tag = "The Tor Project",
        title = "A Critique of Website Traffic Fingerprinting Attacks",
        url = "\url{https://blog.torproject.org/blog/critique-website-traffic-fingerprinting-attacks}",
        note = "[Online; accessed 11-November-2015]"
      }
    #+END_SRC
** [[./top_ml_algo.pdf][Top 10 algorithms in data mining]]
** SO: Sklearn kNN usage with a user defined metric
*** quotes
    - You pass a metric as metric param, and additional metric
      arguments as keyword paramethers to NN constructor:
      #+BEGIN_SRC python
        >>> def mydist(x, y):
        ...     return np.sum((x-y)**2)
        ...
        >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])

        >>> nbrs = NearestNeighbors(n_neighbors=4, algorithm='ball_tree',
        ...            metric='pyfunc', func=mydist)
      #+END_SRC

** Mozilla Add-On SDK
*** Guides
**** XUL Migration Guide
     - Using XPCOM This example uses the action button API, which is
       only available from Firefox 29 onwards.

       Finally, if none of the above techniques work for you, you can
       use the require("chrome") statement to get direct access to the
       Components object, which you can then use to load and access
       any XPCOM object.

       The following complete add-on uses nsIPromptService to display
       an alert dialog:
       #+BEGIN_SRC javascript
         var {Cc, Ci} = require("chrome");

         var promptSvc = Cc["@mozilla.org/embedcomp/prompt-service;1"].
                         getService(Ci.nsIPromptService);

         require("sdk/ui/button/action").ActionButton({
           id: "xpcom-example",
           label: "Hello from XPCOM",
           icon: "./icon-16.png",
           onClick: function() {
             promptSvc.alert(null, "My Add-on", "Hello from XPCOM");
           }
         });
       #+END_SRC

       It's good practice to encapsulate code which uses XPCOM by
       packaging it in its own module. For example, we could package
       the alert feature implemented above using a script like:

       #+BEGIN_SRC javascript
         var {Cc, Ci} = require("chrome");

         var promptSvc = Cc["@mozilla.org/embedcomp/prompt-service;1"].
                     getService(Ci.nsIPromptService);

         exports.alert = function(title, text) {
             promptSvc.alert(null, title, text);
         };
       #+END_SRC

       If we save this as "alert.js" in our add-on's lib directory, we
       can rewrite main.js to use it as follows:

       #+BEGIN_SRC javascript
         require("sdk/ui/button/action").ActionButton({
           id: "xpcom-example",
           label: "Hello from XPCOM",
           icon: "./icon-16.png",
           onClick: function() {
             require("./alert").alert("My Add-on", "Hello from XPCOM");
           }
         });
       #+END_SRC
*** LL apis/
**** system/events
     - Usage The system/events module provides core (low level) API
       for working with the application observer service, also known
       as nsIObserverService. You can find a list of events dispatched
       by firefox codebase here.
       #+BEGIN_SRC javascript
         var events = require("sdk/system/events");
         var { Ci } = require("chrome");

         function listener(event) {
           var channel = event.subject.QueryInterface(Ci.nsIHttpChannel);
           channel.setRequestHeader("User-Agent", "MyBrowser/1.0", false);
         }

         events.on("http-on-modify-request", listener);
       #+END_SRC
**** window/utils
     - getWindowLoadingContext(window)
       Returns the nsILoadContext.
       - Parameters
         window : nsIDOMWindow
       - Returns
	 nsILoadContext
*** Tools
**** [[developer.mozilla.org/en-US/Add-ons/SDK/Tools/console.html][console]] ([[file:///home/chive/IT-gg/js/Mozilla_Addon_SDK_doc/developer.mozilla.org/en-US/Add-ons/SDK/Tools/console.html][local link]])
     - When you run your add-on using jpm run or jpm test, the global
       extensions.sdk.console.logLevel preference is automatically set
       to "info". This means that calls to console.log() will appear
       in the console output.
     - When you install an add-on into Firefox, the logging level will
       be "error" by default (that is, unless you have set one of the
       two preferences). This means that messages written using
       debug(), log(), info(), trace(), and warn() will not appear in
       the console.
** [[file:///home/chive/IT-gg/js/Eloquent%20JavaScript/contents.html][Eloquent Javascript]]
*** quotes
    - (begins 4)
    - The keyword `delete` cuts off properties. Trying to read a
      non-existent property gives the value undefined.
    - var now = new Date();
    - (ends 4)
*** summary
** [[./js/javascript%20-%20Firefox%20Addon%20observer%20http-on-modify-request%20not%20working%20properly%20-%20Stack%20Overflow.html][Firefox Addon observer http-on-modify-request not working properly]]
*** quotes
    - set up
      #+BEGIN_SRC javascript
        httpRequestObserver =
        {
          observe: function(subject, topic, data)
          {
            if (topic == "http-on-modify-request") {
                // [...] do sth here, from answer:
                var httpChannel = subject.QueryInterface(Ci.nsIHttpChannel);
                var uri = httpChannel.URI;
                var domainloc = uri.host;
            }
          },

          register: function()
          {
            var observerService = Cc["@mozilla.org/observer-service;1"]
                    .getService(Ci.nsIObserverService);
            observerService.addObserver(this, "http-on-modify-request", false);
          },

          unregister: function()
          {
            var observerService = Cc["@mozilla.org/observer-service;1"]
                    .getService(Ci.nsIObserverService);
            observerService.removeObserver(this, "http-on-modify-request");
          }
        };
      #+END_SRC
    - register observer
      #+BEGIN_SRC javascript
        httpRequestObserver.register();
      #+END_SRC
    - unload register
      #+BEGIN_SRC javascript
        exports.onUnload = function(reason) {
            httpRequestObserver.unregister();
        };
      #+END_SRC
** [[./Matplotlib.pdf][Matplotlib]]
*** quotes
    - (starts 68)
    - matplotlib.pyplot.savefig(*args, **kwargs)

      Save the current figure.
    - (ends 68.1)
** [[./Gamma distribution - Wikipedia, the free encyclopedia.html]]
*** summary
    0) [@0] Abstract

       - two-parameter

       - parametrization

	 - k, \theta

	 - a, b with a == k and b == 1/\theta

	 - k, \mu = k / \beta

	 - positive real numbers

       - k integer: erlang distribution

       - maximum entropy probability distribution

       - Mean \scriptstyle \mathbf{E}[ X] = k \theta

       - Variance \scriptstyle\operatorname{Var}[ X] = k \theta^2 
*** quotes
    - The common exponential distribution and chi-squared distribution
      are special cases of the gamma distribution.
    - Erlang distribution: the sum of k independent exponentially
      distributed random variables, each of which has a mean of θ
      (which is equivalent to a rate parameter of 1/θ).
    - (ends 0)
    - The skewness is equal to 2/\sqrt{k} , it *depends only on the
      shape parameter* (k) and approaches a normal distribution when k
      is large (approximately when k > 10).
    - (ends 3)
    - If X ~ Gamma(1, 1/λ) (shape -scale parametrization), then X has
      an exponential distribution with rate parameter λ.
    - (ends 6.4)
*** ref
    #+BEGIN_SRC bibtex
       @misc{ wiki:xxx,
         author = "Wikipedia",
         title = "Gamma distribution --- Wikipedia{,} The Free Encyclopedia",
         year = "2015",
         url = "https://en.wikipedia.org/w/index.php?title=Gamma_distribution&oldid=686932783",
         note = "[Online; accessed 14-December-2015]"
       }
    #+END_SRC
** [[./howtoread.pdf][How to Read a Book, v5.0]]
** [[./mozsign/Add-ons_Extension Signing - MozillaWiki.html][Add-ons/Extension Signing]]
*** quotes
    - How do I get my add-ons signed if they are not hosted on
      addons.mozilla.org (AMO)?
      - You will need to create an AMO account and submit your
        add-on. There will be an option where you indicate the add-on
        won't be listed on AMO, and you'll be able to submit your
        add-on files without having them published on the site. Please
        read the Distribution Policy for more details.
      - You can also use the jpm sign command to generate a signed XPI
        that can be self-hosted.
      - There is an API you can use for signing.
** [[./mozsign/jpm - Mozilla | MDN.html][jpm]]
*** quotes
    - This feature is only supported from jpm 1.0.4 onwards.
    - You will need to create API credentials on addons.mozilla.org
      before using this command.

      `jpm sign --api-key ${AMO_API_KEY} --api-secret ${AMO_API_SECRET}`
** [[./crov1297.pdf][Long-Lasting Transient Conditions in Simulations with Heavy-Tailed Workloads]]
** [[./Traffic generation model - Wikipedia, the free encyclopedia.html]]
** [[./dana_vol_11_2__pp_65_130.pdf][Predator foraging in patchy environments: the interrupted Poisson process (IPP) model unit]]
** [[./ide883.pdf][Superposition of Interrupted Poisson Processes and Its Application to Packetized Voice Multiplexers]]
** [[./all - jStat Documentation.html]]
** [[file:./random - How do I generate a Poisson Process? - Stack Overflow.see_title.html]]
** [[./tcpdump - Wikipedia, the free encyclopedia.html]]
*** ref
    #+BEGIN_SRC bibtex
       @misc{ wiki:xxx,
         author = "Wikipedia",
         title = "Tcpdump --- Wikipedia{,} The Free Encyclopedia",
         year = "2015",
         url = "https://en.wikipedia.org/w/index.php?title=Tcpdump&oldid=674322030",
         note = "[Online; accessed 23-December-2015]"
       }
    #+END_SRC
** [[file:Marionette%20Python%20Tests%20-%20Mozilla%20|%20MDN.html][Marionette Python Tests]]
** [[file:Marionette%20-%20Mozilla%20|%20MDN.html::<!DOCTYPE%20html][Marionette (main page)]]
** [[./1999-35.pdf]]
** [[./tcpdump.pdf]]
** [[shell:man npm-faq][man npm-faq]]
*** quotes
    - How do I install node with npm?
       You don't.  Try one of these node version managers:

       Unix:
       · http://github.com/isaacs/nave
       · http://github.com/visionmedia/n
       · http://github.com/creationix/nvm

       Windows:

       · http://github.com/marcelklehr/nodist
       · https://github.com/coreybutler/nvm-windows
       · https://github.com/hakobera/nvmw
       · https://github.com/nanjingboy/nvmw
